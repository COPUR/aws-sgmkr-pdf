# sagemaker-dg-2000.pdf

## Page 1

Amazon SageMaker AI
Developer Guide

• (Optional) Kendra access – This permission grants you access to the document querying  feature,
where you can query documents stored in an Amazon Kendra index using foundation models in
Canvas.

If you select this option, then in the Canvas Kendra Access section, enter the IDs for your
Amazon Kendra indexes to which you want to grant access.

• (Optional) Canvas MLOps – This permission grants you access to the model deployment feature
in Canvas, where you can deploy models for use in production.

In the domain setup’s Step 3: Applications section, choose Conﬁgure Canvas and then do the
following:

1.
For the Canvas storage conﬁguration, specify where you want Canvas to store the application
data, such as model artifacts, batch predictions, datasets, and logs. SageMaker AI creates a

Canvas/ folder inside this bucket to store the data. For more information, see Conﬁgure your
Amazon S3 storage. For this section, do the following:

a.
Select System managed if you want to set the location to the default SageMaker AI-

created bucket that follows the pattern s3://sagemaker-{Region}-{your-account-

id}.

b.
Select Custom S3 to specify your own Amazon S3 bucket as the storage location. Then,
enter the Amazon S3 URI.

c.
(Optional) For Encryption key, specify a KMS key for encrypting Canvas artifacts stored at
the speciﬁed location.

2.
(Optional) For Amazon Q Developer, do the following:

a.
Turn on Enable Amazon Q Developer in SageMaker Canvas for natural language ML to
give your users permissions to leverage generative AI assistance during their ML workﬂow
in Canvas. This option only grants permissions to query Amazon Q Developer for help with
predetermined tasks that can be completed in the Canvas application.

b.
Turn on Enable Amazon Q Developer chat for general AWS questions to give your users
permissions to make generative AI queries related to AWS services.

3.
(Optional) Conﬁgure the Large data processing section if your users plan to process datasets
larger than 5 GB in Canvas. For more detailed information about how to conﬁgure these
options, see Grant Users Permissions to Use Large Data across the ML Lifecycle.

4.
(Optional) For the ML Ops permissions conﬁguration section, do the following:

Getting started
972

## Page 2

Amazon SageMaker AI
Developer Guide

a.
Leave the Enable direct deployment of Canvas models option turned on to give your
users permissions to deploy their models from Canvas to a SageMaker AI endpoint. For
more information about model deployment in Canvas, see Deploy your models to an
endpoint.

b.
Leave the Enable Model Registry registration permissions for all users option turned on
to give your users permissions to register their model version to the SageMaker AI model
registry (it is turned on by default). For more information, see Register a model version in
the SageMaker AI model registry.

c.
If you left the Enable Model Registry registration permissions for all users option turned
on, then select either Register to Model Registry only or Register and approve model in
Model Registry.

5.
(Optional) For the Local ﬁle upload conﬁguration section, turn on the Enable local ﬁle
upload option to give your users permissions to upload ﬁles to Canvas from their local
machines. Turning this option on attaches a cross-origin resource sharing (CORS) policy to
the Amazon S3 bucket speciﬁed in the Canvas storage conﬁguration (and overrides any
existing CORS policy). To learn more about local ﬁle upload permissions, see Grant Your Users
Permissions to Upload Local Files.

6.
(Optional) For the OAuth settings section, do the following:

a.
Choose Add OAuth conﬁguration.

b.
For Data source, select your data source.

c.
For Secret setup, select Create a new secret and enter the information you have from
your identity provider. If you haven’t done the initial OAuth setup with your data source
yet, see Set up connections to data sources with OAuth.

7.
(Optional) For the Canvas Ready-to-use models conﬁguration, do the following:

a.
Leave the Enable Canvas Ready-to-use models option turned on to give your users
permissions to generate predictions with Ready-to-use models in Canvas (it is turned on
by default). This option also gives you permissions to chat with generative-AI powered
models. For more information, see Generative AI foundation models in SageMaker Canvas.

b.
Leave the Enable document query using Amazon Kendra option turned on to give your
users permissions to use foundation models for querying documents stored in an Amazon
Kendra index. Then, from the dropdown menu, select the existing indexes to which you
want to grant access. For more information, see Generative AI foundation models in
SageMaker Canvas.

Getting started
973

## Page 3

Amazon SageMaker AI
Developer Guide

c.
For Amazon Bedrock role, select Create and use a new execution role to create a new
IAM execution role that has a trust relationship with Amazon Bedrock. This IAM role
is assumed by Amazon Bedrock to ﬁne-tune large language models (LLMs) in Canvas.
If you already have an execution role with a trust relationship, then select Use an
existing execution role and choose your role from the dropdown. For more information
about manually conﬁguring permissions for your own execution role, see Grant Users
Permissions to Use Amazon Bedrock and Generative AI Features in Canvas.

8.
Finish conﬁguring the rest of the domain settings using the Use custom setup for Amazon
SageMaker AI procedures.

Note

If you encounter any issues with granting permissions through the console, such as
permissions for Ready-to-use models, see the topic Troubleshooting issues with granting
permissions through the SageMaker AI console.

You should now have a SageMaker AI domain set up and all of the Canvas permissions conﬁgured.

You can edit the Canvas permissions for a domain or a speciﬁc user after the initial domain setup.
Individual user settings override the domain settings. To learn how to edit your Canvas permissions
in the domain settings, see Edit domain settings.

Give yourself permissions to use speciﬁc features in Canvas

The following information outlines the various permissions that you can grant to a Canvas user to
allow the use of various features and functionalities within Canvas. Some of these permissions can
be granted during the domain setup, but some require additional permissions or conﬁguration.
Refer to the speciﬁc permissions information for each feature that you want to enable:

• Local ﬁle upload. The permissions for local ﬁle upload are turned on by default in the Canvas
base permissions when setting up your domain. If you can't upload local ﬁles from your machine
to SageMaker Canvas, you can attach a CORS policy to the Amazon S3 bucket that you speciﬁed
in the Canvas storage conﬁguration. If you allowed SageMaker AI to use the default bucket, the

bucket follows the naming pattern s3://sagemaker-{Region}-{your-account-id}. For
more information, see Grant Your Users Permissions to Upload Local Files.

Getting started
974

## Page 4

Amazon SageMaker AI
Developer Guide

• Custom image and text prediction models. The permissions for building custom image and
text prediction models are turned on by default in the Canvas base permissions when setting
up your domain. However, if you have a custom IAM conﬁguration and don't want to attach the
AmazonSageMakerCanvasFullAccess policy to your user's IAM execution role, then you must
explicitly grant your user the necessary permissions. For more information, see Grant Your Users
Permissions to Build Custom Image and Text Prediction Models.

• Ready-to-use models and foundation models. You might want to use the Canvas Ready-to-
use models to make predictions for your data. With the Ready-to-use models permissions,
you can also chat with generative AI-powered models. The permissions are turned on by
default when setting up your domain, or you can edit the permissions for a domain that
you’ve already created. The Canvas Ready-to-use models permissions option adds the
AmazonSageMakerCanvasAIServicesAccess policy to your execution role. For more information,
see the Get started section of the Ready-to-use models documentation.

For more information about getting started with generative AI foundation models, see
Generative AI foundation models in SageMaker Canvas.

• Fine-tune foundation models. If you'd like to ﬁne-tune foundation models in Canvas,
you can either add the permissions when setting up your domain, or you can edit the
permissions for the domain or user proﬁle after creating your domain. You must add the
AmazonSageMakerCanvasAIServicesAccess policy to the AWS IAM role you chose when setting
up the user proﬁle, and you must also add a trust relationship with Amazon Bedrock to the role.
For instructions on how to add these permissions to your IAM role, see Grant Users Permissions
to Use Amazon Bedrock and Generative AI Features in Canvas.

• Send batch predictions to Quick. You might want to send batch predictions, or datasets of
predictions you generate from a custom model, to Quick for analysis. In QuickSight, you can
build and publish predictive dashboards with your prediction results. For instructions on how to
add these permissions to your Canvas user's IAM role, see Grant Your Users Permissions to Send
Predictions to Quick.

• Deploy Canvas models to a SageMaker AI endpoint. SageMaker AI Hosting oﬀers endpoints
which you can use to deploy your model for use in production. You can deploy models built in
Canvas to a SageMaker AI endpoint and then make predictions programmatically in a production
environment. For more information, see Deploy your models to an endpoint.

• Register model versions to the model registry. You might want to register versions of your
model to the SageMaker AI model registry, which is a repository for tracking the status of
updated versions of your model. A data scientist or MLOps team working in the SageMaker
Model Registry can view the versions of your model that you’ve built and approve or reject them.

Getting started
975

## Page 5

Amazon SageMaker AI
Developer Guide

Then, they can deploy your model version to production or kick oﬀ an automated workﬂow.
Model registration permissions are turned on by default for your domain. You can manage
permissions at the user proﬁle level and grant or remove permissions to speciﬁc users. For more
information, see Register a model version in the SageMaker AI model registry.

• Import data from Amazon Redshift. If you want to import data from Amazon Redshift, you

must give yourself additional permissions. You must add the AmazonRedshiftFullAccess
managed policy to the AWS IAM role you chose when setting up the user proﬁle. For instructions
on how to add the policy to the role, see Grant Users Permissions to Import Amazon Redshift
Data.

Note

The necessary permissions to import through other data sources, such as Amazon
Athena and SaaS platforms, are included in the AmazonSageMakerFullAccess and
AmazonSageMakerCanvasFullAccess policies. If you followed the standard setup
instructions, these policies should already be attached to your execution role. For more
information about these data sources and their permissions, see Connect to data sources.

Step 1: Log in to SageMaker Canvas

When the initial setup is complete, you can access SageMaker Canvas with any of the following
methods, depending on your use case:

• In the SageMaker AI console, choose the Canvas in the left navigation pane. Then, on the Canvas
page, select your user from the dropdown and launch the Canvas application.

• Open SageMaker Studio, and in the Studio interface, go to the Canvas page and launch the
Canvas application.

• Use your organization’s SAML 2.0-based SSO methods, such as Okta or the IAM Identity Center.

When you log into SageMaker Canvas for the ﬁrst time, SageMaker AI creates the application
and a SageMaker AI space for you. The Canvas application’s data is stored in the space. To learn
more about spaces, see Collaboration with shared spaces. The space consists of your user proﬁle’s
applications and a shared directory for all of your applications’ data. If you don’t want to use the
default space created by SageMaker AI and would prefer to create your own space for storing

Getting started
976

## Page 6

Amazon SageMaker AI
Developer Guide

application data, see the page Store SageMaker Canvas application data in your own SageMaker AI
space.

Step 2: Use SageMaker Canvas to get predictions

After you’ve logged in to Canvas, you can start building models and generating predictions for your
data.

You can either use Canvas Ready-to-use models to make predictions without building a model, or
you can build a custom model for your speciﬁc business problem. Review the following information
to decide whether Ready-to-use models or custom models are best for your use case.

• Ready-to-use models. With Ready-to-use models, you can use pre-built models to extract
insights from your data. The Ready-to-use models cover a variety of use cases, such as language
detection and document analysis. To get started making predictions with Ready-to-use models,
see Ready-to-use models.

• Custom models. With custom models, you can build a variety of model types that are
customized to make predictions for your data. Use custom models if you’d like to build a model
that is trained on your business-speciﬁc data and if you’d like to use features such as evaluating
your model’s performance. To get started with building a custom model, see Custom models.

Tutorial: Build an end-to-end machine learning workﬂow in SageMaker
Canvas

This tutorial guides you through an end-to-end machine learning (ML) workﬂow using Amazon
SageMaker Canvas. SageMaker Canvas is a visual no-code interface that you can use to prepare
data and to train and deploy ML models. For the tutorial, you use a NYC taxi dataset to train a
model that predicts the fare amount for a given trip. You get hands-on experience with key ML
tasks such as assessing data quality and addressing data issues, splitting data into training and
test sets, model training and evaluation, making predictions, and deploying your trained model–all
within the SageMaker Canvas application.

Important

This tutorial assumes that you or your administrator have created an AWS account. For
information about creating an AWS account, see Getting started: Are you a ﬁrst time AWS
User?

Tutorial: Build a machine learning workﬂow in Canvas
977

## Page 7

Amazon SageMaker AI
Developer Guide

Setting up

An Amazon SageMaker AI domain is a centralized place to manage all your Amazon SageMaker AI
environments and resources. A domain acts as a virtual boundary for your work in SageMaker AI,
providing isolation and access control for your machine learning (ML) resources.

To get started with Amazon SageMaker Canvas, you or your administrator must navigate to the
SageMaker AI console and create a Amazon SageMaker AI domain. A domain has the storage and
compute resources needed for you to run SageMaker Canvas. Within the domain, you conﬁgure
SageMaker Canvas to access your Amazon S3 buckets and deploy models. Use the following
procedure to set up a quick domain and create a SageMaker Canvas application.

To set up SageMaker Canvas

1.
Navigate to the SageMaker AI console.

2.
On the left-hand navigation, choose SageMaker Canvas.

3.
Choose Create a SageMaker AI domain.

4.
Choose Set up. The domain can take a few minutes to set up.

The preceding procedure used a quick domain set up. You can perform an advanced set up
to control all aspects of the account conﬁguration, including permissions, integrations, and
encryption. For more information about a custom set up, see Use custom setup for Amazon
SageMaker AI.

By default, doing the quick domain set up provides you with permissions to deploy models. If you
have custom permissions set up through a standard domain and you need manually grant model
deployment permissions, see Permissions management.

Flow creation

Amazon SageMaker Canvas is a machine learning platform that enables users to build, train, and
deploy machine learning models without extensive coding or machine learning expertise. One of
the powerful features of Amazon SageMaker Canvas is the ability to import and work with large
datasets from various sources, such as Amazon S3.

For this tutorial, we're using the NYC taxi dataset to predict the fare amount for each trip using a
Amazon SageMaker Canvas Data Wrangler data ﬂow. The following procedure outlines the steps
for importing a modiﬁed version of the NYC taxi dataset into a data ﬂow.

Tutorial: Build a machine learning workﬂow in Canvas
978

## Page 8

Amazon SageMaker AI
Developer Guide

Note

For improved processing, SageMaker Canvas imports a sample of your data. By default, it
randomly samples 50,000 rows.

To import the NYC taxi dataset

1.
From the SageMaker Canvas home page, choose Data Wrangler.

2.
Choose Import data.

3.
Select Tabular.

4.
Choose the toolbox next to data source.

5.
Select Amazon S3 from the dropdown.

6.
For Input S3 endpoint, specify s3://amazon-sagemaker-data-wrangler-

documentation-artifacts/canvas-single-file-nyc-taxi-dataset.csv

7.
Choose Go.

8.
Select the checkbox next to the dataset.

9.
Choose Preview data.

10. Choose Save.

Data Quality and Insights Report 1 (sample)

After importing a dataset into Amazon SageMaker Canvas, you can generate a Data Quality and
Insights report on a sample of the data. Use it to provide valuable insights into the dataset. The
report does the following:

• Assesses the dataset's completeness

• Identiﬁes missing values and outliers

It can identify other potential issues that may impact model performance. It also evaluates the
predictive power of each feature concerning the target variable, allowing you to identify the most
relevant features for problem you're trying to solve.

We can use the insights from the report to predict the fare amount. By specifying the Fare amount
column as the target variable and selecting Regression as the problem type, the report will analyze

Tutorial: Build a machine learning workﬂow in Canvas
979

## Page 9

Amazon SageMaker AI
Developer Guide

the dataset's suitability for predicting continuous values like fare prices. The report should reveal
that features like year and hour_of_day have low predictive power for the chosen target variable,
providing you with valuable insights.

Use the following procedure to get a Data Quality and Insights report on a 50,000 row sample
from the dataset.

To get a report on a sample

1.
Choose Get data insights from the pop up window next to the Data types node.

2.
For Analysis name, specify a name for the report.

3.
For Problem type, choose Regression.

4.
For Target column, choose Fare amount.

5.
Choose Create.

You can review the Data Quality and Insights report on a sample of your data. The report indicates
that the year and hour_of_day features are not predictive of the target variable, Fare amount.

At the top of the navigation, choose the name of the data ﬂow to navigate back to it.

Drop year and hour of day

We're using insights from the report to drop the year and hour_of_day columns to streamline the
feature space and potentially improve model performance.

Amazon SageMaker Canvas provides a user-friendly interface and tools to perform such data
transformations.

Use the following procedure to drop the year and hour_of_day columns from the NYC taxi dataset
using the Data Wrangler tool in Amazon SageMaker Canvas.

1.
Choose the icon next to Data types.

2.
Choose Add step.

3.
In the search bar, write Drop column.

4.
Choose Manage columns.

5.
Choose Drop column.

6.
For Columns to drop, select the year and hour_of_day columns.

Tutorial: Build a machine learning workﬂow in Canvas
980

## Page 10

Amazon SageMaker AI
Developer Guide

7.
Choose Preview to view how your transform changes your data.

8.
Choose Add.

You can use the preceding procedure as the basis to add all of the other transforms in SageMaker
Canvas.

Data Quality and Insights Report 2 (full dataset)

For the previous insights report, we used a sample of the NYC taxi dataset. For our second report,
we're running a comprehensive analysis on the entire dataset to identify potential issues impacting
model performance.

Use the following procedure to create a Data Quality and Insights report on an entire dataset.

To get a report on the entire dataset

1.
Choose the icon next to the Drop columns node.

2.
Choose Get data insights.

3.
For Analysis name, specify a name for the report.

4.
For Problem type, choose Regression.

5.
For Target column, choose Fare amount.

6.
For Data size, choose Full dataset.

7.
Choose Create.

The following is an image from the insights report:

![Page 10 Diagram 1](images/page-0010-img-01.png)

Tutorial: Build a machine learning workﬂow in Canvas
981

## Page 11

Amazon SageMaker AI
Developer Guide

It shows the following issues:

• Duplicate rows

• Skewed target

Duplicate rows can lead to data leakage, where the model is exposed to the same data during
training and testing. They can lead to overly optimistic performance metrics. Removing duplicate
rows ensures that the model is trained on unique instances, reducing the risk of data leakage and
improving the model's ability to generalize.

A skewed target variable distribution, in this case, the Fare amount column, can cause imbalanced
classes, where the model may become biased towards the majority class. This can lead to poor
performance on minority classes, which is particularly problematic in scenarios where accurately
predicting rare or underrepresented instances is important.

Addressing data quality issues

To address these issues and prepare the dataset for modeling, you can search for the following
transformations and apply them:

1. Drop duplicates using the Manage rows transform.

2. Handle outliers in the Fare amount column using the Robust standard deviation numeric

outliers.

3. Handle outliers in the Trip distance and Trip duration columns using the Standard deviation

numeric outliers.

4. Use the Encode categorical to encode the Rate code id, Payment type, Extra ﬂag, and Toll ﬂag

columns as ﬂoats.

If you're not sure about how to apply a transform, see Drop year and hour of day

By addressing these data quality issues and applying appropriate transformations, you can improve
the dataset's suitability for modeling.

Verifying data quality and quick model accuracy

After applying the transforms to address data quality issues, such as removing duplicate rows,
we create our ﬁnal Data Quality and Insights report. This report helps verify that the applied
transformations resolved the issues and that the dataset is now in a suitable state for modeling.

Tutorial: Build a machine learning workﬂow in Canvas
982

## Page 12

Amazon SageMaker AI
Developer Guide

When reviewing the ﬁnal Data Quality and Insights report, you should expect to see no major data
quality issues ﬂagged. The report should indicate that:

• The target variable is no longer skewed

• There are no outliers or duplicate rows

Additionally, the report should provide a quick model score based on a baseline model trained on
the transformed dataset. This score serves as an initial indicator of the model's potential accuracy
and performance.

Use the following procedure to create the Data Quality and Insights report.

To create the Data Quality and Insights report

1.
Choose the icon next to the Drop columns node.

2.
Choose Get data insights.

3.
For Analysis name, specify a name for the report.

4.
For Problem type, choose Regression.

5.
For Target column, choose Fare amount.

6.
For Data size, choose Full dataset.

7.
Choose Create.

Split the data into training and test sets

To train a model and evaluate its performance, we use the Split data transform to split the data
into training and test sets.

By default, SageMaker Canvas uses a Randomized split, but you can also use the following types of
splits:

• Ordered

• Stratiﬁed

• Split by key

You can change the Split percentage or add splits.

Tutorial: Build a machine learning workﬂow in Canvas
983

## Page 13

Amazon SageMaker AI
Developer Guide

For this tutorial, use all of the default settings in the split. You need to double click on the dataset
to view its name. The training dataset has the name Dataset (Train).

Next to the Ordinal encode node apply the Split data transform.

Train model

After you split your data, you can train a model. This model learns from patterns in your data. You
can use it to make predictions or uncover insights.

SageMaker Canvas has both quick builds and standard builds. Use a standard build to train best
performing model on your data.

Before you start training a model, you must ﬁrst export the training dataset as a SageMaker Canvas
dataset.

To export your dataset

1.
Next to the node for the training dataset, choose the icon and select Export.

2.
Select SageMaker Canvas dataset.

3.
Choose Export to export the dataset.

After you've created a dataset, you can train a model on the SageMaker Canvas dataset that
you've created. For information about training a model, see Build a custom numeric or categorical
prediction model.

Evaluate model and make predictions

After training your machine learning model, it's crucial to evaluate its performance to ensure it
meets your requirements and performs well on unseen data. Amazon SageMaker Canvas provides
a user-friendly interface to assess your model's accuracy, review its predictions, and gain insights
into its strengths and weaknesses. You can use the insights to make informed decisions about its
deployment and potential areas for improvement.

Use the following procedure to evaluate a model before you deploy it.

To evaluate a model

1.
Choose My Models.

2.
Choose the model you've created.

Tutorial: Build a machine learning workﬂow in Canvas
984

## Page 14

Amazon SageMaker AI
Developer Guide

3.
Under Versions, select the version corresponding to the model.

You can now view the model evaluation metrics.

After you evaluate the model, you can make predictions on new data. We're using the test dataset
that we've created.

To use the test dataset for predictions we need to convert it into a SageMaker Canvas dataset. The
SageMaker Canvas dataset is in a format that the model can interpret.

Use the following procedure to create a SageMaker Canvas dataset from the test dataset.

To create a SageMaker Canvas dataset

1.
Next to the Dataset (Test) dataset, choose the radio icon.

2.
Select Export.

3.
Select SageMaker Canvas dataset.

4.
For Dataset name, specify a name for the dataset.

5.
Choose Export.

Use the following procedure to make predictions. It assumes that you're still on the Analyze page.

To make predictions on test dataset

1.
Choose Predict.

2.
Choose Manual.

3.
Select the dataset that you've exported.

4.
Choose Generate predictions.

5.
When SageMaker Canvas has ﬁnished generating predictions, select the icon to the right of the
dataset.

6.
Choose Preview to view the predictions.

Deploy a model

After you've evaluated your model, you can deploy it to an endpoint. You can submit requests to
the endpoint to get predictions.

Tutorial: Build a machine learning workﬂow in Canvas
985

## Page 15

Amazon SageMaker AI
Developer Guide

Use the following procedure to deploy a model. It assumes that you're still on the Predict page.

To deploy a model

1.
Choose Deploy.

2.
Choose Create deployment.

3.
Choose Deploy.

Cleaning up

You've successfully completed the tutorial. To avoid incurring additional charges, delete the
resources that you're not using.

Use the following procedure to delete the endpoint that you created. It assumes that you're still on

the Deploy page.

To delete an endpoint

1.
Choose the radio button to the right of your deployment.

2.
Select Delete deployment.

3.
Choose Delete.

After deleting the deployment, delete the datasets that you've created within SageMaker Canvas.
Use the following procedure to delete the datasets.

To delete the datasets

1.
Choose Datasets on the left-hand navigation.

2.
Select the dataset that you've analyzed and the synthetic dataset used for predictions.

3.
Choose Delete.

To avoid incurring additional charges, you must log out of SageMaker Canvas. For more
information, see Logging out of Amazon SageMaker Canvas.

Tutorial: Build a machine learning workﬂow in Canvas
986

## Page 16

Amazon SageMaker AI
Developer Guide

Amazon SageMaker Canvas setup and permissions management (for IT
administrators)

The following pages explain how IT administrators can conﬁgure Amazon SageMaker Canvas
and grant permissions to users within their organizations. You learn how to set up the storage
conﬁguration, manage data encryption and VPCs, control access to speciﬁc capabilities like
generative AI foundation models, integrate with other AWS services like Amazon Redshift, and
more. By following these steps, you can tailor SageMaker Canvas for your users based on your
organization's speciﬁc requirements.

You can also set up SageMaker Canvas for your users with AWS CloudFormation. For more
information, see AWS::SageMaker AI::App in the AWS CloudFormation User Guide.

Topics

• Grant Your Users Permissions to Upload Local Files

• Set Up SageMaker Canvas for Your Users

• Conﬁgure your Amazon S3 storage

• Grant permissions for cross-account Amazon S3 storage

• Grant Users Permissions to Use Large Data across the ML Lifecycle

• Encrypt Your SageMaker Canvas Data with AWS KMS

• Store SageMaker Canvas application data in your own SageMaker AI space

• Grant Your Users Permissions to Build Custom Image and Text Prediction Models

• Grant Users Permissions to Use Amazon Bedrock and Generative AI Features in Canvas

• Update SageMaker Canvas for Your Users

• Request a Quota Increase

• Grant Users Permissions to Import Amazon Redshift Data

• Grant Your Users Permissions to Send Predictions to Quick

• Applications management

• Conﬁgure Amazon SageMaker Canvas in a VPC without internet access

• Set up connections to data sources with OAuth

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
987

## Page 17

Amazon SageMaker AI
Developer Guide

Grant Your Users Permissions to Upload Local Files

If your users are uploading ﬁles from their local machines to SageMaker Canvas, you must attach
a CORS (cross-origin resource sharing) conﬁguration to the Amazon S3 bucket that they're using.
When setting up or editing the SageMaker AI domain or user proﬁle, you can specify either a
custom Amazon S3 location or the default location, which is a SageMaker AI created Amazon

S3 bucket with a name that uses the following pattern: s3://sagemaker-{Region}-{your-

account-id}. SageMaker Canvas adds your users' data to the bucket whenever they upload a ﬁle.

To grant users permissions to upload local ﬁles to the bucket, you can attach a CORS conﬁguration
to it using either of the following procedures. You can use the ﬁrst method when editing the
settings of your domain, where you opt in to allow SageMaker AI to attach the CORS conﬁguration
to the bucket for you. You can also use the ﬁrst method for editing a user proﬁle within a domain.
The second method is the manual method, where you can attach the CORS conﬁguration to the

bucket yourself.

SageMaker AI domain settings method

To grant your users permissions to upload local ﬁles, you can edit the Canvas application
conﬁguration in the domain settings. This attaches a Cross-Origin Resource Sharing (CORS)
conﬁguration to the Canvas storage conﬁguration's Amazon S3 bucket and grants all users in the
domain permission to upload local ﬁles into SageMaker Canvas. By default, the permissions option
is turned on when you set up a new domain, but you can turn this option on and oﬀ as needed.

Note

If you have an existing CORS conﬁguration on the storage conﬁguration Amazon S3 bucket,
turning on the local ﬁle upload option overwrites the existing conﬁguration with the new
conﬁguration.

The following procedure shows how you can turn on this option by editing the domain settings in
the SageMaker AI console.

1.
Go to the SageMaker AI console at https://console.aws.amazon.com/sagemaker/.

2.
In the left navigation pane, choose Domains.

3.
From the list of domains, choose your domain.

4.
On the domain details page, select the App Conﬁgurations tab.

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
988

## Page 18

Amazon SageMaker AI
Developer Guide

5.
Go to the Canvas section and choose Edit.

6.
Turn on the Enable local ﬁle upload toggle. This attaches the CORS conﬁguration and grants
local ﬁle upload permissions.

7.
Choose Submit.

Users in the speciﬁed domain should now have local ﬁle upload permissions.

You can also grant permissions to speciﬁc user proﬁles in a domain by following the preceding
procedure and going into the user proﬁle settings instead of the overall domain settings.

Amazon S3 bucket method

If you want to manually attach the CORS conﬁguration to the SageMaker AI Amazon S3 bucket, use
the following procedure.

1.
Sign in to https://console.aws.amazon.com/s3/.

2.
Choose your bucket. If your domain uses the default SageMaker AI created bucket, the bucket’s

name uses the following pattern: s3://sagemaker-{Region}-{your-account-id}.

3.
Choose Permissions.

4.
Navigate to Cross-origins resource sharing (CORS).

5.
Choose Edit.

6.
Add the following CORS policy:

[
{
"AllowedHeaders": [
"*"
],
"AllowedMethods": [
"POST"
],
"AllowedOrigins": [
"*"
],
"ExposeHeaders": []
}
]

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
989

## Page 19

Amazon SageMaker AI
Developer Guide

7.
Choose Save changes.

In the preceding procedure, the CORS policy must have "POST" listed under AllowedMethods.

After you've gone through the procedure, you should have:

• An IAM role assigned to each of your users.

• Amazon SageMaker Studio Classic runtime permissions for each of your users. SageMaker Canvas
uses Studio Classic to run the commands from your users.

• If the users are uploading ﬁles from their local machines, a CORS policy attached to their
Amazon S3 bucket.

If your users still can't upload the local ﬁles after you update the CORS policy, the browser might
be caching the CORS settings from a previous upload attempt. If they're running into issues,
instruct them to clear their browser cache and try again.

Set Up SageMaker Canvas for Your Users

To set up Amazon SageMaker Canvas, do the following:

• Create an Amazon SageMaker AI domain.

• Create user proﬁles for the domain

• Set up Okta Single Sign On (Okta SSO) for your users.

• Activate link sharing for models.

Use Okta Single-Sign On (Okta SSO) to grant your users access to Amazon SageMaker Canvas.
SageMaker Canvas supports SAML 2.0 SSO methods. The following sections guide you through
procedures to set up Okta SSO.

To set up a domain, see Use custom setup for Amazon SageMaker AI and follow the instructions for
setting up your domain using IAM authentication. You can use the following information to help
you complete the procedure in the section:

• You can ignore the step about creating projects.

• You don't need to provide access to additional Amazon S3 buckets. Your users can use the
default bucket that we provide when we create a role.

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
990

## Page 20

Amazon SageMaker AI
Developer Guide

• To grant your users access to share their notebooks with data scientists, turn on Notebook
Sharing Conﬁguration.

• Use Amazon SageMaker Studio Classic version 3.19.0 or later. For information about updating
Amazon SageMaker Studio Classic, see Shut Down and Update Amazon SageMaker Studio

Classic.

Use the following procedure to set up Okta. For all of the following procedures, you specify the

same IAM role for IAM-role .

Add the SageMaker Canvas application to Okta

Set up the sign-on method for Okta.

1.
Sign in to the Okta Admin dashboard.

2.
Choose Add application. Search for AWS Account Federation.

3.
Choose Add.

4.
Optional: Change the name to Amazon SageMaker Canvas.

5.
Choose Next.

6.
Choose SAML 2.0 as the Sign-On method.

7.
Choose Identity Provider Metadata to open the metadata XML ﬁle. Save the ﬁle locally.

8.
Choose Done.

Set up ID federation in IAM

AWS Identity and Access Management (IAM) is the AWS service that you use to gain access to your
AWS account. You gain access to AWS through an IAM account.

1.
Sign in to the AWS console.

2.
Choose AWS Identity and Access Management (IAM).

3.
Choose Identity Providers.

4.
Choose Create Provider.

5.
For Conﬁgure Provider, specify the following:

• Provider Type – From the dropdown list, choose SAML.

• Provider Name – Specify Okta.

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
991

## Page 21

Amazon SageMaker AI
Developer Guide

• Metadata Document – Upload the XML document that you've saved locally from step 7 of
Add the SageMaker Canvas application to Okta.

6.
Find your identity provider under Identity Providers. Copy its Provider ARN value.

7.
For Roles, choose the IAM role that you're using for Okta SSO access.

8.
Under Trust Relationship for the IAM role, choose Edit Trust Relationship.

9.
Modify the IAM trust relationship policy by specifying the Provider ARN value that you've
copied and add the following policy:

JSON

{
"Version":"2012-10-17",

"Statement": [
{
"Effect": "Allow",
"Principal": {
"Federated": "arn:aws:iam::111122223333:saml-provider/Okta"
},
"Action": [
"sts:AssumeRoleWithSAML",
"sts:TagSession"
],
"Condition": {
"StringEquals": {
"SAML:aud": "https://signin.aws.amazon.com/saml"
}
}
},
{
"Effect": "Allow",
"Principal": {
"Federated": "arn:aws:iam::111122223333:saml-provider/Okta"
},
"Action": [
"sts:SetSourceIdentity"
]
}
]
}

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
992

## Page 22

Amazon SageMaker AI
Developer Guide

10. For Permissions, add the following policy:

JSON

{
"Version":"2012-10-17",
"Statement": [
{
"Sid": "AmazonSageMakerPresignedUrlPolicy",
"Effect": "Allow",
"Action": [
"sagemaker:CreatePresignedDomainUrl"
],

"Resource": "*"
}
]
}

Conﬁgure SageMaker Canvas in Okta

Conﬁgure Amazon SageMaker Canvas in Okta using the following procedure.

To conﬁgure Amazon SageMaker Canvas to use Okta, follow the steps in this section. You must
specify unique user names for each SageMakerStudioProﬁleName ﬁeld. For example, you can use

user.login as a value. If the username is diﬀerent from the SageMaker Canvas proﬁle name,
choose a diﬀerent uniquely identifying attribute. For example, you can use an employee's ID
number for the proﬁle name.

For an example of values that you can set for Attributes, see the code following the procedure.

1.
Under Directory, choose Groups.

2.
Add a group with the following pattern: sagemaker#canvas#IAM-role#AWS-account-id.

3.
In Okta, open the AWS Account Federation application integration conﬁguration.

4.
Select Sign On for the AWS Account Federation application.

5.
Choose Edit and specify the following:

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
993

## Page 23

Amazon SageMaker AI
Developer Guide

• SAML 2.0

• Default Relay State – https://Region.console.aws.amazon.com/sagemaker/home?

region=Region#/studio/canvas/open/StudioId. You can ﬁnd the Studio Classic ID in the
console: https://console.aws.amazon.com/sagemaker/

6.
Choose Attributes.

7.
In the SageMakerStudioProﬁleName ﬁelds, specify unique values for each username. The
usernames must match the usernames that you've created in the AWS console.

Attribute 1:
Name: https://aws.amazon.com/SAML/Attributes/
PrincipalTag:SageMakerStudioUserProfileName
Value: ${user.login}

Attribute 2:
Name: https://aws.amazon.com/SAML/Attributes/TransitiveTagKeys
Value: {"SageMakerStudioUserProfileName"}

8.
Select Environment Type. Choose Regular AWS.

•
If your environment type isn't listed, you can set your ACS URL in the ACS URL ﬁeld. If
your environment type is listed, you don't need to enter your ACS URL

9.
For Identity Provider ARN, specify the ARN you used in step 6 of the preceding procedure.

10. Specify a Session Duration.

11. Choose Join all roles.

12. Turn on Use Group Mapping by specifying the following ﬁelds:

• App Filter – okta

• Group Filter – ^aws\#\S+\#(?IAM-role[\w\-]+)\#(?accountid\d+)$

• Role Value Pattern – arn:aws:iam::$accountid:saml-provider/

Okta,arn:aws:iam::$accountid:role/IAM-role

13. Choose Save/Next.

14. Under Assignments, assign the application to the group that you've created.

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
994

## Page 24

Amazon SageMaker AI
Developer Guide

Add optional policies on access control in IAM

In IAM, you can apply the following policy to the administrator user who creates the user proﬁles.

JSON

{
"Version":"2012-10-17",
"Statement": [
{
"Sid": "CreateSageMakerStudioUserProfilePolicy",
"Effect": "Allow",
"Action": "sagemaker:CreateUserProfile",
"Resource": "*",
"Condition": {
"ForAnyValue:StringEquals": {
"aws:TagKeys": [
"studiouserid"
]
}
}
}
]
}

If you choose to add the preceding policy to the admin user, you must use the following
permissions from Set up ID federation in IAM.

JSON

{
"Version":"2012-10-17",
"Statement": [
{
"Sid": "AmazonSageMakerPresignedUrlPolicy",
"Effect": "Allow",
"Action": [
"sagemaker:CreatePresignedDomainUrl"

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
995

## Page 25

Amazon SageMaker AI
Developer Guide

],
"Resource": "*",
"Condition": {
"StringEquals": {
"sagemaker:ResourceTag/studiouserid": "${aws:PrincipalTag/
SageMakerStudioUserProfileName}"
}
}
}
]
}

Conﬁgure your Amazon S3 storage

When you set up your SageMaker Canvas application, the default storage location for model
artifacts, datasets, and other application data is an Amazon S3 bucket that Canvas creates. This

default Amazon S3 bucket follows the naming pattern s3://sagemaker-{Region}-{your-

account-id} and exists in the same Region as your Canvas application. However, you can
customize the storage location and specify your own Amazon S3 bucket for storing Canvas
application data. You might want to use your own Amazon S3 bucket for storing application data
for any of the following reasons:

• Your organization has internal naming conventions for Amazon S3 buckets.

• You want to enable cross-account access to model artifacts or other Canvas data.

• You want to be compliant with internal security guidelines, such as restricting users to speciﬁc
Amazon S3 buckets or model artifacts.

• You want enhanced visibility and access to logs produced by Canvas, independent of the AWS
console or SageMaker Studio Classic.

By specifying your own Amazon S3 bucket, you can have increased control over your own storage
and be compliant with your organization.

To get started, you can either create a new SageMaker AI domain or user proﬁle, or you can update
an existing domain or user proﬁle. Note that the user proﬁle settings override the domain-level
settings. For example, you can use the default bucket conﬁguration at the domain level, but
you can specify a custom Amazon S3 bucket for an individual user. After specifying your own

Amazon S3 bucket for the domain or user proﬁle, Canvas creates a subfolder called Canvas/

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
996

## Page 26

Amazon SageMaker AI
Developer Guide

<UserProfileName> under the input Amazon S3 URI and saves all artifacts generated in the
Canvas application under this subfolder.

Important

If you update an existing domain or user proﬁle, you no longer have access to your Canvas
artifacts from the previous location. Your ﬁles are still in the old Amazon S3 location, but
you can no longer view them from Canvas. The new conﬁguration takes eﬀect the next
time you log into the application.

For more information about granting cross-account access to your Amazon S3 bucket, see Granting
cross-account object permissions in the Amazon S3 User Guide.

The following sections describe how to specify a custom Amazon S3 bucket for your Canvas
storage conﬁguration. If you’re setting up a new SageMaker AI domain (or a new user in a domain),
then use the New domain setup method or the New user proﬁle setup method. If you have an
existing Canvas user proﬁle and would like to update the proﬁle's storage conﬁguration, use the
Existing user method.

Before you begin

If you’re specifying an Amazon S3 URI from a diﬀerent AWS account, or if you’re using a bucket
that is encrypted with AWS KMS, then you must conﬁgure permissions before proceeding. You
must grant AWS IAM permissions to ensure that Canvas can download and upload objects to and
from your bucket. For detailed information on how to grant the required permissions, see Grant
permissions for cross-account Amazon S3 storage.

Additionally, the ﬁnal Amazon S3 URI for the training folder in your Canvas storage location must

be 128 characters or less. The ﬁnal Amazon S3 URI consists of your bucket path s3://<your-

bucket-name>/<folder-name>/ plus the path that Canvas adds to your bucket: Canvas/

<user-profile-name>/Training. For example, an acceptable path that is less than 128

characters is s3://<amzn-s3-demo-bucket>/<machine-learning>/Canvas/<user-1>/

Training.

New domain setup method

If you’re setting up a new domain and Canvas application, use this section to conﬁgure the storage
location at the domain level. This conﬁguration applies to all new users you create in the domain,
unless you specify a diﬀerent storage location for individual user proﬁles.

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
997

## Page 27

Amazon SageMaker AI
Developer Guide

When doing a Standard setup for your domain, on the Step 3: Conﬁgure Applications - Optional
page, use the following procedure for the Canvas section:

1.
For the Canvas storage conﬁguration, do the following:

a.
Select System managed if you want to set the location to the default SageMaker AI

bucket that follows the pattern s3://sagemaker-{Region}-{your-account-id}.

b.
Select Custom S3 to specify your own Amazon S3 bucket as the storage location. Then,
enter the Amazon S3 URI.

c.
(Optional) For Encryption key, specify a KMS key for encrypting Canvas artifacts stored at
the speciﬁed location.

2.
Finish setting up the domain and choose Submit.

Your domain is now conﬁgured to use the Amazon S3 location you speciﬁed for SageMaker Canvas
application storage.

New user proﬁle setup method

If you’re setting up a new user proﬁle in your domain, use this section to conﬁgure the storage
location for the user. This conﬁguration overrides the domain-level conﬁguration.

When adding a user proﬁle to your domain, for Step 2: Conﬁgure Applications, use the following
procedure for the Canvas section:

1.
For the Canvas storage conﬁguration, do the following:

a.
Select System managed if you want to set the location to the default SageMaker AI

created bucket that follows the pattern s3://sagemaker-{Region}-{your-account-

id}.

b.
Select Custom S3 to specify your own Amazon S3 bucket as the storage location. Then,
enter the Amazon S3 URI.

c.
(Optional) For Encryption key, specify a KMS key for encrypting Canvas artifacts stored at
the speciﬁed location.

2.
Finish setting up the user proﬁle and choose Submit.

Your user proﬁle is now conﬁgured to use the Amazon S3 location you speciﬁed for SageMaker
Canvas application storage.

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
998

## Page 28

Amazon SageMaker AI
Developer Guide

Existing user method

If you have an existing Canvas user proﬁle and would like to update the Amazon S3 storage
location, you can edit the SageMaker AI domain or user proﬁle settings. The change takes eﬀect
the next time you log into the Canvas application.

Note

When you change the storage location for an existing Canvas application, you lose access to
your Canvas artifacts from the previous storage location. The artifacts are still stored in the
old Amazon S3 location, but you can no longer view them from Canvas.

Remember that the user proﬁle settings override the general domain settings, so you can update
the Amazon S3 storage location for speciﬁc user proﬁles without changing it for all of the users.
You can update the storage conﬁguration for an existing domain or user by using the following
procedures.

Update an existing domain

Use the following procedure to update the storage conﬁguration for a domain.

1.
Open the SageMaker AI console at https://console.aws.amazon.com/sagemaker/.

2.
On the left navigation pane, choose Admin conﬁgurations.

3.
Under Admin conﬁgurations, choose Domains.

4.
From the list of domains, choose your domain.

5.
On the Domain details page, choose the App Conﬁgurations tab.

6.
Scroll down to the Canvas section and choose Edit.

7.
The Edit Canvas settings page opens. For the Canvas storage conﬁguration section, do the
following:

a.
Select System managed if you want to set the location to the default SageMaker

AI created bucket that follows the pattern s3://sagemaker-{Region}-{your-

account-id}.

b.
Select Custom S3 to specify your own Amazon S3 bucket as the storage location. Then,
enter the Amazon S3 URI.

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
999

## Page 29

Amazon SageMaker AI
Developer Guide

c.
(Optional) For Encryption key, specify a KMS key for encrypting Canvas artifacts
stored at the speciﬁed location.

8.
Finish any other modiﬁcations you want to make to the domain, and then choose Submit
to save your changes.

Update an existing user proﬁle

Use the following procedure to update the storage conﬁguration for a user proﬁle.

1.
Open the SageMaker AI console at https://console.aws.amazon.com/sagemaker/.

2.
On the left navigation pane, choose Admin conﬁgurations.

3.
Under Admin conﬁgurations, choose domains.

4.
From the list of domains, choose your domain.

5.
From the list of users in the domain, choose the user whose conﬁguration you want to edit.

6.
On the User Details page, choose Edit.

7.
In the navigation pane, choose Canvas settings.

8.
For the Canvas storage conﬁguration, do the following:

a.
Select System managed if you want to set the location to the default SageMaker AI

bucket that follows the pattern s3://sagemaker-{Region}-{your-account-id}.

b.
Select Custom S3 to specify your own Amazon S3 bucket as the storage location. Then,
enter the Amazon S3 URI.

c.
(Optional) For Encryption key, specify a KMS key for encrypting Canvas artifacts
stored at the speciﬁed location.

9.
Finish any other modiﬁcations you want to make to the user proﬁle, and then choose
Submit to save your changes.

The storage location for your Canvas user proﬁle should now be updated. The next time you log
into the Canvas application, you receive a notiﬁcation that the storage location has been updated.
You lose access to any previous artifacts that you created in Canvas. You can still access the ﬁles in
Amazon S3, but you can no longer view them in Canvas.

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1000

## Page 30

Amazon SageMaker AI
Developer Guide

Grant permissions for cross-account Amazon S3 storage

When setting up your SageMaker AI domain or user proﬁle for users to access SageMaker Canvas,
you specify an Amazon S3 storage location for Canvas artifacts. These artifacts include saved
copies of your input datasets, model artifacts, predictions, and other application data. You can
either use the default SageMaker AI created Amazon S3 bucket, or you can customize the storage
location and specify your own bucket for storing Canvas application data.

You can specify an Amazon S3 bucket in another AWS account for storing your Canvas data, but
ﬁrst you must grant cross-account permissions so that Canvas can access the bucket.

The following sections describe how to grant permissions to Canvas for uploading and
downloading objects to and from an Amazon S3 bucket in another account. There are additional
permissions for when your bucket is encrypted with AWS KMS.

Requirements

Before you begin, review the following requirements:

• Cross-account Amazon S3 buckets (and any associated AWS KMS keys) must be in the same AWS
Region as the Canvas user domain or user proﬁle.

• The ﬁnal Amazon S3 URI for the training folder in your Canvas storage location must be 128

characters or less. The ﬁnal S3 URI consists of your bucket path s3://<your-bucket-name>/

<folder-name>/ plus the path that Canvas adds to your bucket: Canvas/<user-profile-

name>/Training. For example, an acceptable path that is less than 128 characters is s3://

<amzn-s3-demo-bucket>/<machine-learning>/Canvas/<user-1>/Training.

Permissions for cross-account Amazon S3 buckets

The following section outlines the basic steps for granting the necessary permissions so that
Canvas can access your Amazon S3 bucket in another account. For more detailed instructions, see
Example 2: Bucket owner granting cross-account bucket permissions in the Amazon S3 User Guide.

1.
Create an Amazon S3 bucket, bucketA, in Account A.

2.
The Canvas user exists in another account called Account B. In the following steps, we refer to

the Canvas user's IAM role as roleB in Account B.

Give the IAM role roleB in Account B permission to download (GetObject) and upload

(PutObject) objects to and from bucketA in Account A by attaching an IAM policy.

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1001

## Page 31

Amazon SageMaker AI
Developer Guide

To limit access to a speciﬁc bucket folder, deﬁne the folder name in the resource element, such

as arn:aws:s3:::<bucketA>/FolderName/*. For more information, see How can I use IAM
policies to grant user-speciﬁc access to speciﬁc folders?

Note

Bucket-level actions, such as GetBucketCors and GetBucketLocation, should be
added on bucket-level resources, not folders.

The following example IAM policy grants the required permissions for roleB to access objects

in bucketA:

JSON

{
"Version":"2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Action": [
"s3:GetObject",
"s3:PutObject",
"s3:DeleteObject"
],
"Resource": [
"arn:aws:s3:::bucketA/FolderName/*"
]
},
{
"Effect": "Allow",
"Action": [
"s3:ListBucket",
"s3:GetBucketCors",
"s3:GetBucketLocation"
],
"Resource": [
"arn:aws:s3:::bucketA"
]
}

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1002

## Page 32

Amazon SageMaker AI
Developer Guide

]
}

3.
Conﬁgure the bucket policy for bucketA in Account A to grant permissions to the IAM role

roleB in Account B.

Note

Admins must also turn oﬀ Block all public access under the bucket Permissions
section.

The following is an example bucket policy for bucketA to grant the necessary permissions to

roleB:

JSON

{
"Version":"2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Principal": {
"AWS": "arn:aws:iam::111122223333:role/roleB"
},
"Action": [
"s3:DeleteObject",
"s3:GetObject",
"s3:PutObject"
],
"Resource": "arn:aws:s3:::bucketA/FolderName/*"
},
{
"Effect": "Allow",
"Principal": {
"AWS": "arn:aws:iam::111122223333:role/roleB"
},
"Action": [
"s3:ListBucket",
"s3:GetBucketCors",
"s3:GetBucketLocation"

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1003

## Page 33

Amazon SageMaker AI
Developer Guide

],
"Resource": "arn:aws:s3:::bucketA"
}
]
}

After conﬁguring the preceding permissions, your Canvas user proﬁle in Account B can now use the
Amazon S3 bucket in Account A as the storage location for Canvas artifacts.

Permissions for cross-account Amazon S3 buckets encrypted with AWS KMS

The following procedure shows you how to grant the necessary permissions so that Canvas can
access your Amazon S3 bucket in another account that is encrypted with AWS KMS. The steps
are similar to the procedure above, but with additional permissions. For more information about
granting cross-account KMS key access, see Allowing users in other accounts to use a KMS key in

the AWS KMS Developer Guide.

1.
Create an Amazon S3 bucket, bucketA, and an Amazon S3 KMS key s3KmsInAccountA in
Account A.

2.
The Canvas user exists in another account called Account B. In the following steps, we refer to

the Canvas user's IAM role as roleB in Account B.

Give the IAM role roleB in Account B permission to do the following:

• Download (GetObject) and upload (PutObject) objects to and from bucketA in Account
A.

• Access the AWS KMS key s3KmsInAccountA in Account A.

The following example IAM policy grants the required permissions for roleB to access objects

in bucketA and use the KMS key s3KmsInAccountA:

JSON

{
"Version":"2012-10-17",
"Statement": [
{
"Effect": "Allow",

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1004

## Page 34

Amazon SageMaker AI
Developer Guide

"Action": [
"s3:GetObject",
"s3:PutObject",
"s3:DeleteObject"
],
"Resource": [
"arn:aws:s3:::bucketA/FolderName/*"
]
},
{
"Effect": "Allow",
"Action": [
"s3:GetBucketCors",
"s3:GetBucketLocation"
],
"Resource": [
"arn:aws:s3:::bucketA"

]
},
{
"Action": [
"kms:DescribeKey",
"kms:CreateGrant",
"kms:RetireGrant",
"kms:GenerateDataKey",
"kms:GenerateDataKeyWithoutPlainText",
"kms:Decrypt"
],
"Effect": "Allow",
"Resource": "arn:aws:kms:us-east-1:111122223333:key/
s3KmsInAccountA"
}
]
}

3.
Conﬁgure the bucket policy for bucketA and the key policy for s3KmsInAccountA in Account

A to grant permissions to the IAM role roleB in Account B.

The following is an example bucket policy for bucketA to grant the necessary permissions to

roleB:

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1005

## Page 35

Amazon SageMaker AI
Developer Guide

JSON

{
"Version":"2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Principal": {
"AWS": "arn:aws:iam::111122223333:role/roleB"
},
"Action": [
"s3:DeleteObject",
"s3:GetObject",
"s3:PutObject"
],
"Resource": "arn:aws:s3:::bucketA/FolderName/*"

},
{
"Effect": "Allow",
"Principal": {
"AWS": "arn:aws:iam::111122223333:role/roleB"
},
"Action": [
"s3:GetBucketCors",
"s3:GetBucketLocation"
],
"Resource": "arn:aws:s3:::bucketA"
}
]
}

The following example is a key policy that you attach to the KMS key s3KmsInAccountA in

Account A to grant roleB access. For more information about how to create and attach a key
policy statement, see Creating a key policy in the AWS KMS Developer Guide.

{
"Sid": "Allow use of the key",
"Effect": "Allow",
"Principal": {
"AWS": [

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1006

## Page 36

Amazon SageMaker AI
Developer Guide

"arn:aws:iam::accountB:role/roleB"
]
},
"Action": [
"kms:DescribeKey",
"kms:CreateGrant",
"kms:RetireGrant",
"kms:GenerateDataKey",
"kms:GenerateDataKeyWithoutPlainText",
"kms:Decrypt"
],
"Resource": "*"
}

After conﬁguring the preceding permissions, your Canvas user proﬁle in Account B can now use the
encrypted Amazon S3 bucket in Account A as the storage location for Canvas artifacts.

Grant Users Permissions to Use Large Data across the ML Lifecycle

Amazon SageMaker Canvas users working with datasets larger than 10 GB in CSV format or 2.5
GB in Parquet format require speciﬁc permissions for large data processing. These permissions are
essential for managing large-scale data throughout the machine learning lifecycle. When datasets
exceed the stated thresholds, or the application's local memory capacity, SageMaker Canvas uses
Amazon EMR Serverless for eﬃcient processing. This applies to:

• Data Import: Importing large datasets with random or stratiﬁed sampling.

• Data Preparation: Exporting processed data from Data Wrangler in Canvas to Amazon S3, to a
new Canvas dataset, or to a Canvas model.

• Model Building: Training models on large datasets.

• Inference: Making predictions on large datasets.

By default, SageMaker Canvas uses EMR Serverless to run these remote jobs with the following app
settings:

• Pre-Initialized capacity: Not conﬁgured

• Application limits: Maximum capacity of 400 vCPUs, max concurrent 16 vCPUs per account, 3000
GB memory, 20000 GB disk

• Metastore conﬁguration: AWS Glue Data Catalog

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1007

## Page 37

Amazon SageMaker AI
Developer Guide

• Application logs: AWS managed storage (enabled), using an AWS owned encryption key

• Application behavior: Auto-starts on job submission and auto-stops after the application is idle
for 15 minutes

To enable these large data processing capabilities, users need the necessary permissions, which can
be granted through the Amazon SageMaker AI domain settings. The method for granting these
permissions depends on how your Amazon SageMaker AI domain was set up initially. We'll cover
three main scenarios:

• Quick domain setup

• Custom domain setup (with public internet access/without VPC)

• Custom domain setup (with VPC and without public internet access)

Each scenario requires speciﬁc steps to ensure that users have the required permissions to leverage
EMR Serverless for large data processing across the entire machine learning lifecycle in SageMaker
Canvas.

Scenario 1: Quick domain setup

If you used the Quick setup option when creating your SageMaker AI domain, follow these steps:

1.
Navigate to the Amazon SageMaker AI domain settings:

a.
Open the Amazon SageMaker AI console at https://console.aws.amazon.com/sagemaker/.

b.
In the left navigation pane, choose Domains.

c.
Select your domain.

d.
Choose the App Conﬁgurations tab.

e.
Scroll to the Canvas section and choose Edit.

2.
Enable large data processing:

a.
In the Large data processing conﬁguration section, turn on Enable EMR Serverless for
large data processing.

b.
Create or select an EMR Serverless role:

i.
Choose Create and use a new execution role to create a new IAM role that
has a trust relationship with EMR Serverless and the AWS managed policy:

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1008

## Page 38

Amazon SageMaker AI
Developer Guide

AmazonSageMakerCanvasEMRServerlessExecutionRolePolicy policy attached. This
IAM role is assumed by Canvas to create EMR Serverless jobs.

ii.
Alternatively, if you already have an execution role with a trust relationship for EMR
Serverless, then select Use an existing execution role and choose your role from the
dropdown.

• The existing role must have a name that begins with the preﬁx

AmazonSageMakerCanvasEMRSExecutionAccess-.

• The role you select should also have at least the permissions described in the AWS
managed policy: AmazonSageMakerCanvasEMRServerlessExecutionRolePolicy
policy.

• The role should have an EMR Serverless trust policy, as shown below:

JSON

{
"Version":"2012-10-17",
"Statement": [
{
"Sid": "EMRServerlessTrustPolicy",
"Effect": "Allow",
"Principal": {
"Service": "emr-serverless.amazonaws.com"
},
"Action": "sts:AssumeRole",
"Condition": {
"StringEquals": {
"aws:SourceAccount": "111122223333"
}
}
}
]
}

3.
(Optional) Add Amazon S3 permissions for custom Amazon S3 buckets:

a.
The Canvas managed policy automatically grants read and write permissions for Amazon

S3 buckets with sagemaker or SageMaker AI in their names. It also grants read

permissions for objects in custom Amazon S3 buckets with the tag "SageMaker":

"true".

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1009

## Page 39

Amazon SageMaker AI
Developer Guide

b.
For custom Amazon S3 buckets without the required tag, add the following policy to your
EMR Serverless role:

c.
JSON

{
"Version":"2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Action": [
"s3:GetObject",
"s3:PutObject",
"s3:DeleteObject"
],
"Resource": [

"arn:aws:s3:::*"
]
}
]
}

d.
We recommend that you scope down the permissions to speciﬁc Amazon S3 buckets that
you want Canvas to access.

4.
Save your changes and restart your SageMaker Canvas application.

Scenario 2: Custom domain setup (with public internet access/without VPC)

If you created or use a custom domain, follow steps 1-3 from Scenario 1, and then do these
additional steps:

1.
Add permissions for the Amazon ECR DescribeImages operation to your Amazon SageMaker
AI execution role, as Canvas utilizes public Amazon ECR Docker images for data preparation
and model training:

a.
Sign in to the AWS console and open the IAM console at https://
console.aws.amazon.com/iam/.

b.
Choose Roles.

c.
In the search box, search for your SageMaker AI execution role by name and select it.

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1010

## Page 40

Amazon SageMaker AI
Developer Guide

d.
Add the following policy to your SageMaker AI execution role. This can be done either by
adding it as a new inline policy or by appending the policy statement to an existing one.
Note that an IAM role can have a maximum of 10 policies attached.

JSON

{
"Version":"2012-10-17",
"Statement": [{
"Sid": "ECRDescribeImagesOperation",
"Effect": "Allow",
"Action": "ecr:DescribeImages",
"Resource": [
"arn:aws:ecr:*:*:repository/sagemaker-data-wrangler-emr-
container",

"arn:aws:ecr:*:*:repository/ap-dataprep-emr"
]
}]
}

2.
Save your changes and restart your SageMaker Canvas application.

Scenario 3: Custom domain setup (with VPC and without public internet access)

If you created or use a custom domain, follow all steps from Scenario 2, then follow these
additional steps:

1.
Ensure your VPC subnets are private:

•
Verify that the route table for your subnets doesn't have an entry mapping 0.0.0.0/0 to
an Internet Gateway.

2.
Add permissions for creating network interfaces:

a.
When using SageMaker Canvas with EMR Serverless for large-scale data processing,
EMR Serverless requires the ability to create Amazon EC2 ENIs to enable network
communication between EMR Serverless applications and your VPC resources.

b.
Add the following policy to your Amazon SageMaker AI execution role. This can be done
either by adding it as a new inline policy or by appending the policy statement to an
existing one. Note that an IAM role can have a maximum of 10 policies attached.

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1011

## Page 41

Amazon SageMaker AI
Developer Guide

JSON

{
"Version":"2012-10-17",
"Statement": [
{
"Sid": "AllowEC2ENICreation",
"Effect": "Allow",
"Action": [
"ec2:CreateNetworkInterface"
],
"Resource": [
"arn:aws:ec2:*:*:network-interface/*"
],
"Condition": {
"StringEquals": {

"aws:CalledViaLast": "ops.emr-
serverless.amazonaws.com"
}
}
}
]
}

3.
(Optional) Restrict ENI creation to speciﬁc subnets:

a.
To further secure your setup by restricting the creation of ENIs to certain subnets within
your VPC, you can tag each subnet with speciﬁc conditions.

b.
Use the following IAM policy to ensure that EMR Serverless applications can only create
Amazon EC2 ENIs within the allowed subnets and security groups:

{
"Sid": "AllowEC2ENICreationInSubnetAndSecurityGroupWithEMRTags",
"Effect": "Allow",
"Action": [
"ec2:CreateNetworkInterface"
],
"Resource": [
"arn:aws:ec2:*:*:subnet/*",
"arn:aws:ec2:*:*:security-group/*"
],

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1012

## Page 42

Amazon SageMaker AI
Developer Guide

"Condition": {
"StringEquals": {
"aws:ResourceTag/KEY": "VALUE"
}
}
}

4.
Follow the steps on the page Conﬁgure Amazon SageMaker Canvas in a VPC without internet
access to set the VPC endpoint for Amazon S3, which is required by EMR Serverless and other
AWS services that are used by SageMaker Canvas.

5.
Save your changes and restart your SageMaker Canvas application.

By following these steps, you can enable large data processing in SageMaker Canvas for various
domain setups, including those with custom VPC conﬁgurations. Remember to restart your
SageMaker Canvas application after making these changes to apply the new permissions.

Encrypt Your SageMaker Canvas Data with AWS KMS

You might have data that you want to encrypt while using Amazon SageMaker Canvas, such as your
private company information or customer data. SageMaker Canvas uses AWS Key Management
Service to protect your data. AWS KMS is a service that you can use to create and manage
cryptographic keys for encrypting your data. For more information about AWS KMS, see AWS Key
Management Service in the AWS KMS Developer Guide.

Amazon SageMaker Canvas provides you with several options for encrypting your data. SageMaker
Canvas provides default encryption within the application for tasks such as building your model
and generating insights. You can also choose to encrypt data stored in Amazon S3 to protect your
data at rest. SageMaker Canvas supports importing encrypted datasets, so you can establish an
encrypted workﬂow. The following sections describe how you can use AWS KMS encryption to
protect your data while building models with SageMaker Canvas.

Encrypt your data in SageMaker Canvas

With SageMaker Canvas, you can use two diﬀerent AWS KMS encryption keys to encrypt your data
in SageMaker Canvas, which you can specify when setting up your domain using the standard
domain setup. These keys are speciﬁed in the following domain setup steps:

• Step 3: Conﬁgure Applications - (Optional) – When conﬁguring the Canvas storage
conﬁguration section, you can specify an Encryption key. This is a KMS key that SageMaker

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1013

## Page 43

Amazon SageMaker AI
Developer Guide

Canvas uses for long-term storage of model objects and datasets, which are stored in the
provided Amazon S3 bucket for your domain. If creating a Canvas application with the CreateApp

API, use the S3KMSKeyId ﬁeld to specify this key.

• Step 6: Conﬁgure storage – SageMaker Canvas uses one key for encrypting the Amazon

SageMaker Studio private space that is created for your Canvas application, which includes
temporary application storage, visualizations, and compute jobs (such as building models). You
can use either the default AWS managed key or specify your own. If you specify your AWS KMS

key, the data stored in the /home/sagemaker-user directory is encrypted with your key. If you

don't specify an AWS KMS key, the data inside /home/sagemaker-user is encrypted with an
AWS managed key. Regardless of whether you specify an AWS KMS key, all of the data outside
of the working directory is encrypted with an AWS Managed Key. To learn more about the Studio
space and your Canvas application storage, see Store SageMaker Canvas application data in
your own SageMaker AI space. If creating a Canvas application with the CreateApp API, use the

KmsKeyID ﬁeld to specify this key.

The preceding keys can be the same or diﬀerent KMS keys.

Prerequisites

To use your own KMS key for either of the previously described purposes, you must ﬁrst grant your
user's IAM role permission to use the key. Then, you can specify the KMS key when setting up your
domain.

The simplest way to grant your role permission to use the key is to modify the key policy. Use the
following procedure to grant your role the necessary permissions.

1.
Open the AWS KMS console.

2.
In the Key Policy section, choose Switch to policy view.

3.
Modify the key's policy to grant permissions for the kms:GenerateDataKey and

kms:Decrypt actions to the IAM role. Additionally, if you're modifying the key policy that

encrypts your Canvas application storage in the Studio space, grant the kms:CreateGrant
action. You can add a statement that's similar to the following:

{
"Sid": "ExampleStmt",
"Action": [
"kms:CreateGrant", #this permission is only required for the key that encrypts
your SageMaker Canvas application storage

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1014

## Page 44

Amazon SageMaker AI
Developer Guide

"kms:Decrypt",
"kms:GenerateDataKey"
],
"Effect": "Allow",
"Principal": {
"AWS": "<arn:aws:iam::111122223333:role/Jane>"
},
"Resource": "*"
}

4.
Choose Save changes.

The less preferred method is to modify the user’s IAM role to grant the user permissions to
use or manage the KMS key. If you use this method, the KMS key policy must also allow access
management through IAM. To learn how to grant permission to a KMS key through the user’s IAM
role, see Specifying KMS keys in IAM policy statements in the AWS KMS Developer Guide.

Encrypt your data in the SageMaker Canvas application

The ﬁrst KMS key you can use in SageMaker Canvas is used for encrypting application data stored
on Amazon Elastic Block Store (Amazon EBS) volumes and in the Amazon Elastic File System that
SageMaker AI creates in your domain. SageMaker Canvas encrypts your data with this key in the
underlying application and temporary storage systems created when using compute instances for
building models and generating insights. SageMaker Canvas passes the key to other AWS services,
such as Autopilot, whenever SageMaker Canvas initiates jobs with them to process your data.

You can specify this key by setting the KmsKeyID in the CreateDomain API call or while doing the
standard domain setup in the console. If you don’t specify your own KMS key, SageMaker AI uses a
default AWS managed KMS key to encrypt your data in the SageMaker Canvas application.

To specify your own KMS key for use in the SageMaker Canvas application through the console,
ﬁrst set up your Amazon SageMaker AI domain using the Standard setup. Use the following
procedure to complete the Network and Storage Section for the domain.

1.
Fill out your desired Amazon VPC settings.

2.
For Encryption key, choose Enter a KMS key ARN.

3.
For KMS ARN, enter the ARN for your KMS key, which should

have a format similar to the following: arn:aws:kms:example-

region-1:123456789098:key/111aa2bb-333c-4d44-5555-a111bb2c33dd

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1015

## Page 45

Amazon SageMaker AI
Developer Guide

Encrypt your SageMaker Canvas data saved in Amazon S3

The second KMS key you can specify is used for data that SageMaker Canvas stores to Amazon

S3. This KMS key is speciﬁed in the S3KMSKeyId ﬁeld in the CreateDomain API call, or while

doing the standard domain setup in the SageMaker AI console. SageMaker Canvas saves
duplicates of your input datasets, application and model data, and output data to the Region’s

default SageMaker AI S3 bucket for your account. The naming pattern for this bucket is s3://

sagemaker-{Region}-{your-account-id}, and SageMaker Canvas stores data in the

Canvas/ folder.

1.
Turn on Enable notebook resource sharing.

2.
For S3 location for shareable notebook resources, leave the default Amazon S3 path. Note
that SageMaker Canvas does not use this Amazon S3 path; this Amazon S3 path is used for
Studio Classic notebooks.

3.
For Encryption key, choose Enter a KMS key ARN.

4.
For KMS ARN, enter the ARN for your KMS key, which should have a format similar to the

following: arn:aws:kms:us-east-1:111122223333:key/111aa2bb-333c-4d44-5555-

a111bb2c33dd

Import encrypted datasets from Amazon S3

Your users might have datasets that have been encrypted with a KMS key. While the preceding
section shows you how to encrypt data in SageMaker Canvas and data stored to Amazon S3, you
must grant your user's IAM role additional permissions if you want to import data from Amazon S3
that is already encrypted with AWS KMS.

To grant your user permissions to import encrypted datasets from Amazon S3 into SageMaker
Canvas, add the following permissions to the IAM execution role that you've used for the user
proﬁle.

"kms:Decrypt",
"kms:GenerateDataKey"

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1016

## Page 46

Amazon SageMaker AI
Developer Guide

To learn how to edit the IAM permissions for a role, see Adding and removing IAM identity
permissions in the IAM User Guide. For more information about KMS keys, see Key policies in AWS
Key Management Service in the AWS KMS Developer Guide.

FAQs

Refer to the following FAQ items for answers to commonly asked questions about SageMaker
Canvas AWS KMS support.

Q: Does SageMaker Canvas retain my KMS key?

A: No. SageMaker Canvas may temporarily cache your key or pass it on to other AWS services (such
as Autopilot), but SageMaker Canvas does not retain your KMS key.

Q: I speciﬁed a KMS key when setting up my domain. Why did my dataset fail to import in
SageMaker Canvas?

A: Your user’s IAM role may not have permissions to use that KMS key. To grant your user
permissions, see the Prerequisites. Another possible error is that you have a bucket policy on your
Amazon S3 bucket that requires the use of a speciﬁc KMS key that doesn’t match the KMS key you
speciﬁed in your domain. Make sure that you specify the same KMS key for your Amazon S3 bucket
and your domain.

Q: How do I ﬁnd the Region’s default SageMaker AI Amazon S3 bucket for my account?

A: The default Amazon S3 bucket follows the naming pattern s3://

sagemaker-{Region}-{your-account-id}. The Canvas/ folder in this bucket stores your
SageMaker Canvas application data.

Q: Can I change the default SageMaker AI Amazon S3 bucket used to store SageMaker Canvas
data?

A: No, SageMaker AI creates this bucket for you.

Q: What does SageMaker Canvas store in the default SageMaker AI Amazon S3 bucket?

A: SageMaker Canvas uses the default SageMaker AI Amazon S3 bucket to store duplicates of your
input datasets, model artifacts, and model outputs.

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1017

## Page 47

Amazon SageMaker AI
Developer Guide

Q: What use cases are supported for using KMS keys with SageMaker Canvas?

A: With SageMaker Canvas, you can use your own encryption keys with AWS KMS for building
regression, binary and multi-class classiﬁcation, and time series forecasting models, as well as for
batch inference with your model.

Store SageMaker Canvas application data in your own SageMaker AI space

Your Amazon SageMaker Canvas application data, such as datasets that you import and your model
artifacts, is stored in a Amazon SageMaker Studio private space. The space consists of a storage
volume for your application data with 100 GB of storage per user proﬁle, the type of the space
(in this case, a Canvas application), and the image for your application's container. When you set
up Canvas and launch your application for the ﬁrst time, SageMaker AI creates a default private
space that is assigned to your user proﬁle and stores your Canvas data. You don't have to do any
additional conﬁguration to set up the space because SageMaker AI automatically creates the space
on your behalf. However, if you don't want to use the default space, you have the option to specify
a space that you created yourself. This can be useful if you want to isolate your data. The following
page shows you how to create and conﬁgure your own Studio space for storing Canvas application
data.

Note

You can only conﬁgure a custom Studio space for new Canvas applications. You can't
modify the space conﬁguration for existing Canvas applications.

Before you begin

Your Amazon SageMaker AI domain or user proﬁle must have at least 100 GB of storage in order to
create and use the SageMaker Canvas application.

If you created your domain through the SageMaker AI console, enough storage is provisioned
by default and you don't need to take any additional action. If you created your domain or
user proﬁle with the CreateDomain or  CreateUserProﬁle APIs, then make sure that you set the

MaximumEbsVolumeSizeInGb value to 100 GB or greater. To set a greater storage value, you can
either create a new domain or user proﬁle, or you can update an existing domain or user proﬁle
using the UpdateDomain or  UpdateUserProﬁle APIs.

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1018

## Page 48

Amazon SageMaker AI
Developer Guide

Create a new space

First, create a new Studio space that is conﬁgured to store Canvas application data. This is the
space that you specify when creating a new Canvas application in the next step.

To create a space, you can use the AWS SDK for Python (Boto3) or the AWS CLI.

SDK for Python (Boto3)

The following example shows you how to use the AWS SDK for Python (Boto3) create_space
method to create a space that you can use for Canvas applications. Make sure to specify these
parameters:

• DomainId: Specify the ID for your SageMaker AI domain. To ﬁnd your ID, you can go to
the SageMaker AI console at https://console.aws.amazon.com/sagemaker/ and locate your
domain in the Domains section.

• SpaceName: Specify a name for the new space.

• EbsVolumeSizeinGb: Specify the storage volume size for your space (in GB). The minimum

value is 5 and the maximum is 16384.

• SharingType: Specify this ﬁeld as Private. For more information, see Amazon SageMaker
Studio spaces.

• OwnerUserProfileName: Specify the user proﬁle name. To ﬁnd user proﬁle names
associated with a domain, you can go to the SageMaker AI console at https://
console.aws.amazon.com/sagemaker/ and locate your domain in the Domains section. In the
domain's settings, you can view the user proﬁles.

• AppType: Specify this ﬁeld as Canvas.

response = client.create_space(
DomainId='<your-domain-id>',
SpaceName='<your-new-space-name>',
SpaceSettings={
'AppType': 'Canvas',
'SpaceStorageSettings': {
'EbsStorageSettings': {
'EbsVolumeSizeInGb': <storage-volume-size>
}
},

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1019

## Page 49

Amazon SageMaker AI
Developer Guide

},
OwnershipSettings={
'OwnerUserProfileName': '<your-user-profile>'
},
SpaceSharingSettings={
'SharingType': 'Private'
}
)

AWS CLI

The following example shows you how to use the AWS CLI create-space method to create a
space that you can use for Canvas applications. Make sure to specify these parameters:

• domain-id: Specify the ID for your domain. To ﬁnd your ID, you can go to the SageMaker
AI console at https://console.aws.amazon.com/sagemaker/ and locate your domain in the
Domains section.

• space-name: Specify a name for the new space.

• EbsVolumeSizeinGb: Specify the storage volume size for your space (in GB). The minimum

value is 5 and the maximum is 16384.

• SharingType: Specify this ﬁeld as Private. For more information, see Amazon SageMaker
Studio spaces.

• OwnerUserProfileName: Specify the user proﬁle name. To ﬁnd user proﬁle names
associated with a domain, you can go to the SageMaker AI console at https://
console.aws.amazon.com/sagemaker/ and locate your domain in the Domains section. In the
domain's settings, you can view the user proﬁles.

• AppType: Specify this ﬁeld as Canvas.

create-space
--domain-id <your-domain-id>
--space-name <your-new-space-name>
--space-settings '{
"AppType": "Canvas",
"SpaceStorageSettings": {
"EbsStorageSettings": {"EbsVolumeSizeInGb": <storage-volume-size>}
},
}'

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1020

## Page 50

Amazon SageMaker AI
Developer Guide

--ownership-settings '{"OwnerUserProfileName": "<your-user-profile>"}'
--space-sharing-settings '{"SharingType": "Private"}'

You should now have a space. Keep track of your space's name for the next step.

Create a new Canvas application

After creating a space, create a new Canvas application that speciﬁes the space as its storage
location.

To create a new Canvas application, you can use the AWS SDK for Python (Boto3) or the AWS CLI.

Important

You must use the AWS SDK for Python (Boto3) or the AWS CLI to create your Canvas
application. Specifying a custom space when creating Canvas applications through the
SageMaker AI console isn't supported.

SDK for Python (Boto3)

The following example shows you how to use the AWS SDK for Python (Boto3) create_app
method to create a new Canvas application. Make sure to specify these parameters:

• DomainId: Specify the ID for your SageMaker AI domain.

• SpaceName: Specify the name of the space that you created in the previous step.

• AppType: Specify this ﬁeld as Canvas.

• AppName: Specify default as the app name.

response = client.create_app(
DomainId='<your-domain-id>',
SpaceName='<your-space-name>',
AppType='Canvas',
AppName='default'
)

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1021

## Page 51

Amazon SageMaker AI
Developer Guide

AWS CLI

The following example shows you how to use the AWS CLI create-app method to create a
new Canvas application. Make sure to specify these parameters:

• DomainId: Specify the ID for your SageMaker AI domain.

• SpaceName: Specify the name of the space that you created in the previous step.

• AppType: Specify this ﬁeld as Canvas.

• AppName: Specify default as the app name.

create-app
--domain-id <your-domain-id>
--space-name <your-space-name>
--app-type Canvas

--app-name default

You should now have a new Canvas application that uses a custom Studio space as the storage
location for application data.

Important

Any time you delete the Canvas application (or log out) and have to re-create the

application, you must provide your space in the SpaceName ﬁeld to make sure that Canvas
uses your space.

The space is attached to the user proﬁle you speciﬁed in the space conﬁguration. You can delete
your Canvas application without deleting the space, and the data stored in the space remains. The
data stored in your space is only deleted if you delete your user proﬁle, or if you directly delete the
space.

Grant Your Users Permissions to Build Custom Image and Text Prediction Models

Important

Custom IAM policies that allow Amazon SageMaker Studio or Amazon SageMaker Studio
Classic to create Amazon SageMaker resources must also grant permissions to add tags to

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1022

## Page 52

Amazon SageMaker AI
Developer Guide

those resources. The permission to add tags to resources is required because Studio and
Studio Classic automatically tag any resources they create. If an IAM policy allows Studio
and Studio Classic to create resources but does not allow tagging, "AccessDenied" errors can
occur when trying to create resources. For more information, see Provide permissions for
tagging SageMaker AI resources.
AWS managed policies for Amazon SageMaker AI that give permissions to create
SageMaker resources already include permissions to add tags while creating those
resources.

In Amazon SageMaker Canvas, you can build custom models to meet your speciﬁc business
need. Two of these custom model types are single-label image predicion and multi-category text
prediction. The permissions to build these model types are included in the AWS Identity and Access
Management (IAM) policy called AmazonSageMakerCanvasFullAccess, which SageMaker AI attaches
by default to your user's IAM execution role if you leave the Canvas base permissions turned on. If
you are using a custom IAM conﬁguration, then you must explicitly add permissions to your user's
IAM execution role so that they can build custom image and text prediction model types. To grant
the necessary permissions to build image and text prediction models, read the following section to
learn how to attach a least-permissions policy to your role.

To add the permissions to the user's IAM role, do the following:

1.
Go to the IAM console.

2.
Choose Roles.

3.
In the search box, search for the user's IAM role by name and select it.

4.
On the page for the user's role, under Permissions, choose Add permissions.

5.
Choose Create inline policy.

6.
Select the JSON tab, and then paste the following least-permissions policy into the editor.

JSON

{
"Version":"2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Action": [

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1023

## Page 53

Amazon SageMaker AI
Developer Guide

"sagemaker:CreateAutoMLJobV2",
"sagemaker:DescribeAutoMLJobV2"
],
"Resource": "*"
}
]
}

7.
Choose Review policy.

8.
Enter a Name for the policy.

9.
Choose Create policy.

For more information about AWS managed policies, see Managed policies and inline policies in the
IAM User Guide.

Grant Users Permissions to Use Amazon Bedrock and Generative AI Features in
Canvas

Generative AI features in Amazon SageMaker Canvas are powered by Amazon Bedrock foundation
models, which are large language models (LLMs) that have the capability to understand and
generate human-like text. This page describes how to grant the permissions necessary for the
following features in SageMaker Canvas:

• Chat with and compare Amazon Bedrock models: Access and start conversational chats with
Amazon Bedrock models through SageMaker Canvas.

• Use the Chat for data prep feature in Data Wrangler : Use natural language to explore, visualize,
and transform your data. This feature is powered by Anthropic Claude 2.

• Fine-tune Amazon Bedrock foundation models: Fine-tune an Amazon Bedrock foundation model
on your own data to receive customized responses.

In order to use these features, you must ﬁrst request access to the speciﬁc Amazon Bedrock model
that you want to use. Then, add the necessary AWS IAM permissions and a trust relationship with
Amazon Bedrock to the user's execution role. To grant the permissions to the role, you can choose
one of the following methods:

• Create a new Amazon SageMaker AI domain or user proﬁle and turn on Amazon Bedrock
permissions. For more information, see Getting started with using Amazon SageMaker Canvas.

• Edit the settings for an existing Amazon SageMaker AI domain or user proﬁle.

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1024

## Page 54

Amazon SageMaker AI
Developer Guide

• Manually add permissions and a trust relationship to a domain's or user's IAM role.

Step 1: Add Amazon Bedrock model access

Access to Amazon Bedrock models isn't granted by default, so you must go to the Amazon Bedrock
console to request access to models for your AWS account.

To learn how to request access to a speciﬁc Amazon Bedrock model, following the procedure
to Add model access on the page Manage access to Amazon Bedrock foundation models in the
Amazon Bedrock User Guide.

Step 2: Grant permissions to the user's IAM role

When setting up your Amazon SageMaker AI domain or user proﬁle, the user's IAM execution
role must have the  AmazonSageMakerCanvasBedrockAccess policy attached, as well as a trust

relationship with Amazon Bedrock, so that your user can access Amazon Bedrock models from
SageMaker Canvas.

You can modify the domain settings and either create a new execution role (to which SageMaker AI
attaches the required permissions for you) or specify an existing role.

Alternatively, you can manually modify the permissions for an existing IAM role through the IAM
console.

Both methods are described in the following sections.

Grant permissions through the domain settings

You can edit your domain or user proﬁle settings to turn on the Canvas Ready-to-use models
conﬁguration setting and specify an Amazon Bedrock role.

To edit your domain settings and grant access to Amazon Bedrock models for Canvas users in the
domain, do the following:

1.
Go to the SageMaker AI console at https://console.aws.amazon.com/sagemaker/.

2.
In the left navigation pane, choose Domains.

3.
From the list of domains, choose your domain.

4.
Choose the App Conﬁgurations tab.

5.
In the Canvas section, choose Edit.

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1025

## Page 55

Amazon SageMaker AI
Developer Guide

6.
The Edit Canvas settings page opens. For the Canvas Ready-to-use models conﬁguration
section, do the following:

a.
Turn on the Enable Canvas Ready-to-use models option.

b.
For Amazon Bedrock role, select Create and use a new execution role to create a new
IAM execution role that has the  AmazonSageMakerCanvasBedrockAccess policy attached
and a trust relationship with Amazon Bedrock. This IAM role is assumed by Amazon
Bedrock when you access Amazon Bedrock models, use the chat for data prep feature, or
ﬁne-tune Amazon Bedrock models in Canvas. If you already have an execution role with a
trust relationship, then select Use an existing execution role and choose your role from
the dropdown.

7.
Choose Submit  to save your changes.

Your users should now have the necessary permissions to access Amazon Bedrock models, use the
chat for data prep feature, and ﬁne-tune Amazon Bedrock models in Canvas.

You can use the same procedure above for editing an individual user’s settings, except go into
the individual user’s proﬁle from the domain page and edit the user settings instead. Permissions
granted to an individual user don’t apply to other users in the domain, while permissions granted
through the domain settings apply to all user proﬁles in the domain.

For more information on editing your domain settings, see View and Edit domains.

Grant permissions manually through IAM

You can manually grant users permissions to access and ﬁne-tune Amazon Bedrock models in
Canvas by adding permissions to the IAM role speciﬁed for the domain or user’s proﬁle. The
IAM role must have the  AmazonSageMakerCanvasBedrockAccess policy attached and a trust
relationship with Amazon Bedrock.

The following section shows you how to attach the policy to your IAM role and create the trust
relationship with Amazon Bedrock.

First, take note of your domain or user proﬁle’s IAM role. Note that permissions granted to an
individual user don’t apply to other users in the domain, while permissions granted through the
domain apply to all user proﬁles in the domain.

To conﬁgure the IAM role and grant permissions to ﬁne-tune foundation models in Canvas, do the
following:

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1026

## Page 56

Amazon SageMaker AI
Developer Guide

1.
Go to the IAM console at https://console.aws.amazon.com/iam/.

2.
In the left navigation pane, choose Roles.

3.
Search for the user's IAM role by name from the list of roles and select it.

4.
On the Permissions tab, choose Add permissions. From the dropdown menu, choose Attach
policies.

5.
Search for the AmazonSageMakerCanvasBedrockAccess policy and select it.

6.
ChooseAdd permissions.

7.
Back on the IAM role’s page, choose the Trust relationships tab.

8.
Choose Edit trust policy.

9.
In the policy editor, ﬁnd the Add a principal option in the right panel and choose Add.

10. In the dialog box, for Principal type, select AWS services.

11. For ARN, enter bedrock.amazonaws.com.

12. Choose Add principal.

13. Choose Update policy.

You should now have an IAM role that has the  AmazonSageMakerCanvasBedrockAccess policy
attached and a trust relationship with Amazon Bedrock. For information about AWS managed
policies, see Managed policies and inline policies in the IAM User Guide.

Update SageMaker Canvas for Your Users

You can update to the latest version of Amazon SageMaker Canvas as either a user or an IT
administrator. You can update Amazon SageMaker Canvas for a single user at a time.

To update the Amazon SageMaker Canvas application, you must delete the previous version.

Important

Deleting the previous version of Amazon SageMaker Canvas doesn't delete the data or
models that the users have created.

Use the following procedure to log in to AWS, open Amazon SageMaker AI domain, and update
Amazon SageMaker Canvas. The users can start using the SageMaker Canvas application when they
log back in.

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1027

## Page 57

Amazon SageMaker AI
Developer Guide

1.
Sign in to the Amazon SageMaker AI console at Amazon SageMaker Runtime.

2.
On the left navigation pane, choose Admin conﬁgurations.

3.
Under Admin conﬁgurations, choose domains.

4.
On the Domains page, choose your domain.

5.
From the list of User proﬁles, choose a user proﬁle.

6.
For the list of Apps, ﬁnd the Canvas application (the App type says Canvas) and choose Delete
app.

7.
Complete the dialog box and choose Conﬁrm action.

The following image shows the user proﬁle page and highlights the Delete app action from the
preceding procedure.

![Page 57 Diagram 1](images/page-0057-img-01.png)

Request a Quota Increase

Your users might use AWS resources in amounts that exceed those speciﬁed by their quotas. If your
users are resource constrained and encounter errors in SageMaker Canvas, you can request a quota
increase for them.

For more details about SageMaker AI quotas and how to request a quota increase, see Quotas.

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1028

## Page 58

Amazon SageMaker AI
Developer Guide

Amazon SageMaker Canvas uses the following services to process the requests of your users:

• Amazon SageMaker Autopilot

• Amazon SageMaker Studio Classic domain

For a list of the available quotas for SageMaker Canvas operations, see Amazon SageMaker AI
endpoints and quotas.

Request an increase for instances to build custom models

When building a custom model, if you encounter an error during post-building analysis that tells

you to increase your quota for ml.m5.2xlarge instances, use the following information to resolve
the issue.

You must increase the SageMaker AI Hosting endpoint quota for the ml.m5.2xlarge instance
type to a non-zero value in your AWS account. After building a model, SageMaker Canvas hosts the
model on a SageMaker AI Hosting endpoint and uses the endpoint to generate the post-building

analysis. If you don't increase the default account quota of 0 for ml.m5.2xlarge instances,
SageMaker Canvas cannot complete this step and generates an error during post-building analysis.

For the procedure to increase the quota, see  Requesting a quota increase in the Service Quotas
User Guide.

Grant Users Permissions to Import Amazon Redshift Data

Your users might have datasets stored in Amazon Redshift. Before users can import data from

Amazon Redshift into SageMaker Canvas, you must add the AmazonRedshiftFullAccess
managed policy to the IAM execution role that you've used for the user proﬁle and add Amazon
Redshift as a service principal to the role's trust policy. You must also associate the IAM execution
role with your Amazon Redshift cluster. Complete the procedures in the following sections to give
your users the required permissions to import Amazon Redshift data.

Add Amazon Redshift permissions to your IAM role

You must grant Amazon Redshift permissions to the IAM role speciﬁed in your user proﬁle.

To add the AmazonRedshiftFullAccess policy to the user's IAM role, do the following.

1.
Sign in to the IAM console at https://console.aws.amazon.com/iam/.

2.
Choose Roles.

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1029

## Page 59

Amazon SageMaker AI
Developer Guide

3.
In the search box, search for the user's IAM role by name and select it.

4.
On the page for the user's role, under Permissions, choose Add permissions.

5.
Choose Attach policies.

6.
Search for the AmazonRedshiftFullAccess managed policy and select it.

7.
Choose Attach policies to attach the policy to the role.

After attaching the policy, the role’s Permissions section should now include

AmazonRedshiftFullAccess.

To add Amazon Redshift as a service principal to the IAM role, do the following.

1.
On the same page for the IAM role, under Trust relationships, choose Edit trust policy.

2.
In the Edit trust policy editor, update the trust policy to add Amazon Redshift as a service
principal. An IAM role that allows Amazon Redshift to access other AWS services on your
behalf has a trust relationship as follows:

JSON

{
"Version":"2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Principal": {
"Service": "redshift.amazonaws.com"
},
"Action": "sts:AssumeRole"
}
]
}

3.
After editing the trust policy, choose Update policy.

You should now have an IAM role that has the policy AmazonRedshiftFullAccess attached to
it and a trust relationship established with Amazon Redshift, giving users permission to import
Amazon Redshift data into SageMaker Canvas. For more information about AWS managed policies,
see Managed policies and inline policies in the IAM User Guide.

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1030

## Page 60

Amazon SageMaker AI
Developer Guide

Associate the IAM role with your Amazon Redshift cluster

In the settings for your Amazon Redshift cluster, you must associate the IAM role that you granted
permissions to in the preceding section.

To associate an IAM role with your cluster, do the following.

1.
Sign in to the Amazon Redshift console at https://console.aws.amazon.com/redshiftv2/.

2.
On the navigation menu, choose Clusters, and then choose the name of the cluster that you
want to update.

3.
In the Actions dropdown menu, choose Manage IAM roles. The Cluster permissions page
appears.

4.
For Available IAM roles, enter either the ARN or the name of the IAM role, or choose the IAM
role from the list.

5.
Choose Associate IAM role to add it to the list of Associated IAM roles.

6.
Choose Save changes to associate the IAM role with the cluster.

Amazon Redshift modiﬁes the cluster to complete the change, and the IAM role to which you
previously granted Amazon Redshift permissions is now associated with your Amazon Redshift
cluster. Your users now have the required permissions to import Amazon Redshift data into
SageMaker Canvas.

Grant Your Users Permissions to Send Predictions to Quick

You must grant your SageMaker Canvas users permissions to send batch predictions to Quick.
In Quick, users can create analyses and reports with a dataset and prepare dashboards to share
their results. For more information about sending prediction to QuickSight for analysis, see Send
predictions to Quick.

To grant the necessary permissions to share batch predictions with users in QuickSight, you must
add a permissions policy to the AWS Identity and Access Management (IAM) execution role that
you’ve used for the user proﬁle. The following section shows you how to attach a least-permissions
policy to your role.

Add the permissions policy to your IAM role

To add the permissions policy, use the following procedure:

1.
Sign in to the IAM console at https://console.aws.amazon.com/iam/.

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1031

## Page 61

Amazon SageMaker AI
Developer Guide

2.
Choose Roles.

3.
In the search box, search for the user's IAM role by name and select it.

4.
On the page for the user's role, under Permissions, choose Add permissions.

5.
Choose Create inline policy.

6.
Select the JSON tab, and then paste the following least-permissions policy into the editor.

Replace the placeholders <your-account-number> with your own AWS account number.

JSON

{
"Version":"2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Action": [
"quicksight:CreateDataSet",
"quicksight:ListUsers",
"quicksight:ListNamespaces",
"quicksight:CreateDataSource",
"quicksight:PassDataSet",
"quicksight:PassDataSource"
],
"Resource": [
"arn:aws:quicksight:*:111122223333:datasource/*",
"arn:aws:quicksight:*:111122223333:user/*",
"arn:aws:quicksight:*:111122223333:namespace/*",
"arn:aws:quicksight:*:111122223333:dataset/*"
]
}
]
}

7.
Choose Review policy.

8.
Enter a Name for the policy.

9.
Choose Create policy.

You should now have a customer-managed IAM policy attached to your execution role that grants
your Canvas users the necessary permissions to send batch predictions to users in QuickSight.

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1032

## Page 62

Amazon SageMaker AI
Developer Guide

Applications management

The following sections describe how you can manage your SageMaker Canvas applications. You can
view, delete, or relaunch your applications from the Domains section of the SageMaker AI console.

Topics

• Check for active applications

• Delete an application

• Relaunch an application

Check for active applications

To check if you have any actively running SageMaker Canvas applications, use the following
procedure.

1.
Open the SageMaker AI console.

2.
On the left navigation pane, choose Dashboard.

3.
In the LCNC section, there is a row for Canvas that tells you how many active apps are running.
Choose the number to view the list of apps.

The Status column displays the status of the application, such as Ready, Pending, or Deleted. If
the application is Ready, then your SageMaker Canvas workspace instance is active. You can delete
the application from the console, or you can reopen Canvas and log out.

Delete an application

If you want to terminate your SageMaker Canvas workspace instance, you can either log out from
the SageMaker Canvas application or delete your application from the SageMaker AI console. A
workspace instance is dedicated for your use from when you start using SageMaker Canvas to the
point when you stop using it. Deleting the application only terminates the workspace instance
and stops workspace instance charges. Models and datasets aren’t aﬀected, but Quick build tasks
automatically restart when you relaunch the application.

To delete your Canvas application through the AWS console, ﬁrst close the browser tab in which
your Canvas application was open. Then, use the following procedure to delete your SageMaker
Canvas application.

1.
Open the SageMaker AI console.

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1033

## Page 63

Amazon SageMaker AI
Developer Guide

2.
On the left navigation pane, choose Admin conﬁgurations.

3.
Under Admin conﬁgurations, choose Domains.

4.
On the Domains page, choose your domain.

5.
On the Domain details page, choose Resources.

6.
Under Applications, ﬁnd the application that says Canvas in the App type column.

7.
Select the checkbox next to the Canvas application and choose Stop.

You have now successfully stopped the application and terminated the workspace instance.

You can also terminate the workspace instance by logging out from within the SageMaker Canvas
application.

Relaunch an application

If you delete or log out of your SageMaker Canvas application and want to relaunch the
application, use the following procedure.

1.
Navigate to the SageMaker AI console.

2.
In the navigation pane, choose Canvas.

3.
On the SageMaker Canvas landing page, in the Get Started box, select your user proﬁle from
the dropdown.

4.
Choose Open Canvas to open the application.

SageMaker Canvas begins launching the application.

You can also use the following secondary procedure if you encounter any issues with the previous
procedure.

1.
Open the SageMaker AI console.

2.
On the left navigation pane, choose Admin conﬁgurations.

3.
Under Admin conﬁgurations, choose domains.

4.
On the Domains page, choose your domain.

5.
On the Domain details page, under User proﬁles, select the user proﬁle name for the
SageMaker Canvas application you want to view.

6.
Choose Launch and select Canvas from the dropdown list.

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1034

## Page 64

Amazon SageMaker AI
Developer Guide

SageMaker Canvas begins launching the application.

Conﬁgure Amazon SageMaker Canvas in a VPC without internet access

The Amazon SageMaker Canvas application runs in a container in an AWS managed Amazon Virtual
Private Cloud (VPC). If you want to further control access to your resources or run SageMaker
Canvas without public internet access, you can conﬁgure your Amazon SageMaker AI domain and
VPC settings. Within your own VPC, you can conﬁgure settings such as security groups (virtual
ﬁrewalls that control inbound and outbound traﬃc from Amazon EC2 instances) and subnets
(ranges of IP addresses in your VPC). To learn more about VPCs, see How Amazon VPC works.

When the SageMaker Canvas application is running in the AWS managed VPC, it can interact with
other AWS services using either an internet connection or through VPC endpoints created in a
customer-managed VPC (without public internet access). SageMaker Canvas applications can access
these VPC endpoints through a Studio Classic-created network interface that provides connectivity
to the customer-managed VPC. The default behavior of the SageMaker Canvas application is to
have internet access. When using an internet connection, the containers for the preceding jobs
access AWS resources over the internet, such as the Amazon S3 buckets where you store training
data and model artifacts.

However, if you have security requirements to control access to your data and job containers, we
recommend that you conﬁgure SageMaker Canvas and your VPC so that your data and containers
aren’t accessible over the internet. SageMaker AI uses the VPC conﬁguration settings you specify
when setting up your domain for SageMaker Canvas.

If you want to conﬁgure your SageMaker Canvas application without internet access, you must
conﬁgure your VPC settings when you onboard to Amazon SageMaker AI domain, set up VPC
endpoints, and grant the necessary AWS Identity and Access Management permissions. For
information about conﬁguring a VPC in Amazon SageMaker AI, see Choose an Amazon VPC. The
following sections describe how to run SageMaker Canvas in a VPC without public internet access.

Conﬁgure Amazon SageMaker Canvas in a VPC without internet access

You can send traﬃc from SageMaker Canvas to other AWS services through your own VPC. If
your own VPC doesn't have public internet access and you've set up your domain in VPC only
mode, then SageMaker Canvas won't have public internet access as well. This includes all requests,
such as accessing datasets in Amazon S3 or training jobs for standard builds, and the requests go
through VPC endpoints in your VPC instead of the public internet. When you onboard to domain
and Choose an Amazon VPC, you can specify your own VPC as the default VPC for the domain,

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1035

## Page 65

Amazon SageMaker AI
Developer Guide

along with your desired security group and subnet settings. Then, SageMaker AI creates a network
interface in your VPC that SageMaker Canvas uses to access VPC endpoints in your VPC.

Make sure that you set up one or more security groups in your VPC with inbound and outbound
rules that allow  TCP traﬃc within the security group. This is required for connectivity between the
Jupyter Server application and the Kernel Gateway applications. You must allow access to at least

ports in the range 8192-65535. Also, make sure to create a distinct security group for each user
proﬁle and add inbound access from that same security group. We do not recommend reusing a
domain level security group for user proﬁles. If the domain level security group allows inbound
access to itself, all applications in the domain have access to all other applications in the domain.
Note that the security group and subnet settings are set after you ﬁnish onboarding to domain.

When onboarding to domain, if you choose Public internet only as the network access type, the
VPC is SageMaker AI managed and allows internet access.

You can change this behavior by choosing VPC only so that SageMaker AI sends all traﬃc to
a network interface that SageMaker AI creates in your speciﬁed VPC. When you choose this
option, you must provide the subnets, security groups, and VPC endpoints that are necessary to
communicate with the SageMaker API and SageMaker AI Runtime, and various AWS services, such
as Amazon S3 and Amazon CloudWatch, that are used by SageMaker Canvas. Note that you can
only import data from Amazon S3 buckets located in the same Region as your VPC.

The following procedures show how you can conﬁgure these settings to use SageMaker Canvas
without the internet.

Step 1: Onboard to Amazon SageMaker AI domain

To send SageMaker Canvas traﬃc to a network interface in your own VPC instead of over the
internet, specify the VPC you want to use when onboarding to Amazon SageMaker AI domain. You
must also specify at least two subnets in your VPC that SageMaker AI can use. Choose Standard
setup and do the following procedure when conﬁguring the Network and Storage Section for the
domain.

1.
Select your desired VPC.

2.
Choose two or more Subnets. If you don’t specify the subnets, SageMaker AI uses all of the
subnets in the VPC.

3.
Choose one or more Security group(s).

4.
Choose VPC Only to turn oﬀ direct internet access in the AWS managed VPC where SageMaker
Canvas is hosted.

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1036

## Page 66

Amazon SageMaker AI
Developer Guide

After disabling internet access, ﬁnish the onboarding process to set up your domain. For more
information about the VPC settings for Amazon SageMaker AI domain, see Choose an Amazon VPC.

Step 2: Conﬁgure VPC endpoints and access

Note

In order to conﬁgure Canvas in your own VPC, you must enable private DNS hostnames for
your VPC endpoints. For more information, see Connect to SageMaker AI Through a VPC
Interface Endpoint.

SageMaker Canvas only accesses other AWS services to manage and store data for its functionality.
For example, it connects to Amazon Redshift if your users access an Amazon Redshift database.
It can connect to an AWS service such as Amazon Redshift using an internet connection or a VPC
endpoint. Use VPC endpoints if you want to set up connections from your VPC to AWS services that
don't use the public internet.

A VPC endpoint creates a private connection to an AWS service that uses a networking path that
is isolated from the public internet. For example, if you set up access to Amazon S3 using a VPC
endpoint from your own VPC, then the SageMaker Canvas application can access Amazon S3 by
going through the network interface in your VPC and then through the VPC endpoint that connects
to Amazon S3. The communication between SageMaker Canvas and Amazon S3 is private.

For more information about conﬁguring VPC endpoints for your VPC, see AWS PrivateLink. If you
are using Amazon Bedrock models in Canvas with a VPC, for more information about controlling
access to your data, see  Protect jobs using a VPC in the Amazon Bedrock User Guide.

The following are the VPC endpoints for each service you can use with SageMaker Canvas:

Service
Endpoint
Endpoint type

AWS Application Auto Scaling
com.amazo

Interface

naws.Region.application-
autoscaling

Amazon Athena
com.amazo

Interface

naws.Region.athena

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1037

## Page 67

Amazon SageMaker AI
Developer Guide

Service
Endpoint
Endpoint type

Amazon SageMaker AI
com.amazo

Interface

naws.Region.sagemaker.api

com.amazo

naws.Region.sagemake
r.runtime

com.amazo

naws.Region.notebook

Amazon SageMaker AI Data
Science Assistant

com.amazo

Interface

naws.Region.sagemaker-
data-science-assistant

AWS Security Token Service
com.amazonaws.Region.sts
Interface

Amazon Elastic Container
Registry (Amazon ECR)

com.amazo

Interface

naws.Region.ecr.api

com.amazo

naws.Region.ecr.dkr

Amazon Elastic Compute
Cloud (Amazon EC2)

com.amazonaws.Region.ec2
Interface

Amazon Simple Storage

com.amazonaws.Region.s3
Gateway

Service (Amazon S3)

Amazon Redshift
com.amazo

Interface

naws.Region.redshift-data

AWS Secrets Manager
com.amazo

Interface

naws.Region.secretsmanager

AWS Systems Manager
com.amazonaws.Region.ssm
Interface

Amazon CloudWatch
com.amazo

Interface

naws.Region.monitoring

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1038

## Page 68

Amazon SageMaker AI
Developer Guide

Service
Endpoint
Endpoint type

Amazon CloudWatch Logs
com.amazonaws.Region.logs
Interface

Amazon Forecast
com.amazo

Interface

naws.Region.forecast

com.amazo

naws.Region.forecastquery

Amazon Textract
com.amazo

Interface

naws.Region.textract

Amazon Comprehend
com.amazo

Interface

naws.Region.comprehend

Amazon Rekognition
com.amazo

Interface

naws.Region.rekognition

AWS Glue
com.amazonaws.Region.glue
Interface

AWS Application Auto Scaling
com.amazo

Interface

naws.Region.application-
autoscaling

Amazon Relational Database
Service (Amazon RDS)

com.amazonaws.Region.rds
Interface

Amazon Bedrock (see note
after table)

com.amazo

Interface

naws.Region.bedrock-
runtime

Amazon Kendra
com.amazo

Interface

naws.Region.kendra

Amazon EMR Serverless
com.amazo

Interface

naws.Region.emr-serverless

Amazon Q Developer (see
note after table)

com.amazonaws.Region.q
Interface

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1039

## Page 69

Amazon SageMaker AI
Developer Guide

Note

The Amazon Q Developer VPC endpoint is currently available only in the US East (N.
Virginia) region. To connect to it from other regions, you can choose one of the following
options based on your security and infrastructure preferences:

• Set up a NAT Gateway. Conﬁgure a NAT Gateway in your VPC's private subnet to enable
internet connectivity for the Q Developer endpoint. For more information, see Setting up
a NAT Gateway in a VPC Private Subnet.

• Enable cross-region VPC endpoint access. Set up cross-region VPC endpoint access for
Q Developer. Use this option to connect securely without requiring internet access. For
more information, see Conﬁguring Cross-Region VPC Endpoint Access.

Note

For Amazon Bedrock, the interface endpoint service name

com.amazonaws.Region.bedrock has been deprecated. Create a new VPC endpoint
with the service name listed in the preceding table.
Additionally, you can't ﬁne-tune foundation models from Canvas VPCs with no internet
access. This is because Amazon Bedrock doesn't support VPC endpoints for model
customization APIs. To learn more about ﬁne-tuning foundation models in Canvas, see
Fine-tune foundation models.

You must also add an endpoint policy for Amazon S3 to control AWS principal access to your VPC
endpoint. For information about how to update your VPC endpoint policy, see Control access to
VPC endpoints using endpoint policies.

The following are two VPC endpoint policies that you can use. Use the ﬁrst policy if you only want
to grant access to the basic functionality of Canvas, such as importing data and creating models.
Use the second policy if you want to grant access to the additional  genenerative AI features in
Canvas.

Basic VPC endpoint policy

The following policy grants the necessary access to your VPC endpoint for basic operations in
Canvas.

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1040

## Page 70

Amazon SageMaker AI
Developer Guide

{
"Effect": "Allow",
"Action": [
"s3:GetObject",
"s3:PutObject",
"s3:DeleteObject",
"s3:CreateBucket",
"s3:GetBucketCors",
"s3:GetBucketLocation"
],
"Resource": [
"arn:aws:s3:::*SageMaker*",
"arn:aws:s3:::*Sagemaker*",
"arn:aws:s3:::*sagemaker*"
]
},

{
"Effect": "Allow",
"Action": [
"s3:ListBucket",
"s3:ListAllMyBuckets"
],
"Resource": "*"
}

Generative AI VPC endpoint policy

The following policy grants the necessary access to your VPC endpoint for basic operations in
Canvas, as well as using generative AI foundation models.

{
"Effect": "Allow",
"Action": [
"s3:GetObject",
"s3:PutObject",
"s3:DeleteObject",
"s3:CreateBucket",
"s3:GetBucketCors",
"s3:GetBucketLocation"
],
"Resource": [
"arn:aws:s3:::*SageMaker*",
"arn:aws:s3:::*Sagemaker*",

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1041

## Page 71

Amazon SageMaker AI
Developer Guide

"arn:aws:s3:::*sagemaker*",
"arn:aws:s3:::*fmeval/datasets*",
"arn:aws:s3:::*jumpstart-cache-prod*"
]
},
{
"Effect": "Allow",
"Action": [
"s3:ListBucket",
"s3:ListAllMyBuckets"
],
"Resource": "*"
}

Step 3: Grant IAM permissions

The SageMaker Canvas user must have the necessary AWS Identity and Access Management
permissions to allow connection to the VPC endpoints. The IAM role to which you give permissions
must be the same one you used when onboarding to Amazon SageMaker AI domain. You can

attach the SageMaker AI managed AmazonSageMakerFullAccess policy to the IAM role for the
user to give the user the required permissions. If you require more restrictive IAM permissions and

use custom policies instead, then give the user’s role the ec2:DescribeVpcEndpointServices
permission. SageMaker Canvas requires these permissions to verify the existence of the required
VPC endpoints for standard build jobs. If it detects these VPC endpoints, then standard build jobs
run by default in your VPC. Otherwise, they will run in the default AWS managed VPC.

For instructions on how to attach the AmazonSageMakerFullAccess IAM policy to your user’s
IAM role, see Adding and removing IAM identity permissions.

To grant your user’s IAM role the granular ec2:DescribeVpcEndpointServices permission, use
the following procedure.

1.
Sign in to the AWS Management Console and open the IAM console.

2.
In the navigation pane, choose Roles.

3.
In the list, choose the name of the role to which you want to grant permissions.

4.
Choose the Permissions tab.

5.
Choose Add permissions and then choose Create inline policy.

6.
Choose the JSON tab and enter the following policy, which grants the

ec2:DescribeVpcEndpointServices permission:

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1042

## Page 72

Amazon SageMaker AI
Developer Guide

JSON

{
"Version":"2012-10-17",
"Statement": [
{
"Sid": "VisualEditor0",
"Effect": "Allow",
"Action": "ec2:DescribeVpcEndpointServices",
"Resource": "*"
}
]
}

7.
Choose Review policy, and then enter a Name for the policy (for example,

VPCEndpointPermissions).

8.
Choose Create policy.

The user’s IAM role should now have permissions to access the VPC endpoints conﬁgured in your
VPC.

(Optional) Step 4: Override security group settings for speciﬁc users

If you are an administrator, you might want diﬀerent users to have diﬀerent VPC settings, or user-
speciﬁc VPC settings. When you override the default VPC’s security group settings for a speciﬁc
user, these settings are passed on to the SageMaker Canvas application for that user.

You can override the security groups that a speciﬁc user has access to in your VPC when you set
up a new user proﬁle in Studio Classic. You can use the CreateUserProﬁle SageMaker API call

(or create_user_proﬁle with the AWS CLI), and then in the UserSettings, you can specify the

SecurityGroups for the user.

Set up connections to data sources with OAuth

The following section describes the steps you must take to set up OAuth connections to data
sources from SageMaker Canvas. OAuth is a common authentication platform for granting access
to resources without sharing passwords. With OAuth, you can quickly connect to your data from
Canvas and import it for building models. Canvas currently supports OAuth for Snowﬂake and
Salesforce Data Cloud.

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1043

## Page 73

Amazon SageMaker AI
Developer Guide

Note

You can only establish one OAuth connection for each data source.

Set up OAuth for Salesforce Data Cloud

To set up OAuth for Salesforce Data Cloud, follow these general steps:

1.
Sign in to Salesforce Data Cloud.

2.
In Salesforce Data Cloud, create a new app connection and do the following:

a.
Enable OAuth settings.

b.
When prompted for a callback URL (or the URL of the resource accessing your data),
specify the URL for your Canvas application. The Canvas application URL follows this

format: https://<domain-id>.studio.<region>.sagemaker.aws/canvas/

default

c.
Copy the consumer key and secret.

d.
Copy your authorization URL and token URL.

For more detailed instructions about performing the preceding tasks in Salesforce Data Cloud, see
Import data from Salesforce Data Cloud in the Data Wrangler documentation for importing data
from Salesforce Data Cloud.

After enabling access from Salesforce Data Cloud and getting your connection information, you
must create an AWS Secrets Manager secret to store the information and add it to your Amazon
SageMaker AI domain or user proﬁle. Note that you can add a secret to both a domain and user
proﬁle, but Canvas looks for secrets in the user proﬁle ﬁrst.

To add a secret to your domain or user proﬁle, do the following:

1.
Go to the Amazon SageMaker AI console.

2.
Choose domains in the navigation pane.

3.
From the list of domains, choose your domain.

a.
If adding your secret to your domain, do the following:

i.
Choose the domain.

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1044

## Page 74

Amazon SageMaker AI
Developer Guide

ii.
On the domain settings page, choose the domain settings tab.

iii.
Choose Edit.

b.
If adding the secret to your user proﬁle, do the following:

i.
Choose the user’s domain.

ii.
On the domain settings page, choose the user proﬁle.

iii.
On the User Details page, choose Edit.

4.
In the navigation pane, choose Canvas settings.

5.
For OAuth settings, choose Add OAuth conﬁguration.

6.
For Data source, select Salesforce Data Cloud.

7.
For Secret Setup, select Create a new secret. Alternatively, if you already created an AWS
Secrets Manager secret with your credentials, enter the ARN for the secret. If creating a new
secret, do the following:

a.
For Identity Provider, select SALESFORCE.

b.
For Client ID, Client Secret, Authorization URL, and Token URL, enter all of the
information you gathered from Salesforce Data Cloud in the previous procedure.

8.
Save your domain or user proﬁle settings.

You should now be able to create a connection to your data in Salesforce Data Cloud from Canvas.

Set up OAuth for Snowﬂake

To set up authentication for Snowﬂake, Canvas supports identity providers that you can use
instead of having users directly enter their credentials into Canvas.

The following are links to the Snowﬂake documentation for the identity providers that Canvas
supports:

• Azure AD

• Okta

• Ping Federate

The following process describes the general steps you must take. For more detailed instructions
about performing these steps, you can refer to the Setting up Snowﬂake OAuth Access section in
the Data Wrangler documentation for importing data from Snowﬂake.

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1045

## Page 75

Amazon SageMaker AI
Developer Guide

To set up OAuth for Snowﬂake, do the following:

1.
Register Canvas as an application with the identity provider. This requires specifying

a redirect URL to Canvas, which should follow this format: https://<domain-

id>.studio.<region>.sagemaker.aws/canvas/default

2.
Within the identity provider, create a server or API that sends OAuth tokens to Canvas so
that Canvas can access Snowﬂake. When setting up the server, use the authorization code
and refresh token grant types, specify the access token lifetime, and set a refresh token
policy. Additionally, within the External OAuth Security Integration for Snowﬂake, enable

external_oauth_any_role_mode.

3.
Get the following information from the identity provider: token URL, authorization URL, client
ID, client secret. For Azure AD, also retrieve the OAuth scope credentials.

4.
Store the information retrieved in the previous step in an AWS Secrets Manager secret.

a.
For Okta and Ping Federate, the secret should look like the following format:

{"token_url":"https://identityprovider.com/oauth2/example-portion-of-URL-path/
v2/token",
"client_id":"example-client-id", "client_secret":"example-client-secret",
"identity_provider":"OKTA"|"PING_FEDERATE",
"authorization_url":"https://identityprovider.com/oauth2/example-portion-of-
URL-path/v2/authorize"}

b.
For Azure AD, the secret should also include the OAuth scope credentials as the

datasource_oauth_scope ﬁeld.

After conﬁguring the identity provider and the secret, you must create an AWS Secrets Manager
secret to store the information and add it to your Amazon SageMaker AI domain or user proﬁle.
Note that you can add a secret to both a domain and user proﬁle, but Canvas looks for secrets in
the user proﬁle ﬁrst.

To add a secret to your domain or user proﬁle, do the following:

1.
Go to the Amazon SageMaker AI console.

2.
Choose domains in the navigation pane.

3.
From the list of domains, choose your domain.

a.
If adding your secret to your domain, do the following:

Amazon SageMaker Canvas setup and permissions management (for IT administrators)
1046

## Page 76

Amazon SageMaker AI
Developer Guide

i.
Choose the domain.

ii.
On the domain settings page, choose the domain settings tab.

iii.
Choose Edit.

b.
If adding the secret to your user proﬁle, do the following:

i.
Choose the user’s domain.

ii.
On the domain settings page, choose the user proﬁle.

iii.
On the User Details page, choose Edit.

4.
In the navigation pane, choose Canvas settings.

5.
For OAuth settings, choose Add OAuth conﬁguration.

6.
For Data source, select Snowﬂake.

7.
For Secret Setup, select Create a new secret. Alternatively, if you already created an AWS

Secrets Manager secret with your credentials, enter the ARN for the secret. If creating a new
secret, do the following:

a.
For Identity Provider, select SNOWFLAKE.

b.
For Client ID, Client Secret, Authorization URL, and Token URL, enter all of the
information you gathered from the identity provider in the previous procedure.

8.
Save your domain or user proﬁle settings.

You should now be able to create a connection to your data in Snowﬂake from Canvas.

Generative AI assistance for solving ML problems in Canvas using
Amazon Q Developer

While using Amazon SageMaker Canvas, you can chat with Amazon Q Developer in natural
language to leverage generative AI and solve problems. Q Developer is an assistant that helps you
translate your goals into machine learning (ML) tasks and describes each step of the ML workﬂow.
Q Developer helps Canvas users reduce the amount of time, eﬀort, and data science expertise
required to leverage ML and make data-driven decisions for their organizations.

Through a conversation with Q Developer, you can initiate actions in Canvas such as preparing
data, building an ML model, making predictions, and deploying a model. Q Developer makes
suggestions for next steps and provides you with context as you complete each step. It also informs

Generative AI assistance using Q Developer
1047

## Page 77

Amazon SageMaker AI
Developer Guide

you of results; for example, Canvas can transform your dataset according to best practices, and Q
Developer can list the transforms that were used and why.

Amazon Q Developer is available in SageMaker Canvas at no additional cost to both Amazon Q

Developer Pro Tier and Free Tier users. However, standard charges apply for resources such as the

SageMaker Canvas workspace instance and any resources used for building or deploying models.
For more information about pricing, see Amazon SageMaker Canvas pricing.

Use of Amazon Q is licensed to you under MIT's 0 License and subject to the AWS Responsible AI
Policy. When you use Q Developer from outside the US, Q Developer processes data across US
regions. For more information, see Cross region inference in Amazon Q Developer.

Note

Amazon Q Developer in SageMaker Canvas doesn't use user content to improve the service,
regardless of whether you use the Free-tier or Pro-tier subscription. For service telemetry
purposes, Q Developer might track your usage, such as the number of questions asked and
whether recommendations were accepted or rejected. This telemetry data doesn't include
personally identiﬁable information such as IP address.

How it works

Amazon Q Developer is a generative AI powered assistant available in SageMaker Canvas that you
can query using natural language. Q Developer makes suggestions for each step of the machine
learning workﬂow, explaining concepts and providing you with options and more details as
needed. You can use Q Developer for help with regression, binary classiﬁcation, and multi-class
classiﬁcation use cases.

For example, to predict customer churn, upload a dataset of historical customer churn information
to Canvas through Q Developer. Q Developer suggests an appropriate ML model type and steps to
ﬁx dataset issues, build a model, and make predictions.

Important

Amazon Q Developer is intended for conversations about machine learning problems
within SageMaker Canvas. It guides users through Canvas actions and optionally answers
questions about AWS services. Q Developer processes model inputs only in English. For

Generative AI assistance using Q Developer
1048

## Page 78

Amazon SageMaker AI
Developer Guide

more information about how you can use Q Developer, see  Amazon Q Developer features
in the Amazon Q Developer User Guide.

Supported regions

Amazon Q Developer is available within SageMaker Canvas in the following AWS Regions:

• US East (N. Virginia)

• US East (Ohio)

• US West (Oregon)

• Asia Paciﬁc (Mumbai)

• Asia Paciﬁc (Seoul)

• Asia Paciﬁc (Singapore)

• Asia Paciﬁc (Sydney)

• Asia Paciﬁc (Tokyo)

• Europe (Frankfurt)

• Europe (Ireland)

• Europe (Paris)

Amazon Q Developer capabilities available in Canvas

The following list summarizes the Canvas tasks with which Q Developer can provide assistance:

• Describe your objective – Q Developer can suggest an ML model type and general approach to
solve your problem.

• Import and analyze datasets – Tell Q Developer where your dataset is stored or upload a ﬁle to
save it as a Canvas dataset. Prompt Q Developer to identify any issues in your dataset, such as
outliers or missing values. Q Developer provides summary statistics about your dataset and lists
any identiﬁed issues.

Q Developer supports queries about the following statistics for individual columns:

• Numeric columns – number of valid values, feature type, mean, median, minimum,

maximum, standard deviation, 25th percentile, 75th percentile, number of

outliers

Generative AI assistance using Q Developer
1049

## Page 79

Amazon SageMaker AI
Developer Guide

• Categorical columns – number of missing values, number of valid values,

feature type, most frequent, most frequent category, most frequent category

count, least frequent, least frequent category, least frequent category

count, categories

• Fix dataset issues – Prompt Q Developer to use Canvas's data transformation capabilities to
create a revised version of your dataset. Canvas creates a Data Wrangler data ﬂow and applies
transforms according to data science best practices. For more information, see Data preparation.

If you want to do more advanced data analysis or data preparation tasks than you can
accomplish with Q Developer, then we recommend that you go to the Data Wrangler data ﬂow
interface.

• Train a model – Q Developer tells you the recommended ML model type for your problem and
a proposed model building conﬁguration. You can use the suggested default settings to do a
quick build, or you can modify the conﬁguration and do a standard build. When ready, prompt Q
Developer to build your Canvas model.

All of the custom model types are supported. For more information about model types and quick
versus standard builds, see How custom models work.

• Evaluate model accuracy – After building a model, Q Developer provides a summary of how
the model scores across various metrics. These metrics help you determine the usefulness and
accuracy of your model. Q Developer can explain any concept or metric in detail.

To view full details and visualizations, open the model from the chat or the My Models page of
Canvas. For more information, see Model evaluation.

• Get predictions for new data – You can upload a new dataset and prompt Q Developer to help
you open the prediction feature of Canvas.

Q Developer opens a new window in the application where you can either make a single
prediction or make batch predictions with a new dataset. For more information, see Predictions
with custom models.

• Deploy a model – To deploy your model for production, ask Q Developer to help you deploy
your model through Canvas. Q Developer opens a new window in which you can conﬁgure your
deployment.

After deploying, view your deployment details either 1) on the My Models page of Canvas in the
model's Deploy tab, or 2) on the ML Ops page in the Deployments tab. For more information,
see Deploy your models to an endpoint.

Generative AI assistance using Q Developer
1050

## Page 80

Amazon SageMaker AI
Developer Guide

Prerequisites

To use Amazon Q Developer to build ML models in SageMaker Canvas, complete the following
prerequisites:

Set up a Canvas application

Make sure that you have a Canvas application set up. For information about how to set up a Canvas
application, see Getting started with using Amazon SageMaker Canvas.

Grant Q Developer permissions

To access Q Developer while using Canvas, you must attach the necessary permissions
to the AWS IAM role used for your SageMaker AI domain or user proﬁle. You can do
this through the console described in this section. If you encounter any permissions
issues due to using the console method, then manually attach the AWS managed policy
AmazonSageMakerCanvasSMDataScienceAssistantAccess to the IAM role.

Permissions attached at the domain level apply to all user proﬁles in the domain, unless individual
permissions are granted or revoked at the user proﬁle level.

SageMaker AI console method

You can grant permissions by editing the SageMaker AI domain or user proﬁle settings.

To grant permissions through the domain settings in the SageMaker AI console, do the
following:

1.
Open the Amazon SageMaker AI console at https://console.aws.amazon.com/sagemaker/.

2.
On the left navigation pane, choose Admin conﬁgurations.

3.
Under Admin conﬁgurations, choose Domains.

4.
From the list of domains, select your domain.

5.
On the Domain details page, select the App conﬁgurations tab.

6.
In the Canvas section, choose Edit.

7.
On the Edit Canvas settings page, go to the Amazon Q Developer section and do the
following:

a.
Turn on Enable Amazon Q Developer in SageMaker Canvas for natural language ML
to add the permissions to chat with Q Developer in Canvas to your domain's execution
role.

Generative AI assistance using Q Developer
1051

## Page 81

Amazon SageMaker AI
Developer Guide

b.
(Optional) Turn on Enable Amazon Q Developer chat for general AWS questions
if you want to ask Q Developer questions about various AWS services (for example:
Describe how Athena works).

Note

When making general AWS queries to Q Developer, your requests route
through the US East (N. Virginia) AWS Region. To prevent your data from
routing through US East (N. Virginia), turn oﬀ the Enable Amazon Q Developer
chat for general AWS questions toggle.

Manual method

Attach the  AmazonSageMakerCanvasSMDataScienceAssistantAccess policy to the AWS IAM role
used for your domain or user proﬁle. For more information about how to do this, see  Adding
and removing IAM identity permissions in the AWS IAM User Guide.

(Optional) Conﬁgure access to Q Developer from your VPC

If you have a VPC that is conﬁgured without public internet access, you can add a VPC endpoint
for Q Developer. For more information, see Conﬁgure Amazon SageMaker Canvas in a VPC without
internet access.

Getting started

To use Amazon Q Developer to build ML models in SageMaker Canvas, do the following:

1.
Open your SageMaker Canvas application.

2.
In the left navigation pane, choose Amazon Q.

3.
Choose Start a new conversation to open a new chat.

When you start a new chat, Q Developer prompts you to state your problem or provide a dataset.

Generative AI assistance using Q Developer
1052

## Page 82

Amazon SageMaker AI
Developer Guide

![Page 82 Diagram 1](images/page-0082-img-01.png)

After importing your data, you can ask Q Developer to provide you with summary statistics about
your dataset, or you can ask questions about speciﬁc columns. For a list of the diﬀerent statistics
that Q Developer supports, see the preceding section Amazon Q Developer capabilities available in

Generative AI assistance using Q Developer
1053

## Page 83

Amazon SageMaker AI
Developer Guide

Canvas. The following screenshot shows an example of asking for dataset statistics and the most
frequent category in a product category column.

![Page 83 Diagram 1](images/page-0083-img-01.png)

Q Developer tracks any Canvas artifacts you import or create during the conversation, such as
transformed datasets and models. You can access them from the chat or other Canvas application
tabs. For example, if Q Developer ﬁxes issues in your dataset, you can access the new, transformed
dataset from the following places:

• The artifacts sidebar in the Q Developer chat interface

• The Datasets page of Canvas, where you can view both your original and transformed datasets.
The transformed dataset has the Built by Amazon Q label added to it.

• The Data Wrangler page of Canvas, where Q Developer creates a new data ﬂow for your dataset

Generative AI assistance using Q Developer
1054

## Page 84

Amazon SageMaker AI
Developer Guide

The following screenshot shows the original dataset and the transformed dataset in the sidebar of
a chat.

![Page 84 Diagram 1](images/page-0084-img-01.png)

When your data is ready, ask Q Developer to help build a Canvas model. Q Developer might
prompt you to conﬁrm a few ﬁelds and review the build conﬁguration. If you use the default build
conﬁguration, then your model is built using a quick build. If you want to customize any part of
your build conﬁguration, such as selecting the algorithms used or changing the objective metric,
then your model is built with a standard build.

The following screenshot shows how you can prompt Q Developer to initiate a Canvas model build
with only a few prompts. This example uses the default conﬁguration to start a quick build.

Generative AI assistance using Q Developer
1055

## Page 85

Amazon SageMaker AI
Developer Guide

![Page 85 Diagram 1](images/page-0085-img-01.png)

Generative AI assistance using Q Developer
1056

## Page 86

Amazon SageMaker AI
Developer Guide

After building your model, you can perform additional actions using either natural language in
the chat or the artifacts sidebar menu. For example, you can view model details and metrics, make
predictions, or deploy the model. The following screenshot shows the sidebar where you can
choose these additional options.

![Page 86 Diagram 1](images/page-0086-img-01.png)

You can also perform any of these actions by going to the My Models page of Canvas and selecting
your model. From your model's page, you can navigate to the Analyze, Predict, and Deploy tabs to
view model metrics and visualizations, make predictions, and manage deployments, respectively.

Logging Q Developer conversations with AWS CloudTrail

AWS CloudTrail is a service that records actions taken by users, roles, or AWS services in Amazon
SageMaker AI. CloudTrail captures API calls resulting from your interactions with Amazon Q
Developer (a conversational AI assistant) while using SageMaker Canvas (a no-code ML interface).
CloudTrail data shows request details, the IP address of the requester, who made the request, and
when.

Your interactions with Q Developer are sent as SendConversation API calls to the SageMaker
AI Data Science Assistant service, which is an internal service that Canvas leverages on the

backend. The event source for SendConversation API calls is sagemaker-data-science-

assistant.amazonaws.com.

Generative AI assistance using Q Developer
1057

## Page 87

Amazon SageMaker AI
Developer Guide

Note

For privacy and security reasons, the content of your conversations is hidden in the

logs, appearing as HIDDEN_DUE_TO_SECURITY_REASONS in the request and response
elements.

To learn more about CloudTrail, see the AWS CloudTrail User Guide. To learn more about CloudTrail
in SageMaker AI, see Logging Amazon SageMaker AI API calls using AWS CloudTrail.

The following is an example log ﬁle entry for the SendConversation API:

{
"eventVersion":"1.10",
"userIdentity": {
"type":"AssumedRole",
"principalId":"AROA123456789EXAMPLE:user-Isengard",
"arn":"arn:aws:sts::111122223333:assumed-role/Admin/user",
"accountId":"111122223333",
"accessKeyId":"ASIAIOSFODNN7EXAMPLE",
"sessionContext": {
"sessionIssuer": {
"type":"Role",
"principalId":"AROA123456789EXAMPLE",
"arn":"arn:aws:iam::111122223333:role/Admin",
"accountId":"111122223333",
"userName":"Admin"
},
"attributes": {
"creationDate":"2024-11-11T22:04:37Z",
"mfaAuthenticated":"false"
}
}
},
"eventTime":"2024-11-11T22:09:22Z",
"eventSource":"sagemaker-data-science-assistant.amazonaws.com",
"eventName":"SendConversation",
"awsRegion":"us-west-2",
"sourceIPAddress":"192.0.2.0",
"userAgent":"Boto3/1.33.13 md/Botocore#1.33.13 ua/2.0 os/
linux#5.10.227-198.884.amzn2int.x86_64 md/arch#x86_64 lang/python#3.7.16 md/
pyimpl#CPython cfg/retry-mode#legacy Botocore/1.33.13",

Generative AI assistance using Q Developer
1058

## Page 88

Amazon SageMaker AI
Developer Guide

"requestParameters": {
"conversation": [
{
"utteranceId":"a1b2c3d4-5678-90ab-cdef-EXAMPLE11111",
"utterance":"HIDDEN_DUE_TO_SECURITY_REASONS",
"timestamp":"Feb 4, 2020, 7:46:29 AM",
"utteranceType":"User"
}
],
"utteranceId":"a1b2c3d4-5678-90ab-cdef-EXAMPLE11111"
},
"responseElements": {
"responseCode":"CHAT_RESPONSE",
"conversationId":"1234567890abcdef0",
"response": {
"chat": {
"body":"HIDDEN_DUE_TO_SECURITY_REASONS"

}
}
},
"requestID":"a1b2c3d4-5678-90ab-cdef-EXAMPLE11111",
"eventID":"a1b2c3d4-5678-90ab-cdef-EXAMPLE11111",
"readOnly":false,
"eventType":"AwsApiCall",
"managementEvent":true,
"recipientAccountId":"123456789012",
"eventCategory":"Management",
"tlsDetails": {
"tlsVersion":"TLSv1.2",
"cipherSuite":"ECDHE-RSA-AES128-GCM-SHA256",
"clientProvidedHostHeader":"gamma.us-west-2.data-science-
assistant.sagemaker.aws.dev"
}
}

Data import

Amazon SageMaker Canvas supports importing tabular, image, and document data. You can import
datasets from your local machine, Amazon services such as Amazon S3 and Amazon Redshift, and
external data sources. When importing datasets from Amazon S3, you can bring a dataset of any
size. Use the datasets that you import to build models and make predictions for other datasets.

Data import
1059

## Page 89

Amazon SageMaker AI
Developer Guide

Each use case for which you can build a custom model accepts diﬀerent types of input. For
example, if you want to build a single-label image classiﬁcation model, then you should import
image data. For more information about the diﬀerent model types and the data they accept, see
How custom models work. You can import data and build custom models in SageMaker Canvas for
the following data types:

• Tabular (CSV, Parquet, or tables)

• Categorical – Use categorical data to build custom categorical prediction models for 2 and 3+
category prediction.

• Numeric – Use numeric data to build custom numeric prediction models.

• Text – Use text data to build custom multi-category text prediction models.

• Timeseries – Use timeseries data to build custom time series forecasting models.

• Image (JPG or PNG) – Use image data to build custom single-label image prediction models.

• Document (PDF, JPG, PNG, TIFF) – Document data is only supported for SageMaker Canvas
Ready-to-use models. To learn more about Ready-to-use models that can make predictions for
document data, see Ready-to-use models.

You can import data into Canvas from the following data sources:

• Local ﬁles on your computer

• Amazon S3 buckets

• Amazon Redshift provisioned clusters (not Amazon Redshift Serverless)

• AWS Glue Data Catalog through Amazon Athena

• Amazon Aurora

• Amazon Relational Database Service (Amazon RDS)

• Salesforce Data Cloud

• Snowﬂake

• Databricks, SQLServer, MariaDB, and other popular databases through JDBC connectors

• Over 40 external SaaS platforms, such as SAP OData

For a full list of data sources from which you can import, see the following table:

Data import
1060

## Page 90

Amazon SageMaker AI
Developer Guide

Source
Type
Supported data types

Local ﬁle upload
Local
Tabular, Image, Document

Amazon Aurora
Amazon internal
Tabular

Amazon S3 bucket
Amazon internal
Tabular, Image, Document

Amazon RDS
Amazon internal
Tabular

Amazon Redshift provision
ed clusters (not Redshift
Serverless)

Amazon internal
Tabular

AWS Glue Data Catalog
(through Amazon Athena)

Amazon internal
Tabular

Databricks
External
Tabular

Snowﬂake
External
Tabular

Salesforce Data Cloud
External
Tabular

SQLServer
External
Tabular

MySQL
External
Tabular

PostgreSQL
External
Tabular

MariaDB
External
Tabular

Amplitude
External SaaS platform
Tabular

CircleCI
External SaaS platform
Tabular

DocuSign Monitor
External SaaS platform
Tabular

Domo
External SaaS platform
Tabular

Datadog
External SaaS platform
Tabular

Data import
1061

## Page 91

Amazon SageMaker AI
Developer Guide

Source
Type
Supported data types

Dynatrace
External SaaS platform
Tabular

Facebook Ads
External SaaS platform
Tabular

Facebook Page Insights
External SaaS platform
Tabular

Google Ads
External SaaS platform
Tabular

Google Analytics 4
External SaaS platform
Tabular

Google Search Console
External SaaS platform
Tabular

GitHub
External SaaS platform
Tabular

GitLab
External SaaS platform
Tabular

Infor Nexus
External SaaS platform
Tabular

Instagram Ads
External SaaS platform
Tabular

Jira Cloud
External SaaS platform
Tabular

LinkedIn Ads
External SaaS platform
Tabular

LinkedIn Ads
External SaaS platform
Tabular

Mailchimp
External SaaS platform
Tabular

Marketo
External SaaS platform
Tabular

Microsoft Teams
External SaaS platform
Tabular

Mixpanel
External SaaS platform
Tabular

Okta
External SaaS platform
Tabular

Salesforce
External SaaS platform
Tabular

Salesforce Marketing Cloud
External SaaS platform
Tabular

Data import
1062

## Page 92

Amazon SageMaker AI
Developer Guide

Source
Type
Supported data types

Salesforce Pardot
External SaaS platform
Tabular

SAP OData
External SaaS platform
Tabular

SendGrid
External SaaS platform
Tabular

ServiceNow
External SaaS platform
Tabular

Singular
External SaaS platform
Tabular

Slack
External SaaS platform
Tabular

Stripe
External SaaS platform
Tabular

Trend Micro
External SaaS platform
Tabular

Typeform
External SaaS platform
Tabular

Veeva
External SaaS platform
Tabular

Zendesk
External SaaS platform
Tabular

Zendesk Chat
External SaaS platform
Tabular

Zendesk Sell
External SaaS platform
Tabular

Zendesk Sunshine
External SaaS platform
Tabular

Zoom Meetings
External SaaS platform
Tabular

For instructions on how to import data and information regarding input data requirements, such as
the maximum ﬁle size for images, see Create a dataset.

Canvas also provides several sample datasets in your application to help you get started. To learn
more about the SageMaker AI-provided sample datasets you can experiment with, see Use sample
datasets.

Data import
1063

## Page 93

Amazon SageMaker AI
Developer Guide

After you import a dataset into Canvas, you can update the dataset at any time. You can do a
manual update or you can set up a schedule for automatic dataset updates. For more information,
see Update a dataset.

For more information speciﬁc to each dataset type, see the following sections:

Tabular

To import data from an external data source (such as a Snowﬂake database or a SaaS platform),
you must authenticate and connect to the data source in the Canvas application. For more
information, see Connect to data sources.

If you want to import datasets larger than 5 GB from Amazon S3 into Canvas, you can achieve
faster sampling by using Amazon Athena to query and sample the data from Amazon S3.

After creating datasets in Canvas, you can prepare and transform your data using the data
preparation functionality of Data Wrangler. You can use Data Wrangler to handle missing
values, transform your features, join multiple datasets into a single dataset, and more. For more
information, see Data preparation.

Tip

As long as your data is arranged into tables, you can join datasets from various sources,
such as Amazon Redshift, Amazon Athena, or Snowﬂake.

Image

For information about how to edit an image dataset and perform tasks such as assigning or
reassigning labels, adding images, or deleting images, see Edit an image dataset.

Create a dataset

Note

If you're importing datasets larger than 5 GB into Amazon SageMaker Canvas, we
recommend that you use the Data Wrangler feature in Canvas to create a data ﬂow. Data
Wrangler supports advanced data preparation features such as joining and concatenating
data. After you create a data ﬂow, you can export your data ﬂow as a Canvas dataset and
begin building a model. For more information, see Export to create a model.

Data import
1064

## Page 94

Amazon SageMaker AI
Developer Guide

The following sections describe how to create a dataset in Amazon SageMaker Canvas. For custom
models, you can create datasets for tabular and image data. For Ready-to-use models, you can
use tabular and image datasets as well as document datasets. Choose your workﬂow based on the
following information:

• For categorical, numeric, text, and timeseries data, see Import tabular data.

• For image data, see Import image data.

• For document data, see Import document data.

A dataset can consist of multiple ﬁles. For example, you might have multiple ﬁles of inventory data
in CSV format. You can upload these ﬁles together as a dataset as long as the schema (or column
names and data types) of the ﬁles match.

Canvas also supports managing multiple versions of your dataset. When you create a dataset,

the ﬁrst version is labeled as V1. You can create a new version of your dataset by updating your
dataset. You can do a manual update, or you can set up an automated schedule for updating your
dataset with new data. For more information, see Update a dataset.

When you import your data into Canvas, make sure that it meets the requirements in the following
table. The limitations are speciﬁc to the type of model you’re building.

Limit
2 category,
3+ category,
numeric, and
time series
models

Text
prediction
models

Image
prediction
models

*Document
data for
Ready-to-
use models

Supported ﬁle types
CSV and
Parquet
(local upload,
Amazon S3,
or databases)

CSV and
Parquet
(local upload,
Amazon S3,
or databases)

JPG, PNG
PDF, JPG,
PNG, TIFF

JSON
(databases)

JSON
(databases)

Maximum ﬁle size
Local upload:
5 GB

Local upload:
5 GB

30 MB per
image

5 MB per
document

Data import
1065

## Page 95

Amazon SageMaker AI
Developer Guide

Limit
2 category,
3+ category,
numeric, and
time series
models

Text
prediction
models

Image
prediction
models

*Document
data for
Ready-to-
use models

Data sources:
PBs

Data sources:
PBs

Maximum number of ﬁles you
can upload at a time

30
30
N/A
N/A

Maximum number of columns
1,000
1,000
N/A
N/A

Maximum number of entries
(rows, images, or documents)
for Quick builds

N/A
7500 rows
5000 images
N/A

Maximum number of entries
(rows, images, or documents)
for Standard builds

N/A
150,000 rows
180,000
images

N/A

Minimum number of entries
(rows) for Quick builds

2 category:
500 rows

N/A
N/A
N/A

3+ category,
numeric,
time series:
N/A

Minimum number of entries
(rows, images, or documents)
for Standard builds

250 rows
50 rows
50 images
N/A

Minimum number of entries
(rows or images) per label

N/A
25 rows
25 rows
N/A

Data import
1066

## Page 96

Amazon SageMaker AI
Developer Guide

Limit
2 category,
3+ category,
numeric, and
time series
models

Text
prediction
models

Image
prediction
models

*Document
data for
Ready-to-
use models

Minimum number of labels
2 category: 2

2
2
N/A

3+ category:
3

Numeric,
time series:
N/A

Minimum sample size for
random sampling

500
N/A
N/A
N/A

Maximum sample size for
random sampling

200,000
N/A
N/A
N/A

Maximum number of labels
2 category: 2

1000
1000
N/A

3+ category,
numeric,
time series:
N/A

*Document data is currently only supported for Ready-to-use models that accept document data.
You can't build a custom model with document data.

Also note the following restrictions:

• When importing data from an Amazon S3 bucket, make sure that your Amazon S3 bucket name

doesn't contain a .. If your bucket name contains a ., you might experience errors when trying to
import data into Canvas.

• For tabular data, Canvas disallows selecting any ﬁle with extensions other
than .csv, .parquet, .parq, and .pqt for both local upload and Amazon S3 import. CSV ﬁles can

Data import
1067

## Page 97

Amazon SageMaker AI
Developer Guide

use any common or custom delimiter, and they must not have newline characters except when
denoting a new row.

• For tabular data using Parquet ﬁles, note the following:

• Parquet ﬁles can't include complex types like maps and lists.

• The column names of Parquet ﬁles can't contain spaces.

• If using compression, Parquet ﬁles must use either gzip or snappy compression types. For more
information about the preceding compression types, see the gzip documentation and the
snappy documentation.

• For image data, if you have any unlabeled images, you must label them before building your
model. For information about how to assign labels to images within the Canvas application, see
Edit an image dataset.

• If you set up automatic dataset updates or automatic batch prediction conﬁgurations, you can
only create a total of 20 conﬁgurations in your Canvas application. For more information, see
How to manage automations.

After you import a dataset, you can view your datasets on the Datasets page at any time.

Import tabular data

With tabular datasets, you can build categorical, numeric, time series forecasting, and text
prediction models. Review the limitations table in the preceding Import a dataset section to
ensure that your data meets the requirements for tabular data.

Use the following procedure to import a tabular dataset into Canvas:

1.
Open your SageMaker Canvas application.

2.
In the left navigation pane, choose Datasets.

3.
Choose Import data.

4.
From the dropdown menu, choose Tabular.

5.
In the popup dialog box, in the Dataset name ﬁeld, enter a name for the dataset and choose
Create.

6.
On the Create tabular dataset page, open the Data Source dropdown menu.

7.
Choose your data source:

• To upload ﬁles from your computer, choose Local upload.

Data import
1068

## Page 98

Amazon SageMaker AI
Developer Guide

• To import data from another source, such as an Amazon S3 bucket or a Snowﬂake database,
search for your data source in the Search data source bar. Then, choose the tile for your
desired data source.

Note

You can only import data from the tiles that have an active connection. If you want
to connect to a data source that is unavailable to you, contact your administrator. If
you’re an administrator, see Connect to data sources.

The following screenshot shows the Data Source dropdown menu.

![Page 98 Diagram 1](images/page-0098-img-01.png)

8.
(Optional) If you’re connecting to an Amazon Redshift or Snowﬂake database for the ﬁrst time,
a dialog box appears to create a connection. Fill out the dialog box with your credentials and
choose Create connection. If you already have a connection, choose your connection.

9.
From your data source, select your ﬁles to import. For local upload and importing from
Amazon S3, you can select ﬁles. For Amazon S3 only, you also have the option to directly enter
the S3 URI, alias, or ARN of your bucket or S3 access point in the Input S3 endpoint ﬁeld, and
then choose ﬁles to import. For database sources, you can drag-and-drop data tables from the
left navigation pane.

Data import
1069

## Page 99

Amazon SageMaker AI
Developer Guide

10. (Optional) For tabular data sources that support SQL querying (such as Amazon Redshift,

Amazon Athena, or Snowﬂake), you can choose Edit in SQL to make SQL queries before
importing them.

The following screenshot shows the Edit SQL view for an Amazon Athena data source.

![Page 99 Diagram 1](images/page-0099-img-01.png)

11. Choose Preview dataset to preview your data before importing it.

12. In the Import settings, enter a Dataset name or use the default dataset name.

13. (Optional) For data that you import from Amazon S3, you are shown the Advanced settings

and can ﬁll out the following ﬁelds:

a.
Toggle the Use ﬁrst row as header option on if you want to use the ﬁrst row of your
dataset as the column names. If you selected multiple ﬁles, this applies to each ﬁle.

b.
If you're importing a CSV ﬁle, for the File encoding (CSV) dropdown, select your dataset

ﬁle’s encoding. UTF-8 is the default.

c.
For the Delimiter dropdown, select the delimiter that separates each cell in your data. The

default delimiter is ,. You can also specify a custom delimiter.

d.
Select Multi-line detection if you’d like Canvas to manually parse your entire dataset for
multi-line cells. By default, this option is not selected and Canvas determines whether or
not to use multi-line support by taking a sample of your data. However, Canvas might not
detect any multi-line cells in the sample. If you have multi-line cells, we recommend that

Data import
1070

## Page 100

Amazon SageMaker AI
Developer Guide

you select the Multi-line detection option to force Canvas to check your entire dataset for
multi-line cells.

14. When you’re ready to import your data, choose Create dataset.

While your dataset is importing into Canvas, you can see your datasets listed on the Datasets page.
From this page, you can View your dataset details.

When the Status of your dataset shows as Ready, Canvas successfully imported your data and you
can proceed with building a model.

If you have a connection to a data source, such as an Amazon Redshift database or a SaaS
connector, you can return to that connection. For Amazon Redshift and Snowﬂake, you can add
another connection by creating another dataset, returning to the Import data page, and choosing
the Data Source tile for that connection. From the dropdown menu, you can open the previous

connection or choose Add connection.

Note

For SaaS platforms, you can only have one connection per data source.

Import image data

With image datasets, you can build single-label image prediction custom models, which predict a
label for an image. Review the limitations in the preceding Import a dataset section to ensure that
your image dataset meets the requirements for image data.

Note

You can only import image datasets from local ﬁle upload or an Amazon S3 bucket. Also,
for image datasets, you must have at least 25 images per label.

Use the following procedure to import an image dataset into Canvas:

1.
Open your SageMaker Canvas application.

2.
In the left navigation pane, choose Datasets.

3.
Choose Import data.

Data import
1071

## Page 101

Amazon SageMaker AI
Developer Guide

4.
From the dropdown menu, choose Image.

5.
In the popup dialog box, in the Dataset name ﬁeld, enter a name for the dataset and choose
Create.

6.
On the Import page, open the Data Source dropdown menu.

7.
Choose your data source. To upload ﬁles from your computer, choose Local upload. To import
ﬁles from Amazon S3, choose Amazon S3.

8.
From your computer or Amazon S3 bucket, select the images or folders of images that you
want to upload.

9.
When you’re ready to import your data, choose Import data.

While your dataset is importing into Canvas, you can see your datasets listed on the Datasets page.
From this page, you can View your dataset details.

When the Status of your dataset shows as Ready, Canvas successfully imported your data and you
can proceed with building a model.

When you are building your model, you can edit your image dataset, and you can assign or re-
assign labels, add images, or delete images from your dataset. For more information about how to
edit your image dataset, see Edit an image dataset.

Import document data

The Ready-to-use models for expense analysis, identity document analysis, document analysis, and
document queries support document data. You can’t build a custom model with document data.

With document datasets, you can generate predictions for expense analysis, identity document
analysis, document analysis, and document queries Ready-to-use models. Review the limitations
table in the Create a dataset section to ensure that your document dataset meets the requirements
for document data.

Note

You can only import document datasets from local ﬁle upload or an Amazon S3 bucket.

Use the following procedure to import a document dataset into Canvas:

1.
Open your SageMaker Canvas application.

Data import
1072

## Page 102

Amazon SageMaker AI
Developer Guide

2.
In the left navigation pane, choose Datasets.

3.
Choose Import data.

4.
From the dropdown menu, choose Document.

5.
In the popup dialog box, in the Dataset name ﬁeld, enter a name for the dataset and choose
Create.

6.
On the Import page, open the Data Source dropdown menu.

7.
Choose your data source. To upload ﬁles from your computer, choose Local upload. To import
ﬁles from Amazon S3, choose Amazon S3.

8.
From your computer or Amazon S3 bucket, select the document ﬁles that you want to upload.

9.
When you’re ready to import your data, choose Import data.

While your dataset is importing into Canvas, you can see your datasets listed on the Datasets page.
From this page, you can View your dataset details.

When the Status of your dataset shows as Ready, Canvas has successfully imported your data.

On the Datasets page, you can choose your dataset to preview it, which shows you up to the ﬁrst
100 documents of your dataset.

View your dataset details

For each of your datasets, you can view all of the ﬁles in a dataset, the dataset’s version history,
and any auto update conﬁgurations for the dataset. From the Datasets page, you can also initiate
actions such as Update a dataset or How custom models work.

To view the details for a dataset, do the following:

1. Open the SageMaker Canvas application.

2. In the left navigation pane, choose Datasets.

3. From the list of datasets, choose your dataset.

On the Data tab, you can see a preview of your data. If you choose Dataset details, you can see
all of the ﬁles that are part of your dataset. Choose a ﬁle to see only the data from that ﬁle in the
preview. For image datasets, the preview only shows you the ﬁrst 100 images of your dataset.

Data import
1073

## Page 103

Amazon SageMaker AI
Developer Guide

On the Version history tab, you can see a list of all of the versions of your dataset. A new version
is made whenever you update a dataset. To learn more about updating a dataset, see Update a
dataset. The following screenshot shows the Version history tab in the Canvas application.

![Page 103 Diagram 1](images/page-0103-img-01.png)

On the Auto updates tab, you can enable auto updates for the dataset and set up a conﬁguration
to update your dataset on a regular schedule. To learn more about setting up auto updates for a
dataset, see Conﬁgure automatic updates for a dataset. The following screenshot shows the Auto
updates tab with auto updates turned on and a list of auto update jobs that have been performed
on the dataset.

Data import
1074

## Page 104

Amazon SageMaker AI
Developer Guide

![Page 104 Diagram 1](images/page-0104-img-01.png)

Update a dataset

After importing your initial dataset into Amazon SageMaker Canvas, you might have additional
data that you want to add to your dataset. For example, you might get inventory data at the end
of every week that you want to add to your dataset. Instead of importing your data multiple times,
you can update your existing dataset and add or remove ﬁles from it.

Note

You can only update datasets that you have imported through local upload or Amazon S3.

You can update your dataset either manually or automatically. For more information about
automatic dataset updates, see Conﬁgure automatic updates for a dataset.

Data import
1075

## Page 105

Amazon SageMaker AI
Developer Guide

Every time you update your dataset, Canvas creates a new version of your dataset. You can only use
the latest version of your dataset to build a model or generate predictions. For more information
about viewing the version history of your dataset, see View your dataset details.

You can also use dataset updates with automated batch predictions, which starts a batch
prediction job whenever you update your dataset. For more information, see Batch predictions in
SageMaker Canvas.

The following section describes how to do manual updates to your dataset.

Manually update a dataset

To do a manual update, do the following:

1. Open the SageMaker Canvas application.

2. In the left navigation pane, choose Datasets.

3. From the list of datasets, choose the dataset you want to update.

4. Choose the Update dataset dropdown menu and choose Manual update. You are taken to the

import data workﬂow.

5. From the Data source dropdown menu, choose either Local upload or Amazon S3.

6. The page shows you a preview of your data. From here, you can add or remove ﬁles from the

dataset. If you’re importing tabular data, the schema of the new ﬁles (column names and data
types) must match the schema of the existing ﬁles. Additionally, your new ﬁles must not exceed
the maximum dataset size or ﬁle size. For more information about these limitations, see  Import
a dataset.

Note

If you add a ﬁle with the same name as an existing ﬁle in your dataset, the new ﬁle
overwrites the old version of the ﬁle.

7. When you’re ready to save your changes, choose Update dataset.

You should now have a new version of your dataset.

On the Datasets page, you can choose the Version history tab to see all of the versions of your
dataset and the history of both manual and automatic updates you’ve made.

Data import
1076

## Page 106

Amazon SageMaker AI
Developer Guide

Conﬁgure automatic updates for a dataset

After importing your initial dataset into Amazon SageMaker Canvas, you might have additional
data that you want to add to your dataset. For example, you might get inventory data at the end
of every week that you want to add to your dataset. Instead of importing your data multiple times,
you can update your existing dataset and add or remove ﬁles from it.

Note

You can only update datasets that you have imported through local upload or Amazon S3.

With automatic dataset updates, you specify a location where Canvas checks for ﬁles at a
frequency you specify. If you import new ﬁles during the update, the schema of the ﬁles must
match the existing dataset exactly.

Every time you update your dataset, Canvas creates a new version of your dataset. You can only use
the latest version of your dataset to build a model or generate predictions. For more information
about viewing the version history of your dataset, see View your dataset details.

You can also use dataset updates with automated batch predictions, which starts a batch
prediction job whenever you update your dataset. For more information, see Batch predictions in
SageMaker Canvas.

The following section describes how to do automatic updates to your dataset.

An automatic update is when you set up a conﬁguration for Canvas to update your dataset at a
given frequency. We recommend that you use this option if you regularly receive new ﬁles of data
that you want to add to your dataset.

When you set up the auto update conﬁguration, you specify an Amazon S3 location where you
upload your ﬁles and a frequency at which Canvas checks the location and imports ﬁles. Each
instance of Canvas updating your dataset is referred to as a job. For each job, Canvas imports all of
the ﬁles in the Amazon S3 location. If you have new ﬁles with the same names as existing ﬁles in
your dataset, Canvas overwrites the old ﬁles with the new ﬁles.

For automatic dataset updates, Canvas doesn’t perform schema validation. If the schema of ﬁles
imported during an automatic update don’t match the schema of the existing ﬁles or exceed the
size limitations (see Import a dataset for a table of ﬁle size limitations), then you get errors when
your jobs run.

Data import
1077

## Page 107

Amazon SageMaker AI
Developer Guide

Note

You can only set up a maximum of 20 automatic conﬁgurations in your Canvas application.
Additionally, Canvas only does automatic updates while you’re logged in to your Canvas
application. If you log out of your Canvas application, automatic updates pause until you
log back in.

To conﬁgure automatic updates for your dataset, do the following:

1. Open the SageMaker Canvas application.

2. In the left navigation pane, choose Datasets.

3. From the list of datasets, choose the dataset you want to update.

4. Choose the Update dataset dropdown menu and choose Automatic update. You are taken to

the Auto updatestab for the dataset.

5. Turn on the Auto update enabled toggle.

6. For Specify a data source, enter the Amazon S3 path to a folder where you plan to regularly

upload ﬁles.

7. For Choose a frequency, select Hourly, Weekly, or Daily.

8. For Specify a starting time, use the calendar and time picker to select when you want the ﬁrst

auto update job to start.

9. When you’re ready to create the auto update conﬁguration, choose Save.

Canvas begins the ﬁrst job of your auto update cadence at the speciﬁed starting time.

View your automatic dataset update jobs

To view the job history for your automatic dataset updates in Amazon SageMaker Canvas, on your
dataset details page, choose the Auto updates tab.

Each automatic update to a dataset shows as a job in the Auto updates tab under the Job history
section. For each job, you can see the following:

• Job created – The timestamp for when Canvas started updating the dataset.

• Files – The number of ﬁles in the dataset.

• Cells (Columns x Rows) – The number of columns and rows in the dataset.

Data import
1078

## Page 108

Amazon SageMaker AI
Developer Guide

• Status – The status of the dataset after the update. If the job was successful, the status is Ready.
If the job failed for any reason, the status is Failed, and you can hover over the status for more
details.

Edit your automatic dataset update conﬁguration

You might want to make changes to your auto update conﬁguration for a dataset, such as changing
the frequency of the updates. You might also want to turn oﬀ your automatic update conﬁguration
to pause the updates to your dataset.

To make changes to your auto update conﬁguration for a dataset, go to the Auto updates tab of
your dataset and choose Edit to make changes to the conﬁguration.

To pause your dataset updates, turn oﬀ your automatic conﬁguration. You can turn oﬀ auto
updates by going to the Auto updates tab of your dataset and turning the Enable auto updates
toggle oﬀ. You can turn this toggle back on at any time to resume the update schedule.

To learn how to delete your conﬁguration, see Delete an automatic conﬁguration.

Connect to data sources

In Amazon SageMaker Canvas, you can import data from a location outside of your local ﬁle system
through an AWS service, a SaaS platform, or other databases using JDBC connectors. For example,
you might want to import tables from a data warehouse in Amazon Redshift, or you might want to
import Google Analytics data.

When you go through the Import workﬂow to import data in the Canvas application, you can
choose your data source and then select the data that you want to import. For certain data sources,
like Snowﬂake and Amazon Redshift, you must specify your credentials and add a connection to
the data source.

The following screenshot shows the data sources toolbar in the Import workﬂow, with all of
the available data sources highlighted. You can only import data from the data sources that are
available to you. Contact your administrator if your desired data source isn’t available.

Data import
1079

## Page 109

Amazon SageMaker AI
Developer Guide

![Page 109 Diagram 1](images/page-0109-img-01.png)

The following sections provide information about establishing connections to external data
sources and and importing data from them. Review the following section ﬁrst to determine what
permissions you need to import data from your data source.

Permissions

Review the following information to ensure that you have the necessary permissions to import
data from your data source:

• Amazon S3: You can import data from any Amazon S3 bucket as long as your user has
permissions to access the bucket. For more information about using AWS IAM to control access to
Amazon S3 buckets, see Identity and access management in Amazon S3 in the Amazon S3 User
Guide.

• Amazon Athena: If you have the AmazonSageMakerFullAccess policy and the
AmazonSageMakerCanvasFullAccess policy attached to your user’s execution role, then you can
query your AWS Glue Data Catalog with Amazon Athena. If you’re part of an Athena workgroup,
make sure that the Canvas user has permissions to run Athena queries on the data. For more
information, see Using workgroups for running queries in the Amazon Athena User Guide.

• Amazon DocumentDB: You can import data from any Amazon DocumentDB database as long
as you have the credentials (username and password) to connect to the database and have the
minimum base Canvas permissions attached to your user’s execution role. For more information
about Canvas permissions, see the Prerequisites for setting up Amazon SageMaker Canvas.

Data import
1080

## Page 110

Amazon SageMaker AI
Developer Guide

• Amazon Redshift: To give yourself the necessary permissions to import data from Amazon
Redshift, see Grant Users Permissions to Import Amazon Redshift Data.

• Amazon RDS: If you have the AmazonSageMakerCanvasFullAccess policy attached to your user’s
execution role, then you’ll be able to access your Amazon RDS databases from Canvas.

• SaaS platforms: If you have the AmazonSageMakerFullAccess policy and the
AmazonSageMakerCanvasFullAccess policy attached to your user’s execution role, then you have
the necessary permissions to import data from SaaS platforms. See Use SaaS connectors with
Canvas for more information about connecting to a speciﬁc SaaS connector.

• JDBC connectors: For database sources such as Databricks, MySQL or MariaDB, you must enable
username and password authentication on the source database before attempting to connect
from Canvas. If you’re connecting to a Databricks database, you must have the JDBC URL that
contains the necessary credentials.

Connect to a database stored in AWS

You might want to import data that you’ve stored in AWS. You can import data from Amazon S3,
use Amazon Athena to query a database in the AWS Glue Data Catalog, import data from Amazon
RDS, or make a connection to a provisioned Amazon Redshift database (not Redshift Serverless).

You can create multiple connections to Amazon Redshift. For Amazon Athena, you can access any
databases that you have in your AWS Glue Data Catalog. For Amazon S3, you can import data from
a bucket as long as you have the necessary permissions.

Review the following sections for more detailed information.

Connect to data in Amazon S3, Amazon Athena, or Amazon RDS

For Amazon S3, you can import data from an Amazon S3 bucket as long as you have permissions to
access the bucket.

For Amazon Athena, you can access databases in your AWS Glue Data Catalog as long as you have
permissions through your Amazon Athena workgroup.

For Amazon RDS, if you have the AmazonSageMakerCanvasFullAccess policy attached to your
user’s role, then you’ll be able to import data from your Amazon RDS databases into Canvas.

To import data from an Amazon S3 bucket, or to run queries and import data tables with Amazon
Athena, see Create a dataset. You can only import tabular data from Amazon Athena, and you can
import tabular and image data from Amazon S3.

Data import
1081

## Page 111

Amazon SageMaker AI
Developer Guide

Connect to an Amazon DocumentDB database

Amazon DocumentDB is a fully managed, serverless, document database service. You can import
unstructured document data stored in an Amazon DocumentDB database into SageMaker Canvas
as a tabular dataset, and then you can build machine learning models with the data.

Important

Your SageMaker AI domain must be conﬁgured in VPC only mode to add connections to
Amazon DocumentDB. You can only access Amazon DocumentDB clusters in the same
Amazon VPC as your Canvas application. Additionally, Canvas can only connect to TLS-
enabled Amazon DocumentDB clusters. For more information about how to set up Canvas
in VPC only mode, see Conﬁgure Amazon SageMaker Canvas in a VPC without internet
access.

To import data from Amazon DocumentDB databases, you must have credentials to access
the Amazon DocumentDB database and specify the username and password when creating
a database connection. You can conﬁgure more granular permissions and restrict access by
modifying the Amazon DocumentDB user permissions. To learn more about access control in
Amazon DocumentDB, see Database Access Using Role-Based Access Control in the Amazon
DocumentDB Developer Guide.

When you import from Amazon DocumentDB, Canvas converts your unstructured data into a
tabular dataset by mapping the ﬁelds to columns in a table. Additional tables are created for each
complex ﬁeld (or nested structure) in the data, where the columns correspond to the sub-ﬁelds
of the complex ﬁeld. For more detailed information about this process and examples of schema
conversion, see the  Amazon DocumentDB JDBC Driver Schema Discovery GitHub page.

Canvas can only make a connection to a single database in Amazon DocumentDB. To import data
from a diﬀerent database, you must create a new connection.

You can import data from Amazon DocumentDB into Canvas by using the following methods:

• Create a dataset. You can import your Amazon DocumentDB data and create a tabular dataset
in Canvas. If you choose this method, make sure that you follow the  Import tabular data
procedure.

• Create a data ﬂow. You can create a data preparation pipeline in Canvas and add your Amazon
DocumentDB database as a data source.

Data import
1082

## Page 112

Amazon SageMaker AI
Developer Guide

To proceed with importing your data, follow the procedure for one of the methods linked in the
preceding list.

When you reach the step in either workﬂow to choose a data source (Step 6 for creating a dataset,
or Step 8 for creating a data ﬂow), do the following:

1.
For Data Source, open the dropdown menu and choose DocumentDB.

2.
Choose Add connection.

3.
In the dialog box, specify your Amazon DocumentDB credentials:

a.
Enter a Connection name. This is a name used by Canvas to identify this connection.

b.
For Cluster, select the cluster in Amazon DocumentDB that stores your data. Canvas
automatically populates the dropdown menu with Amazon DocumentDB clusters in the
same VPC as your Canvas application.

c.
Enter the Username for your Amazon DocumentDB cluster.

d.
Enter the Password for your Amazon DocumentDB cluster.

e.
Enter the name of the Database to which you want to connect.

f.
The Read preference option determines which types of instances on your cluster Canvas
reads the data from. Select one of the following:

• Secondary preferred – Canvas defaults to reading from the cluster’s secondary
instances, but if a secondary instance isn’t available, then Canvas reads from a primary
instance.

• Secondary – Canvas only reads from the cluster’s secondary instances, which prevents
the read operations from interfering with the cluster’s regular read and write
operations.

g.
Choose Add connection. The following image shows the dialog box with the preceding
ﬁelds for an Amazon DocumentDB connection.

Data import
1083

## Page 113

Amazon SageMaker AI
Developer Guide

![Page 113 Diagram 1](images/page-0113-img-01.png)

You should now have an Amazon DocumentDB connection, and you can use your Amazon
DocumentDB data in Canvas to create either a dataset or a data ﬂow.

Connect to an Amazon Redshift database

You can import data from Amazon Redshift, a data warehouse where your organization keeps its
data. Before you can import data from Amazon Redshift, the AWS IAM role you use must have the

AmazonRedshiftFullAccess managed policy attached. For instructions on how to attach this
policy, see Grant Users Permissions to Import Amazon Redshift Data.

To import data from Amazon Redshift, you do the following:

1. Create a connection to an Amazon Redshift database.

2. Choose the data that you're importing.

3. Import the data.

Data import
1084

## Page 114

Amazon SageMaker AI
Developer Guide

You can use the Amazon Redshift editor to drag datasets onto the import pane and import them
into SageMaker Canvas. For more control over the values returned in the dataset, you can use the
following:

• SQL queries

• Joins

With SQL queries, you can customize how you import the values in the dataset. For example, you
can specify the columns returned in the dataset or the range of values for a column.

You can use joins to combine multiple datasets from Amazon Redshift into a single dataset. You
can drag your datasets from Amazon Redshift into the panel that gives you the ability to join the
datasets.

You can use the SQL editor to edit the dataset that you've joined and convert the joined dataset
into a single node. You can join another dataset to the node. You can import the data that you've
selected into SageMaker Canvas.

Use the following procedure to import data from Amazon Redshift.

1.
In the SageMaker Canvas application, go to the Datasets page.

2.
Choose Import data, and from the dropdown menu, choose Tabular.

3.
Enter a name for the dataset and choose Create.

4.
For Data Source, open the dropdown menu and choose Redshift.

5.
Choose Add connection.

6.
In the dialog box, specify your Amazon Redshift credentials:

a.
For Authentication method, choose IAM.

b.
Enter the Cluster identiﬁer to specify to which cluster you want to connect. Enter only the
cluster identiﬁer and not the full endpoint of the Amazon Redshift cluster.

c.
Enter the Database name of the database to which you want to connect.

d.
Enter a Database user to identify the user you want to use to connect to the database.

e.
For ARN, enter the IAM role ARN of the role that the Amazon Redshift cluster should
assume to move and write data to Amazon S3. For more information about this role, see
Authorizing Amazon Redshift to access other AWS services on your behalf in the Amazon
Redshift Management Guide.

Data import
1085

## Page 115

Amazon SageMaker AI
Developer Guide

f.
Enter a Connection name. This is a name used by Canvas to identify this connection.

7.
From the tab that has the name of your connection, drag the .csv ﬁle that you're importing to
the Drag and drop table to import pane.

8.
Optional: Drag additional tables to the import pane. You can use the GUI to join the tables. For
more speciﬁcity in your joins, choose Edit in SQL.

9.
Optional: If you're using SQL to query the data, you can choose Context to add context to the
connection by specifying values for the following:

• Warehouse

• Database

• Schema

10. Choose Import data.

The following image shows an example of ﬁelds speciﬁed for an Amazon Redshift connection.

![Page 115 Diagram 1](images/page-0115-img-01.png)

The following image shows the page used to join datasets in Amazon Redshift.

Data import
1086

## Page 116

Amazon SageMaker AI
Developer Guide

![Page 116 Diagram 1](images/page-0116-img-01.png)

The following image shows an SQL query being used to edit a join in Amazon Redshift.

![Page 116 Diagram 2](images/page-0116-img-02.png)

Data import
1087

## Page 117

Amazon SageMaker AI
Developer Guide

Connect to your data with JDBC connectors

With JDBC, you can connect to your databases from sources such as Databricks, SQLServer, MySQL,
PostgreSQL, MariaDB, Amazon RDS, and Amazon Aurora.

You must make sure that you have the necessary credentials and permissions to create the
connection from Canvas.

• For Databricks, you must provide a JDBC URL. The URL formatting can vary between
Databricks instances. For information about ﬁnding the URL and the specifying
the parameters within it, see JDBC conﬁguration and connection parameters in the
Databricks documentation. The following is an example of how a URL can be formatted:

jdbc:spark://aws-sagemaker-datawrangler.cloud.databricks.com:443/

default;transportMode=http;ssl=1;httpPath=sql/protocolv1/

o/3122619508517275/0909-200301-cut318;AuthMech=3;UID=token;PWD=personal-

access-token

• For other database sources, you must set up username and password authentication, and then
specify those credentials when connecting to the database from Canvas.

Additionally, your data source must either be accessible through the public internet, or if your
Canvas application is running in VPC only mode, then the data source must run in the same VPC.
For more information about conﬁguring an Amazon RDS database in a VPC, see Amazon VPC VPCs
and Amazon RDS in the Amazon RDS User Guide.

After you’ve conﬁgured your data source credentials, you can sign in to the Canvas application and
create a connection to the data source. Specify your credentials (or, for Databricks, the URL) when
creating the connection.

Connect to data sources with OAuth

Canvas supports using OAuth as an authentication method for connecting to your data in
Snowﬂake and Salesforce Data Cloud. OAuth is a common authentication platform for granting
access to resources without sharing passwords.

Note

You can only establish one OAuth connection for each data source.

Data import
1088

## Page 118

Amazon SageMaker AI
Developer Guide

To authorize the connection, you must following the initial setup described in Set up connections
to data sources with OAuth.

After setting up the OAuth credentials, you can do the following to add a Snowﬂake or Salesforce
Data Cloud connection with OAuth:

1.
Sign in to the Canvas application.

2.
Create a tabular dataset. When prompted to upload data, choose Snowﬂake or Salesforce Data
Cloud as your data source.

3.
Create a new connection to your Snowﬂake or Salesforce Data Cloud data source. Specify
OAuth as the authentication method and enter your connection details.

You should now be able to import data from your databases in Snowﬂake or Salesforce Data Cloud.

Connect to a SaaS platform

You can import data from Snowﬂake and over 40 other external SaaS platforms. For a full list of
the connectors, see the table on Data import.

Note

You can only import tabular data, such as data tables, from SaaS platforms.

Use Snowﬂake with Canvas

Snowﬂake is a data storage and analytics service, and you can import your data from Snowﬂake
into SageMaker Canvas. For more information about Snowﬂake, see the Snowﬂake documentation.

You can import data from your Snowﬂake account by doing the following:

1. Create a connection to the Snowﬂake database.

2. Choose the data that you're importing by dragging and dropping the table from the left

navigation menu into the editor.

3. Import the data.

You can use the Snowﬂake editor to drag datasets onto the import pane and import them into
SageMaker Canvas. For more control over the values returned in the dataset, you can use the
following:

Data import
1089

## Page 119

Amazon SageMaker AI
Developer Guide

• SQL queries

• Joins

With SQL queries, you can customize how you import the values in the dataset. For example, you
can specify the columns returned in the dataset or the range of values for a column.

You can join multiple Snowﬂake datasets into a single dataset before you import into Canvas using
SQL or the Canvas interface. You can drag your datasets from Snowﬂake into the panel that gives
you the ability to join the datasets, or you can edit the joins in SQL and convert the SQL into a
single node. You can join other nodes to the node that you've converted. You can then combine the
datasets that you've joined into a single node and join the nodes to a diﬀerent Snowﬂake dataset.
Finally, you can import the data that you've selected into Canvas.

Use the following procedure to import data from Snowﬂake to Amazon SageMaker Canvas.

1.
In the SageMaker Canvas application, go to the Datasets page.

2.
Choose Import data, and from the dropdown menu, choose Tabular.

3.
Enter a name for the dataset and choose Create.

4.
For Data Source, open the dropdown menu and choose Snowﬂake.

5.
Choose Add connection.

6.
In the Add a new Snowﬂake connection dialog box, specify your Snowﬂake credentials. For
the Authentication method, choose one of the following:

• Basic - username password – Provide your Snowﬂake account ID, username, and password.

• ARN – For improved protection of your Snowﬂake credentials, provide the ARN of an AWS
Secrets Manager secret that contains your credentials. For more information, see  Create an
AWS Secrets Manager secret in the AWS Secrets Manager User Guide.

Your secret should have your Snowﬂake credentials stored in the following JSON format:

{"accountid": "ID",
"username": "username",
"password": "password"}

• OAuth – OAuth lets you authenticate without providing a password but requires additional
setup. For more information about setting up OAuth credentials for Snowﬂake, see Set up
connections to data sources with OAuth.

Data import
1090

## Page 120

Amazon SageMaker AI
Developer Guide

7.
Choose Add connection.

8.
From the tab that has the name of your connection, drag the .csv ﬁle that you're importing to
the Drag and drop table to import pane.

9.
Optional: Drag additional tables to the import pane. You can use the user interface to join the
tables. For more speciﬁcity in your joins, choose Edit in SQL.

10. Optional: If you're using SQL to query the data, you can choose Context to add context to the

connection by specifying values for the following:

• Warehouse

• Database

• Schema

Adding context to a connection makes it easier to specify future queries.

11. Choose Import data.

The following image shows an example of ﬁelds speciﬁed for a Snowﬂake connection.

Data import
1091

## Page 121

Amazon SageMaker AI
Developer Guide

![Page 121 Diagram 1](images/page-0121-img-01.png)

The following image shows the page used to add context to a connection.

Data import
1092

## Page 122

Amazon SageMaker AI
Developer Guide

![Page 122 Diagram 1](images/page-0122-img-01.png)

The following image shows the page used to join datasets in Snowﬂake.

Data import
1093

## Page 123

Amazon SageMaker AI
Developer Guide

![Page 123 Diagram 1](images/page-0123-img-01.png)

The following image shows a SQL query being used to edit a join in Snowﬂake.

Data import
1094

## Page 124

Amazon SageMaker AI
Developer Guide

![Page 124 Diagram 1](images/page-0124-img-01.png)

Use SaaS connectors with Canvas

Note

For SaaS platforms besides Snowﬂake, you can only have one connection per data source.

Before you can import data from a SaaS platform, your administrator must authenticate and create
a connection to the data source. For more information about how administrators can create a
connection with a SaaS platform, see Managing Amazon AppFlow connections in the Amazon
AppFlow User Guide.

If you’re an administrator getting started with Amazon AppFlow for the ﬁrst time, see Getting
started in the Amazon AppFlow User Guide.

Data import
1095

## Page 125

Amazon SageMaker AI
Developer Guide

To import data from a SaaS platform, you can follow the standard Import tabular data procedure,
which shows you how to import tabular datasets into Canvas.

Sample datasets in Canvas

SageMaker Canvas provides sample datasets addressing unique use cases so you can start building,
training, and validating models quickly without writing any code. The use cases associated with
these datasets highlight the capabilities of SageMaker Canvas, and you can leverage these datasets
to get started with building models. You can ﬁnd the sample datasets in the Datasets page of your
SageMaker Canvas application.

The following datasets are the samples that SageMaker Canvas provides by default. These datasets
cover use cases such as predicting house prices, loan defaults, and readmission for diabetic
patients; forecasting sales; predicting machine failures to streamline predictive maintenance in
manufacturing units; and generating supply chain predictions for transportation and logistics.

The datasets are stored in the sample_dataset folder in the default Amazon S3 bucket that
SageMaker AI creates for your account in a Region.

• canvas-sample-diabetic-readmission.csv: This dataset contains historical data including
over ﬁfteen features with patient and hospital outcomes. You can use this dataset to predict
whether high-risk diabetic patients are likely to get readmitted to the hospital within 30 days of
discharge, after 30 days, or not at all. Use the redadmitted column as the target column, and
use the 3+ category prediction model type with this dataset. To learn more about how to build
a model with this dataset, see the SageMaker Canvas workshop page. This dataset was obtained
from the UCI Machine Learning Repository.

• canvas-sample-housing.csv: This dataset contains data on the characteristics tied to a given
housing price. You can use this dataset to predict housing prices. Use the median_house_value
column as the target column, and use the numeric prediction model type with this dataset. To
learn more about building a model with this dataset, see the SageMaker Canvas workshop page.
This is the California housing dataset obtained from the StatLib repository.

• canvas-sample-loans.csv: This dataset contains complete loan data for all loans issued from
2007–2011, including the current loan status and latest payment information. You can use this
dataset to predict whether a customer will repay a loan. Use the loan_status column as the
target column, and use the 3+ category prediction model type with this dataset. To learn more
about how to build a model with this dataset, see the SageMaker Canvas workshop page. This
data uses the LendingClub data obtained from Kaggle.

Data import
1096

## Page 126

Amazon SageMaker AI
Developer Guide

• canvas-sample-maintenance.csv: This dataset contains data on the characteristics tied to a
given maintenance failure type. You can use this dataset to predict which failure will occur in the
future. Use the Failure Type column as the target column, and use the 3+ category prediction
model type with this dataset. To learn more about how to build a model with this dataset,
see the SageMaker Canvas workshop page. This dataset was obtained from the UCI Machine
Learning Repository.

• canvas-sample-shipping-logs.csv: This dataset contains complete shipping data for all products
delivered, including estimated time shipping priority, carrier, and origin. You can use this
dataset to predict the estimated time of arrival of the shipment in number of days. Use the
ActualShippingDays column as the target column, and use the numeric prediction model type
with this dataset. To learn more about how to build a model with this data, see the SageMaker
Canvas workshop page. This is a synthetic dataset created by Amazon.

• canvas-sample-sales-forecasting.csv: This dataset contains historical time series sales data
for retail stores. You can use this dataset to forecast sales for a particular retail store. Use the
sales column as the target column, and use the time series forecasting model type with this
dataset. To learn more about how to build a model with this dataset, see the SageMaker Canvas
workshop page. This is a synthetic dataset created by Amazon.

Re-import a deleted sample dataset

Amazon SageMaker Canvas provides you with sample datasets for various use cases that highlight
the capabilities of Canvas. To learn more about the sample datasets that are available, see Sample
datasets in Canvas. If you no longer wish to use the sample datasets, you can delete them from the
Datasets page of your SageMaker Canvas application. However, these datasets are still stored in
the Amazon S3 bucket that you speciﬁed as the Canvas storage location, so you can always access
them later.

If you used the default Amazon S3 bucket, the bucket name follows the pattern

sagemaker-{region}-{account ID}. You can ﬁnd the sample datasets in the directory path

Canvas/sample_dataset.

If you delete a sample dataset from your SageMaker Canvas application and want to access the
sample dataset again, use the following procedure.

1.
Navigate to the Datasets page in your SageMaker Canvas application.

2.
Choose Import data.

Data import
1097

## Page 127

Amazon SageMaker AI
Developer Guide

3.
From the list of Amazon S3 buckets, select the bucket that is your Canvas storage location.
If using the default SageMaker AI-created Amazon S3 bucket, it follows the naming pattern

sagemaker-{region}-{account ID}.

4.
Select the Canvas folder.

5.
Select the sample_dataset folder, which contains all of the sample datasets for SageMaker
Canvas.

6.
Select the dataset you want to import, and then choose Import data.

Data preparation

Note

Previously, Amazon SageMaker Data Wrangler was part of the SageMaker Studio Classic
experience. Now, if you update to using the new Studio experience, you must use
SageMaker Canvas to access Data Wrangler and receive the latest feature updates. If you
have been using Data Wrangler in Studio Classic until now and want to migrate to Data
Wrangler in Canvas, you might have to grant additional permissions so that you can create
and use a Canvas application. For more information, see (Optional) Migrate from Data
Wrangler in Studio Classic to SageMaker Canvas.
To learn how to migrate your data ﬂows from Data Wrangler in Studio Classic, see
(Optional) Migrate data from Studio Classic to Studio.

Use Amazon SageMaker Data Wrangler in Amazon SageMaker Canvas to prepare, featurize and
analyze your data. You can integrate a Data Wrangler data preparation ﬂow into your machine
learning (ML) workﬂows to simplify and streamline data pre-processing and feature engineering
using little to no coding. You can also add your own Python scripts and transformations to
customize workﬂows.

• Data Flow – Create a data ﬂow to deﬁne a series of ML data prep steps. You can use a ﬂow to
combine datasets from diﬀerent data sources, identify the number and types of transformations
you want to apply to datasets, and deﬁne a data prep workﬂow that can be integrated into an
ML pipeline.

• Transform – Clean and transform your dataset using standard transforms like string, vector, and
numeric data formatting tools. Featurize your data using transforms like text and date/time
embedding and categorical encoding.

Data preparation
1098

## Page 128

Amazon SageMaker AI
Developer Guide

• Generate Data Insights – Automatically verify data quality and detect abnormalities in your data
with Data Wrangler Data Quality and Insights Report.

• Analyze – Analyze features in your dataset at any point in your ﬂow. Data Wrangler includes
built-in data visualization tools like scatter plots and histograms, as well as data analysis tools
like target leakage analysis and quick modeling to understand feature correlation.

• Export – Export your data preparation workﬂow to a diﬀerent location. The following are
example locations:

• Amazon Simple Storage Service (Amazon S3) bucket

• Amazon SageMaker Feature Store – Store the features and their data in a centralized store.

• Automate data preparation – Create machine learning workﬂows from your data ﬂow.

• Amazon SageMaker Pipelines – Build workﬂows that manage your SageMaker AI data
preparation, model training, and model deployment jobs.

• Serial inference pipeline – Create a serial inference pipeline from your data ﬂow. Use it to make
predictions on new data.

• Python script – Store the data and their transformations in a Python script for your custom
workﬂows.

Create a data ﬂow

Use a Data Wrangler ﬂow in SageMaker Canvas, or data ﬂow, to create and modify a data
preparation pipeline. We recommend that you use Data Wrangler for datasets larger than 5 GB.

To get started, use the following procedure to import your data into a data ﬂow.

1.
Open SageMaker Canvas.

2.
In the left-hand navigation, choose Data Wrangler.

3.
Choose Import and prepare.

4.
From the dropdown menu, choose either Tabular or Image.

5.
For Select a data source, choose your data source and select the data that you want to
import. You have the option to select up to 30 ﬁles or one folder. If you have a dataset already
imported into Canvas, choose Canvas dataset as your source. Otherwise, connect to a data
source such as Amazon S3 or Snowﬂake and browse through your data. For information about
connecting to a data source or importing data, see the following pages:

• Data import

Data preparation
1099

## Page 129

Amazon SageMaker AI
Developer Guide

• Connect to data sources

6.
After selecting the data that you want to import, choose Next.

7.
(Optional) For the Import settings section when importing a tabular dataset, expand the
Advanced dropdown menu. You can specify the following advanced settings for data ﬂow
imports:

• Sampling method – Select the sampling method and sample size you'd like to use. For more
information about how to change your sample, see the section Edit the data ﬂow sampling
conﬁguration.

• File encoding (CSV) – Select your dataset ﬁle’s encoding. UTF-8 is the default.

• Skip ﬁrst rows – Enter the number of rows you’d like to skip importing if you have
redundant rows at the beginning of your dataset.

• Delimiter – Select the delimiter that separates each item in your data. You can also specify a
custom delimiter.

• Multi-line detection – Select this option if you’d like Canvas to manually parse your entire
dataset for multi-line cells. Canvas determines whether or not to use multi-line support by
taking a sample of your data, but Canvas might not detect any multi-line cells in the sample.
In this case, we recommend that you select the Multi-line detection option to force Canvas
to check your entire dataset for multi-line cells.

8.
Choose Import.

You should now have a new data ﬂow, and you can begin adding transform steps and analyses.

How the data ﬂow UI works

To help you navigate your data ﬂow, Data Wrangler has the following tabs in the top navigation
pane:

• Data ﬂow – This tab provides you with a visual view of your data ﬂow step where you can add or
remove transforms, and export data.

• Data – This tab gives you a preview of your data so that you can check the results of your
transforms. You can also see an ordered list of your data ﬂow steps and edit or reorder the steps.

Data preparation
1100

## Page 130

Amazon SageMaker AI
Developer Guide

Note

In this tab, you can only preview data visualizations (such as the distribution of values
per column) for Amazon S3 data sources. Visualizations for other data sources, such as
Amazon Athena, aren't supported.

• Analyses – In this tab, you can see separate sub-tabs for each analysis you create. For example,
if you create a histogram and a Data Quality and Insights (DQI) report, Canvas creates a tab for
each.

When you import a dataset, the original dataset appears on the data ﬂow and is named Source.
SageMaker Canvas automatically infers the types of each column in your dataset and creates a new
dataframe named Data types. You can select this frame to update the inferred data types.

The datasets, transformations, and analyses that you use in the data ﬂow are represented as steps.
Each time you add a transform step, you create a new dataframe. When multiple transform steps
(other than Join or Concatenate) are added to the same dataset, they are stacked.

Under the Combine data option, Join and Concatenate create standalone steps that contain the
new joined or concatenated dataset.

Edit the data ﬂow sampling conﬁguration

When importing tabular data into a Data Wrangler data ﬂow, you can opt to take a sample of your
dataset to speed up the data exploration and cleaning process. Running exploratory transforms on
a sample of your dataset is often faster than running transforms on your entire dataset, and when
you're ready to export your dataset and build a model, you can apply the transforms to the full
dataset.

Canvas supports the following sampling methods:

• FirstK – Canvas selects the ﬁrst K items from your dataset, where K is a number you specify. This
sampling method is simple but can introduce bias if your dataset isn't randomly ordered.

• Random – Canvas selects items from the dataset at random, with each item having an
equal probability of being chosen. This sampling method helps ensure that the sample is
representative of the entire dataset.

• Stratiﬁed – Canvas divides the dataset into groups (or strata) based on one or more attributes
(for example, age and income level). Then, a proportional number of items are randomly selected

Data preparation
1101

## Page 131

Amazon SageMaker AI
Developer Guide

from each group. This method ensures that all relevant subgroups are adequately represented in
the sample.

You can edit your sampling conﬁguration at any time to change the size of the sample used for
data exploration.

To make changes to your sampling conﬁguration, do the following:

1.
In your data ﬂow graph, select your data source node.

2.
Choose Sampling on the bottom navigation bar.

3.
The Sampling dialog box opens. For the Sampling method dropdown, select your desired
sampling method.

4.
For Maximum sample size, enter the number of rows you want to sample.

5.
Choose Update to save your changes.

The changes to your sampling conﬁguration should now be applied.

Add a step to your data ﬂow

In your Data Wrangler data ﬂows, you can add steps that represent data transformations and
analyses.

To add a step to your data ﬂow, select + next to any dataset node or previously added step. Then,
select one of the following options:

• Edit data types (For a Data types step only): If you have not added any transforms to a Data
types step, you can double-click on the Data types step in your ﬂow to open the Data tab and
edit the data types that Data Wrangler inferred when importing your dataset.

• Add transform: Adds a new transform step. See Transform data to learn more about the data
transformations you can add.

• Get data insights: Add analyses, such as histograms or custom visualizations. You can use this
option to analyze your data at any point in the data ﬂow. See Perform exploratory data analysis
(EDA) to learn more about the analyses you can add.

• Join: Find this option under Combine data to join two datasets and add the resulting dataset to
the data ﬂow. To learn more, see Join Datasets.

• Concatenate: Find this option under Combine data to concatenate two datasets and add the
resulting dataset to the data ﬂow. To learn more, see Concatenate Datasets.

Data preparation
1102

## Page 132

Amazon SageMaker AI
Developer Guide

Edit data ﬂow steps

In Amazon SageMaker Canvas, you can edit individual steps in your data ﬂows to transform your
dataset without having to create a new data ﬂow. The following page covers how to edit join and
concatenate steps, as well as data source steps.

Edit join and concatenate steps

Within your data ﬂows, you have the ﬂexibility to edit your join and concatenate steps. You can
make necessary adjustments to your data processing workﬂow, ensuring that your data is properly
combined and transformed without having to redo your entire data ﬂow.

To edit a join or concatenate step in your data ﬂow, do the following:

1.
Open your data ﬂow.

2.
Choose the plus icon (+) next to the join or concatenate node that you want to edit.

3.
From the context menu, choose Edit.

4.
A side panel opens where you can edit the details of your join or concatenation. Modify your
step ﬁelds, such as the type of join. To swap out a data node and select a diﬀerent one to join
or concatenate, choose the delete icon next to the node and then, in the data ﬂow view, select
the new node that you want to include in your transformation.

Note

When swapping out a node during the editing process, you can only select steps that
occur before the join or concatenate operation. You can swap either the left or right
node, but you can only swap one node at a time. Additionally, you cannot select a
source node as a replacement.

5.
Choose Preview to view the result of the combining operation.

6.
Choose Update to save your changes.

Your data ﬂow should now be updated.

Edit or replace a data source step

You might need to make changes to your data source or dataset without deleting the transforms
and data ﬂow steps applied to your original data. Within Data Wrangler, you can edit or replace

Data preparation
1103

## Page 133

Amazon SageMaker AI
Developer Guide

your data source conﬁguration while keeping the steps of your data ﬂow. When editing a data
source, you can change the import settings, such as the sampling size or method and any advanced
settings. You can also add more ﬁles with the same schema, or for query-based data sources such
as Amazon Athena, you can edit the query. When replacing a data source, you have the option to
select a diﬀerent dataset, or even import the data from a diﬀerent data source altogether, as long
as the schema of the new data matches the original data.

To edit a data source conﬁguration, do the following:

1.
In the Canvas application, go to the Data Wrangler page.

2.
Choose your data ﬂow to view it.

3.
In the Data ﬂow tab that shows your data ﬂow steps, ﬁnd the Source node that you want to
edit.

4.
Choose the ellipsis icon next to the Source node.

5.
From the context menu, choose Edit.

6.
For Amazon S3 data sources and local upload, you have the option to select or upload more
ﬁles with the same schema as your original data. For query-based data sources such as
Amazon Athena, you can remove and select diﬀerent tables in the visual query builder, or you
can edit the SQL query directly. When you're done, choose Next.

7.
For the Import settings, make any desired changes.

8.
When you're done, choose Save changes.

Your data source should now be updated.

To replace a data source, do the following:

1.
In the Canvas application, go to the Data Wrangler page.

2.
Choose your data ﬂow to view it.

3.
In the Data ﬂow tab that shows your data ﬂow steps, ﬁnd the Source node that you want to
edit.

4.
Choose the ellipsis icon next to the Source node.

5.
From the context menu, choose Replace.

6.
Go through the create a data ﬂow experience to select another data source and data.

7.
When you’ve selected your data and are ready to update the source node, choose Save.

Data preparation
1104

## Page 134

Amazon SageMaker AI
Developer Guide

You should now see the Source node updated in your data ﬂow.

Reorder steps in your data ﬂow

After adding steps to your data ﬂow, you have the option to reorder steps instead of deleting and
re-adding them in the correct order. For example, you might decide to move a transform to impute
missing values before a step to format strings.

Note

You can’t change the order of certain step types, such as deﬁning your data source,
changing data types, joining, concatenating, or splitting. Steps that can’t be reordered are
grayed out in the Canvas application UI.

To reorder your data ﬂow steps, do the following:

1.
While editing a data ﬂow in Data Wrangler, choose the Data tab. A side panel called Steps lists
your data ﬂow steps in order.

2.
Hover over a transform step and choose the More options icon
(

)
next to that step.

3.
From the context menu, choose Reorder.

4.
Drag and drop your data ﬂow steps into your desired order.

5.
When you’ve ﬁnished, choose Save.

Your data ﬂow steps and graph should now reﬂect the changes you’ve made.

Delete a step from your data ﬂow

Within your data ﬂows, you have the ﬂexibility to delete your join and concatenate steps and
choose whether or not to still apply any subsequent transforms to your data.

To delete a join or concatenate step from your data ﬂow, do the following:

1.
Open your data ﬂow.

2.
Choose the plus icon (+) next to the join or concatenate node that you want to delete.

Data preparation
1105

## Page 135

Amazon SageMaker AI
Developer Guide

3.
In the context menu, choose Delete.

4.
(Optional) If you have transformation steps following the join or concatenate step, then
you can choose whether or not to keep the subsequent transformation steps and add them
separately to each data node. In the Delete join side panel, choose a node to deselect it and
remove any subsequent transformation steps. You can leave both nodes selected to keep all
transformation steps, or you can deselect both nodes to discard all transformation steps.

The following screenshot shows this step with only the second of two data nodes selected.
When the join is successfully deleted, then the subsequent Rename column transform is only
kept by the second data node.

![Page 135 Diagram 1](images/page-0135-img-01.png)

5.
Choose Delete.

The join or concatenate step should now be removed from your data ﬂow.

Perform exploratory data analysis (EDA)

Data Wrangler includes built-in analyses that help you generate visualizations and data analyses in
a few clicks. You can also create custom analyses using your own code.

You add an analysis to a dataframe by selecting a step in your data ﬂow, and then choosing Add
analysis. To access an analysis you've created, select the step that contains the analysis, and select
the analysis.

Data preparation
1106

## Page 136

Amazon SageMaker AI
Developer Guide

Analyses are generated using a sample of up to 200,000 rows of your dataset, and you can
conﬁgure the sample size. For more information about changing the sample size of your data ﬂow,
see Edit the data ﬂow sampling conﬁguration.

Note

Analyses are optimized for data with 1000 or fewer columns. You may experience some
latency when generating analyses for data with additional columns.

You can add the following analysis to a dataframe:

• Data visualizations, including histograms and scatter plots.

• A quick summary of your dataset, including number of entries, minimum and maximum values
(for numeric data), and most and least frequent categories (for categorical data).

• A quick model of the dataset, which can be used to generate an importance score for each
feature.

• A target leakage report, which you can use to determine if one or more features are strongly
correlated with your target feature.

• A custom visualization using your own code.

Use the following sections to learn more about these options.

Get insights on data and data quality

Use the Data Quality and Insights Report to perform an analysis of the data that you've imported
into Data Wrangler. We recommend that you create the report after you import your dataset. You
can use the report to help you clean and process your data. It gives you information such as the
number of missing values and the number of outliers. If you have issues with your data, such as
target leakage or imbalance, the insights report can bring those issues to your attention.

Use the following procedure to create a Data Quality and Insights report. It assumes that you've
already imported a dataset into your Data Wrangler ﬂow.

To create a Data Quality and Insights report

1.
Choose the ellipsis icon next to a node in your Data Wrangler ﬂow.

Data preparation
1107

## Page 137

Amazon SageMaker AI
Developer Guide

2.
Select Get data insights.

3.
For Analysis type, select Data Quality and Insights Report.

4.
For Analysis name, specify a name for the insights report.

5.
For Problem type, specify Regression or Classiﬁcation.

6.
For Target column, specify the target column.

7.
For Data size, specify one of the following:

• Sampled dataset – Uses the interactive sample from your data ﬂow, which can contain up
to 200,000 rows of your dataset. For information about how to edit the size of your sample,
see Edit the data ﬂow sampling conﬁguration.

• Full dataset – Uses the full dataset from your data source to create the report.

Note

Creating a Data Quality and Insights report on the full dataset uses an Amazon
SageMaker processing job. A SageMaker Processing job provisions the additional
compute resources required to get insights for all of your data. For more information
about SageMaker Processing jobs, see Data transformation workloads with SageMaker
Processing.

8.
Choose Create.

The following topics show the sections of the report:

Topics

• Summary

• Target column

• Quick model

• Feature summary

• Samples

• Deﬁnitions

You can either download the report or view it online. To download the report, choose the
download button at the top right corner of the screen.

Data preparation
1108

## Page 138

Amazon SageMaker AI
Developer Guide

Summary

The insights report has a brief summary of the data that includes general information such as
missing values, invalid values, feature types, outlier counts, and more. It can also include high
severity warnings that point to probable issues with the data. We recommend that you investigate
the warnings.

Target column

When you create the Data Quality and Insights Report, Data Wrangler gives you the option to
select a target column. A target column is a column that you're trying to predict. When you choose
a target column, Data Wrangler automatically creates a target column analysis. It also ranks the
features in the order of their predictive power. When you select a target column, you must specify
whether you’re trying to solve a regression or a classiﬁcation problem.

For classiﬁcation, Data Wrangler shows a table and a histogram of the most common classes. A
class is a category. It also presents observations, or rows, with a missing or invalid target value.

For regression, Data Wrangler shows a histogram of all the values in the target column. It also
presents observations, or rows, with a missing, invalid, or outlier target value.

Quick model

The Quick model provides an estimate of the expected predicted quality of a model that you train
on your data.

Data Wrangler splits your data into training and validation folds. It uses 80% of the samples for
training and 20% of the values for validation. For classiﬁcation, the sample is stratiﬁed split. For
a stratiﬁed split, each data partition has the same ratio of labels. For classiﬁcation problems, it's
important to have the same ratio of labels between the training and classiﬁcation folds. Data
Wrangler trains the XGBoost model with the default hyperparameters. It applies early stopping on
the validation data and performs minimal feature preprocessing.

For classiﬁcation models, Data Wrangler returns both a model summary and a confusion matrix.

To learn more about the information that the classiﬁcation model summary returns, see
Deﬁnitions.

A confusion matrix gives you the following information:

• The number of times the predicted label matches the true label.

Data preparation
1109

## Page 139

Amazon SageMaker AI
Developer Guide

• The number of times the predicted label doesn't match the true label.

The true label represents an actual observation in your data. For example, if you're using a model
to detect fraudulent transactions, the true label represents a transaction that is actually fraudulent
or non-fraudulent. The predicted label represents the label that your model assigns to the data.

You can use the confusion matrix to see how well the model predicts the presence or the absence
of a condition. If you're predicting fraudulent transactions, you can use the confusion matrix to
get a sense of both the sensitivity and the speciﬁcity of the model. The sensitivity refers to the
model's ability to detect fraudulent transactions. The speciﬁcity refers to the model's ability to
avoid detecting non-fraudulent transactions as fraudulent.

Feature summary

When you specify a target column, Data Wrangler orders the features by their prediction power.
Prediction power is measured on the data after it is split into 80% training and 20% validation
folds. Data Wrangler ﬁts a model for each feature separately on the training fold. It applies
minimal feature preprocessing and measures prediction performance on the validation data.

It normalizes the scores to the range [0,1]. Higher prediction scores indicate columns that are more
useful for predicting the target on their own. Lower scores point to columns that aren’t predictive
of the target column.

It’s uncommon for a column that isn’t predictive on its own to be predictive when it’s used in
tandem with other columns. You can conﬁdently use the prediction scores to determine whether a
feature in your dataset is predictive.

A low score usually indicates the feature is redundant. A score of 1 implies perfect predictive
abilities, which often indicates target leakage. Target leakage usually happens when the dataset
contains a column that isn’t available at the prediction time. For example, it could be a duplicate of
the target column.

Samples

Data Wrangler provides information about whether your samples are anomalous or if there are
duplicates in your dataset.

Data Wrangler detects anomalous samples using the isolation forest algorithm. The isolation forest
associates an anomaly score with each sample (row) of the dataset. Low anomaly scores indicate

Data preparation
1110

## Page 140

Amazon SageMaker AI
Developer Guide

anomalous samples. High scores are associated with non-anomalous samples. Samples with a
negative anomaly score are usually considered anomalous and samples with positive anomaly score
are considered non-anomalous.

When you look at a sample that might be anomalous, we recommend that you pay attention to
unusual values. For example, you might have anomalous values that result from errors in gathering
and processing the data. The following is an example of the most anomalous samples according
to the Data Wrangler’s implementation of the isolation forest algorithm. We recommend using
domain knowledge and business logic when you examine the anomalous samples.

Data Wrangler detects duplicate rows and calculates the ratio of duplicate rows in your data. Some
data sources could include valid duplicates. Other data sources could have duplicates that point
to problems in data collection. Duplicate samples that result from faulty data collection could
interfere with machine learning processes that rely on splitting the data into independent training
and validation folds.

The following are elements of the insights report that can be impacted by duplicated samples:

• Quick model

• Prediction power estimation

• Automatic hyperparameter tuning

You can remove duplicate samples from the dataset using the Drop duplicates transform under
Manage rows. Data Wrangler shows you the most frequently duplicated rows.

Deﬁnitions

The following are deﬁnitions for the technical terms that are used in the data insights report.

Feature types

The following are the deﬁnitions for each of the feature types:

• Numeric – Numeric values can be either ﬂoats or integers, such as age or income. The
machine learning models assume that numeric values are ordered and a distance is deﬁned
over them. For example, 3 is closer to 4 than to 10 and 3 < 4 < 10.

• Categorical – The column entries belong to a set of unique values, which is usually much
smaller than the number of entries in the column. For example, a column of length 100

could contain the unique values Dog, Cat, and Mouse. The values could be numeric, text, or a

Data preparation
1111

## Page 141

Amazon SageMaker AI
Developer Guide

combination of both. Horse, House, 8, Love, and 3.1 would all be valid values and could be
found in the same categorical column. The machine learning model does not assume order or
distance on the values of categorical features, as opposed to numeric features, even when all
the values are numbers.

• Binary – Binary features are a special categorical feature type in which the cardinality of the
set of unique values is 2.

• Text – A text column contains many non-numeric unique values. In extreme cases, all the
elements of the column are unique. In an extreme case, no two entries are the same.

• Datetime – A datetime column contains information about the date or time. It can have
information about both the date and time.

Feature statistics

The following are deﬁnitions for each of the feature statistics:

• Prediction power – Prediction power measures how useful the column is in predicting the
target.

• Outliers (in numeric columns) – Data Wrangler detects outliers using two statistics that are
robust to outliers: median and robust standard deviation (RSTD). RSTD is derived by clipping
the feature values to the range [5 percentile, 95 percentile] and calculating the standard
deviation of the clipped vector. All values larger than median + 5 * RSTD or smaller than
median - 5 * RSTD are considered to be outliers.

• Skew (in numeric columns) – Skew measures the symmetry of the distribution and is deﬁned
as the third moment of the distribution divided by the third power of the standard deviation.
The skewness of the normal distribution or any other symmetric distribution is zero. Positive
values imply that the right tail of the distribution is longer than the left tail. Negative values
imply that the left tail of the distribution is longer than the right tail. As a rule of thumb, a
distribution is considered skewed when the absolute value of the skew is larger than 3.

• Kurtosis (in numeric columns) – Pearson's kurtosis measures the heaviness of the tail of the
distribution. It's deﬁned as the fourth moment of the distribution divided by the square of
the second moment. The kurtosis of the normal distribution is 3. Kurtosis values lower than 3
imply that the distribution is concentrated around the mean and the tails are lighter than the
tails of the normal distribution. Kurtosis values higher than 3 imply heavier tails or outliers.

• Missing values – Null-like objects, empty strings and strings composed of only white spaces
are considered missing.

Data preparation
1112

## Page 142

Amazon SageMaker AI
Developer Guide

• Valid values for numeric features or regression target – All values that you can cast to ﬁnite
ﬂoats are valid. Missing values are not valid.

• Valid values for categorical, binary, or text features, or for classiﬁcation target – All values
that are not missing are valid.

• Datetime features – All values that you can cast to a datetime object are valid. Missing values
are not valid.

• Invalid values – Values that are either missing or you can't properly cast. For example, in a

numeric column, you can't cast the string "six" or a null value.

Quick model metrics for regression

The following are the deﬁnitions for the quick model metrics:

• R2 or coeﬃcient of determination) – R2 is the proportion of the variation in the target that
is predicted by the model. R2 is in the range of [-infty, 1]. 1 is the score of the model that
predicts the target perfectly and 0 is the score of the trivial model that always predicts the
target mean.

• MSE or mean squared error – MSE is in the range [0, infty]. 0 is the score of the model that
predicts the target perfectly.

• MAE or mean absolute error – MAE is in the range [0, infty] where 0 is the score of the model
that predicts the target perfectly.

• RMSE or root mean square error – RMSE is in the range [0, infty] where 0 is the score of the
model that predicts the target perfectly.

• Max error – The maximum absolute value of the error over the dataset. Max error is in the
range [0, infty]. 0 is the score of the model that predicts the target perfectly.

• Median absolute error – Median absolute error is in the range [0, infty]. 0 is the score of the
model that predicts the target perfectly.

Quick model metrics for classiﬁcation

The following are the deﬁnitions for the quick model metrics:

• Accuracy – Accuracy is the ratio of samples that are predicted accurately. Accuracy is in the
range [0, 1]. 0 is the score of the model that predicts all samples incorrectly and 1 is the score
of the perfect model.

Data preparation
1113

## Page 143

Amazon SageMaker AI
Developer Guide

• Balanced accuracy – Balanced accuracy is the ratio of samples that are predicted accurately
when the class weights are adjusted to balance the data. All classes are given the same
importance, regardless of their frequency. Balanced accuracy is in the range [0, 1]. 0 is the
score of the model that predicts all samples wrong. 1 is the score of the perfect model.

• AUC (binary classiﬁcation) – This is the area under the receiver operating characteristic
curve. AUC is in the range [0, 1] where a random model returns a score of 0.5 and the perfect
model returns a score of 1.

• AUC (OVR) – For multiclass classiﬁcation, this is the area under the receiver operating
characteristic curve calculated separately for each label using one versus rest. Data Wrangler
reports the average of the areas. AUC is in the range [0, 1] where a random model returns a
score of 0.5 and the perfect model returns a score of 1.

• Precision – Precision is deﬁned for a speciﬁc class. Precision is the fraction of true positives
out of all the instances that the model classiﬁed as that class. Precision is in the range [0, 1].
1 is the score of the model that has no false positives for the class. For binary classiﬁcation,
Data Wrangler reports the precision of the positive class.

• Recall – Recall is deﬁned for a speciﬁc class. Recall is the fraction of the relevant class
instances that are successfully retrieved. Recall is in the range [0, 1]. 1 is the score of the
model that classiﬁes all the instances of the class correctly. For binary classiﬁcation, Data
Wrangler reports the recall of the positive class.

• F1 – F1 is deﬁned for a speciﬁc class. It's the harmonic mean of the precision and recall. F1 is
in the range [0, 1]. 1 is the score of the perfect model. For binary classiﬁcation, Data Wrangler
reports the F1 for classes with positive values.

Textual patterns

Patterns describe the textual format of a string using an easy to read format. The following are
examples of textual patterns:

• "{digits:4-7}" describes a sequence of digits that have a length between 4 and 7.

• "{alnum:5}" describes an alpha-numeric string with a length of exactly 5.

Data Wrangler infers the patterns by looking at samples of non-empty strings from your data.
It can describe many of the commonly used patterns. The conﬁdence expressed as a percentage
indicates how much of the data is estimated to match the pattern. Using the textual pattern,
you can see which rows in your data you need to correct or drop.

Data preparation
1114

## Page 144

Amazon SageMaker AI
Developer Guide

The following describes the patterns that Data Wrangler can recognize:

Pattern
Textual Format

{alnum}
Alphanumeric strings

{any}
Any string of word characters

{digits}
A sequence of digits

{lower}
A lowercase word

{mixed}
A mixed-case word

{name}
A word beginning with a capital letter

{upper}
An uppercase word

{whitespace}
Whitespace characters

A word character is either an underscore or a character that might appear in a word in any

language. For example, the strings 'Hello_word' and 'écoute' both consist of word
characters. 'H' and 'é' are both examples of word characters.

Bias report

SageMaker Canvas provides the bias report in Data Wrangler to help uncover potential biases
in your data. The bias report analyzes the relationship between the target column (label) and
a column that you believe might contain bias (facet variable). For example, if you are trying to
predict customer conversion, the facet variable may be the age of the customer. The bias report
can help you determine whether or not your data is biased toward a certain age group.

To generate a bias report in Canvas, do the following:

1.
In your data ﬂow in Data Wrangler, choose the More options icon
(

)
next to a node in the ﬂow.

2.
From the context menu, choose Get data insights.

Data preparation
1115

## Page 145

Amazon SageMaker AI
Developer Guide

3.
The Create analysis side panel opens. For the Analysis type dropdown menu, select Bias
Report.

4.
In the Analysis name ﬁeld, enter a name for the bias report.

5.
For the Select the column your model predicts (target) dropdown menu, select your target
column.

6.
For Is your predicted column a value or threshold?, select Value if your target column has
categorical values or Threshold if it has numerical values.

7.
For Predicted value (or Predicted threshold, depending on your selection in the previous
step), enter the target column value or values that correspond to a positive outcome. For

example, if predicting customer conversion, your value might be yes to indicate that a
customer was converted.

8.
For the Select the column to analyze for bias dropdown menu, select the column that you
believe might contain bias, also known as the facet variable.

9.
For Is your column a value or threshold?, select Value if the facet variable has categorical
values or Threshold if it has numerical values.

10. For Column value(s) to analyze for bias (or Column threshold to analyze for bias, depending

on your selection in the previous step), enter the value or values that you want to analyze for
potential bias. For example, if you're checking for bias against customers over a certain age,
use the beginning of that age range as your threshold.

11. For Choose bias metrics, select the bias metrics you'd like to include in your bias report. Hover

over the info icons for more information about each metric.

12. (Optional) When prompted with the option Would you like to analyze additional metrics?,

select Yes to view and include more bias metrics.

13. When you're ready to create the bias report, choose Add.

Once generated, the report gives you an overview of the bias metrics you selected. You can view
the bias report at any time from the Analyses tab of your data ﬂow.

Histogram

Use histograms to see the counts of feature values for a speciﬁc feature. You can inspect the
relationships between features using the Color by option.

You can use the Facet by feature to create histograms of one column, for each value in another
column.

Data preparation
1116

## Page 146

Amazon SageMaker AI
Developer Guide

Scatter plot

Use the Scatter Plot feature to inspect the relationship between features. To create a scatter plot,
select a feature to plot on the X axis and the Y axis. Both of these columns must be numeric typed
columns.

You can color scatter plots by an additional column.

Additionally, you can facet scatter plots by features.

Table summary

Use the Table Summary analysis to quickly summarize your data.

For columns with numerical data, including log and ﬂoat data, a table summary reports the
number of entries (count), minimum (min), maximum (max), mean, and standard deviation (stddev)
for each column.

For columns with non-numerical data, including columns with string, Boolean, or date/time data,
a table summary reports the number of entries (count), least frequent value (min), and most
frequent value (max).

Quick model

Use the Quick Model visualization to quickly evaluate your data and produce importance scores
for each feature. A feature importance score score indicates how useful a feature is at predicting
a target label. The feature importance score is between [0, 1] and a higher number indicates that
the feature is more important to the whole dataset. On the top of the quick model chart, there is a
model score. A classiﬁcation problem shows an F1 score. A regression problem has a mean squared
error (MSE) score.

When you create a quick model chart, you select a dataset you want evaluated, and a target label
against which you want feature importance to be compared. Data Wrangler does the following:

• Infers the data types for the target label and each feature in the dataset selected.

• Determines the problem type. Based on the number of distinct values in the label column, Data
Wrangler determines if this is a regression or classiﬁcation problem type. Data Wrangler sets
a categorical threshold to 100. If there are more than 100 distinct values in the label column,
Data Wrangler classiﬁes it as a regression problem; otherwise, it is classiﬁed as a classiﬁcation
problem.

Data preparation
1117

## Page 147

Amazon SageMaker AI
Developer Guide

• Pre-processes features and label data for training. The algorithm used requires encoding features
to vector type and encoding labels to double type.

• Trains a random forest algorithm with 70% of data. Spark’s RandomForestRegressor is used to
train a model for regression problems. The RandomForestClassiﬁer is used to train a model for
classiﬁcation problems.

• Evaluates a random forest model with the remaining 30% of data. Data Wrangler evaluates
classiﬁcation models using an F1 score and evaluates regression models using an MSE score.

• Calculates feature importance for each feature using the Gini importance method.

Target leakage

Target leakage occurs when there is data in a machine learning training dataset that is strongly
correlated with the target label, but is not available in real-world data. For example, you may
have a column in your dataset that serves as a proxy for the column you want to predict with your
model.

When you use the Target Leakage analysis, you specify the following:

• Target: This is the feature about which you want your ML model to be able to make predictions.

• Problem type: This is the ML problem type on which you are working. Problem type can either
be classiﬁcation or regression.

• (Optional) Max features: This is the maximum number of features to present in the visualization,
which shows features ranked by their risk of being target leakage.

For classiﬁcation, the target leakage analysis uses the area under the receiver operating
characteristic, or AUC - ROC curve for each column, up to Max features. For regression, it uses a
coeﬃcient of determination, or R2 metric.

The AUC - ROC curve provides a predictive metric, computed individually for each column using
cross-validation, on a sample of up to around 1000 rows. A score of 1 indicates perfect predictive
abilities, which often indicates target leakage. A score of 0.5 or lower indicates that the information
on the column could not provide, on its own, any useful information towards predicting the target.
Although it can happen that a column is uninformative on its own but is useful in predicting
the target when used in tandem with other features, a low score could indicate the feature is
redundant.

Data preparation
1118

## Page 148

Amazon SageMaker AI
Developer Guide

Multicollinearity

Multicollinearity is a circumstance where two or more predictor variables are related to each
other. The predictor variables are the features in your dataset that you're using to predict a target
variable. When you have multicollinearity, the predictor variables are not only predictive of the
target variable, but also predictive of each other.

You can use the Variance Inﬂation Factor (VIF), Principal Component Analysis (PCA), or Lasso
feature selection as measures for the multicollinearity in your data. For more information, see the
following.

Variance Inﬂation Factor (VIF)

The Variance Inﬂation Factor (VIF) is a measure of collinearity among variable pairs. Data
Wrangler returns a VIF score as a measure of how closely the variables are related to each other.
A VIF score is a positive number that is greater than or equal to 1.

A score of 1 means that the variable is uncorrelated with the other variables. Scores greater
than 1 indicate higher correlation.

Theoretically, you can have a VIF score with a value of inﬁnity. Data Wrangler clips high scores
to 50. If you have a VIF score greater than 50, Data Wrangler sets the score to 50.

You can use the following guidelines to interpret your VIF scores:

• A VIF score less than or equal to 5 indicates that the variables are moderately correlated with
the other variables.

• A VIF score greater than or equal to 5 indicates that the variables are highly correlated with
the other variables.

Principle Component Analysis (PCA)

Principal Component Analysis (PCA) measures the variance of the data along diﬀerent
directions in the feature space. The feature space consists of all the predictor variables that you
use to predict the target variable in your dataset.

For example, if you're trying to predict who survived on the RMS Titanic after it hit an iceberg,
your feature space can include the passengers' age, gender, and the fare that they paid.

Data preparation
1119

## Page 149

Amazon SageMaker AI
Developer Guide

From the feature space, PCA generates an ordered list of variances. These variances are also
known as singular values. The values in the list of variances are greater than or equal to 0. We
can use them to determine how much multicollinearity there is in our data.

When the numbers are roughly uniform, the data has very few instances of multicollinearity.

When there is a lot of variability among the values, we have many instances of multicollinearity.
Before it performs PCA, Data Wrangler normalizes each feature to have a mean of 0 and a
standard deviation of 1.

Note

PCA in this circumstance can also be referred to as Singular Value Decomposition (SVD).

Lasso feature selection

Lasso feature selection uses the L1 regularization technique to only include the most predictive
features in your dataset.

For both classiﬁcation and regression, the regularization technique generates a coeﬃcient for
each feature. The absolute value of the coeﬃcient provides an importance score for the feature.
A higher importance score indicates that it is more predictive of the target variable. A common
feature selection method is to use all the features that have a non-zero lasso coeﬃcient.

Detect anomalies in time series data

You can use the anomaly detection visualization to see outliers in your time series data. To
understand what determines an anomaly, you need to understand that we decompose the time
series into a predicted term and an error term. We treat the seasonality and trend of the time series
as the predicted term. We treat the residuals as the error term.

For the error term, you specify a threshold as the number of standard of deviations the residual
can be away from the mean for it to be considered an anomaly. For example, you can specify a
threshold as being 3 standard deviations. Any residual greater than 3 standard deviations away
from the mean is an anomaly.

You can use the following procedure to perform an Anomaly detection analysis.

1.
Open your Data Wrangler data ﬂow.

2.
In your data ﬂow, under Data types, choose the +, and select Add analysis.

Data preparation
1120

## Page 150

Amazon SageMaker AI
Developer Guide

3.
For Analysis type, choose Time Series.

4.
For Visualization, choose Anomaly detection.

5.
For Anomaly threshold, choose the threshold that a value is considered an anomaly.

6.
Choose Preview to generate a preview of the analysis.

7.
Choose Add to add the transform to the Data Wrangler data ﬂow.

Seasonal trend decomposition in time series data

You can determine whether there's seasonality in your time series data by using the Seasonal Trend
Decomposition visualization. We use the STL (Seasonal Trend decomposition using LOESS) method
to perform the decomposition. We decompose the time series into its seasonal, trend, and residual
components. The trend reﬂects the long term progression of the series. The seasonal component is
a signal that recurs in a time period. After removing the trend and the seasonal components from

the time series, you have the residual.

You can use the following procedure to perform a Seasonal-Trend decomposition analysis.

1.
Open your Data Wrangler data ﬂow.

2.
In your data ﬂow, under Data types, choose the +, and select Add analysis.

3.
For Analysis type, choose Time Series.

4.
For Visualization, choose Seasonal-Trend decomposition.

5.
For Anomaly threshold, choose the threshold that a value is considered an anomaly.

6.
Choose Preview to generate a preview of the analysis.

7.
Choose Add to add the transform to the Data Wrangler data ﬂow.

Create custom visualizations

You can add an analysis to your Data Wrangler ﬂow to create a custom visualization. Your dataset,
with all the transformations you've applied, is available as a Pandas DataFrame. Data Wrangler uses

the df variable to store the dataframe. You access the dataframe by calling the variable.

You must provide the output variable, chart, to store an Altair output chart. For example, you can
use the following code block to create a custom histogram using the Titanic dataset.

import altair as alt
df = df.iloc[:30]

Data preparation
1121

## Page 151

Amazon SageMaker AI
Developer Guide

df = df.rename(columns={"Age": "value"})
df = df.assign(count=df.groupby('value').value.transform('count'))
df = df[["value", "count"]]
base = alt.Chart(df)
bar = base.mark_bar().encode(x=alt.X('value', bin=True, axis=None), y=alt.Y('count'))
rule = base.mark_rule(color='red').encode(
x='mean(value):Q',
size=alt.value(5))
chart = bar + rule

To create a custom visualization:

1.
Next to the node containing the transformation that you'd like to visualize, choose the +.

2.
Choose Add analysis.

3.
For Analysis type, choose Custom Visualization.

4.
For Analysis name, specify a name.

5.
Enter your code in the code box.

6.
Choose Preview to preview your visualization.

7.
Choose Save to add your visualization.

If you don’t know how to use the Altair visualization package in Python, you can use custom code
snippets to help you get started.

Data Wrangler has a searchable collection of visualization snippets. To use a visualization snippet,
choose Search example snippets and specify a query in the search bar.

The following example uses the Binned scatterplot code snippet. It plots a histogram for 2
dimensions.

The snippets have comments to help you understand the changes that you need to make to the
code. You usually need to specify the column names of your dataset in the code.

import altair as alt

# Specify the number of top rows for plotting
rows_number = 1000
df = df.head(rows_number)
# You can also choose bottom rows or randomly sampled rows

Data preparation
1122

## Page 152

Amazon SageMaker AI
Developer Guide

# df = df.tail(rows_number)
# df = df.sample(rows_number)

chart = (
alt.Chart(df)
.mark_circle()
.encode(
# Specify the column names for binning and number of bins for X and Y axis
x=alt.X("col1:Q", bin=alt.Bin(maxbins=20)),
y=alt.Y("col2:Q", bin=alt.Bin(maxbins=20)),
size="count()",
)
)

# :Q specifies that label column has quantitative type.
# For more details on Altair typing refer to

# https://altair-viz.github.io/user_guide/encoding.html#encoding-data-types

Transform data

Amazon SageMaker Data Wrangler provides numerous ML data transforms to streamline cleaning
and featurizing your data. Using the interactive data preparation tools in Data Wrangler, you can
sample datasets of any size with a variety of sampling techniques and start exploring your data in
a matter of minutes. After ﬁnalizing your data transforms on the sampled data, you can then scale
the data ﬂow to apply those transformations to the entire dataset.

When you add a transform, it adds a step to the data ﬂow. Each transform you add modiﬁes
your dataset and produces a new dataframe. All subsequent transforms apply to the resulting
dataframe.

Data Wrangler includes built-in transforms, which you can use to transform columns without any
code. If you know how you want to prepare your data but don't know how to get started or which
transforms to use, you can use the chat for data prep feature to interact conversationally with Data
Wrangler and apply transforms using natural language. For more information, see Chat for data
prep.

You can also add custom transformations using PySpark, Python (User-Deﬁned Function), pandas,
and PySpark SQL. Some transforms operate in place, while others create a new output column in
your dataset.

Data preparation
1123

## Page 153

Amazon SageMaker AI
Developer Guide

You can apply transforms to multiple columns at once. For example, you can delete multiple
columns in a single step.

You can apply the Process numeric and Handle missing transforms only to a single column.

Use this page to learn more about the built-in and custom transforms oﬀered by Data Wrangler.

Join Datasets

You can join datasets directly in your data ﬂow. When you join two datasets, the resulting joined
dataset appears in your ﬂow. The following join types are supported by Data Wrangler.

• Left outer – Include all rows from the left table. If the value for the column joined on a left table
row does not match any right table row values, that row contains null values for all right table
columns in the joined table.

• Left anti – Include rows from the left table that do not contain values in the right table for the
joined column.

• Left semi – Include a single row from the left table for all identical rows that satisfy the criteria
in the join statement. This excludes duplicate rows from the left table that match the criteria of
the join.

• Right outer – Include all rows from the right table. If the value for the joined column in a right
table row does not match any left table row values, that row contains null values for all left table
columns in the joined table.

• Inner – Include rows from left and right tables that contain matching values in the joined
column.

• Full outer – Include all rows from the left and right tables. If the row value for the joined column
in either table does not match, separate rows are created in the joined table. If a row doesn’t
contain a value for a column in the joined table, null is inserted for that column.

• Cartesian cross – Include rows which combine each row from the ﬁrst table with each row from
the second table. This is a Cartesian product of rows from tables in the join. The result of this
product is the size of the left table times the size of the right table. Therefore, we recommend
caution in using this join between very large datasets.

Use the following procedure to join two datasets. You should have already imported two data
sources into your data ﬂow.

Data preparation
1124

## Page 154

Amazon SageMaker AI
Developer Guide

1.
Select the More options icon
(

)
next to the left node that you want to join. The ﬁrst node you select is always the left table in
your join.

2.
Hover over Combine data, and then choose Join.

3.
Select the right node. The second node you select is always the right table in your join.

4.
The Join type ﬁeld is set to Inner join by default. Select the dropdown menu to change the
join type.

5.
For Join keys, verify the columns from the left and right tables that you want to use to join the
data. You can add or remove additional join keys.

6.
For Name of join, enter a name for the joined data, or use the default name.

7.
(Optional) Choose Preview to preview the joined data.

8.
Choose Add to complete the join.

Note

If you receive a notice that Canvas didn't identify any matching rows when joining your
data, we recommend that you either verify that you've selected the correct columns, or
update your sample to try to ﬁnd matching rows. You can choose a diﬀerent sampling
strategy or change the size of the sample. For information about how to edit the sample,
see Edit the data ﬂow sampling conﬁguration.

You should now see a join node added to your data ﬂow.

Concatenate Datasets

Concatenating combines two datasets by appending the rows from one dataset to another.

Use the following procedure to concatenate two datasets. You should have already imported two
data sources into your data ﬂow.

To concatenate two datasets:

1.
Select the More options icon
(

)

Data preparation
1125

## Page 155

Amazon SageMaker AI
Developer Guide

next to the left node that you want to concatenate. The ﬁrst node you select is always the left
table in your concatenate operation.

2.
Hover over Combine data, and then choose Concatenate.

3.
Select the right node. The second node you select is always the right table in your concatenate.

4.
(Optional) Select the checkbox next to Remove duplicates after concatenation to remove
duplicate columns.

5.
(Optional) Select the checkbox next to Add column to indicate source dataframe to add a
column to the resulting dataframe that lists the source dataset for each record.

a.
For Indicator column name, enter a name for the added column.

b.
For First dataset indicating string, enter the value you want to use to mark records from
the ﬁrst dataset (or the left node).

c.
For Second dataset indicating string, enter the value you want to use to mark records
from the second dataset (or the right node).

6.
For Name of concatenate, enter a name for the concatenation.

7.
(Optional) Choose Preview to preview the concatenated data.

8.
Choose Add to add the new dataset to your data ﬂow.

You should now see a concatenate node added to your data ﬂow.

Balance Data

You can balance the data for datasets with an underrepresented category. Balancing a dataset can
help you create better models for binary classiﬁcation.

Note

You can't balance datasets containing column vectors.

You can use the Balance data operation to balance your data using one of the following operators:

• Random oversampling – Randomly duplicates samples in the minority category. For example,
if you're trying to detect fraud, you might only have cases of fraud in 10% of your data. For an
equal proportion of fraudulent and non-fraudulent cases, this operator randomly duplicates
fraud cases within the dataset 8 times.

Data preparation
1126

## Page 156

Amazon SageMaker AI
Developer Guide

• Random undersampling – Roughly equivalent to random oversampling. Randomly removes
samples from the overrepresented category to get the proportion of samples that you desire.

• Synthetic Minority Oversampling Technique (SMOTE) – Uses samples from the underrepresented
category to interpolate new synthetic minority samples. For more information about SMOTE, see

the following description.

You can use all transforms for datasets containing both numeric and non-numeric features. SMOTE
interpolates values by using neighboring samples. Data Wrangler uses the R-squared distance
to determine the neighborhood to interpolate the additional samples. Data Wrangler only uses
numeric features to calculate the distances between samples in the underrepresented group.

For two real samples in the underrepresented group, Data Wrangler interpolates the numeric
features by using a weighted average. It randomly assigns weights to those samples in the range of
[0, 1]. For numeric features, Data Wrangler interpolates samples using a weighted average of the
samples. For samples A and B, Data Wrangler could randomly assign a weight of 0.7 to A and 0.3 to
B. The interpolated sample has a value of 0.7A + 0.3B.

Data Wrangler interpolates non-numeric features by copying from either of the interpolated real
samples. It copies the samples with a probability that it randomly assigns to each sample. For
samples A and B, it can assign probabilities 0.8 to A and 0.2 to B. For the probabilities it assigned, it
copies A 80% of the time.

Custom Transforms

The Custom Transforms group allows you to use Python (User-Deﬁned Function), PySpark, pandas,

or PySpark (SQL) to deﬁne custom transformations. For all three options, you use the variable df
to access the dataframe to which you want to apply the transform. To apply your custom code

to your dataframe, assign the dataframe with the transformations that you've made to the df
variable. If you're not using Python (User-Deﬁned Function), you don't need to include a return
statement. Choose Preview to preview the result of the custom transform. Choose Add to add the
custom transform to your list of Previous steps.

You can import the popular libraries with an import statement in the custom transform code
block, such as the following:

• NumPy version 1.19.0

• scikit-learn version 0.23.2

• SciPy version 1.5.4

Data preparation
1127

## Page 157

Amazon SageMaker AI
Developer Guide

• pandas version 1.0.3

• PySpark version 3.0.0

Important

Custom transform doesn't support columns with spaces or special characters in the name.

We recommend that you specify column names that only have alphanumeric characters
and underscores. You can use the Rename column transform in the Manage columns
transform group to remove spaces from a column's name. You can also add a Python
(Pandas) Custom transform similar to the following to remove spaces from multiple

columns in a single step. This example changes columns named A column and B column

to A_column and B_column respectively.

df.rename(columns={"A column": "A_column", "B column": "B_column"})

If you include print statements in the code block, the result appears when you select Preview. You
can resize the custom code transformer panel. Resizing the panel provides more space to write
code.

The following sections provide additional context and examples for writing custom transform code.

Python (User-Deﬁned Function)

The Python function gives you the ability to write custom transformations without needing to
know Apache Spark or pandas. Data Wrangler is optimized to run your custom code quickly. You
get similar performance using custom Python code and an Apache Spark plugin.

To use the Python (User-Deﬁned Function) code block, you specify the following:

• Input column – The input column where you're applying the transform.

• Mode – The scripting mode, either pandas or Python.

• Return type – The data type of the value that you're returning.

Using the pandas mode gives better performance. The Python mode makes it easier for you to
write transformations by using pure Python functions.

Data preparation
1128

## Page 158

Amazon SageMaker AI
Developer Guide

PySpark

The following example extracts date and time from a timestamp.

from pyspark.sql.functions import from_unixtime, to_date, date_format
df = df.withColumn('DATE_TIME', from_unixtime('TIMESTAMP'))
df = df.withColumn( 'EVENT_DATE', to_date('DATE_TIME')).withColumn(
'EVENT_TIME', date_format('DATE_TIME', 'HH:mm:ss'))

pandas

The following example provides an overview of the dataframe to which you are adding transforms.

df.info()

PySpark (SQL)

The following example creates a new dataframe with four columns: name, fare, pclass, survived.

SELECT name, fare, pclass, survived FROM df

If you don’t know how to use PySpark, you can use custom code snippets to help you get started.

Data Wrangler has a searchable collection of code snippets. You can use to code snippets to
perform tasks such as dropping columns, grouping by columns, or modelling.

To use a code snippet, choose Search example snippets and specify a query in the search bar. The
text you specify in the query doesn’t have to match the name of the code snippet exactly.

The following example shows a Drop duplicate rows code snippet that can delete rows with similar
data in your dataset. You can ﬁnd the code snippet by searching for one of the following:

• Duplicates

• Identical

• Remove

The following snippet has comments to help you understand the changes that you need to make.
For most snippets, you must specify the column names of your dataset in the code.

Data preparation
1129

## Page 159

Amazon SageMaker AI
Developer Guide

# Specify the subset of columns
# all rows having identical values in these columns will be dropped

subset = ["col1", "col2", "col3"]
df = df.dropDuplicates(subset)

# to drop the full-duplicate rows run
# df = df.dropDuplicates()

To use a snippet, copy and paste its content into the Custom transform ﬁeld. You can copy and
paste multiple code snippets into the custom transform ﬁeld.

Custom Formula

Use Custom formula to deﬁne a new column using a Spark SQL expression to query data in the
current dataframe. The query must use the conventions of Spark SQL expressions.

Important

Custom formula doesn't support columns with spaces or special characters in the name.
We recommend that you specify column names that only have alphanumeric characters
and underscores. You can use the Rename column transform in the Manage columns
transform group to remove spaces from a column's name. You can also add a Python
(Pandas) Custom transform similar to the following to remove spaces from multiple

columns in a single step. This example changes columns named A column and B column

to A_column and B_column respectively.

df.rename(columns={"A column": "A_column", "B column": "B_column"})

You can use this transform to perform operations on columns, referencing the columns by name.
For example, assuming the current dataframe contains columns named col_a and col_b, you
can use the following operation to produce an Output column that is the product of these two
columns with the following code:

col_a * col_b

Data preparation
1130

## Page 160

Amazon SageMaker AI
Developer Guide

Other common operations include the following, assuming a dataframe contains col_a and col_b
columns:

• Concatenate two columns: concat(col_a, col_b)

• Add two columns: col_a + col_b

• Subtract two columns: col_a - col_b

• Divide two columns: col_a / col_b

• Take the absolute value of a column: abs(col_a)

For more information, see the Spark documentation on selecting data.

Reduce Dimensionality within a Dataset

Reduce the dimensionality in your data by using Principal Component Analysis (PCA). The
dimensionality of your dataset corresponds to the number of features. When you use
dimensionality reduction in Data Wrangler, you get a new set of features called components. Each
component accounts for some variability in the data.

The ﬁrst component accounts for the largest amount of variation in the data. The second
component accounts for the second largest amount of variation in the data, and so on.

You can use dimensionality reduction to reduce the size of the data sets that you use to train
models. Instead of using the features in your dataset, you can use the principal components
instead.

To perform PCA, Data Wrangler creates axes for your data. An axis is an aﬃne combination of
columns in your dataset. The ﬁrst principal component is the value on the axis that has the largest
amount of variance. The second principal component is the value on the axis that has the second
largest amount of variance. The nth principal component is the value on the axis that has the nth
largest amount of variance.

You can conﬁgure the number of principal components that Data Wrangler returns. You can either
specify the number of principal components directly or you can specify the variance threshold
percentage. Each principal component explains an amount of variance in the data. For example,
you might have a principal component with a value of 0.5. The component would explain 50% of
the variation in the data. When you specify a variance threshold percentage, Data Wrangler returns
the smallest number of components that meet the percentage that you specify.

Data preparation
1131

## Page 161

Amazon SageMaker AI
Developer Guide

The following are example principal components with the amount of variance that they explain in
the data.

• Component 1 – 0.5

• Component 2 – 0.45

• Component 3 – 0.05

If you specify a variance threshold percentage of 94 or 95, Data Wrangler returns Component 1

and Component 2. If you specify a variance threshold percentage of 96, Data Wrangler returns all
three principal components.

You can use the following procedure to run PCA on your dataset.

To run PCA on your dataset, do the following.

1.
Open your Data Wrangler data ﬂow.

2.
Choose the +, and select Add transform.

3.
Choose Add step.

4.
Choose Dimensionality Reduction.

5.
For Input Columns, choose the features that you're reducing into the principal components.

6.
(Optional) For Number of principal components, choose the number of principal components
that Data Wrangler returns in your dataset. If specify a value for the ﬁeld, you can't specify a
value for Variance threshold percentage.

7.
(Optional) For Variance threshold percentage, specify the percentage of variation in the
data that you want explained by the principal components. Data Wrangler uses the default

value of 95 if you don't specify a value for the variance threshold. You can't specify a variance
threshold percentage if you've speciﬁed a value for Number of principal components.

8.
(Optional) Deselect Center to not use the mean of the columns as the center of the data. By
default, Data Wrangler centers the data with the mean before scaling.

9.
(Optional) Deselect Scale to not scale the data with the unit standard deviation.

10. (Optional) Choose Columns to output the components to separate columns. Choose Vector to

output the components as a single vector.

11. (Optional) For Output column, specify a name for an output column. If you're outputting the

components to separate columns, the name that you specify is a preﬁx. If you're outputting
the components to a vector, the name that you specify is the name of the vector column.

Data preparation
1132

## Page 162

Amazon SageMaker AI
Developer Guide

12. (Optional) Select Keep input columns. We don't recommend selecting this option if you plan

on only using the principal components to train your model.

13. Choose Preview.

14. Choose Add.

Encode Categorical

Categorical data is usually composed of a ﬁnite number of categories, where each category
is represented with a string. For example, if you have a table of customer data, a column that
indicates the country a person lives in is categorical. The categories would be Afghanistan, Albania,
Algeria, and so on. Categorical data can be nominal or ordinal. Ordinal categories have an inherent
order, and nominal categories do not. The highest degree obtained (High school, Bachelors,
Masters, and so on) is an example of ordinal categories.

Encoding categorical data is the process of creating a numerical representation for categories. For
example, if your categories are Dog and Cat, you may encode this information into two vectors,

[1,0] to represent Dog, and [0,1] to represent Cat.

When you encode ordinal categories, you may need to translate the natural order of categories
into your encoding. For example, you can represent the highest degree obtained with the following

map: {"High school": 1, "Bachelors": 2, "Masters":3}.

Use categorical encoding to encode categorical data that is in string format into arrays of integers.

The Data Wrangler categorical encoders create encodings for all categories that exist in a column at
the time the step is deﬁned. If new categories have been added to a column when you start a Data
Wrangler job to process your dataset at time t, and this column was the input for a Data Wrangler
categorical encoding transform at time t-1, these new categories are considered missing in the
Data Wrangler job. The option you select for Invalid handling strategy is applied to these missing
values. Examples of when this can occur are:

• When you use a .ﬂow ﬁle to create a Data Wrangler job to process a dataset that was updated
after the creation of the data ﬂow. For example, you may use a data ﬂow to regularly process
sales data each month. If that sales data is updated weekly, new categories may be introduced
into columns for which an encode categorical step is deﬁned.

• When you select Sampling when you import your dataset, some categories may be left out of
the sample.

Data preparation
1133

## Page 163

Amazon SageMaker AI
Developer Guide

In these situations, these new categories are considered missing values in the Data Wrangler job.

You can choose from and conﬁgure an ordinal and a one-hot encode. Use the following sections to
learn more about these options.

Both transforms create a new column named Output column name. You specify the output format
of this column with Output style:

• Select Vector to produce a single column with a sparse vector.

• Select Columns to create a column for every category with an indicator variable for whether the
text in the original column contains a value that is equal to that category.

Ordinal Encode

Select Ordinal encode to encode categories into an integer between 0 and the total number of
categories in the Input column you select.

Invalid handing strategy: Select a method to handle invalid or missing values.

• Choose Skip if you want to omit the rows with missing values.

• Choose Keep to retain missing values as the last category.

• Choose Error if you want Data Wrangler to throw an error if missing values are encountered in
the Input column.

• Choose Replace with NaN to replace missing with NaN. This option is recommended if your ML
algorithm can handle missing values. Otherwise, the ﬁrst three options in this list may produce
better results.

One-Hot Encode

Select One-hot encode for Transform to use one-hot encoding. Conﬁgure this transform using the
following:

• Drop last category: If True, the last category does not have a corresponding index in the one-
hot encoding. When missing values are possible, a missing category is always the last one and

setting this to True means that a missing value results in an all zero vector.

• Invalid handing strategy: Select a method to handle invalid or missing values.

• Choose Skip if you want to omit the rows with missing values.

Data preparation
1134

## Page 164

Amazon SageMaker AI
Developer Guide

• Choose Keep to retain missing values as the last category.

• Choose Error if you want Data Wrangler to throw an error if missing values are encountered in
the Input column.

• Is input ordinal encoded: Select this option if the input vector contains ordinal encoded data.

This option requires that input data contain non-negative integers. If True, input i is encoded as a
vector with a non-zero in the ith location.

Similarity encode

Use similarity encoding when you have the following:

• A large number of categorical variables

• Noisy data

The similarity encoder creates embeddings for columns with categorical data. An embedding is a
mapping of discrete objects, such as words, to vectors of real numbers. It encodes similar strings to
vectors containing similar values. For example, it creates very similar encodings for "California" and
"Calfornia".

Data Wrangler converts each category in your dataset into a set of tokens using a 3-gram
tokenizer. It converts the tokens into an embedding using min-hash encoding.

The similarity encodings that Data Wrangler creates:

• Have low dimensionality

• Are scalable to a large number of categories

• Are robust and resistant to noise

For the preceding reasons, similarity encoding is more versatile than one-hot encoding.

To add the similarity encoding transform to your dataset, use the following procedure.

To use similarity encoding, do the following.

1.
Sign in to the Amazon SageMaker AI Console.

2.
Choose Open Studio Classic.

3.
Choose Launch app.

Data preparation
1135

## Page 165

Amazon SageMaker AI
Developer Guide

4.
Choose Studio.

5.
Specify your data ﬂow.

6.
Choose a step with a transformation.

7.
Choose Add step.

8.
Choose Encode categorical.

9.
Specify the following:

• Transform – Similarity encode

• Input column – The column containing the categorical data that you're encoding.

• Target dimension – (Optional) The dimension of the categorical embedding vector. The
default value is 30. We recommend using a larger target dimension if you have a large
dataset with many categories.

• Output style – Choose Vector for a single vector with all of the encoded values. Choose

Column to have the encoded values in separate columns.

• Output column – (Optional) The name of the output column for a vector encoded output.
For a column-encoded output, this is the preﬁx of the column names followed by listed
number.

Featurize Text

Use the Featurize Text transform group to inspect string-typed columns and use text embedding
to featurize these columns.

This feature group contains two features, Character statistics and Vectorize. Use the following
sections to learn more about these transforms. For both options, the Input column must contain
text data (string type).

Character Statistics

Use Character statistics to generate statistics for each row in a column containing text data.

This transform computes the following ratios and counts for each row, and creates a new column
to report the result. The new column is named using the input column name as a preﬁx and a suﬃx
that is speciﬁc to the ratio or count.

• Number of words: The total number of words in that row. The suﬃx for this output column is -

stats_word_count.

Data preparation
1136

## Page 166

Amazon SageMaker AI
Developer Guide

• Number of characters: The total number of characters in that row. The suﬃx for this output

column is -stats_char_count.

• Ratio of upper: The number of uppercase characters, from A to Z, divided by all characters in the

column. The suﬃx for this output column is -stats_capital_ratio.

• Ratio of lower: The number of lowercase characters, from a to z, divided by all characters in the

column. The suﬃx for this output column is -stats_lower_ratio.

• Ratio of digits: The ratio of digits in a single row over the sum of digits in the input column. The

suﬃx for this output column is -stats_digit_ratio.

• Special characters ratio: The ratio of non-alphanumeric (characters like #$&%:@) characters

to over the sum of all characters in the input column. The suﬃx for this output column is -

stats_special_ratio.

Vectorize

Text embedding involves mapping words or phrases from a vocabulary to vectors of real numbers.
Use the Data Wrangler text embedding transform to tokenize and vectorize text data into term
frequency–inverse document frequency (TF-IDF) vectors.

When TF-IDF is calculated for a column of text data, each word in each sentence is converted to
a real number that represents its semantic importance. Higher numbers are associated with less
frequent words, which tend to be more meaningful.

When you deﬁne a Vectorize transform step, Data Wrangler uses the data in your dataset to deﬁne
the count vectorizer and TF-IDF methods . Running a Data Wrangler job uses these same methods.

You conﬁgure this transform using the following:

• Output column name: This transform creates a new column with the text embedding. Use this
ﬁeld to specify a name for this output column.

• Tokenizer: A tokenizer converts the sentence into a list of words, or tokens.

Choose Standard to use a tokenizer that splits by white space and converts each word to

lowercase. For example, "Good dog" is tokenized to ["good","dog"].

Choose Custom to use a customized tokenizer. If you choose Custom, you can use the following
ﬁelds to conﬁgure the tokenizer:

Data preparation
1137

## Page 167

Amazon SageMaker AI
Developer Guide

• Minimum token length: The minimum length, in characters, for a token to be valid. Defaults

to 1. For example, if you specify 3 for minimum token length, words like a, at, in are
dropped from the tokenized sentence.

• Should regex split on gaps: If selected, regex splits on gaps. Otherwise, it matches tokens.

Defaults to True.

• Regex pattern: Regex pattern that deﬁnes the tokenization process. Defaults to ' \\ s+'.

• To lowercase: If chosen, Data Wrangler converts all characters to lowercase before

tokenization. Defaults to True.

To learn more, see the Spark documentation on Tokenizer.

• Vectorizer: The vectorizer converts the list of tokens into a sparse numeric vector. Each token
corresponds to an index in the vector and a non-zero indicates the existence of the token in the
input sentence. You can choose from two vectorizer options, Count and Hashing.

• Count vectorize allows customizations that ﬁlter infrequent or too common tokens. Count
vectorize parameters include the following:

• Minimum term frequency: In each row, terms (tokens) with smaller frequency are ﬁltered.
If you specify an integer, this is an absolute threshold (inclusive). If you specify a fraction

between 0 (inclusive) and 1, the threshold is relative to the total term count. Defaults to 1.

• Minimum document frequency: Minimum number of rows in which a term (token) must
appear to be included. If you specify an integer, this is an absolute threshold (inclusive). If
you specify a fraction between 0 (inclusive) and 1, the threshold is relative to the total term

count. Defaults to 1.

• Maximum document frequency: Maximum number of documents (rows) in which a term
(token) can appear to be included. If you specify an integer, this is an absolute threshold
(inclusive). If you specify a fraction between 0 (inclusive) and 1, the threshold is relative to

the total term count. Defaults to 0.999.

• Maximum vocabulary size: Maximum size of the vocabulary. The vocabulary is made up of

all terms (tokens) in all rows of the column. Defaults to 262144.

• Binary outputs: If selected, the vector outputs do not include the number of appearances of

a term in a document, but rather are a binary indicator of its appearance. Defaults to False.

To learn more about this option, see the Spark documentation on CountVectorizer.

• Hashing is computationally faster. Hash vectorize parameters includes the following:

Data preparation
1138

## Page 168

Amazon SageMaker AI
Developer Guide

• Number of features during hashing: A hash vectorizer maps tokens to a vector index
according to their hash value. This feature determines the number of possible hash values.
Large values result in fewer collisions between hash values but a higher dimension output
vector.

To learn more about this option, see the Spark documentation on FeatureHasher

• Apply IDF applies an IDF transformation, which multiplies the term frequency with the standard
inverse document frequency used for TF-IDF embedding. IDF parameters include the following:

• Minimum document frequency : Minimum number of documents (rows) in which a
term (token) must appear to be included. If count_vectorize is the chosen vectorizer, we
recommend that you keep the default value and only modify the min_doc_freq ﬁeld in Count

vectorize parameters. Defaults to 5.

• Output format:The output format of each row.

• Select Vector to produce a single column with a sparse vector.

• Select Flattened to create a column for every category with an indicator variable for whether
the text in the original column contains a value that is equal to that category. You can only
choose ﬂattened when Vectorizer is set as Count vectorizer.

Transform Time Series

In Data Wrangler, you can transform time series data. The values in a time series dataset are
indexed to speciﬁc time. For example, a dataset that shows the number of customers in a store for
each hour in a day is a time series dataset. The following table shows an example of a time series
dataset.

Hourly number of customers in a store

Number of customers
Time (hour)

4
09:00

10
10:00

14
11:00

25
12:00

Data preparation
1139

## Page 169

Amazon SageMaker AI
Developer Guide

Number of customers
Time (hour)

20
13:00

18
14:00

For the preceding table, the Number of Customers column contains the time series data. The time
series data is indexed on the hourly data in the Time (hour) column.

You might need to perform a series of transformations on your data to get it in a format that you
can use for your analysis. Use the Time series transform group to transform your time series data.
For more information about the transformations that you can perform, see the following sections.

Topics

• Group by a Time Series

• Resample Time Series Data

• Handle Missing Time Series Data

• Validate the Timestamp of Your Time Series Data

• Standardizing the Length of the Time Series

• Extract Features from Your Time Series Data

• Use Lagged Features from Your Time Series Data

• Create a Datetime Range In Your Time Series

• Use a Rolling Window In Your Time Series

Group by a Time Series

You can use the group by operation to group time series data for speciﬁc values in a column.

For example, you have the following table that tracks the average daily electricity usage in a
household.

Average daily household electricity usage

Data preparation
1140

## Page 170

Amazon SageMaker AI
Developer Guide

Household ID
Daily timestamp
Electricity usage
(kWh)

Number of
household occupants

household_0
1/1/2020
30
2

household_0
1/2/2020
40
2

household_0
1/4/2020
35
3

household_1
1/2/2020
45
3

household_1
1/3/2020
55
4

If you choose to group by ID, you get the following table.

Electricity usage grouped by household ID

Household ID
Electricity usage series
(kWh)

Number of household
occupants series

household_0
[30, 40, 35]
[2, 2, 3]

household_1
[45, 55]
[3, 4]

Each entry in the time series sequence is ordered by the corresponding timestamp. The ﬁrst

element of the sequence corresponds to the ﬁrst timestamp of the series. For household_0, 30 is

the ﬁrst value of the Electricity Usage Series. The value of 30 corresponds to the ﬁrst timestamp

of 1/1/2020.

You can include the starting timestamp and ending timestamp. The following table shows how
that information appears.

Electricity usage grouped by household ID

Data preparation
1141

## Page 171

Amazon SageMaker AI
Developer Guide

Household ID
Electricity
usage series
(kWh)

Number of
household
occupants series

Start_time
End_time

household_0
[30, 40, 35]
[2, 2, 3]
1/1/2020
1/4/2020

household_1
[45, 55]
[3, 4]
1/2/2020
1/3/2020

You can use the following procedure to group by a time series column.

1.
Open your Data Wrangler data ﬂow.

2.
In your data ﬂow, under Data types, choose the +, and select Add transform.

3.
Choose Add step.

4.
Choose Time Series.

5.
Under Transform, choose Group by.

6.
Specify a column in Group by this column.

7.
For Apply to columns, specify a value.

8.
Choose Preview to generate a preview of the transform.

9.
Choose Add to add the transform to the Data Wrangler data ﬂow.

Resample Time Series Data

Time series data usually has observations that aren't taken at regular intervals. For example, a
dataset could have some observations that are recorded hourly and other observations that are
recorded every two hours.

Many analyses, such as forecasting algorithms, require the observations to be taken at regular
intervals. Resampling gives you the ability to establish regular intervals for the observations in
your dataset.

You can either upsample or downsample a time series. Downsampling increases the interval
between observations in the dataset. For example, if you downsample observations that are taken
either every hour or every two hours, each observation in your dataset is taken every two hours.
The hourly observations are aggregated into a single value using an aggregation method such as
the mean or median.

Data preparation
1142

## Page 172

Amazon SageMaker AI
Developer Guide

Upsampling reduces the interval between observations in the dataset. For example, if you
upsample observations that are taken every two hours into hourly observations, you can use an
interpolation method to infer hourly observations from the ones that have been taken every two
hours. For information on interpolation methods, see pandas.DataFrame.interpolate.

You can resample both numeric and non-numeric data.

Use the Resample operation to resample your time series data. If you have multiple time series in
your dataset, Data Wrangler standardizes the time interval for each time series.

The following table shows an example of downsampling time series data by using the mean as the
aggregation method. The data is downsampled from every two hours to every hour.

Hourly temperature readings over a day before downsampling

Timestamp
Temperature (Celsius)

12:00
30

1:00
32

2:00
35

3:00
32

4:00
30

Temperature readings downsampled to every two hours

Timestamp
Temperature (Celsius)

12:00
30

2:00
33.5

4:00
35

You can use the following procedure to resample time series data.

Data preparation
1143

## Page 173

Amazon SageMaker AI
Developer Guide

1.
Open your Data Wrangler data ﬂow.

2.
In your data ﬂow, under Data types, choose the +, and select Add transform.

3.
Choose Add step.

4.
Choose Resample.

5.
For Timestamp, choose the timestamp column.

6.
For Frequency unit, specify the frequency that you're resampling.

7.
(Optional) Specify a value for Frequency quantity.

8.
Conﬁgure the transform by specifying the remaining ﬁelds.

9.
Choose Preview to generate a preview of the transform.

10. Choose Add to add the transform to the Data Wrangler data ﬂow.

Handle Missing Time Series Data

If you have missing values in your dataset, you can do one of the following:

• For datasets that have multiple time series, drop the time series that have missing values that are
greater than a threshold that you specify.

• Impute the missing values in a time series by using other values in the time series.

Imputing a missing value involves replacing the data by either specifying a value or by using an
inferential method. The following are the methods that you can use for imputation:

• Constant value – Replace all the missing data in your dataset with a value that you specify.

• Most common value – Replace all the missing data with the value that has the highest frequency
in the dataset.

• Forward ﬁll – Use a forward ﬁll to replace the missing values with the non-missing value that
precedes the missing values. For the sequence: [2, 4, 7, NaN, NaN, NaN, 8], all of the missing
values are replaced with 7. The sequence that results from using a forward ﬁll is [2, 4, 7, 7, 7, 7,
8].

• Backward ﬁll – Use a backward ﬁll to replace the missing values with the non-missing value that
follows the missing values. For the sequence: [2, 4, 7, NaN, NaN, NaN, 8], all of the missing values
are replaced with 8. The sequence that results from using a backward ﬁll is [2, 4, 7, 8, 8, 8, 8].

• Interpolate – Uses an interpolation function to impute the missing values. For more information
on the functions that you can use for interpolation, see pandas.DataFrame.interpolate.

Data preparation
1144

## Page 174

Amazon SageMaker AI
Developer Guide

Some of the imputation methods might not be able to impute of all the missing value in your
dataset. For example, a Forward ﬁll can't impute a missing value that appears at the beginning of
the time series. You can impute the values by using either a forward ﬁll or a backward ﬁll.

You can either impute missing values within a cell or within a column.

The following example shows how values are imputed within a cell.

Electricity usage with missing values

Household ID
Electricity usage series (kWh)

household_0
[30, 40, 35, NaN, NaN]

household_1
[45, NaN, 55]

Electricity usage with values imputed using a forward ﬁll

Household ID
Electricity usage series (kWh)

household_0
[30, 40, 35, 35, 35]

household_1
[45, 45, 55]

The following example shows how values are imputed within a column.

Average daily household electricity usage with missing values

Household ID
Electricity usage (kWh)

household_0
30

household_0
40

household_0
NaN

household_1
NaN

household_1
NaN

Data preparation
1145

## Page 175

Amazon SageMaker AI
Developer Guide

Average daily household electricity usage with values imputed using a forward ﬁll

Household ID
Electricity usage (kWh)

household_0
30

household_0
40

household_0
40

household_1
40

household_1
40

You can use the following procedure to handle missing values.

1.
Open your Data Wrangler data ﬂow.

2.
In your data ﬂow, under Data types, choose the +, and select Add transform.

3.
Choose Add step.

4.
Choose Handle missing.

5.
For Time series input type, choose whether you want to handle missing values inside of a cell
or along a column.

6.
For Impute missing values for this column, specify the column that has the missing values.

7.
For Method for imputing values, select a method.

8.
Conﬁgure the transform by specifying the remaining ﬁelds.

9.
Choose Preview to generate a preview of the transform.

10. If you have missing values, you can specify a method for imputing them under Method for

imputing values.

11. Choose Add to add the transform to the Data Wrangler data ﬂow.

Validate the Timestamp of Your Time Series Data

You might have time stamp data that is invalid. You can use the Validate time stamp function to
determine whether the timestamps in your dataset are valid. Your timestamp can be invalid for one
or more of the following reasons:

Data preparation
1146

## Page 176

Amazon SageMaker AI
Developer Guide

• Your timestamp column has missing values.

• The values in your timestamp column are not formatted correctly.

If you have invalid timestamps in your dataset, you can't perform your analysis successfully. You
can use Data Wrangler to identify invalid timestamps and understand where you need to clean
your data.

The time series validation works in one of the two ways:

You can conﬁgure Data Wrangler to do one of the following if it encounters missing values in your
dataset:

• Drop the rows that have the missing or invalid values.

• Identify the rows that have the missing or invalid values.

• Throw an error if it ﬁnds any missing or invalid values in your dataset.

You can validate the timestamps on columns that either have the timestamp type or the string

type. If the column has the string type, Data Wrangler converts the type of the column to

timestamp and performs the validation.

You can use the following procedure to validate the timestamps in your dataset.

1.
Open your Data Wrangler data ﬂow.

2.
In your data ﬂow, under Data types, choose the +, and select Add transform.

3.
Choose Add step.

4.
Choose Validate timestamps.

5.
For Timestamp Column, choose the timestamp column.

6.
For Policy, choose whether you want to handle missing timestamps.

7.
(Optional) For Output column, specify a name for the output column.

8.
If the date time column is formatted for the string type, choose Cast to datetime.

9.
Choose Preview to generate a preview of the transform.

10. Choose Add to add the transform to the Data Wrangler data ﬂow.

Data preparation
1147

## Page 177

Amazon SageMaker AI
Developer Guide

Standardizing the Length of the Time Series

If you have time series data stored as arrays, you can standardize each time series to the same
length. Standardizing the length of the time series array might make it easier for you to perform
your analysis on the data.

You can standardize your time series for data transformations that require the length of your data
to be ﬁxed.

Many ML algorithms require you to ﬂatten your time series data before you use them. Flattening
time series data is separating each value of the time series into its own column in a dataset.
The number of columns in a dataset can't change, so the lengths of the time series need to be
standardized between you ﬂatten each array into a set of features.

Each time series is set to the length that you specify as a quantile or percentile of the time series
set. For example, you can have three sequences that have the following lengths:

• 3

• 4

• 5

You can set the length of all of the sequences as the length of the sequence that has the 50th
percentile length.

Time series arrays that are shorter than the length you've speciﬁed have missing values added. The
following is an example format of standardizing the time series to a longer length: [2, 4, 5, NaN,
NaN, NaN].

You can use diﬀerent approaches to handle the missing values. For information on those
approaches, see Handle Missing Time Series Data.

The time series arrays that are longer than the length that you specify are truncated.

You can use the following procedure to standardize the length of the time series.

1.
Open your Data Wrangler data ﬂow.

2.
In your data ﬂow, under Data types, choose the +, and select Add transform.

3.
Choose Add step.

Data preparation
1148

## Page 178

Amazon SageMaker AI
Developer Guide

4.
Choose Standardize length.

5.
For Standardize the time series length for the column, choose a column.

6.
(Optional) For Output column, specify a name for the output column. If you don't specify a
name, the transform is done in place.

7.
If the datetime column is formatted for the string type, choose Cast to datetime.

8.
Choose Cutoﬀ quantile and specify a quantile to set the length of the sequence.

9.
Choose Flatten the output to output the values of the time series into separate columns.

10. Choose Preview to generate a preview of the transform.

11. Choose Add to add the transform to the Data Wrangler data ﬂow.

Extract Features from Your Time Series Data

If you're running a classiﬁcation or a regression algorithm on your time series data, we recommend
extracting features from the time series before running the algorithm. Extracting features might
improve the performance of your algorithm.

Use the following options to choose how you want to extract features from your data:

• Use Minimal subset to specify extracting 8 features that you know are useful in downstream
analyses. You can use a minimal subset when you need to perform computations quickly. You can
also use it when your ML algorithm has a high risk of overﬁtting and you want to provide it with
fewer features.

• Use Eﬃcient subset to specify extracting the most features possible without extracting features
that are computationally intensive in your analyses.

• Use All features to specify extracting all features from the tune series.

• Use Manual subset to choose a list of features that you think explain the variation in your data
well.

Use the following the procedure to extract features from your time series data.

1.
Open your Data Wrangler data ﬂow.

2.
In your data ﬂow, under Data types, choose the +, and select Add transform.

3.
Choose Add step.

4.
Choose Extract features.

Data preparation
1149

## Page 179

Amazon SageMaker AI
Developer Guide

5.
For Extract features for this column, choose a column.

6.
(Optional) Select Flatten to output the features into separate columns.

7.
For Strategy, choose a strategy to extract the features.

8.
Choose Preview to generate a preview of the transform.

9.
Choose Add to add the transform to the Data Wrangler data ﬂow.

Use Lagged Features from Your Time Series Data

For many use cases, the best way to predict the future behavior of your time series is to use its
most recent behavior.

The most common uses of lagged features are the following:

• Collecting a handful of past values. For example, for time, t + 1, you collect t, t - 1, t - 2, and t - 3.

• Collecting values that correspond to seasonal behavior in the data. For example, to predict the
occupancy in a restaurant at 1:00 PM, you might want to use the features from 1:00 PM on the
previous day. Using the features from 12:00 PM or 11:00 AM on the same day might not be as
predictive as using the features from previous days.

1.
Open your Data Wrangler data ﬂow.

2.
In your data ﬂow, under Data types, choose the +, and select Add transform.

3.
Choose Add step.

4.
Choose Lag features.

5.
For Generate lag features for this column, choose a column.

6.
For Timestamp Column, choose the column containing the timestamps.

7.
For Lag, specify the duration of the lag.

8.
(Optional) Conﬁgure the output using one of the following options:

• Include the entire lag window

• Flatten the output

• Drop rows without history

9.
Choose Preview to generate a preview of the transform.

10. Choose Add to add the transform to the Data Wrangler data ﬂow.

Data preparation
1150

## Page 180

Amazon SageMaker AI
Developer Guide

Create a Datetime Range In Your Time Series

You might have time series data that don't have timestamps. If you know that the observations
were taken at regular intervals, you can generate timestamps for the time series in a separate
column. To generate timestamps, you specify the value for the start timestamp and the frequency

of the timestamps.

For example, you might have the following time series data for the number of customers at a

restaurant.

Time series data on the number of customers at a restaurant

Number of customers

10

14

24

40

30

20

If you know that the restaurant opened at 5:00 PM and that the observations are taken hourly, you
can add a timestamp column that corresponds to the time series data. You can see the timestamp
column in the following table.

Time series data on the number of customers at a restaurant

Number of customers
Timestamp

10
1:00 PM

14
2:00 PM

24
3:00 PM

Data preparation
1151

## Page 181

Amazon SageMaker AI
Developer Guide

Number of customers
Timestamp

40
4:00 PM

30
5:00 PM

20
6:00 PM

Use the following procedure to add a datetime range to your data.

1.
Open your Data Wrangler data ﬂow.

2.
In your data ﬂow, under Data types, choose the +, and select Add transform.

3.
Choose Add step.

4.
Choose Datetime range.

5.
For Frequency type, choose the unit used to measure the frequency of the timestamps.

6.
For Starting timestamp, specify the start timestamp.

7.
For Output column, specify a name for the output column.

8.
(Optional) Conﬁgure the output using the remaining ﬁelds.

9.
Choose Preview to generate a preview of the transform.

10. Choose Add to add the transform to the Data Wrangler data ﬂow.

Use a Rolling Window In Your Time Series

You can extract features over a time period. For example, for time, t, and a time window length
of 3, and for the row that indicates the tth timestamp, we append the features that are extracted
from the time series at times t - 3, t -2, and t - 1. For information on extracting features, see
Extract Features from Your Time Series Data.

You can use the following procedure to extract features over a time period.

1.
Open your Data Wrangler data ﬂow.

2.
In your data ﬂow, under Data types, choose the +, and select Add transform.

3.
Choose Add step.

4.
Choose Rolling window features.

5.
For Generate rolling window features for this column, choose a column.

Data preparation
1152

## Page 182

Amazon SageMaker AI
Developer Guide

6.
For Timestamp Column, choose the column containing the timestamps.

7.
(Optional) For Output Column, specify the name of the output column.

8.
For Window size, specify the window size.

9.
For Strategy, choose the extraction strategy.

10. Choose Preview to generate a preview of the transform.

11. Choose Add to add the transform to the Data Wrangler data ﬂow.

Featurize Datetime

Use Featurize date/time to create a vector embedding representing a datetime ﬁeld. To use this
transform, your datetime data must be in one of the following formats:

• Strings describing datetime: For example, "January 1st, 2020, 12:44pm".

• A Unix timestamp: A Unix timestamp describes the number of seconds, milliseconds,
microseconds, or nanoseconds from 1/1/1970.

You can choose to Infer datetime format and provide a Datetime format. If you provide a
datetime format, you must use the codes described in the Python documentation. The options you
select for these two conﬁgurations have implications for the speed of the operation and the ﬁnal
results.

• The most manual and computationally fastest option is to specify a Datetime format and select
No for Infer datetime format.

• To reduce manual labor, you can choose Infer datetime format and not specify a datetime
format. It is also a computationally fast operation; however, the ﬁrst datetime format
encountered in the input column is assumed to be the format for the entire column. If there are
other formats in the column, these values are NaN in the ﬁnal output. Inferring the datetime
format can give you unparsed strings.

• If you don't specify a format and select No for Infer datetime format, you get the most robust
results. All the valid datetime strings are parsed. However, this operation can be an order of
magnitude slower than the ﬁrst two options in this list.

When you use this transform, you specify an Input column which contains datetime data in one of
the formats listed above. The transform creates an output column named Output column name.
The format of the output column depends on your conﬁguration using the following:

Data preparation
1153

## Page 183

Amazon SageMaker AI
Developer Guide

• Vector: Outputs a single column as a vector.

• Columns: Creates a new column for every feature. For example, if the output contains a year,
month, and day, three separate columns are created for year, month, and day.

Additionally, you must choose an Embedding mode. For linear models and deep networks, we
recommend choosing cyclic. For tree-based algorithms, we recommend choosing ordinal.

Format String

The Format string transforms contain standard string formatting operations. For example, you
can use these operations to remove special characters, normalize string lengths, and update string
casing.

This feature group contains the following transforms. All transforms return copies of the strings in
the Input column and add the result to a new, output column.

Name
Function

Left pad
Left-pad the string with a given Fill character
to the given width. If the string is longer than
width, the return value is shortened to width
characters.

Right pad
Right-pad the string with a given Fill
character to the given width. If the string
is longer than width, the return value is

shortened to width characters.

Center (pad on either side)
Center-pad the string (add padding on both
sides of the string) with a given Fill character
to the given width. If the string is longer than
width, the return value is shortened to width
characters.

Prepend zeros
Left-ﬁll a numeric string with zeros, up to
a given width. If the string is longer than
width, the return value is shortened to width
characters.

Data preparation
1154

## Page 184

Amazon SageMaker AI
Developer Guide

Name
Function

Strip left and right
Returns a copy of the string with the leading
and trailing characters removed.

Strip characters from left
Returns a copy of the string with leading
characters removed.

Strip characters from right
Returns a copy of the string with trailing
characters removed.

Lower case
Convert all letters in text to lowercase.

Upper case
Convert all letters in text to uppercase.

Capitalize
Capitalize the ﬁrst letter in each sentence.

Swap case
Converts all uppercase characters to lowercase
and all lowercase characters to uppercase
characters of the given string, and returns it.

Add preﬁx or suﬃx
Adds a preﬁx and a suﬃx the string column.
You must specify at least one of Preﬁx and
Suﬃx.

Remove symbols
Removes given symbols from a string. All
listed characters are removed. Defaults to
white space.

Handle Outliers

Machine learning models are sensitive to the distribution and range of your feature values.
Outliers, or rare values, can negatively impact model accuracy and lead to longer training times.
Use this feature group to detect and update outliers in your dataset.

When you deﬁne a Handle outliers transform step, the statistics used to detect outliers are
generated on the data available in Data Wrangler when deﬁning this step. These same statistics are
used when running a Data Wrangler job.

Data preparation
1155

## Page 185

Amazon SageMaker AI
Developer Guide

Use the following sections to learn more about the transforms this group contains. You specify an
Output name and each of these transforms produces an output column with the resulting data.

Robust standard deviation numeric outliers

This transform detects and ﬁxes outliers in numeric features using statistics that are robust to
outliers.

You must deﬁne an Upper quantile and a Lower quantile for the statistics used to calculate
outliers. You must also specify the number of Standard deviations from which a value must vary
from the mean to be considered an outlier. For example, if you specify 3 for Standard deviations, a
value must fall more than 3 standard deviations from the mean to be considered an outlier.

The Fix method is the method used to handle outliers when they are detected. You can choose
from the following:

• Clip: Use this option to clip the outliers to the corresponding outlier detection bound.

• Remove: Use this option to remove rows with outliers from the dataframe.

• Invalidate: Use this option to replace outliers with invalid values.

Standard Deviation Numeric Outliers

This transform detects and ﬁxes outliers in numeric features using the mean and standard
deviation.

You specify the number of Standard deviations a value must vary from the mean to be considered
an outlier. For example, if you specify 3 for Standard deviations, a value must fall more than 3
standard deviations from the mean to be considered an outlier.

The Fix method is the method used to handle outliers when they are detected. You can choose
from the following:

• Clip: Use this option to clip the outliers to the corresponding outlier detection bound.

• Remove: Use this option to remove rows with outliers from the dataframe.

• Invalidate: Use this option to replace outliers with invalid values.

Data preparation
1156

## Page 186

Amazon SageMaker AI
Developer Guide

Quantile Numeric Outliers

Use this transform to detect and ﬁx outliers in numeric features using quantiles. You can deﬁne an
Upper quantile and a Lower quantile. All values that fall above the upper quantile or below the
lower quantile are considered outliers.

The Fix method is the method used to handle outliers when they are detected. You can choose
from the following:

• Clip: Use this option to clip the outliers to the corresponding outlier detection bound.

• Remove: Use this option to remove rows with outliers from the dataframe.

• Invalidate: Use this option to replace outliers with invalid values.

Min-Max Numeric Outliers

This transform detects and ﬁxes outliers in numeric features using upper and lower thresholds. Use
this method if you know threshold values that demark outliers.

You specify a Upper threshold and a Lower threshold, and if values fall above or below those
thresholds respectively, they are considered outliers.

The Fix method is the method used to handle outliers when they are detected. You can choose
from the following:

• Clip: Use this option to clip the outliers to the corresponding outlier detection bound.

• Remove: Use this option to remove rows with outliers from the dataframe.

• Invalidate: Use this option to replace outliers with invalid values.

Replace Rare

When you use the Replace rare transform, you specify a threshold and Data Wrangler ﬁnds all
values that meet that threshold and replaces them with a string that you specify. For example, you
may want to use this transform to categorize all outliers in a column into an "Others" category.

• Replacement string: The string with which to replace outliers.

• Absolute threshold: A category is rare if the number of instances is less than or equal to this
absolute threshold.

Data preparation
1157

## Page 187

Amazon SageMaker AI
Developer Guide

• Fraction threshold: A category is rare if the number of instances is less than or equal to this
fraction threshold multiplied by the number of rows.

• Max common categories: Maximum not-rare categories that remain after the operation. If the
threshold does not ﬁlter enough categories, those with the top number of appearances are

classiﬁed as not rare. If set to 0 (default), there is no hard limit to the number of categories.

Handle Missing Values

Missing values are a common occurrence in machine learning datasets. In some situations, it is
appropriate to impute missing data with a calculated value, such as an average or categorically
common value. You can process missing values using the Handle missing values transform group.
This group contains the following transforms.

Fill Missing

Use the Fill missing transform to replace missing values with a Fill value you deﬁne.

Impute Missing

Use the Impute missing transform to create a new column that contains imputed values where
missing values were found in input categorical and numerical data. The conﬁguration depends on
your data type.

For numeric data, choose an imputing strategy, the strategy used to determine the new value to
impute. You can choose to impute the mean or the median over the values that are present in your
dataset. Data Wrangler uses the value that it computes to impute the missing values.

For categorical data, Data Wrangler imputes missing values using the most frequent value in the
column. To impute a custom string, use the Fill missing transform instead.

Add Indicator for Missing

Use the Add indicator for missing transform to create a new indicator column, which contains a

Boolean "false" if a row contains a value, and "true" if a row contains a missing value.

Drop Missing

Use the Drop missing option to drop rows that contain missing values from the Input column.

Manage Columns

You can use the following transforms to quickly update and manage columns in your dataset:

Data preparation
1158

## Page 188

Amazon SageMaker AI
Developer Guide

Name
Function

Drop Column
Delete a column.

Duplicate Column
Duplicate a column.

Rename Column
Rename a column.

Move Column
Move a column's location in the dataset.
Choose to move your column to the start or
end of the dataset, before or after a reference
column, or to a speciﬁc index.

Manage Rows

Use this transform group to quickly perform sort and shuﬄe operations on rows. This group
contains the following:

• Sort: Sort the entire dataframe by a given column. Select the check box next to Ascending order
for this option; otherwise, deselect the check box and descending order is used for the sort.

• Shuﬄe: Randomly shuﬄe all rows in the dataset.

Manage Vectors

Use this transform group to combine or ﬂatten vector columns. This group contains the following
transforms.

• Assemble: Use this transform to combine Spark vectors and numeric data into a single column.
For example, you can combine three columns: two containing numeric data and one containing
vectors. Add all the columns you want to combine in Input columns and specify a Output
column name for the combined data.

• Flatten: Use this transform to ﬂatten a single column containing vector data. The input column
must contain PySpark vectors or array-like objects. You can control the number of columns
created by specifying a Method to detect number of outputs. For example, if you select Length
of ﬁrst vector, the number of elements in the ﬁrst valid vector or array found in the column
determines the number of output columns that are created. All other input vectors with too
many items are truncated. Inputs with too few items are ﬁlled with NaNs.

Data preparation
1159

## Page 189

Amazon SageMaker AI
Developer Guide

You also specify an Output preﬁx, which is used as the preﬁx for each output column.

Process Numeric

Use the Process Numeric feature group to process numeric data. Each scalar in this group is
deﬁned using the Spark library. The following scalars are supported:

• Standard Scaler: Standardize the input column by subtracting the mean from each value and
scaling to unit variance. To learn more, see the Spark documentation for StandardScaler.

• Robust Scaler: Scale the input column using statistics that are robust to outliers. To learn more,
see the Spark documentation for RobustScaler.

• Min Max Scaler: Transform the input column by scaling each feature to a given range. To learn
more, see the Spark documentation for MinMaxScaler.

• Max Absolute Scaler: Scale the input column by dividing each value by the maximum absolute

value. To learn more, see the Spark documentation for MaxAbsScaler.

Sampling

After you've imported your data, you can use the Sampling transformer to take one or more
samples of it. When you use the sampling transformer, Data Wrangler samples your original
dataset.

You can choose one of the following sample methods:

• Limit: Samples the dataset starting from the ﬁrst row up to the limit that you specify.

• Randomized: Takes a random sample of a size that you specify.

• Stratiﬁed: Takes a stratiﬁed random sample.

You can stratify a randomized sample to make sure that it represents the original distribution of
the dataset.

You might be performing data preparation for multiple use cases. For each use case, you can take a
diﬀerent sample and apply a diﬀerent set of transformations.

The following procedure describes the process of creating a random sample.

To take a random sample from your data.

Data preparation
1160

## Page 190

Amazon SageMaker AI
Developer Guide

1.
Choose the + to the right of the dataset that you've imported. The name of your dataset is
located below the +.

2.
Choose Add transform.

3.
Choose Sampling.

4.
For Sampling method, choose the sampling method.

5.
For Approximate sample size, choose the approximate number of observations that you want
in your sample.

6.
(Optional) Specify an integer for Random seed to create a reproducible sample.

The following procedure describes the process of creating a stratiﬁed sample.

To take a stratiﬁed sample from your data.

1.
Choose the + to the right of the dataset that you've imported. The name of your dataset is
located below the +.

2.
Choose Add transform.

3.
Choose Sampling.

4.
For Sampling method, choose the sampling method.

5.
For Approximate sample size, choose the approximate number of observations that you want
in your sample.

6.
For Stratify column, specify the name of the column that you want to stratify on.

7.
(Optional) Specify an integer for Random seed to create a reproducible sample.

Search and Edit

Use this section to search for and edit speciﬁc patterns within strings. For example, you can ﬁnd
and update strings within sentences or documents, split strings by delimiters, and ﬁnd occurrences
of speciﬁc strings.

The following transforms are supported under Search and edit. All transforms return copies of the
strings in the Input column and add the result to a new output column.

Data preparation
1161

## Page 191

Amazon SageMaker AI
Developer Guide

Name
Function

Find substring
Returns the index of the ﬁrst occurrence of
the Substring for which you searched , You

can start and end the search at Start and End
respectively.

Find substring (from right)
Returns the index of the last occurrence of
the Substring for which you searched. You
can start and end the search at Start and End
respectively.

Matches preﬁx
Returns a Boolean value if the string contains
a given Pattern. A pattern can be a character
sequence or regular expression. Optionally,
you can make the pattern case sensitive.

Find all occurrences
Returns an array with all occurrences of a
given pattern. A pattern can be a character
sequence or regular expression.

Extract using regex
Returns a string that matches a given Regex
pattern.

Extract between delimiters
Returns a string with all characters found
between Left delimiter and Right delimiter.

Extract from position
Returns a string, starting from Start position
in the input string, that contains all characters
up to the start position plus Length.

Find and replace substring
Returns a string with all matches of a given
Pattern (regular expression) replaced by
Replacement string.

Replace between delimiters
Returns a string with the substring found
between the ﬁrst appearance of a Left
delimiter and the last appearance of a Right

Data preparation
1162

## Page 192

Amazon SageMaker AI
Developer Guide

Name
Function

delimiter replaced by Replacement string. If
no match is found, nothing is replaced.

Replace from position
Returns a string with the substring between
Start position and Start position plus Length
replaced by Replacement string. If Start
position plus Length is greater than the
length of the replacement string, the output
contains ….

Convert regex to missing
Converts a string to None if invalid and returns
the result. Validity is deﬁned with a regular
expression in Pattern.

Split string by delimiter
Returns an array of strings from the input
string, split by Delimiter, with up to Max
number of splits (optional). The delimiter
defaults to white space.

Split data

Use the Split data transform to split your dataset into two or three datasets. For example, you can
split your dataset into a dataset used to train your model and a dataset used to test it. You can
determine the proportion of the dataset that goes into each split. For example, if you’re splitting
one dataset into two datasets, the training dataset can have 80% of the data while the testing
dataset has 20%.

Splitting your data into three datasets gives you the ability to create training, validation, and test
datasets. You can see how well the model performs on the test dataset by dropping the target
column.

Your use case determines how much of the original dataset each of your datasets get and the
method you use to split the data. For example, you might want to use a stratiﬁed split to make sure
that the distribution of the observations in the target column are the same across datasets. You
can use the following split transforms:

Data preparation
1163

## Page 193

Amazon SageMaker AI
Developer Guide

• Randomized split — Each split is a random, non-overlapping sample of the original dataset. For
larger datasets, using a randomized split might be computationally expensive and take longer
than an ordered split.

• Ordered split – Splits the dataset based on the sequential order of the observations. For
example, for an 80/20 train-test split, the ﬁrst observations that make up 80% of the dataset
go to the training dataset. The last 20% of the observations go to the testing dataset. Ordered
splits are eﬀective in keeping the existing order of the data between splits.

• Stratiﬁed split – Splits the dataset to make sure that the number of observations in the input
column have proportional representation. For an input column that has the observations 1, 1,
1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, an 80/20 split on the column would mean that
approximately 80% of the 1s, 80% of the 2s, and 80% of the 3s go to the training set. About
20% of each type of observation go to the testing set.

• Split by key – Avoids data with the same key occurring in more than one split. For example, if you
have a dataset with the column 'customer_id' and you're using it as a key, no customer id is in
more than one split.

After you split the data, you can apply additional transformations to each dataset. For most use
cases, they aren't necessary.

Data Wrangler calculates the proportions of the splits for performance. You can choose an error
threshold to set the accuracy of the splits. Lower error thresholds more accurately reﬂect the
proportions that you specify for the splits. If you set a higher error threshold, you get better
performance, but lower accuracy.

For perfectly split data, set the error threshold to 0. You can specify a threshold between 0 and 1
for better performance. If you specify a value greater than 1, Data Wrangler interprets that value
as 1.

If you have 10000 rows in your dataset and you specify an 80/20 split with an error of 0.001, you
would get observations approximating one of the following results:

• 8010 observations in the training set and 1990 in the testing set

• 7990 observations in the training set and 2010 in the testing set

The number of observations for the testing set in the preceding example is in the interval between
8010 and 7990.

Data preparation
1164

## Page 194

Amazon SageMaker AI
Developer Guide

By default, Data Wrangler uses a random seed to make the splits reproducible. You can specify a
diﬀerent value for the seed to create a diﬀerent reproducible split.

Randomized split

Use the following procedure to perform a randomized split on your dataset.

To split your dataset randomly, do the following

1.
Choose the + next to the node containing the dataset that you're splitting.

2.
Choose Add transform.

3.
Choose Split data.

4.
(Optional) For Splits, specify the names and proportions of each split. The proportions
must sum to 1.

5.
(Optional) Choose the + to create an additional split.

•
Specify the names and proportions of all the splits. The proportions must sum to 1.

6.
(Optional) Specify a value for Error threshold other than the default value.

7.
(Optional) Specify a value for Random seed.

8.
Choose Preview.

9.
Choose Add.

Ordered split

Use the following procedure to perform an ordered split on your dataset.

To make an ordered split in your dataset, do the following.

1.
Choose the + next to the node containing the dataset that you're splitting.

2.
Choose Add transform.

3.
For Transform, choose Ordered split.

4.
Choose Split data.

5.
(Optional) For Splits, specify the names and proportions of each split. The proportions
must sum to 1.

6.
(Optional) Choose the + to create an additional split.

Data preparation
1165

## Page 195

Amazon SageMaker AI
Developer Guide

•
Specify the names and proportions of all the splits. The proportions must sum to 1.

7.
(Optional) Specify a value for Error threshold other than the default value.

8.
(Optional) For Input column, specify a column with numeric values. Uses the values of the

columns to infer which records are in each split. The smaller values are in one split with the
larger values in the other splits.

9.
(Optional) Select Handle duplicates to add noise to duplicate values and create a dataset
of entirely unique values.

10. (Optional) Specify a value for Random seed.

11. Choose Preview.

12. Choose Add.

Stratiﬁed split

Use the following procedure to perform a stratiﬁed split on your dataset.

To make a stratiﬁed split in your dataset, do the following.

1.
Choose the + next to the node containing the dataset that you're splitting.

2.
Choose Add transform.

3.
Choose Split data.

4.
For Transform, choose Stratiﬁed split.

5.
(Optional) For Splits, specify the names and proportions of each split. The proportions
must sum to 1.

6.
(Optional) Choose the + to create an additional split.

•
Specify the names and proportions of all the splits. The proportions must sum to 1.

7.
For Input column, specify a column with up to 100 unique values. Data Wrangler can't
stratify a column with more than 100 unique values.

8.
(Optional) Specify a value for Error threshold other than the default value.

9.
(Optional) Specify a value for Random seed to specify a diﬀerent seed.

10. Choose Preview.

11. Choose Add.

Data preparation
1166

## Page 196

Amazon SageMaker AI
Developer Guide

Split by column keys

Use the following procedure to split by the column keys in your dataset.

To split by the column keys in your dataset, do the following.

1.
Choose the + next to the node containing the dataset that you're splitting.

2.
Choose Add transform.

3.
Choose Split data.

4.
For Transform, choose Split by key.

5.
(Optional) For Splits, specify the names and proportions of each split. The proportions
must sum to 1.

6.
(Optional) Choose the + to create an additional split.

•
Specify the names and proportions of all the splits. The proportions must sum to 1.

7.
For Key columns, specify the columns with values that you don't want to appear in both
datasets.

8.
(Optional) Specify a value for Error threshold other than the default value.

9.
Choose Preview.

10. Choose Add.

Parse Value as Type

Use this transform to cast a column to a new type. The supported Data Wrangler data types are:

• Long

• Float

• Boolean

• Date, in the format dd-MM-yyyy, representing day, month, and year respectively.

• String

Validate String

Use the Validate string transforms to create a new column that indicates that a row of text data
meets a speciﬁed condition. For example, you can use a Validate string transform to verify that a

Data preparation
1167

## Page 197

Amazon SageMaker AI
Developer Guide

string only contains lowercase characters. The following transforms are supported under Validate
string.

The following transforms are included in this transform group. If a transform outputs a Boolean

value, True is represented with a 1 and False is represented with a 0.

Name
Function

String length
Returns True if a string length equals

speciﬁed length. Otherwise, returns False.

Starts with
Returns True if a string starts will a speciﬁed

preﬁx. Otherwise, returns False.

Ends with
Returns True if a string length equals

speciﬁed length. Otherwise, returns False.

Is alphanumeric
Returns True if a string only contains
numbers and letters. Otherwise, returns

False.

Is alpha (letters)
Returns True if a string only contains letters.

Otherwise, returns False.

Is digit
Returns True if a string only contains digits.

Otherwise, returns False.

Is space
Returns True if a string only contains
numbers and letters. Otherwise, returns

False.

Is title
Returns True if a string contains any white

spaces. Otherwise, returns False.

Is lowercase
Returns True if a string only contains lower

case letters. Otherwise, returns False.

Is uppercase
Returns True if a string only contains upper

case letters. Otherwise, returns False.

Data preparation
1168

## Page 198

Amazon SageMaker AI
Developer Guide

Name
Function

Is numeric
Returns True if a string only contains

numbers. Otherwise, returns False.

Is decimal
Returns True if a string only contains decimal

numbers. Otherwise, returns False.

Unnest JSON Data

If you have a .csv ﬁle, you might have values in your dataset that are JSON strings. Similarly, you
might have nested data in columns of either a Parquet ﬁle or a JSON document.

Use the Flatten structured operator to separate the ﬁrst level keys into separate columns. A ﬁrst
level key is a key that isn't nested within a value.

For example, you might have a dataset that has a person column with demographic information on
each person stored as JSON strings. A JSON string might look like the following.

"{"seq": 1,"name": {"first": "Nathaniel","last": "Ferguson"},"age": 59,"city":
"Posbotno","state": "WV"}"

The Flatten structured operator converts the following ﬁrst level keys into additional columns in
your dataset:

• seq

• name

• age

• city

• state

Data Wrangler puts the values of the keys as values under the columns. The following shows the
column names and values of the JSON.

Data preparation
1169

## Page 199

Amazon SageMaker AI
Developer Guide

seq, name,                                    age, city, state
1, {"first": "Nathaniel","last": "Ferguson"}, 59, Posbotno, WV

For each value in your dataset containing JSON, the Flatten structured operator creates columns

for the ﬁrst-level keys. To create columns for nested keys, call the operator again. For the preceding
example, calling the operator creates the columns:

• name_ﬁrst

• name_last

The following example shows the dataset that results from calling the operation again.

seq, name,                                    age, city, state, name_first, name_last

1, {"first": "Nathaniel","last": "Ferguson"}, 59, Posbotno, WV, Nathaniel, Ferguson

Choose Keys to ﬂatten on to specify the ﬁrst-level keys that want to extract as separate columns.
If you don't specify any keys, Data Wrangler extracts all the keys by default.

Explode Array

Use Explode array to expand the values of the array into separate output rows. For example, the
operation can take each value in the array, [[1, 2, 3,], [4, 5, 6], [7, 8, 9]] and create a new column
with the following rows:

[1, 2, 3]
[4, 5, 6]
[7, 8, 9]

Data Wrangler names the new column, input_column_name_ﬂatten.

You can call the Explode array operation multiple times to get the nested values of the array into
separate output columns. The following example shows the result of calling the operation multiple
times on a dataset with a nested array.

Putting the values of a nested array into separate columns

Data preparation
1170

## Page 200

Amazon SageMaker AI
Developer Guide

id
array
id
array_items
id
array_ite
ms_items

1
[ [cat, dog],

1
[cat, dog]
1
cat

[bat, frog] ]

2
[[rose,
petunia], [lily,
daisy]]

1
[bat, frog]
1
dog

2
[rose,
petunia]

1
bat

2
[lily, daisy]
1
frog

2
2
rose

2
2
petunia

2
2
lily

2
2
daisy

Transform Image Data

Use Data Wrangler to import and transform the images that you're using for your machine learning
(ML) pipelines. After you've prepared your image data, you can export it from your Data Wrangler
ﬂow to your ML pipeline.

You can use the information provided here to familiarize yourself with importing and transforming
image data in Data Wrangler. Data Wrangler uses OpenCV to import images. For more information
about supported image formats, see Image ﬁle reading and writing.

After you've familiarized yourself with the concepts of transforming your image data, go through
the following tutorial, Prepare image data with Amazon SageMaker Data Wrangler.

The following industries and use cases are examples where applying machine learning to
transformed image data can be useful:

• Manufacturing – Identifying defects in items from the assembly line

Data preparation
1171

## Page 201

Amazon SageMaker AI
Developer Guide

• Food – Identifying spoiled or rotten food

• Medicine – Identifying lesions in tissues

When you work with image data in Data Wrangler, you go through the following process:

1. Import – Select the images by choosing the directory containing them in your Amazon S3

bucket.

2. Transform – Use the built-in transformations to prepare the images for your machine learning

pipeline.

3. Export – Export the images that you’ve transformed to a location that can be accessed from the

pipeline.

Use the following procedure to import your image data.

To import your image data

1.
Navigate to the Create connection page.

2.
Choose Amazon S3.

3.
Specify the Amazon S3 ﬁle path that contains the image data.

4.
For File type, choose Image.

5.
(Optional) Choose Import nested directories to import images from multiple Amazon S3
paths.

6.
Choose Import.

Data Wrangler uses the open-source imgaug library for its built-in image transformations. You can
use the following built-in transformations:

• ResizeImage

• EnhanceImage

• CorruptImage

• SplitImage

• DropCorruptedImages

• DropImageDuplicates

Data preparation
1172

## Page 202

Amazon SageMaker AI
Developer Guide

• Brightness

• ColorChannels

• Grayscale

• Rotate

Use the following procedure to transform your images without writing code.

To transform the image data without writing code

1.
From your Data Wrangler ﬂow, choose the + next to the node representing the images that
you've imported.

2.
Choose Add transform.

3.
Choose Add step.

4.
Choose the transform and conﬁgure it.

5.
Choose Preview.

6.
Choose Add.

In addition to using the transformations that Data Wrangler provides, you can also use your own
custom code snippets. For more information about using custom code snippets, see Custom
Transforms. You can import the OpenCV and imgaug libraries within your code snippets and use
the transforms associated with them. The following is an example of a code snippet that detects
edges within the images.

# A table with your image data is stored in the `df` variable
import cv2
import numpy as np
from pyspark.sql.functions import column

from sagemaker_dataprep.compute.operators.transforms.image.constants import
DEFAULT_IMAGE_COLUMN, IMAGE_COLUMN_TYPE
from sagemaker_dataprep.compute.operators.transforms.image.decorators import
BasicImageOperationDecorator, PandasUDFOperationDecorator

@BasicImageOperationDecorator
def my_transform(image: np.ndarray) -> np.ndarray:

Data preparation
1173

## Page 203

Amazon SageMaker AI
Developer Guide

# To use the code snippet on your image data, modify the following lines within the
function
HYST_THRLD_1, HYST_THRLD_2 = 100, 200
edges = cv2.Canny(image,HYST_THRLD_1,HYST_THRLD_2)
return edges

@PandasUDFOperationDecorator(IMAGE_COLUMN_TYPE)
def custom_image_udf(image_row):
return my_transform(image_row)

df = df.withColumn(DEFAULT_IMAGE_COLUMN,
custom_image_udf(column(DEFAULT_IMAGE_COLUMN)))

When apply transformations in your Data Wrangler ﬂow, Data Wrangler only applies them to
a sample of the images in your dataset. To optimize your experience with the application, Data
Wrangler doesn't apply the transforms to all of your images.

Filter data

Use Data Wrangler to ﬁlter the data in your columns. When you ﬁlter the data in a column, you
specify the following ﬁelds:

• Column name – The name of the column that you're using to ﬁlter the data.

• Condition – The type of ﬁlter that you're applying to values in the column.

• Value – The value or category in the column to which you're applying the ﬁlter.

You can ﬁlter on the following conditions:

• = – Returns values that match the value or category that you specify.

• != – Returns values that don't match the value or category that you specify.

• >= – For Long or Float data, ﬁlters for values that are greater than or equal to the value that you
specify.

• <= – For Long or Float data, ﬁlters for values that are less than or equal to the value that you
specify.

• > – For Long or Float data, ﬁlters for values that are greater than the value that you specify.

Data preparation
1174

## Page 204

Amazon SageMaker AI
Developer Guide

• < – For Long or Float data, ﬁlters for values that are less than the value that you specify.

For a column that has the categories, male and female, you can ﬁlter out all the male values. You

could also ﬁlter for all the female values. Because there are only male and female values in the

column, the ﬁlter returns a column that only has female values.

You can also add multiple ﬁlters. The ﬁlters can be applied across multiple columns or the same
column. For example, if you're creating a column that only has values within a certain range, you
add two diﬀerent ﬁlters. One ﬁlter speciﬁes that the column must have values greater than the
value that you provide. The other ﬁlter speciﬁes that the column must have values less than the
value that you provide.

Use the following procedure to add the ﬁlter transform to your data.

To ﬁlter your data

1.
From your Data Wrangler ﬂow, choose the + next to the node with the data that you're
ﬁltering.

2.
Choose Add transform.

3.
Choose Add step.

4.
Choose Filter data.

5.
Specify the following ﬁelds:

• Column name – The column that you're ﬁltering.

• Condition – The condition of the ﬁlter.

• Value – The value or category in the column to which you're applying the ﬁlter.

6.
(Optional) Choose + following the ﬁlter that you've created.

7.
Conﬁgure the ﬁlter.

8.
Choose Preview.

9.
Choose Add.

Chat for data prep

Important

For administrators:

Data preparation
1175

## Page 205

Amazon SageMaker AI
Developer Guide

• Chat for data prep requires the AmazonSageMakerCanvasAIServicesAccess
policy. For more information, see AWS managed policy:
AmazonSageMakerCanvasAIServicesAccess

• Chat for data prep requires access to Amazon Bedrock and the Anthropic Claude model
within it. For more information, see Add model access.

• You must run SageMaker Canvas data prep in the same AWS Region as the Region where
you're running your model. Chat for data prep is available in the US East (N. Virginia), US
West (Oregon), and Europe (Frankfurt) AWS Regions.

In addition to using the built-in transforms and analyses, you can use natural language to explore,
visualize, and transform your data in a conversational interface. Within the conversational
interface, you can use natural language queries to understand and prepare your data to build ML
models.

The following are examples of some prompts that you can use:

• Summarize my data

• Drop column example-column-name

• Replace missing values with median

• Plot histogram of prices

• What is the most expensive item sold?

• How many distinct items were sold?

• Sort data by region

When you’re transforming your data using your prompts, you can view a preview that shows how
data is being transformed. You can choose to add it as step in your Data Wrangler ﬂow based on
what you see in the preview.

The responses to your prompts generate code for your transformations and analyses. You can
modify the code to update the output from the prompt. For example, you can modify the code for
an analysis to change the values of the axes of a graph.

Use the following procedure to start chatting with your data:

Data preparation
1176

## Page 206

Amazon SageMaker AI
Developer Guide

To chat with your data

1.
Open the SageMaker Canvas data ﬂow.

2.
Choose the speech bubble.

![Page 206 Diagram 1](images/page-0206-img-01.png)

3.
Specify a prompt.

4.
(Optional) If an analysis has been generated by your query, choose Add to analyses to
reference it for later.

![Page 206 Diagram 2](images/page-0206-img-02.png)

Data preparation
1177

## Page 207

Amazon SageMaker AI
Developer Guide

5.
(Optional) If you've transformed your data using a prompt, do the following.

a.
Choose Preview to view the results.

b.
(Optional) Modify the code in the transform and choose Update.

c.
(Optional) If you're happy with the results of the transform, choose Add to steps to add it
to the steps panel on the right-hand navigation.

![Page 207 Diagram 1](images/page-0207-img-01.png)

After you’ve prepared your data using natural language, you can create a model using your
transformed data. For more information about creating a model, see How custom models work.

How data processing works in Data Wrangler

While working with data interactively in an Amazon SageMaker Data Wrangler data ﬂow, Amazon
SageMaker Canvas only applies the transformations to a sample dataset for you to preview. After
ﬁnishing your data ﬂow in SageMaker Canvas, you can process all of your data and save it in a
location that is suitable for your machine learning workﬂows.

There are several options for how to proceed after you've ﬁnished transforming your data in Data
Wrangler:

Data preparation
1178

## Page 208

Amazon SageMaker AI
Developer Guide

• Create a model. You can create a Canvas model, where you directly start creating a model with
your prepared data. You can create a model either after processing your entire dataset, or by
exporting just the sample data you worked with in Data Wrangler. Canvas saves your processed
data (either the entire dataset or the sample data) as a Canvas dataset.

We recommend that you use your sample data for quick iterations, but that you use your entire
data when you want to train your ﬁnal model. When building tabular models, datasets larger
than 5 GB are automatically downsampled to 5 GB, and for time series forecasting models,
datasets larger than 30 GB are downsampled to 30 GB.

To learn more about creating a model, see How custom models work.

• Export the data. You can export your data for use in machine learning workﬂows. When you
choose to export your data, you have several options:

• You can save your data in the Canvas application as a dataset. For more information about the

supported ﬁle types for Canvas datasets and additional requirements when importing data
into Canvas, see Create a dataset.

• You can save your data to Amazon S3. Depending on the Canvas memory availability, your
data is processed in the application and then exported to Amazon S3. If the size of your
dataset exceeds what Canvas can process, then by default, Canvas uses an EMR Serverless job
to scale to multiple compute instances, process your full dataset, and export it to Amazon S3.
You can also manually conﬁgure a SageMaker Processing job to have more granular control
over the compute resources used to process your data.

• Export a data ﬂow. You might want to save the code for your data ﬂow so that you can modify
or run your transformations outside of Canvas. Canvas provides you with the option to save your
data ﬂow transformations as Python code in a Jupyter notebook, which you can then export to
Amazon S3 for use elsewhere in your machine learning workﬂows.

When you export your data from a data ﬂow and save it either as a Canvas dataset or to Amazon
S3, Canvas creates a new destination node in your data ﬂow, which is a ﬁnal node that shows
you where your processed data is stored. You can add additional destination nodes to your ﬂow
if you'd like to perform multiple export operations. For example, you can export the data from
diﬀerent points in your data ﬂow to only apply some of the transformations, or you can export
transformed data to diﬀerent Amazon S3 locations. For more information about how to add or edit
a destination node, see Add destination nodes and Edit a destination node.

Data preparation
1179

## Page 209

Amazon SageMaker AI
Developer Guide

For more information about setting up a schedule with Amazon EventBridge to automatically
process and export your data on a schedule, see Create a schedule to automatically process new
data.

Export to create a model

In just a few clicks from your data ﬂow, you can export your transformed data and start creating
an ML model in Canvas. Canvas saves your data as a Canvas dataset, and you're taken to the model
build conﬁguration page for a new model.

To create a Canvas model with your transformed data:

1.
Navigate to your data ﬂow.

2.
Choose the ellipsis icon next to the node that you're exporting.

3.
From the context menu, choose Create model.

4.
In the Export to create a model side panel, enter a Dataset name for the new dataset.

5.
Leave the Process entire dataset option selected to process and export your entire dataset
before proceeding with building a model. Turn this option oﬀ to train your model using the
interactive sample data you are working with in your data ﬂow.

6.
Enter a Model name to name the new model.

7.
Select a Problem type, or the type of model that you want to build. For more information
about the supported model types in SageMaker Canvas, see How custom models work.

8.
Select the Target column, or the value that you want the model to predict.

9.
Choose Export and create model.

The Build tab for a new Canvas model should open, and you can ﬁnish conﬁguring and training
your model. For more information about how to build a model, see Build a model.

Export data

Export data to apply the transforms from your data ﬂow to the full imported dataset. You can
export any node in your data ﬂow to the following locations:

• SageMaker Canvas dataset

• Amazon S3

Data preparation
1180

## Page 210

Amazon SageMaker AI
Developer Guide

If you want to train models in Canvas, you can export your full, transformed dataset as a Canvas
dataset. If you want to use your transformed data in machine learning workﬂows external to
SageMaker Canvas, you can export your dataset to Amazon S3.

Export to a Canvas dataset

Use the following procedure to export a SageMaker Canvas dataset from a node in your data ﬂow.

To export a node in your ﬂow as a SageMaker Canvas dataset

1.
Navigate to your data ﬂow.

2.
Choose the ellipsis icon next to the node that you're exporting.

3.
In the context menu, hover over Export, and then select Export data to Canvas dataset.

4.
In the Export to Canvas dataset side panel, enter a Dataset name for the new dataset.

5.
Leave the Process entire dataset option selected if you want SageMaker Canvas to process
and save your full dataset. Turn this option oﬀ to only apply the transforms to the sample data
you are working with in your data ﬂow.

6.
Choose Export.

You should now be able to go to the Datasets page of the Canvas application and see your new
dataset.

Export to Amazon S3

When exporting your data to Amazon S3, you can scale to transform and process data of any size.
Canvas automatically processes your data locally if the application's memory can handle the size of
your dataset. If your dataset size exceeds the local memory capacity of 5 GB, then Canvas initiates
a remote job on your behalf to provision additional compute resources and process the data more
quickly. By default, Canvas uses Amazon EMR Serverless to run these remote jobs. However, you
can manually conﬁgure Canvas to use either EMR Serverless or a SageMaker Processing job with
your own settings.

Note

When running an EMR Serverless job, by default the job inherits the IAM role, KMS key
settings, and tags of your Canvas application.

Data preparation
1181

## Page 211

Amazon SageMaker AI
Developer Guide

The following summarizes the options for remote jobs in Canvas:

• EMR Serverless: This is the default option that Canvas uses for remote jobs. EMR Serverless
automatically provisions and scales compute resources to process your data so that you
don't have to worry about choosing the right compute resources for your workload. For more
information about EMR Serverless, see the EMR Serverless User Guide.

• SageMaker Processing: SageMaker Processing jobs oﬀer more advanced options and granular
control over the compute resources used to process your data. For example, you can specify
the type and count of the compute instances, conﬁgure the job in your own VPC and control
network access, automate processing jobs, and more. For more information about automating
processing jobs see Create a schedule to automatically process new data. For more general
information about SageMaker Processing jobs, see Data transformation workloads with
SageMaker Processing.

The following ﬁle types are supported when exporting to Amazon S3:

• CSV

• Parquet

To get started, review the following prerequisites.

Prerequisites for EMR Serverless jobs

To create a remote job that uses EMR Serverless resources, you must have the necessary
permissions. You can grant permissions either through the Amazon SageMaker AI domain or user
proﬁle settings, or you can manually conﬁgure your user's AWS IAM role. For instructions on how to
grant users permissions to perform large data processing, see Grant Users Permissions to Use Large
Data across the ML Lifecycle.

If you don't want to conﬁgure these policies but still need to process large datasets through Data
Wrangler, you can alternatively use a SageMaker Processing job.

Use the following procedures to export your data to Amazon S3. To conﬁgure a remote job, follow
the optional advanced steps.

To export a node in your ﬂow to Amazon S3

1.
Navigate to your data ﬂow.

Data preparation
1182

## Page 212

Amazon SageMaker AI
Developer Guide

2.
Choose the ellipsis icon next to the node that you're exporting.

3.
In the context menu, hover over Export, and then select Export data to Amazon S3.

4.
In the Export to Amazon S3 side panel, you can change the Dataset name for the new
dataset.

5.
For the S3 location, enter the Amazon S3 location to which you want to export the dataset.
You can enter the S3 URI, alias, or ARN of the S3 location or S3 access point. For more
information access points, see Managing data access with Amazon S3 access points in the
Amazon S3 User Guide.

6.
(Optional) For the Advanced settings, specify values for the following ﬁelds:

a.
File type – The ﬁle format of your exported data.

b.
Delimiter – The delimiter used to separate values in the ﬁle.

c.
Compression – The compression method used to reduce the ﬁle size.

d.
Number of partitions – The number of dataset ﬁles that Canvas writes as the output of
the job.

e.
Choose columns – You can choose a subset of columns from the data to include in the
partitions.

7.
Leave the Process entire dataset option selected if you want Canvas to apply your data ﬂow
transforms to your entire dataset and export the result. If you deselect this option, Canvas only
applies the transforms to the sample of your dataset used in the interactive Data Wrangler
data ﬂow.

Note

If you only export a sample of your data, Canvas processes your data in the application
and doesn't create a remote job for you.

8.
Leave the Auto job conﬁguration option selected if you want Canvas to automatically
determine whether to run the job using Canvas application memory or an EMR Serverless job.
If you deselect this option and manually conﬁgure your job, then you can choose to use either
an EMR Serverless or a SageMaker Processing job. For instructions on how to conﬁgure an
EMR Serverless or a SageMaker Processing job, see the section after this procedure before you
export your data.

9.
Choose Export.

Data preparation
1183

## Page 213

Amazon SageMaker AI
Developer Guide

The following procedures show how to manually conﬁgure the remote job settings for either EMR
Serverless or SageMaker Processing when exporting your full dataset to Amazon S3.

EMR Serverless

To conﬁgure an EMR Serverless job while exporting to Amazon S3, do the following:

1.
In the Export to Amazon S3 side panel, turn oﬀ the Auto job conﬁguration option.

2.
Select EMR Serverless.

3.
For Job name, enter a name for your EMR Serverless job. The name can contain letters,
numbers, hyphens, and underscores.

4.
For IAM role, enter the user's IAM execution role. This role should have the required
permissions to run EMR Serverless applications. For more information, see Grant Users
Permissions to Use Large Data across the ML Lifecycle.

5.
(Optional) For KMS key, specify the key ID or ARN of an AWS KMS key to encrypt the job
logs. If you don't enter a key, Canvas uses a default key for EMR Serverless.

6.
(Optional) For Monitoring conﬁguration, enter the name of an Amazon CloudWatch Logs
log group to which you want to publish your logs.

7.
(Optional) For Tags, add metadata tags to the EMR Serverless job consisting of key-value
pairs. These tags can be used to categorize and search for jobs.

8.
Choose Export to start the job.

SageMaker Processing

To conﬁgure a SageMaker Processing job while exporting to Amazon S3, do the following:

1.
In the Export to Amazon S3 side panel, turn oﬀ the Auto job conﬁguration option.

2.
Select SageMaker Processing.

3.
For Job name, enter a name for your SageMaker AI Processing job.

4.
For Instance type, select the type of compute instance to run the processing job.

5.
For Instance count, specify the number of compute instances to launch.

6.
For IAM role, enter the user's IAM execution role. This role should have the required
permissions for SageMaker AI to create and run processing jobs on your behalf. These
permissions are granted if you have the AmazonSageMakerFullAccess policy attached to
your IAM role.

Data preparation
1184

## Page 214

Amazon SageMaker AI
Developer Guide

7.
For Volume size, enter the storage size in GB for the ML storage volume that is attached to
each processing instance. Choose the size based on your expected input and output data
size.

8.
(Optional) For Volume KMS key, specify a KMS key to encrypt the storage volume. If you
don't specify a key, the default Amazon EBS encryption key is used.

9.
(Optional) For KMS key, specify a KMS key to encrypt input and output Amazon S3 data
sources used by the processing job.

10. (Optional) For Spark memory conﬁguration, do the following:

a.
Enter Driver memory in MB for the Spark driver node that handles job coordination
and scheduling.

b.
Enter Executor memory in MB for the Spark executor nodes that run individual tasks
in the job.

11. (Optional) For Network conﬁguration, do the following:

a.
For Subnet conﬁguration, enter the IDs of the VPC subnets for the processing
instances to be launched in. By default, the job uses the settings of your default VPC.

b.
For Security group conﬁguration, enter the IDs of the security groups to control
inbound and outbound connectivity rules.

c.
Turn on the Enable inter-container traﬃc encryption option to encrypt network
communication between processing containers during the job.

12. (Optional) For Associate schedules, you can choose create an Amazon EventBridge

schedule to have the processing job run on recurring intervals. Choose Create new
schedule and ﬁll out the dialog box. For more information about ﬁlling out this section and
running processing jobs on a schedule, see Create a schedule to automatically process new
data.

13. (Optional) Add Tags as key-value pairs so that you can categorize and search for processing

jobs.

14. Choose Export to start the processing job.

After exporting your data, you should ﬁnd the fully processed dataset in the speciﬁed Amazon S3
location.

Data preparation
1185

## Page 215

Amazon SageMaker AI
Developer Guide

Export a data ﬂow

Exporting your data ﬂow translates the operations that you've made in Data Wrangler and exports
it into a Jupyter notebook of Python code that you can modify and run. This can be helpful for

integrating the code for your data transformations into your machine learning pipelines.

You can choose any data node in your data ﬂow and export it. Exporting the data node exports the
transformation that the node represents and the transformations that precede it.

To export a data ﬂow as a Jupyter notebook

1.
Navigate to your data ﬂow.

2.
Choose the ellipsis icon next to the node that you want to export.

3.
In the context menu, hover over Export, and then hover over Export via Jupyter notebook.

4.
Choose one of the following:

• SageMaker Pipelines

• Amazon S3

• SageMaker AI Inference Pipeline

• SageMaker AI Feature Store

• Python Code

5.
The Export data ﬂow as notebook dialog box opens. Select one of the following:

• Download a local copy

• Export to S3 location

6.
If you selected Export to S3 location, enter the Amazon S3 location to which you want to
export the notebook.

7.
Choose Export.

Your Jupyter notebook should either download to your local machine, or you can ﬁnd it saved in
the Amazon S3 location you speciﬁed.

Add destination nodes

A destination node in SageMaker Canvas speciﬁes where to store your processed and transformed
data. When you choose to export your transformed data to Amazon S3, Canvas uses the speciﬁed

Data preparation
1186

## Page 216

Amazon SageMaker AI
Developer Guide

destination node location, applying all the transformations you've conﬁgured in your data ﬂow. For
more information about export jobs to Amazon S3, see the preceding section Export to Amazon S3.

By default, choosing to export your data to Amazon S3 adds a destination node to your data ﬂow.
However, you can add multiple destination nodes to your ﬂow, allowing you to simultaneously
export diﬀerent sets of transformations or variations of your data to diﬀerent Amazon S3
locations. For example, you can create one destination node that exports the data after applying

all transformations, and another destination node that exports the data after only certain initial
transformations, such as a join operation. This ﬂexibility enables you to export and store diﬀerent
versions or subsets of your transformed data in separate S3 locations for various use cases.

Use the following procedure to add a destination node to your data ﬂow.

To add a destination node

1.
Navigate to your data ﬂow.

2.
Choose the ellipsis icon next to the node where you want to place the destination node.

3.
In the context menu, hover over Export, and then select Add destination.

4.
In the Export destination side panel, enter a Dataset name to name the output.

5.
For Amazon S3 location, enter the Amazon S3 location to which you want to export the
output. You can enter the S3 URI, alias, or ARN of the S3 location or S3 access point. For more
information access points, see Managing data access with Amazon S3 access points in the
Amazon S3 User Guide.

6.
For Export settings, specify the following ﬁelds:

a.
File type – The ﬁle format of the exported data.

b.
Delimiter – The delimiter used to separate values in the ﬁle.

c.
Compression – The compression method used to reduce the ﬁle size.

7.
For Partitioning, specify the following ﬁelds:

a.
Number of partitions – The number of dataset ﬁles that SageMaker Canvas writes as the
output of the job.

b.
Choose columns – You can choose a subset of columns from the data to include in the
partitions.

8.
Choose Add if you want to simply add a destination node to your data ﬂow, or choose Add and
then choose Export if you want to add the node and initiate an export job.

Data preparation
1187

## Page 217

Amazon SageMaker AI
Developer Guide

You should now see a new destination node in your ﬂow.

Edit a destination node

A destination node in a Amazon SageMaker Canvas data ﬂow speciﬁes the Amazon S3 location
where your processed and transformed data is stored, applying all the conﬁgured transformations
in your data ﬂow. You can edit the conﬁguration of an existing destination node and then choose
to re-run the job to overwrite the data in the speciﬁed Amazon S3 location. For more information
about adding a new destination node, see Add destination nodes.

Use the following procedure to edit a destination node in your data ﬂow and initiate an export job.

To edit a destination node

1.
Navigate to your data ﬂow.

2.
Choose the ellipsis icon next to the destination node that you want to edit.

3.
In the context menu, choose Edit.

4.
The Edit destination side panel opens. From this panel, you can edit details such as the
dataset name, the Amazon S3 location, and the export and partitioning settings.

5.
(Optional) In Additional nodes to export, you can select more destination nodes to process
when you run the export job.

6.
Leave the Process entire dataset option selected if you want Canvas to apply your data ﬂow
transforms to your entire dataset and export the result. If you deselect this option, Canvas only
applies the transforms to the sample of your dataset used in the interactive Data Wrangler
data ﬂow.

7.
Leave the Auto job conﬁguration option selected if you want Canvas to automatically
determine whether to run the job using Canvas application memory or an EMR Serverless job.
If you deselect this option and manually conﬁgure your job, then you can choose to use either
an EMR Serverless or a SageMaker Processing job. For instructions on how to conﬁgure an EMR
Serverless or a SageMaker Processing job, see the preceding section Export to Amazon S3.

8.
When you're done making changes, choose Update.

Saving changes to your destination node conﬁguration doesn't automatically re-run a job or
overwrite data that has already been processed and exported. Export your data again to run a job
with the new conﬁguration. If you decide to export your data again with a job, Canvas uses the
updated destination node conﬁguration to transform and output the data to the speciﬁed location,
overwriting any existing data.

Data preparation
1188

## Page 218

Amazon SageMaker AI
Developer Guide

Create a schedule to automatically process new data

Note

The following section only applies to SageMaker Processing jobs. If you used the default
Canvas settings or EMR Serverless to create a remote job to apply transforms to your full
dataset, this section doesn’t apply.

If you're processing data periodically, you can create a schedule to run the processing job
automatically. For example, you can create a schedule that runs a processing job automatically
when you get new data. For more information about processing jobs, see Export to Amazon S3.

When you create a job, you must specify an IAM role that has permissions to create the job. You
can use the AmazonSageMakerCanvasDataPrepFullAccess policy to add permissions.

Add the following trust policy to the role to allow EventBridge to assume it.

{
"Effect": "Allow",
"Principal": {
"Service": "events.amazonaws.com"
},
"Action": "sts:AssumeRole"
}

Important

When you create a schedule, Data Wrangler creates an eventRule in EventBridge. You
incur charges for both the event rules that you create and the instances used to run the
processing job.
For information about EventBridge pricing, see Amazon EventBridge pricing. For
information about processing job pricing, see Amazon SageMaker Pricing.

You can set a schedule using one of the following methods:

• CRON expressions

Data preparation
1189

## Page 219

Amazon SageMaker AI
Developer Guide

Note

Data Wrangler doesn't support the following expressions:

• LW#

• Abbreviations for days

• Abbreviations for months

• RATE expressions

• Recurring – Set an hourly or daily interval to run the job.

• Speciﬁc time – Set speciﬁc days and times to run the job.

The following sections provide procedures on scheduling jobs when ﬁlling out the SageMaker AI
Processing job settings while exporting your data to Amazon S3. All of the following instructions
begin in the Associate schedules section of the SageMaker Processing job settings.

CRON

Use the following procedure to create a schedule with a CRON expression.

1.
In the Export to Amazon S3 side panel, make sure you've turned oﬀ the Auto job
conﬁguration toggle and have the SageMaker Processing option selected.

2.
In the SageMaker Processing job settings, open the Associate schedules section and
choose Create new schedule.

3.
The Create new schedule dialog box opens. For Schedule Name, specify the name of the
schedule.

4.
For Run Frequency, choose CRON.

5.
For each of the Minutes, Hours, Days of month, Month, and Day of week ﬁelds, enter valid
CRON expression values.

6.
Choose Create.

7.
(Optional) Choose Add another schedule to run the job on an additional schedule.

Data preparation
1190

## Page 220

Amazon SageMaker AI
Developer Guide

Note

You can associate a maximum of two schedules. The schedules are independent and
don't aﬀect each other unless the times overlap.

8.
Choose one of the following:

• Schedule and run now – The job runs immediately and subsequently runs on the
schedules.

• Schedule only – The job only runs on the schedules that you specify.

9.
Choose Export after you've ﬁlled out the rest of the export job settings.

RATE

Use the following procedure to create a schedule with a RATE expression.

1.
In the Export to Amazon S3 side panel, make sure you've turned oﬀ the Auto job
conﬁguration toggle and have the SageMaker Processing option selected.

2.
In the SageMaker Processing job settings, open the Associate schedules section and
choose Create new schedule.

3.
The Create new schedule dialog box opens. For Schedule Name, specify the name of the
schedule.

4.
For Run Frequency, choose Rate.

5.
For Value, specify an integer.

6.
For Unit, select one of the following:

• Minutes

• Hours

• Days

7.
Choose Create.

8.
(Optional) Choose Add another schedule to run the job on an additional schedule.

Data preparation
1191

## Page 221

Amazon SageMaker AI
Developer Guide

Note

You can associate a maximum of two schedules. The schedules are independent and
don't aﬀect each other unless the times overlap.

9.
Choose one of the following:

• Schedule and run now – The job runs immediately and subsequently runs on the
schedules.

• Schedule only – The job only runs on the schedules that you specify.

10. Choose Export after you've ﬁlled out the rest of the export job settings.

Recurring

Use the following procedure to create a schedule that runs a job on a recurring basis.

1.
In the Export to Amazon S3 side panel, make sure you've turned oﬀ the Auto job
conﬁguration toggle and have the SageMaker Processing option selected.

2.
In the SageMaker Processing job settings, open the Associate schedules section and
choose Create new schedule.

3.
The Create new schedule dialog box opens. For Schedule Name, specify the name of the
schedule.

4.
For Run Frequency, choose Recurring.

5.
For Every x hours, specify the hourly frequency that the job runs during the day. Valid

values are integers in the inclusive range of 1 and 23.

6.
For On days, select one of the following options:

• Every Day

• Weekends

• Weekdays

• Select Days

•
(Optional) If you've selected Select Days, choose the days of the week to run the job.

Data preparation
1192

## Page 222

Amazon SageMaker AI
Developer Guide

Note

The schedule resets every day. If you schedule a job to run every ﬁve hours, it runs
at the following times during the day:

• 00:00

• 05:00

• 10:00

• 15:00

• 20:00

7.
Choose Create.

8.
(Optional) Choose Add another schedule to run the job on an additional schedule.

Note

You can associate a maximum of two schedules. The schedules are independent and
don't aﬀect each other unless the times overlap.

9.
Choose one of the following:

• Schedule and run now – The job runs immediately and subsequently runs on the
schedules.

• Schedule only – The job only runs on the schedules that you specify.

10. Choose Export after you've ﬁlled out the rest of the export job settings.

Speciﬁc time

Use the following procedure to create a schedule that runs a job at speciﬁc times.

1.
In the Export to Amazon S3 side panel, make sure you've turned oﬀ the Auto job
conﬁguration toggle and have the SageMaker Processing option selected.

2.
In the SageMaker Processing job settings, open the Associate schedules section and
choose Create new schedule.

Data preparation
1193

## Page 223

Amazon SageMaker AI
Developer Guide

3.
The Create new schedule dialog box opens. For Schedule Name, specify the name of the
schedule.

4.
For Run Frequency, choose Start time.

5.
For Start time, enter a time in UTC format (for example, 09:00). The start time defaults to

the time zone where you are located.

6.
For On days, select one of the following options:

• Every Day

• Weekends

• Weekdays

• Select Days

•
(Optional) If you've selected Select Days, choose the days of the week to run the job.

7.
Choose Create.

8.
(Optional) Choose Add another schedule to run the job on an additional schedule.

Note

You can associate a maximum of two schedules. The schedules are independent and
don't aﬀect each other unless the times overlap.

9.
Choose one of the following:

• Schedule and run now – The job runs immediately and subsequently runs on the
schedules.

• Schedule only – The job only runs on the schedules that you specify.

10. Choose Export after you've ﬁlled out the rest of the export job settings.

You can use the SageMaker AI AWS Management Console to view the jobs that are scheduled to
run. Your processing jobs run within Pipelines. Each processing job has its own pipeline. It runs
as a processing step within the pipeline. You can view the schedules that you've created within a
pipeline. For information about viewing a pipeline, see View the details of a pipeline.

Use the following procedure to view the jobs that you've scheduled.

To view the jobs you've scheduled, do the following.

Data preparation
1194

## Page 224

Amazon SageMaker AI
Developer Guide

1.
Open Amazon SageMaker Studio Classic.

2.
Open Pipelines

3.
View the pipelines for the jobs that you've created.

The pipeline running the job uses the job name as a preﬁx. For example, if you've created a job

named housing-data-feature-enginnering, the name of the pipeline is canvas-data-

prep-housing-data-feature-engineering.

4.
Choose the pipeline containing your job.

5.
View the status of the pipelines. Pipelines with a Status of Succeeded have run the processing
job successfully.

To stop the processing job from running, do the following:

To stop a processing job from running, delete the event rule that speciﬁes the schedule. Deleting
an event rule stops all the jobs associated with the schedule from running. For information about
deleting a rule, see Disabling or deleting an Amazon EventBridge rule.

You can stop and delete the pipelines associated with the schedules as well. For information about
stopping a pipeline, see StopPipelineExecution. For information about deleting a pipeline, see
DeletePipeline.

Automate data preparation in SageMaker Canvas

After you transform your data in data ﬂow, you can export the transforms to your machine
learning workﬂows. When you export your transforms, SageMaker Canvas creates a Jupyter
notebook. You must run the notebook within Amazon SageMaker Studio Classic. For information
about getting started with Studio Classic, contact your administrator.

Automate data preparation using Pipelines

When you want to build and deploy large-scale machine learning (ML) workﬂows, you can use
Pipelines to create workﬂows that manage and deploy SageMaker AI jobs. With Pipelines, you
can build workﬂows that manage your SageMaker AI data preparation, model training, and
model deployment jobs. You can use the ﬁrst-party algorithms that SageMaker AI oﬀers by using
Pipelines. For more information on Pipelines, see SageMaker Pipelines.

When you export one or more steps from your data ﬂow to Pipelines, Data Wrangler creates a
Jupyter notebook that you can use to deﬁne, instantiate, run, and manage a pipeline.

Data preparation
1195

## Page 225

Amazon SageMaker AI
Developer Guide

Use a Jupyter Notebook to Create a Pipeline

Use the following procedure to create a Jupyter notebook to export your Data Wrangler ﬂow to
Pipelines.

Use the following procedure to generate a Jupyter notebook and run it to export your Data
Wrangler ﬂow to Pipelines.

1.
Choose the + next to the node that you want to export.

2.
Choose Export data ﬂow.

3.
Choose Pipelines (via Jupyter Notebook).

4.
Download the Jupyter notebook or copy it to an Amazon S3 location. We recommend
copying it to an Amazon S3 location that you can access within Studio Classic. Contact your
administrator if you need guidance on a suitable location.

5.
Run the Jupyter notebook.

You can use the Jupyter notebook that Data Wrangler produces to deﬁne a pipeline. The pipeline
includes the data processing steps that are deﬁned by your Data Wrangler ﬂow.

You can add additional steps to your pipeline by adding steps to the steps list in the following
code in the notebook:

pipeline = Pipeline(
name=pipeline_name,
parameters=[instance_type, instance_count],
steps=[step_process], #Add more steps to this list to run in your Pipeline
)

For more information on deﬁning pipelines, see Deﬁne SageMaker AI Pipeline.

Automate data preparation using an inference endpoint

Use your Data Wrangler ﬂow to process data at the time of inference by creating a SageMaker
AI serial inference pipeline from your Data Wrangler ﬂow. An inference pipeline is a series of
steps that results in a trained model making predictions on new data. A serial inference pipeline
within Data Wrangler transforms the raw data and provides it to the machine learning model for
a prediction. You create, run, and manage the inference pipeline from a Jupyter notebook within
Studio Classic. For more information about accessing the notebook, see Use a Jupyter notebook to
create an inference endpoint.

Data preparation
1196

## Page 226

Amazon SageMaker AI
Developer Guide

Within the notebook, you can either train a machine learning model or specify one that you've
already trained. You can either use Amazon SageMaker Autopilot or XGBoost to train the model
using the data that you've transformed in your Data Wrangler ﬂow.

The pipeline provides the ability to perform either batch or real-time inference. You can also add
the Data Wrangler ﬂow to SageMaker Model Registry. For more information about hosting models,
see Multi-model endpoints.

Important

You can't export your Data Wrangler ﬂow to an inference endpoint if it has the following
transformations:

• Join

• Concatenate

• Group by

If you must use the preceding transforms to prepare your data, use the following
procedure.

To prepare your data for inference with unsupported transforms

1.
Create a Data Wrangler ﬂow.

2.
Apply the preceding transforms that aren't supported.

3.
Export the data to an Amazon S3 bucket.

4.
Create a separate Data Wrangler ﬂow.

5.
Import the data that you've exported from the preceding ﬂow.

6.
Apply the remaining transforms.

7.
Create a serial inference pipeline using the Jupyter notebook that we provide.

For information about exporting your data to an Amazon S3 bucket see Export data.
For information about opening the Jupyter notebook used to create the serial inference
pipeline, see Use a Jupyter notebook to create an inference endpoint.

Data preparation
1197

## Page 227

Amazon SageMaker AI
Developer Guide

Data Wrangler ignores transforms that remove data at the time of inference. For example, Data
Wrangler ignores the Handle Missing Values transform if you use the Drop missing conﬁguration.

If you've reﬁt transforms to your entire dataset, the transforms carry over to your inference
pipeline. For example, if you used the median value to impute missing values, the median
value from reﬁtting the transform is applied to your inference requests. You can either reﬁt the
transforms from your Data Wrangler ﬂow when you're using the Jupyter notebook or when you're

exporting your data to an inference pipeline. .

The serial inference pipeline supports the following data types for the input and output strings.
Each data type has a set of requirements.

Supported datatypes

• text/csv – the datatype for CSV strings

• The string can't have a header.

• Features used for the inference pipeline must be in the same order as features in the training
dataset.

• There must be a comma delimiter between features.

• Records must be delimited by a newline character.

The following is an example of a validly formatted CSV string that you can provide in an
inference request.

abc,0.0,"Doe, John",12345\ndef,1.1,"Doe, Jane",67890

• application/json – the datatype for JSON strings

• The features used in the dataset for the inference pipeline must be in the same order as the
features in the training dataset.

• The data must have a speciﬁc schema. You deﬁne schema as a single instances object that

has a set of features. Each features object represents an observation.

The following is an example of a validly formatted JSON string that you can provide in an
inference request.

{
"instances": [

Data preparation
1198

## Page 228

Amazon SageMaker AI
Developer Guide

{
"features": ["abc", 0.0, "Doe, John", 12345]
},
{
"features": ["def", 1.1, "Doe, Jane", 67890]
}
]
}

Use a Jupyter notebook to create an inference endpoint

Use the following procedure to export your Data Wrangler ﬂow to create an inference pipeline.

To create an inference pipeline using a Jupyter notebook, do the following.

1.
Choose the + next to the node that you want to export.

2.
Choose Export data ﬂow.

3.
Choose SageMaker AI Inference Pipeline (via Jupyter Notebook).

4.
Download the Jupyter notebook or copy it to an Amazon S3 location. We recommend
copying it to an Amazon S3 location that you can access within Studio Classic. Contact your
administrator if you need guidance on a suitable location.

5.
Run the Jupyter notebook.

When you run the Jupyter notebook, it creates an inference ﬂow artifact. An inference ﬂow artifact
is a Data Wrangler ﬂow ﬁle with additional metadata used to create the serial inference pipeline.
The node that you're exporting encompasses all of the transforms from the preceding nodes.

Important

Data Wrangler needs the inference ﬂow artifact to run the inference pipeline. You can't use
your own ﬂow ﬁle as the artifact. You must create it by using the preceding procedure.

Automate data preparation using Python Code

To export all steps in your data ﬂow to a Python ﬁle that you can manually integrate into any data
processing workﬂow, use the following procedure.

Data preparation
1199

## Page 229

Amazon SageMaker AI
Developer Guide

Use the following procedure to generate a Jupyter notebook and run it to export your Data
Wrangler ﬂow to Python code.

1.
Choose the + next to the node that you want to export.

2.
Choose Export data ﬂow.

3.
Choose Python Code.

4.
Download the Jupyter notebook or copy it to an Amazon S3 location. We recommend
copying it to an Amazon S3 location that you can access within Studio Classic. Contact your
administrator if you need guidance on a suitable location.

5.
Run the Jupyter notebook.

You might need to conﬁgure the Python script to make it run in your pipeline. For example,
if you're running a Spark environment, make sure that you are running the script from an
environment that has permission to access AWS resources.

Generative AI foundation models in SageMaker Canvas

Amazon SageMaker Canvas provides generative AI foundation models that you can use to start
conversational chats. These content generation models are trained on large amounts of text data
to learn the statistical patterns and relationships between words, and they can produce coherent
text that is statistically similar to the text on which they were trained. You can use this capability to
increase your productivity by doing the following:

• Generate content, such as document outlines, reports, and blogs

• Summarize text from large corpuses of text, such as earnings call transcripts, annual reports, or
chapters of user manuals

• Extract insights and key takeaways from large passages of text, such as meeting notes or
narratives

• Improve text and catch grammatical errors or typos

The foundation models are a combination of Amazon SageMaker JumpStart and Amazon Bedrock
large language models (LLMs). Canvas oﬀers the following models:

Generative AI foundation models
1200

## Page 230

Amazon SageMaker AI
Developer Guide

Model
Type
Description

Amazon Titan
Amazon Bedrock model
Amazon Titan is a powerful,
general-purpose language

model that you can use for
tasks such as summariza
tion, text generation (such
as creating a blog post),
classiﬁcation, open-ende
d Q&A, and information
extraction. It is pretrained
on large datasets, making it
suitable for complex tasks
and reasoning. To continue
supporting best practices
in the responsible use of AI,
Amazon Titan foundation
models are built to detect
and remove harmful content
in the data, reject inappropr
iate content in the user input,
and ﬁlter model outputs
that contain inappropriate
content (such as hate speech,
profanity, and violence).

Anthropic Claude Instant
Amazon Bedrock model
Anthropic's Claude Instant is
a faster and more cost-eﬀe
ctive yet still very capable
model. This model can
handle a range of tasks
including casual dialogue,
text analysis, summariza
tion, and document question
answering. Just like Claude-2,
Claude Instant can support

Generative AI foundation models
1201

## Page 231

Amazon SageMaker AI
Developer Guide

Model
Type
Description

up to 100,000 tokens in each
prompt, equivalent to about
200 pages of information.

Anthropic Claude-2
Amazon Bedrock model
Claude-2 is Anthropic's most
powerful model, which excels
at a wide range of tasks from
sophisticated dialogue and
creative content generatio
n to detailed instruction
following. Claude-2 can take
up to 100,000 tokens in
each prompt, equivalent to
about 200 pages of informati
on. It can generate longer
responses compared to its
prior version. It supports
use cases such as question
answering, information
extraction, removing PII,
content generation, multiple-
choice classiﬁcation, roleplay,
comparing text, summariza
tion, and document Q&A with
citation.

Generative AI foundation models
1202

## Page 232

Amazon SageMaker AI
Developer Guide

Model
Type
Description

Falcon-7B-Instruct
JumpStart model
Falcon-7B-Instruct has 7
billion parameters and was
ﬁne-tuned on a mixture of
chat and instruct datasets.
It is suitable as a virtual
assistant and performs best
when following instructions
or engaging in conversat
ion. Since the model was
trained on large amounts of
English-language web data,
it carries the stereotypes
and biases commonly found
online and is not suitable for
languages other than English.
Compared to Falcon-40B-
Instruct, Falcon-7B-Instruct
is a slightly smaller and more
compact model.

Generative AI foundation models
1203

## Page 233

Amazon SageMaker AI
Developer Guide

Model
Type
Description

Falcon-40B-Instruct
JumpStart model
Falcon-40B-Instruct has 40
billion parameters and was
ﬁne-tuned on a mixture of
chat and instruct datasets.
It is suitable as a virtual
assistant and performs best
when following instructions
or engaging in conversat
ion. Since the model was
trained on large amounts of
English-language web data,
it carries the stereotypes
and biases commonly found
online and is not suitable for
languages other than English.
Compared to Falcon-7B-
Instruct, Falcon-40B-Instruc
t is a slightly larger and more
powerful model.

Generative AI foundation models
1204

## Page 234

Amazon SageMaker AI
Developer Guide

Model
Type
Description

Jurassic-2 Mid
Amazon Bedrock model
Jurassic-2 Mid is a high-perf
ormance text generation
model trained on a massive
corpus of text (current up
to mid 2022). It is highly
versatile, general-purpose,
and capable of composing
human-like text and solving
complex tasks such as
question answering, text
classiﬁcation, and many
others. This model oﬀers
zero-shot instruction capabilit
ies, allowing it to be directed
with only natural language
and without the use of
examples. It performs up to
30% faster than its predecess
or, the Jurassic-1 model.

Jurassic-2 Mid is AI21’s
mid-sized model, carefully
designed to strike the right
balance between exceptional
quality and aﬀordability.

Generative AI foundation models
1205

## Page 235

Amazon SageMaker AI
Developer Guide

Model
Type
Description

Jurassic-2 Ultra
Amazon Bedrock model
Jurassic-2 Ultra is a high-
performance text generatio
n model trained on a massive
corpus of text (current up
to mid 2022). It is highly
versatile, general-purpose,
and capable of composing
human-like text and solving
complex tasks such as
question answering, text
classiﬁcation, and many
others. This model oﬀers
zero-shot instruction capabilit
ies, allowing it to be directed
with only natural language
and without the use of
examples. It performs up to
30% faster than its predecess
or, the Jurassic-1 model.

Compared to Jurassic-2 Mid,
Jurassic-2 Ultra is a slightly
larger and more powerful
model.

Generative AI foundation models
1206

## Page 236

Amazon SageMaker AI
Developer Guide

Model
Type
Description

Llama-2-7b-Chat
JumpStart model
Llama-2-7b-Chat is a
foundation model by Meta
that is suitable for engaging
in meaningful and coherent
conversations, generating
new content, and extractin
g answers from existing
notes. Since the model was
trained on large amounts of
English-language internet
data, it carries the biases and
limitations commonly found
online and is best-suited for
tasks in English.

Generative AI foundation models
1207

## Page 237

Amazon SageMaker AI
Developer Guide

Model
Type
Description

Llama-2-13B-Chat
Amazon Bedrock model
Llama-2-13B-Chat by Meta
was ﬁne-tuned on conversat
ional data after initial
training on internet data.
It is optimized for natural
dialog and engaging chat
abilities, making it well-suit
ed as a conversational agent.
Compared to the smaller
Llama-2-7b-Chat, Llama-2-1
3B-Chat has nearly twice as
many parameters, allowing
it to remember more context
and produce more nuanced
conversational responses. Like
Llama-2-7b-Chat, Llama-2-1
3B-Chat was trained on
English-language data and
is best-suited for tasks in
English.

Generative AI foundation models
1208

## Page 238

Amazon SageMaker AI
Developer Guide

Model
Type
Description

Llama-2-70B-Chat
Amazon Bedrock model
Like Llama-2-7b-Chat and
Llama-2-13B-Chat, the
Llama-2-70B-Chat model
by Meta is optimized for
engaging in natural and
meaningful dialog. With 70
billion parameters, this large
conversational model can
remember more extensive
context and produce highly
coherent responses when
compared to the more
compact model versions.
However, this comes at the
cost of slower responses and
higher resource requireme
nts. Llama-2-70B-Chat was
trained on large amounts of
English-language internet
data and is best-suited for
tasks in English.

Generative AI foundation models
1209

## Page 239

Amazon SageMaker AI
Developer Guide

Model
Type
Description

Mistral-7B
JumpStart model
Mistral-7B by Mistral.AI is an
excellent general purpose
language model suitable
for a wide range of natural
language (NLP) tasks like
text generation, summariza
tion, and question answering
. It utilizes grouped-query
attention (GQA) which
allows for faster inference
speeds, making it perform
comparably to models with
twice or three times as many
parameters. It was trained
on a mixture of text data
including books, websites,
and scientiﬁc papers in the
English language, so it is best-
suited for tasks in English.

Generative AI foundation models
1210

## Page 240

Amazon SageMaker AI
Developer Guide

Model
Type
Description

Mistral-7B-Chat
JumpStart model
Mistral-7B-Chat is a conversat
ional model by Mistral.AI
based on Mistral-7B. While
Mistral-7B is best for general
NLP tasks, Mistral-7B-Chat
has been further ﬁne-tune
d on conversational data
to optimize its abilities for
natural, engaging chat. As
a result, Mistral-7B-Chat
generates more human-like
responses and remembers the
context of previous responses
. Like Mistral-7B, this model
is best-suited for English
language tasks.

MPT-7B-Instruct
JumpStart model
MPT-7B-Instruct is a model
for long-form instruction
following tasks and can
assist you with writing tasks
including text summarization
and question-answering to
save you time and eﬀort. This
model was trained on large
amounts of ﬁne-tuned data
and can handle larger inputs,
such as complex documents
. Use this model when you
want to process large bodies
of text or want the model to
generate long responses.

Generative AI foundation models
1211

## Page 241

Amazon SageMaker AI
Developer Guide

The foundation models from Amazon Bedrock are currently only available in the US East (N.
Virginia) and US West (Oregon) Regions. Additionally, when using foundation models from Amazon
Bedrock, you are charged based on the volume of input tokens and output tokens, as speciﬁed by
each model provider. For more information, see the Amazon Bedrock pricing page. The JumpStart
foundation models are deployed on SageMaker AI Hosting instances, and you are charged for
the duration of usage based on the instance type used. For more information about the cost of
diﬀerent instance types, see the Amazon SageMaker AI Hosting: Real-Time Inference section on the
SageMaker pricing page.

Document querying is an additional feature that you can use to query and get insights from
documents stored in indexes using Amazon Kendra. With this functionality, you can generate
content from the context of those documents and receive responses that are speciﬁc to your
business use case, as opposed to responses that are generic to the large amounts of data on which
the foundation models were trained. For more information about indexes in Amazon Kendra, see
the Amazon Kendra Developer Guide.

If you would like to get responses from any of the foundation models that are customized to your
data and use case, you can ﬁne-tune foundation models. To learn more, see Fine-tune foundation
models.

If you'd like to get predictions from an Amazon SageMaker JumpStart foundation model through
an application or website, you can deploy the model to a SageMaker AI endpoint. SageMaker AI
endpoints host your model, and you can send requests to the endpoint through your application
code to receive predictions from the model. For more information, see Deploy your models to an
endpoint.

Complete the prerequisites for foundation models in SageMaker Canvas

The following sections outline the prerequisites for interacting with foundation models and using
the document query feature in Canvas. The rest of the content on this page assumes that you’ve
met the prerequisites for foundation models. The document query feature requires additional
permissions.

Prerequisites for foundation models

The permissions you need for interacting with models are included in the Canvas Ready-
to-use models permissions. To use the generative AI-powered models in Canvas, you must
turn on the Canvas Ready-to-use models conﬁguration permissions when setting up
your Amazon SageMaker AI domain. For more information, see Prerequisites for setting up
Amazon SageMaker Canvas. The Canvas Ready-to-use models conﬁguration attaches the

Generative AI foundation models
1212

## Page 242

Amazon SageMaker AI
Developer Guide

AmazonSageMakerCanvasAIServicesAccess policy to your Canvas user's AWS Identity and Access
Management (IAM) execution role. If you encounter any issues with granting permissions, see the
topic Troubleshooting issues with granting permissions through the SageMaker AI console.

If you’ve already set up your domain, you can edit your domain settings and turn on the
permissions. For instructions on how to edit your domain settings, see Edit domain settings. When
editing the settings for your domain, go to the Canvas settings and turn on the Enable Canvas
Ready-to-use models option.

Certain JumpStart foundation models also require that you request a SageMaker AI instance quota
increase. Canvas hosts the models that you’re currently interacting with on these instances, but the
default quota for your account may be insuﬃcient. If you run into an error while running any of the
following models, request a quota increase for the associated instance types:

• Falcon-40B – ml.g5.12xlarge, ml.g5.24xlarge

• Falcon-13B – ml.g5.2xlarge, ml.g5.4xlarge, ml.g5.8xlarge

• MPT-7B-Instruct – ml.g5.2xlarge, ml.g5.4xlarge, ml.g5.8xlarge

For the preceding instances types, request an increase from 0 to 1 for the endpoint usage quota.
For more information about how to increase an instance quota for your account, see Requesting a
quota increase in the Service Quotas User Guide.

Prerequisites for document querying

Note

Document querying is supported in the following AWS Regions: US East (N. Virginia),
US East (Ohio), US West (Oregon), Europe (Ireland), Asia Paciﬁc (Singapore), Asia Paciﬁc
(Sydney), Asia Paciﬁc (Tokyo), and Asia Paciﬁc (Mumbai).

The document querying feature requires that you already have an Amazon Kendra index that stores
your documents and document metadata. For more information about Amazon Kendra, see the
Amazon Kendra Developer Guide. To learn more about the quotas for querying indexes, see Quotas
in the Amazon Kendra Developer Guide.

You must also make sure that your Canvas user proﬁle has the necessary permissions for document
querying. The AmazonSageMakerCanvasFullAccess policy must be attached to the AWS IAM

Generative AI foundation models
1213

## Page 243

Amazon SageMaker AI
Developer Guide

execution role for the SageMaker AI domain that hosts your Canvas application (this policy is
attached by default to all new and existing Canvas user proﬁles). You must also speciﬁcally grant
document querying permissions and specify access to one or more Amazon Kendra indexes.

If your Canvas administrator is setting up a new domain or user proﬁle, have them set up the
domain by following the instructions in Prerequisites for setting up Amazon SageMaker Canvas.
While setting up the domain, they can turn on the document querying permissions through the
Canvas Ready-to-use models conﬁguration.

The Canvas administrator can manage document querying permissions at the user proﬁle level as
well. For example, if the administrator wants to grant document querying permissions to some
user proﬁles but remove permissions for others, they can edit the permissions for a speciﬁc user.

The following procedure shows how to turn on document querying permissions for a speciﬁc user
proﬁle:

1.
Open the SageMaker AI console at https://console.aws.amazon.com/sagemaker/.

2.
On the left navigation pane, choose Admin conﬁgurations.

3.
Under Admin conﬁgurations, choose domains.

4.
From the list of domains, select the user proﬁle’s domain.

5.
On the domain details page, choose the User proﬁle whose permissions you want to edit.

6.
On the User Details page, choose Edit.

7.
In the left navigation pane, choose Canvas settings.

8.
In the Canvas Ready-to-use models conﬁguration section, turn on the Enable document
query using Amazon Kendra toggle.

9.
In the dropdown, select one or more Amazon Kendra indexes to which you want to grant
access.

10. Choose Submit to save the changes to your domain settings.

You should now be able to use Canvas foundation models to query documents in the speciﬁed
Amazon Kendra indexes.

Start a new conversation to generate, extract, or summarize content

To get started with generative AI foundation models in Canvas, you can initiate a new chat session
with one of the models. For JumpStart models, you are charged while the model is active, so

Generative AI foundation models
1214

## Page 244

Amazon SageMaker AI
Developer Guide

you must start up models when you want to use them and shut them down when you are done
interacting. If you do not shut down a JumpStart model, Canvas shuts it down after 2 hours of
inactivity. For Amazon Bedrock models (such as Amazon Titan), you are charged by prompt; the
models are already active and don’t need to be started up or shut down. You are charged directly
for use of these models by Amazon Bedrock.

To open a chat with a model, do the following:

1.
Open the SageMaker Canvas application.

2.
In the left navigation pane, choose Ready-to-use models.

3.
Choose Generate, extract and summarize content.

4.
On the welcome page, you’ll receive a recommendation to start up the default model. You can
start the recommended model, or you can choose Select another model from the dropdown
to choose a diﬀerent one.

5.
If you selected a JumpStart foundation model, you have to start it up before it is available for
use. Choose Start up the model, and then the model is deployed to a SageMaker AI instance.
It might take several minutes for this to complete. When the model is ready, you can enter
prompts and ask the model questions.

If you selected a foundation model from Amazon Bedrock, you can start using it instantly by
entering a prompt and asking questions.

Depending on the model, you can perform various tasks. For example, you can enter a passage
of text and ask the model to summarize it. Or, you can ask the model to come up with a short
summary of the market trends in your domain.

The model’s responses in a chat are based on the context of your previous prompts. If you want to
ask a new question in the chat that is unrelated to the previous conversation topic, we recommend
that you start a new chat with the model.

Extract information from documents with document querying

Note

This section assumes that you’ve completed the section above Prerequisites for document
querying.

Generative AI foundation models
1215

## Page 245

Amazon SageMaker AI
Developer Guide

Document querying is a feature that you can use while interacting with foundation models in
Canvas. With document querying, you can access a corpus of documents stored in an Amazon
Kendra index, which holds the contents of your documents and is structured in a way to make
documents searchable. You can ask speciﬁc questions that are targeted to the data in your Amazon
Kendra index, and the foundation model returns answers to your questions. For example, you can
query an internal knowledge base of IT information and ask questions such as “How do I connect to
my company’s network?” For more information about setting up an index, see the Amazon Kendra
Developer Guide.

When using the document query feature, the foundation models restrict their responses to the
content of the documents in your index with a technique called Retrieval Augmented Generation
(RAG). This technique bundles the most relevant information from the index along with the user's
prompt and sends it to the foundation model to get a response. Responses are limited to what
can be found in your index, preventing the model from giving you incorrect responses based on
external data. For more information about this process, see the blog post Quickly build high-
accuracy Generative AI applications on enterprise data.

To get started, in a chat with a foundation model in Canvas, turn on the Document query toggle at
the top of the page. From the dropdown, select the Amazon Kendra index that you want to query.
Then, you can begin asking questions related to the documents in your index.

Important

Document querying supports the Compare model outputs feature. Any existing chat history
is overwritten when you start a new chat to compare model outputs.

Start up models

Note

The following section describe starting up models, which only applies to the JumpStart
foundation models, such as Falcon-40B-Instruct. You can access Amazon Bedrock models,
such as Amazon Titan, instantly at any time.

Generative AI foundation models
1216

## Page 246

Amazon SageMaker AI
Developer Guide

You can start up as many JumpStart models as you like. Each active JumpStart model incurs
charges on your account, so we recommend that you don’t start up more models than you are
currently using.

To start up another model, you can do the following:

1.
On the Generate, extract and summarize content page, choose New chat.

2.
Choose the model from the dropdown menu. If you want to choose a model not displayed in
the dropdown, choose Start up another model, and then select the model that you want to
start up.

3.
Choose Start up model.

The model should begin starting up, and within a few minutes you can chat with the model.

Shut down models

We highly recommend that you shut down models that you aren’t using. The models automatically
shut down after 2 hours of inactivity. However, to manually shut down a model, you can do the
following:

1.
On the Generate, extract and summarize content page, open the chat for the model that you
want to shut down.

2.
On the chat page, choose the More options icon
(

).

3.
Choose Shut down model.

4.
In the Shut down model conﬁrmation box, choose Shut down.

The model begins shutting down. If your chat compares two or more models, you can shut
down an individual model from the chat page by choosing the model’s More options icon
(

)
and then choosing Shut down model.

Compare model outputs

You might want to compare the output of diﬀerent models side by side to see which model output
you prefer. This can help you decide which model is best suited to your use case. You can compare
up to three models in chats.

Generative AI foundation models
1217

## Page 247

Amazon SageMaker AI
Developer Guide

Note

Each individual model incurs charges on your account.

You must start a new chat to add models for comparison. To compare the output of models side by
side in a chat, do the following:

1.
In a chat, choose New chat.

2.
Choose Compare, and use the dropdown menu to select the model that you want to add. To
add a third model, choose Compare again to add another model.

Note

If you want to use a JumpStart model that isn’t currently active, you are prompted to
start up the model.

When the models are active, you see the two models side by side in the chat. You can submit your
prompt, and each model responds in the same chat, as shown in the following screenshot.

Generative AI foundation models
1218

## Page 248

Amazon SageMaker AI
Developer Guide

![Page 248 Diagram 1](images/page-0248-img-01.png)

When you’re done interacting, make sure to shut down any JumpStart models individually to avoid
incurring further charges.

Fine-tune foundation models

The foundation models that you can access through Amazon SageMaker Canvas can help you
with a range of general purpose tasks. However, if you have a speciﬁc use case and would like to
customized responses based on your own data, you can ﬁne-tune a foundation model.

To ﬁne-tune a foundation model, you provide a dataset that consists of sample prompts and model
responses. Then, you train the foundation model on the data. Finally, the ﬁne-tuned foundation
model is able to provide you with more speciﬁc responses.

The following list contains the foundation models that you can ﬁne-tune in Canvas:

• Titan Express

• Falcon-7B

• Falcon-7B-Instruct

• Falcon-40B-Instruct

Generative AI foundation models
1219

## Page 249

Amazon SageMaker AI
Developer Guide

• Falcon-40B

• Flan-T5-Large

• Flan-T5-Xl

• Flan-T5-Xxl

• MPT-7B

• MPT-7B-Instruct

You can access more detailed information about each foundation model in the Canvas application
while ﬁne-tuning a model. For more information, see Fine-tune the model.

This topic describes how to ﬁne-tune foundation models in Canvas.

Before you begin

Before ﬁne-tuning a foundation model, make sure that you have the permissions for Ready-to-
use models in Canvas and an AWS Identity and Access Management execution role that has a trust
relationship with Amazon Bedrock, which allows Amazon Bedrock to assume your role while ﬁne-
tuning foundation models.

While setting up or editing your Amazon SageMaker AI domain, you must 1) turn on the Canvas
Ready-to-use models conﬁguration permissions, and 2) create or specify an Amazon Bedrock role,
which is an IAM execution role to which SageMaker AI attaches a trust relationship with Amazon
Bedrock. For more information about conﬁguring these settings, see Prerequisites for setting up
Amazon SageMaker Canvas.

You can conﬁgure the Amazon Bedrock role manually if you would rather use your own IAM
execution role (instead of letting SageMaker AI create one on your behalf). For more information
about conﬁguring your own IAM execution role’s trust relationship with Amazon Bedrock, see Grant
Users Permissions to Use Amazon Bedrock and Generative AI Features in Canvas.

You must also have a dataset that is formatted for ﬁne-tuning large language models (LLMs). The
following is a list of requirements for your dataset:

• The dataset must be tabular and contain at least two columns of text data–one input column
(which contains example prompts to the model) and one output column (which contains example
responses from the model).

An example is the following:

Generative AI foundation models
1220

## Page 250

Amazon SageMaker AI
Developer Guide

Input
Output

What are your shipping terms?
We oﬀer free shipping on all orders over $50.
Orders under $50 have a shipping fee of

$5.99.

How can I return an item?
To return an item, please visit our returns
center and follow the instructions. You must
provide your order number and the reason
for the return.

I'm having trouble with my product. What
can I do?

Please contact our customer support team
and we will be happy to help you troublesh
oot the issue.

• We recommend that the dataset has at least 100 text pairs (rows of corresponding input and
output items). This ensures that the foundation model has enough data for ﬁne-tuning and
increases the accuracy of its responses.

• Each input and output item should contain a maximum of 512 characters. Anything longer is
reduced to 512 characters when ﬁne-tuning the foundation model.

When ﬁne-tuning an Amazon Bedrock model, you must adhere to the Amazon Bedrock quotas. For
more information, see Model customization quotas in the Amazon Bedrock User Guide.

For more information about general dataset requirements and limitations in Canvas, see Create a
dataset.

Fine-tune a foundation model

You can ﬁne-tune a foundation model by using any of the following methods in the Canvas
application:

• While in a Generate, extract and summarize content chat with a foundation model, choose the
Fine-tune model icon
(

).

Generative AI foundation models
1221

## Page 251

Amazon SageMaker AI
Developer Guide

• While in a chat with a foundation model, if you’ve re-generated the response two or more times,
then Canvas oﬀers you the option to Fine-tune model. The following screenshot shows you what
this looks like.

• On the My models page, you can create a new model by choosing New model, and then select
Fine-tune foundation model.

• On the Ready-to-use models home page, you can choose Create your own model, and then in
the Create new model dialog box, choose Fine-tune foundation model.

• While browsing your datasets in the Data Wrangler tab, you can select a dataset and choose
Create a model. Then, choose Fine-tune foundation model.

After you’ve begun to ﬁne-tune a model, do the following:

Select a dataset

On the Select tab of ﬁne-tuning a model, you choose the data on which you’d like to train the
foundation model.

Either select an existing dataset or create a new dataset that meets the requirements listed in
the Before you begin section. For more information about how to create a dataset, see Create a
dataset.

When you’ve selected or created a dataset and you’re ready to move on, choose Select dataset.

Fine-tune the model

After selecting your data, you’re now ready to begin training and ﬁne-tune the model.

On the Fine-tune tab, do the following:

1.
(Optional) Choose Learn more about our foundation models to access more information
about each model and help you decide which foundation model or models to deploy.

2.
For Select up to 3 base models, open the dropdown menu and check up to 3 foundation
models (up to 2 JumpStart models and 1 Amazon Bedrock model) that you’d like to ﬁne-
tune during the training job. By ﬁne-tuning multiple foundation models, you can compare

Generative AI foundation models
1222

## Page 252

Amazon SageMaker AI
Developer Guide

their performance and ultimately choose the one best suited to your use case as the default
model. For more information about default models, see View model candidates in the model
leaderboard.

3.
For Select Input column, select the column of text data in your dataset that contains the
example model prompts.

4.
For Select Output column, select the column of text data in your dataset that contains the
example model responses.

5.
(Optional) To conﬁgure advanced settings for the training job, choose Conﬁgure model. For
more information about the advanced model building settings, see Advanced model building
conﬁgurations.

In the Conﬁgure model pop-up window, do the following:

a.
For Hyperparameters, you can adjust the Epoch count, Batch size, Learning rate, and
Learning rate warmup steps for each model you selected. For more information about
these parameters, see the  Hyperparameters section in the JumpStart documentation.

b.
For Data split, you can specify percentages for how to divide your data between the
Training set and Validation set.

c.
For Max job runtime, you can set the maximum amount of time that Canvas runs the
build job. This feature is only available for JumpStart foundation models.

d.
After conﬁguring the settings, choose Save.

6.
Choose Fine-tune to begin training the foundation models you selected.

After the ﬁne-tuning job begins, you can leave the page. When the model shows as Ready on the
My models page, it’s ready for use, and you can now analyze the performance of your ﬁne-tuned
foundation model.

Analyze the ﬁne-tuned foundation model

On the Analyze tab of your ﬁne-tuned foundation model, you can see the model’s performance.

The Overview tab on this page shows you the perplexity and loss scores, along with analyses that
visualize the model’s improvement over time during training. The following screenshot shows the
Overview tab.

Generative AI foundation models
1223

## Page 253

Amazon SageMaker AI
Developer Guide

![Page 253 Diagram 1](images/page-0253-img-01.png)

On this page, you can see the following visualizations:

• The Perplexity Curve measures how well the model predicts the next word in a sequence, or
how grammatical the model’s output is. Ideally, as the model improves during training, the score
decreases and results in a curve that lowers and ﬂattens over time.

• The Loss Curve quantiﬁes the diﬀerence between the correct output and the model’s predicted
output. A loss curve that decreases and ﬂattens over time indicates that the model is improving
its ability to make accurate predictions.

The Advanced metrics tab shows you the hyperparameters and additional metrics for your model.
It looks like the following screenshot:

Generative AI foundation models
1224

## Page 254

Amazon SageMaker AI
Developer Guide

![Page 254 Diagram 1](images/page-0254-img-01.png)

The Advanced metrics tab contains the following information:

• The Explainability section contains the Hyperparameters, which are the values set before
the job to guide the model’s ﬁne-tuning. If you didn’t specify custom hyperparameters in the
model’s advanced settings in the Fine-tune the model section, then Canvas selects default
hyperparameters for you.

For JumpStart models, you can also see the advanced metric ROUGE (Recall-Oriented
Understudy for Gisting Evaluation), which evaluates the quality of summaries generated by the
model. It measures how well the model can summarize the main points of a passage.

• The Artifacts section provides you with links to artifacts generated during the ﬁne-tuning job.
You can access the training and validation data saved in Amazon S3, as well as the link to the
model evaluation report (to learn more, see the following paragraph).

To get more model evaluation insights, you can download a report that is generated using
SageMaker Clarify, which is a feature that can help you detect bias in your model and data. First,
generate the report by choosing Generate evaluation report at the bottom of the page. After
the report has generated, you can download the full report by choosing Download report or by
returning to the Artifacts section.

Generative AI foundation models
1225

## Page 255

Amazon SageMaker AI
Developer Guide

You can also access a Jupyter notebook that shows you how to replicate your ﬁne-tuning job in
Python code. You can use this to replicate or make programmatic changes to your ﬁne-tuning job
or get a deeper understanding of how Canvas ﬁne-tunes your model. To learn more about model
notebooks and how to access them, see Download a model notebook.

For more information about how to interpret the information in the Analyze tab of your ﬁne-tuned
foundation model, see the topic Model evaluation.

After analyzing the Overview and Advanced metrics tabs, you can also choose to open the Model
leaderboard, which shows you the list of the base models trained during the build. The model
with the lowest loss score is considered the best performing model and is selected as the Default
model, which is the model whose analysis you see in the Analyze tab. You can only test and deploy
the default model. For more information about the model leaderboard and how to change the
default model, see View model candidates in the model leaderboard.

Test a ﬁne-tuned foundation model in a chat

After analyzing the performance of a ﬁne-tuned foundation model, you might want to test it out
or compare its responses with the base model. You can test a ﬁne-tuned foundation model in a
chat in the Generate, extract and summarize content feature.

Start a chat with a ﬁne-tuned model by choosing one of the following methods:

• On the ﬁne-tuned model’s Analyze tab, choose Test in Ready-to-use foundation models.

• On the Canvas Ready-to-use models page, choose Generate, extract and summarize content.
Then, choose New chat and select the version of the model that you want to test.

The model starts up in a chat, and you can interact with it like any other foundation model. You
can add more models to the chat and compare their outputs. For more information about the
functionality of chats, see Generative AI foundation models in SageMaker Canvas.

Operationalize ﬁne-tuned foundation models

After ﬁne-tuning your model in Canvas, you can do the following:

• Register the model to the SageMaker Model Registry for integration into your organizations
MLOps processes. For more information, see Register a model version in the SageMaker AI model
registry.

Generative AI foundation models
1226

## Page 256

Amazon SageMaker AI
Developer Guide

• Deploy the model to a SageMaker AI endpoint and send requests to the model from your
application or website to get predictions (or inference). For more information, see Deploy your
models to an endpoint.

Important

You can only register and deploy JumpStart based ﬁne-tuned foundation models, not
Amazon Bedrock based models.

Ready-to-use models

With Amazon SageMaker Canvas Ready-to-use models, you can make predictions on your data
without writing a single line of code or having to build a model—all you have to bring is your data.
The Ready-to-use models use pre-built models to generate predictions without requiring you to
spend the time, expertise, or cost required to build a model, and you can choose from a variety of
use cases ranging from language detection to expense analysis.

Canvas integrates with existing AWS services, such as Amazon Textract, Amazon Rekognition,
and Amazon Comprehend, to analyze your data and make predictions or extract insights. You can
use the predictive power of these services from within the Canvas application to get high quality
predictions for your data.

Canvas supports the following Ready-to-use models types:

Ready-to-use model
Description
Supported data type

Sentiment analysis
Detect sentiment in lines of
text, which can be positive,
negative, neutral, or mixed.
Currently, you can only do
sentiment analysis for English
language text.

Plain text or tabular (CSV,
Parquet)

Entities extraction
Extract entities, which are
real-world objects such
as people, places, and
commercial items, or units

Plain text or tabular (CSV,
Parquet)

Ready-to-use models
1227

## Page 257

Amazon SageMaker AI
Developer Guide

Ready-to-use model
Description
Supported data type

such as dates and quantities,
from text.

Language detection
Determine the dominant
language in text such as
English, French, or German.

Plain text or tabular (CSV,
Parquet)

Personal information
detection

Detect personal informati
on that could be used to
identify an individual,
such as addresses, bank
account numbers, and phone
numbers, from text.

Plain text or tabular (CSV,
Parquet)

Object detection in images
Detect objects, concepts,
scenes, and actions in your
images.

Image (JPG, PNG)

Text detection in images
Detect text in your images.
Image (JPG, PNG)

Expense analysis
Extract information from
invoices and receipts, such
as date, number, item prices,
total amount, and payment
terms.

Document (PDF, JPG, PNG,
TIFF)

Identity document analysis
Extract information from
passports, driver licenses, and
other identity documentation
issued by the US Government.

Document (PDF, JPG, PNG,
TIFF)

Document analysis
Analyze documents and
forms for relationships
among detected text.

Document (PDF, JPG, PNG,
TIFF)

Ready-to-use models
1228

## Page 258

Amazon SageMaker AI
Developer Guide

Ready-to-use model
Description
Supported data type

Document queries
Extract information from
structured documents such as
paystubs, bank statements,
W-2s, and mortgage applicati
on forms by asking questions
using natural language.

Document (PDF)

Get started

To get started with Ready-to-use models, review the following information.

Prerequisites

To use Ready-to-use models in Canvas, you must turn on the Canvas Ready-to-use models
conﬁguration permissions when setting up your Amazon SageMaker AI domain. The Canvas
Ready-to-use models conﬁguration attaches the AmazonSageMakerCanvasAIServicesAccess
policy to your Canvas user's AWS Identity and Access Management (IAM) execution role. If you
encounter any issues with granting permissions, see the topic Troubleshooting issues with granting
permissions through the SageMaker AI console.

If you’ve already set up your domain, you can edit your domain settings and turn on the
permissions. For instructions on how to edit your domain settings, see Edit domain settings. When
editing the settings for your domain, go to the Canvas settings and turn on the Enable Canvas
Ready-to-use models option.

(Optional) Opt out of AI services data storage

Certain AWS AI services store and use your data to make improvements to the service. You can opt
out of having your data stored or used for service improvements. To learn more about how to opt
out, see  AI services opt-out policies in the AWS Organizations User Guide.

How to use Ready-to-use models

To get started with Ready-to-use models, do the following:

1. (Optional) Import your data. You can import a tabular, image, or document dataset to generate

batch predictions, or a dataset of predictions, with Ready-to-use models. To get started with
importing a dataset, see Create a data ﬂow.

Ready-to-use models
1229

## Page 259

Amazon SageMaker AI
Developer Guide

2. Generate predictions. You can generate single or batch predictions with your chosen Ready-to-

use model. To get started with making predictions, see Make predictions for text data.

Make predictions for text data

The following procedures describe how to make both single and batch predictions for text
datasets. Each Ready-to-use model supports both Single predictions and Batch predictions for
your dataset. A Single prediction is when you only need to make one prediction. For example, you
have one image from which you want to extract text, or one paragraph of text for which you want
to detect the dominant language. A Batch prediction is when you’d like to make predictions for
an entire dataset. For example, you might have a CSV ﬁle of customer reviews for which you’d like
to analyze the customer sentiment, or you might have image ﬁles in which you’d like to detect
objects.

You can use these procedures for the following Ready-to-use model types: sentiment analysis,
entities extraction, language detection, and personal information detection.

Note

For sentiment analysis, you can only use English language text.

Single predictions

To make a single prediction for Ready-to-use models that accept text data, do the following:

1.
In the left navigation pane of the Canvas application, choose Ready-to-use models.

2.
On the Ready-to-use models page, choose the Ready-to-use model for your use case. For text
data, it should be one of the following: Sentiment analysis, Entities extraction, Language
detection, or Personal information detection.

3.
On the Run predictions page for your chosen Ready-to-use model, choose Single prediction.

4.
For Text ﬁeld, enter the text for which you’d like to get a prediction.

5.
Choose Generate prediction results to get your prediction.

In the right pane Prediction results, you receive an analysis of your text in addition to a
Conﬁdence score for each result or label. For example, if you chose language detection and

Ready-to-use models
1230

## Page 260

Amazon SageMaker AI
Developer Guide

entered a passage of text in French, you might get French with a 95% conﬁdence score and traces
of other languages, like English, with a 5% conﬁdence score.

The following screenshot shows the results for a single prediction using language detection where
the model is 100% conﬁdent that the passage is English.

![Page 260 Diagram 1](images/page-0260-img-01.png)

Batch predictions

To make batch predictions for Ready-to-use models that accept text data, do the following:

1.
In the left navigation pane of the Canvas application, choose Ready-to-use models.

2.
On the Ready-to-use models page, choose the Ready-to-use model for your use case. For text
data, it should be one of the following: Sentiment analysis, Entities extraction, Language
detection, or Personal information detection.

3.
On the Run predictions page for your chosen Ready-to-use model, choose Batch prediction.

4.
Choose Select dataset if you’ve already imported your dataset. If not, choose Import new
dataset, and then you are directed through the import data workﬂow.

5.
From the list of available datasets, select your dataset and choose Generate predictions to get
your predictions.

Ready-to-use models
1231

## Page 261

Amazon SageMaker AI
Developer Guide

After the prediction job ﬁnishes running, on the Run predictions page, you see an output dataset
listed under Predictions. This dataset contains your results, and if you select the More options icon
(

),
you can Preview the output data. Then, you can choose Download to download the results.

Make predictions for image data

The following procedures describe how to make both single and batch predictions for image
datasets. Each Ready-to-use model supports both Single predictions and Batch predictions for
your dataset. A Single prediction is when you only need to make one prediction. For example, you
have one image from which you want to extract text, or one paragraph of text for which you want
to detect the dominant language. A Batch prediction is when you’d like to make predictions for
an entire dataset. For example, you might have a CSV ﬁle of customer reviews for which you’d like
to analyze the customer sentiment, or you might have image ﬁles in which you’d like to detect

objects.

You can use these procedures for the following Ready-to-use model types: object detection images
and text detection in images.

Single predictions

To make a single prediction for Ready-to-use models that accept image data, do the following:

1.
In the left navigation pane of the Canvas application, choose Ready-to-use models.

2.
On the Ready-to-use models page, choose the Ready-to-use model for your use case. For
image data, it should be one of the following: Object detection images or Text detection in
images.

3.
On the Run predictions page for your chosen Ready-to-use model, choose Single prediction.

4.
Choose Upload image.

5.
You are prompted to select an image to upload from your local computer. Select the image
from your local ﬁles, and then the prediction results generate.

In the right pane Prediction results, you receive an analysis of your image in addition to a
Conﬁdence score for each object or text detected. For example, if you chose object detection in
images, you receive a list of objects in the image along with a conﬁdence score of how certain the
model is that each object was accurately detected, such as 93%.

Ready-to-use models
1232

## Page 262

Amazon SageMaker AI
Developer Guide

The following screenshot shows the results for a single prediction using the object detection
in images solution, where the model predicts objects such as a clock tower and bus with 100%
conﬁdence.

![Page 262 Diagram 1](images/page-0262-img-01.png)

Batch predictions

To make batch predictions for Ready-to-use models that accept image data, do the following:

1.
In the left navigation pane of the Canvas application, choose Ready-to-use models.

2.
On the Ready-to-use models page, choose the Ready-to-use model for your use case. For
image data, it should be one of the following: Object detection images or Text detection in
images.

3.
On the Run predictions page for your chosen Ready-to-use model, choose Batch prediction.

4.
Choose Select dataset if you’ve already imported your dataset. If not, choose Import new
dataset, and then you are directed through the import data workﬂow.

5.
From the list of available datasets, select your dataset and choose Generate predictions to get
your predictions.

After the prediction job ﬁnishes running, on the Run predictions page, you see an output dataset
listed under Predictions. This dataset contains your results, and if you select the More options icon
(

),

Ready-to-use models
1233

## Page 263

Amazon SageMaker AI
Developer Guide

you can choose View prediction results to preview the output data. Then, you can choose
Download prediction and download the results as a CSV or a ZIP ﬁle.

Make predictions for document data

The following procedures describe how to make both single and batch predictions for document
datasets. Each Ready-to-use model supports both Single predictions and Batch predictions for
your dataset. A Single prediction is when you only need to make one prediction. For example, you
have one image from which you want to extract text, or one paragraph of text for which you want
to detect the dominant language. A Batch prediction is when you’d like to make predictions for
an entire dataset. For example, you might have a CSV ﬁle of customer reviews for which you’d like
to analyze the customer sentiment, or you might have image ﬁles in which you’d like to detect
objects.

You can use these procedures for the following Ready-to-use model types: expense analysis,
identity document analysis, and document analysis.

Note

For document queries, only single predictions are currently supported.

Single predictions

To make a single prediction for Ready-to-use models that accept document data, do the following:

1.
In the left navigation pane of the Canvas application, choose Ready-to-use models.

2.
On the Ready-to-use models page, choose the Ready-to-use model for your use case. For
document data, it should be one of the following: Expense analysis, Identity document
analysis, or Document analysis.

3.
On the Run predictions page for your chosen Ready-to-use model, choose Single prediction.

4.
If your Ready-to-use model is identity document analysis or document analysis, complete the
following actions. If you’re doing expense analysis or document queries, skip this step and go
to Step 5 or Step 6, respectively.

a.
Choose Upload document.

b.
You are prompted to upload a PDF, JPG, or PNG ﬁle from your local computer. Select the
document from your local ﬁles, and then the prediction results will generate.

Ready-to-use models
1234

## Page 264

Amazon SageMaker AI
Developer Guide

5.
If your Ready-to-use model is expense analysis, do the following:

a.
Choose Upload invoice or receipt.

b.
You are prompted to upload a PDF, JPG, PNG, or TIFF ﬁle from your local computer. Select
the document from your local ﬁles, and then the prediction results will generate.

6.
If your Ready-to-use model is document queries, do the following:

a.
Choose Upload document.

b.
You are prompted to upload a PDF ﬁle from your local computer. Select the document
from your local ﬁles. Your PDF must be 1–100 pages long.

Note

If you're in the Asia Paciﬁc (Seoul), Asia Paciﬁc (Singapore), Asia Paciﬁc (Sydney), or
Europe (Frankfurt) regions, then the maximum PDF size for document queries is 20
pages.

c.
In the right side pane, enter queries to search for information in the document. The
number of characters you can have in a single query is from 1–200. You can add up to 15
queries at a time.

d.
Choose Submit queries, and then the results generate with answers to your queries. You
are billed once for each submissions of queries you make.

In the right pane Prediction results, you’ll receive an analysis of your document.

The following information describes the results for each type of solution:

• For expense analysis, the results are categorized into Summary ﬁelds, which include ﬁelds such
as the total on a receipt, and Line item ﬁelds, which include ﬁelds such as individual items on a
receipt. The identiﬁed ﬁelds are highlighted on the document image in the output.

• For identity document analysis, the output shows you the ﬁelds that the Ready-to-use model
identiﬁed, such as ﬁrst and last name, address, or date of birth. The identiﬁed ﬁelds are
highlighted on the document image in the output.

• For document analysis, the results are categorized into Raw text, Forms, Tables, and Signatures.
Raw text includes all of the extracted text, while Forms, Tables, and Signatures only include
information on the form that falls into those categories. For example, Tables only includes

Ready-to-use models
1235

## Page 265

Amazon SageMaker AI
Developer Guide

information extracted from tables in the document. The identiﬁed ﬁelds are highlighted on the
document image in the output.

• For document queries, Canvas returns answers to each of your queries. You can open the
collapsible query dropdown to view a result, along with a conﬁdence score for the prediction. If
Canvas ﬁnds multiple answers in the document, then you might have more than one result for
each query.

The following screenshot shows the results for a single prediction using the document analysis
solution.

![Page 265 Diagram 1](images/page-0265-img-01.png)

Batch predictions

To make batch predictions for Ready-to-use models that accept document data, do the following:

1.
In the left navigation pane of the Canvas application, choose Ready-to-use models.

2.
On the Ready-to-use models page, choose the Ready-to-use model for your use case. For
image data, it should be one of the following: Expense analysis, Identity document analysis,
or Document analysis.

3.
On the Run predictions page for your chosen Ready-to-use model, choose Batch prediction.

Ready-to-use models
1236

## Page 266

Amazon SageMaker AI
Developer Guide

4.
Choose Select dataset if you’ve already imported your dataset. If not, choose Import new
dataset, and then you are directed through the import data workﬂow.

5.
From the list of available datasets, select your dataset and choose Generate predictions. If
your use case is document analysis, continue to Step 6.

6.
(Optional) If your use case is Document analysis, another dialog box called Select features to
include in batch prediction appears. You can select Forms, Tables, and Signatures to group
the results by those features. Then, choose Generate predictions.

After the prediction job ﬁnishes running, on the Run predictions page, you see an output dataset
listed under Predictions. This dataset contains your results, and if you select the More options icon
(

),
you can choose View prediction results to preview the analysis of your document data.

The following information describes the results for each type of solution:

• For expense analysis, the results are categorized into Summary ﬁelds, which include ﬁelds such
as the total on a receipt, and Line item ﬁelds, which include ﬁelds such as individual items on a
receipt. The identiﬁed ﬁelds are highlighted on the document image in the output.

• For identity document analysis, the output shows you the ﬁelds that the Ready-to-use model
identiﬁed, such as ﬁrst and last name, address, or date of birth. The identiﬁed ﬁelds are
highlighted on the document image in the output.

• For document analysis, the results are categorized into Raw text, Forms, Tables, and Signatures.
Raw text includes all of the extracted text, while Forms, Tables, and Signatures only include
information on the form that falls into those categories. For example, Tables only includes
information extracted from tables in the document. The identiﬁed ﬁelds are highlighted on the
document image in the output.

After previewing your results, you can choose Download prediction and download the results as a
ZIP ﬁle.

Custom models

In Amazon SageMaker Canvas, you can train custom machine learning models tailored to your
speciﬁc data and use case. By training a custom model on your data, you are able to capture
characteristics and trends that are speciﬁc and most representative of your data. For example, you

Custom models
1237

## Page 267

Amazon SageMaker AI
Developer Guide

might want to create a custom time series forecasting model that you train on inventory data from
your warehouse to manage your logistics operations.

Canvas supports training a range of model types. After training a custom model, you can evaluate
the model's performance and accuracy. Once satisﬁed with a model, you can make predictions on
new data, and you also have the option to share the custom model with data scientists for further
analysis or to deploy it to a SageMaker AI hosted endpoint for real-time inference, all from within
the Canvas application.

You can train a Canvas custom model on the following types of datasets:

• Tabular (including numeric, categorical, timeseries, and text data)

• Image

The following table shows the types of custom models that you can build in Canvas, along with
their supported data types and data sources.

Model type
Example use case
Supported data
types

Supported data
sources

Numeric prediction
Predicting house
prices based on
features like square
footage

Numeric
Local upload,
Amazon S3, SaaS
connectors

2 category prediction
Predicting whether

Binary or categorical
Local upload,

or not a customer is
likely to churn

Amazon S3, SaaS
connectors

3+ category predictio
n

Predicting patient
outcomes after being
discharged from the
hospital

Categorical
Local upload,
Amazon S3, SaaS
connectors

Time series forecasti
ng

Predicting your
inventory for the next
quarter

Timeseries
Local upload,
Amazon S3, SaaS
connectors

Custom models
1238

## Page 268

Amazon SageMaker AI
Developer Guide

Model type
Example use case
Supported data
types

Supported data
sources

Single-label image
prediction

Predicting types
of manufacturing
defects in images

Image (JPG, PNG)
Local upload,
Amazon S3

Multi-category text
prediction

Predicting categorie
s of products, such as
clothing, electronics,
or household goods,
based on product
descriptions

Source column: text

Local upload,
Amazon S3

Target column: binary
or categorical

Get started

To get started with building and generating predictions from a custom model, do the following:

• Determine your use case and type of model that you want to build. For more information about
the custom model types, see How custom models work. For more information about the data
types and sources supported for custom models, see Data import.

• Import your data into Canvas. You can build a custom model with any tabular or image dataset
that meets the input requirements. For more information about the input requirements, see
Create a dataset.

To learn more about sample datasets provided by SageMaker AI with which you can experiment,
see Sample datasets in Canvas.

• Build your custom model. You can do a Quick build to get your model and start making
predictions more quickly, or you can do a Standard build for greater accuracy.

For numeric, categorical, and time series forecasting model types, you can clean and prepare
your data with the Data Wrangler feature. In Data Wrangler, you can create a data ﬂow and use
various data preparation techniques, such as applying advanced transforms or joining datasets.
For image prediction models, you can Edit an image dataset to update your labels or add and
delete images. Note that you can't use these features for multi-category text prediction models.

• Evaluate your model's performance and determine how well it might perform on real-world data.

• Make single or batch predictions with your model.

Custom models
1239

## Page 269

Amazon SageMaker AI
Developer Guide

How custom models work

Use Amazon SageMaker Canvas to build a custom model on the dataset that you've imported.
Use the model that you've built to make predictions on new data. SageMaker Canvas uses the
information in the dataset to build up to 250 models and choose the one that performs the best.

When you begin building a model, Canvas automatically recommends one or more model types.
Model types fall into one of the following categories:

• Numeric prediction – This is known as regression in machine learning. Use the numeric
prediction model type when you want to make predictions for numeric data. For example, you
might want to predict the price of houses based on features such as the house’s square footage.

• Categorical prediction – This is known as classiﬁcation in machine learning. When you want to
categorize data into groups, use the categorical prediction model types:

• 2 category prediction – Use the 2 category prediction model type (also known as binary
classiﬁcation in machine learning) when you have two categories that you want to predict for
your data. For example, you might want to determine whether a customer is likely to churn.

• 3+ category prediction – Use the 3+ category prediction model type (also known as multi-
class classiﬁcation in machine learning) when you have three or more categories that you
want to predict for your data. For example, you might want to predict a customer's loan status
based on features such as previous payments.

• Time series forecasting – Use time series forecasts when you want to make predictions over a
period of time. For example, you might want to predict the number of items you’ll sell in the
next quarter. For information about time series forecasts, see Time Series Forecasts in Amazon
SageMaker Canvas.

• Image prediction – Use the single-label image prediction model type (also known as single-label
image classiﬁcation in machine learning) when you want to assign labels to images. For example,
you might want to classify diﬀerent types of manufacturing defects in images of your product.

• Text prediction – Use the multi-category text prediction model type (also known as multi-
class text classiﬁcation in machine learning) when you want to assign labels to passages of text.
For example, you might have a dataset of customer reviews for a product, and you want to
determine whether customers liked or disliked the product. You might have your model predict

whether a given passage of text is Positive, Negative, or Neutral.

For a table of the supported input data types for each model type, see Custom models.

Custom models
1240

## Page 270

Amazon SageMaker AI
Developer Guide

For each tabular data model that you build (which includes numeric, categorical, time series
forecasting, and text prediction models), you choose the Target column. The Target column is the
column that contains the information that you want to predict. For example, if you're building a
model to predict whether people have cancelled their subscriptions, the Target column contains

data points that are either a yes or a no about someone's cancellation status.

For image prediction models, you build the model with a dataset of images that have been
assigned labels. For the unlabeled images that you provide, the model predicts a label. For
example, if you’re building a model to predict whether an image is a cat or a dog, you provide
images labeled as cats or dogs when building the model. Then, the model can accept unlabeled
images and predict them as either cats or dogs.

What happens when you build a model

To build your model, you can choose either a Quick build or a Standard build. The Quick build has

a shorter build time, but the Standard build generally has a higher accuracy.

For tabular and time series forecasting models, Canvas uses downsampling to reduce the size of
datasets larger than 5 GB or 30 GB, respectively. Canvas downsamples with the stratiﬁed sampling
method. The table below lists the size of the downsample by model type. To control the sampling
process, you can use Data Wrangler in Canvas to sample using your preferred sampling technique.
For time series data, you can resample to aggregate data points. For more information about
sampling, see Sampling. For more information about resampling time series data, see Resample
Time Series Data.

If you choose to do a Quick build on a dataset with more than 50,000 rows, then Canvas samples
your data down to 50,000 rows for a shorter model training time.

The following table summarizes key characteristics of the model building process, including
average build times for each model and build type, the size of the downsample when building
models with large datasets, and the minimum and maximum number of data points you should
have for each build type.

Limit
Numeric and
categorical
prediction

Time series
forecasting

Image
prediction

Text
prediction

Quick build time
2‐20 minutes
2‐20 minutes
15‐30
minutes

15‐30
minutes

Custom models
1241

## Page 271

Amazon SageMaker AI
Developer Guide

Limit
Numeric and
categorical
prediction

Time series
forecasting

Image
prediction

Text
prediction

Standard build time
2‐4 hours
2‐4 hours
2‐5 hours
2‐5 hours

Downsample size (the
reduced size of a large
dataset after Canvas
downsamples)

5 GB
30 GB
N/A
N/A

Minimum number of entries
(rows) for Quick builds

2 category:
500 rows

N/A
N/A
N/A

3+ category,
numeric,
time series:
N/A

Minimum number of entries
(rows, images, or documents)
for Standard builds

250
50
50
N/A

Maximum number of entries
(rows, images, or documents)
for Quick builds

N/A
N/A
5000
7500

Maximum number of entries
(rows, images, or documents)
for Standard builds

N/A
150,000
180,000
N/A

Maximum number of columns
1,000
1,000
N/A
N/A

Canvas predicts values by using the information in the rest of the dataset, depending on the model
type:

• For categorical prediction, Canvas puts each row into one of the categories listed in the Target
column.

Custom models
1242

## Page 272

Amazon SageMaker AI
Developer Guide

• For numeric prediction, Canvas uses the information in the dataset to predict the numeric values
in the Target column.

• For time series forecasting, Canvas uses historical data to predict values for the Target column in
the future.

• For image prediction, Canvas uses images that have been assigned labels to predict labels for
unlabeled images.

• For text prediction, Canvas analyzes text data that has been assigned labels to predict labels for
passages of unlabeled text.

Additional features to help you build your model

Before building your model, you can use Data Wrangler in Canvas to prepare your data using 300+
built-in transforms and operators. Data Wrangler supports transforms for both tabular and image
datasets. Additionally, you can connect to data sources outside of Canvas, create jobs to apply
transforms to your entire dataset, and export your fully prepared and cleaned data for use in ML
workﬂows outside of Canvas. For more information, see Data preparation.

To see visualizations and analytics to explore your data and determine which features to include in
your model, you can use Data Wrangler’s built-in analyses. You can also access a Data Quality and
Insights Report that highlights potential issues with your dataset and provides recommendations
for how to ﬁx them. For more information, see Perform exploratory data analysis (EDA).

In addition to the more advanced data preparation and exploration functionality provided through
Data Wrangler, Canvas provides some basic features that you can use:

• To ﬁlter your data and access a set of basic data transforms, see Prepare data for model building.

• To access simple visualizations and analytics for feature exploration, see Data exploration and
analysis.

• To learn more about additional features such as previewing your model, validating your dataset,
and changing the size of the random sample used to build your model, see Preview your model.

For tabular datasets with multiple columns (such as datasets for building categorical, numeric, or
time series forecasting model types), you might have rows with missing data points. While Canvas
builds the model, it automatically adds missing values. Canvas uses the values in your dataset to
perform a mathematical approximation for the missing values. For the highest model accuracy, we
recommend adding in the missing data if you can ﬁnd it. Note that the missing data feature is not
supported for text prediction or image prediction models.

Custom models
1243

## Page 273

Amazon SageMaker AI
Developer Guide

Get started

To get started with building a custom model, see Build a model and follow the procedure for the
type of model that you want to build.

Preview your model

Note

The following functionality is only available for custom models built with tabular datasets.
Multi-category text prediction models are also excluded.

SageMaker Canvas provides you with a tool to preview your model before you begin building. This
gives you an estimated accuracy score and also gives you a preliminary idea of how each column
might impact the model.

To preview the model score, when you're on the Build tab of your model, choose Preview model.

The model preview generates an Estimated accuracy prediction of how well the model might
analyze your data. The accuracy of a Quick build or a Standard build represents how well the
model can perform on real data and is generally higher than the Estimated accuracy.

The model preview also provides you with the Column Impact scores, which can indicate the
importance of each column to the model's predictions.

The following screenshot shows a model preview in the Canvas application.

Custom models
1244

## Page 274

Amazon SageMaker AI
Developer Guide

![Page 274 Diagram 1](images/page-0274-img-01.png)

Amazon SageMaker Canvas automatically handles missing values in your dataset while it builds the
model. It infers the missing values by using adjacent values that are present in the dataset.

If you're satisﬁed with your model preview and want to proceed with building a model, then see
Build a model.

Data validation

Before you build your model, SageMaker Canvas checks your dataset for issues that might cause
your build to fail. If SageMaker Canvas ﬁnds any issues, then it warns you on the Build page before
you attempt to build a model.

You can choose Validate data to see a list of the issues with your dataset. You can then use the
SageMaker Canvas Data Wrangler data preparation features, or your own tools, to ﬁx your dataset
before starting a build. If you don’t ﬁx the issues with your dataset, then your build fails.

If you make changes to your dataset to ﬁx the issues, you have the option to re-validate your
dataset before attempting a build. We recommend that you re-validate your dataset before
building.

The following table shows the issues that SageMaker Canvas checks for in your dataset and how to
resolve them.

Custom models
1245

## Page 275

Amazon SageMaker AI
Developer Guide

Issue
Resolution

Wrong model type for your data
Try another model type or use a diﬀerent
dataset.

Missing values in your target column
Replace the missing values, drop rows with
missing values, or use a diﬀerent dataset.

Too many unique labels in your target column
Verify that you've used the correct column for
your target column, or use a diﬀerent dataset.

Too many non-numeric values in your target
column

Choose a diﬀerent target column, select
another model type, or use a diﬀerent dataset.

One or more column names contain double
underscores

Rename the columns to remove any double
underscores, and try again.

None of the rows in your dataset are complete
Replace the missing values, or use a diﬀerent
dataset.

Too many unique labels for the number of
rows in your data

Check that you're using the right target
column, increase the number of rows in your
dataset, consolidate similar labels, or use a
diﬀerent dataset.

Random sample

SageMaker Canvas uses the random sampling method to sample your dataset. The random sample
method means that each row has an equal chance of being picked for the sample. You can choose a
column in the preview to get summary statistics for the random sample, such as the mean and the
mode.

By default, SageMaker Canvas uses a random sample size of 20,000 rows from your dataset
for datasets with more than 20,000 rows. For datasets smaller than 20,000 rows, the default
sample size is the number of rows in your dataset. You can increase or decrease the sample size by
choosing Random sample in the Build tab of the SageMaker Canvas application. You can use the
slider to select your desired sample size, and then choose Update to change the sample size. The
maximum sample size you can choose for a dataset is 40,000 rows, and the minimum sample size is

Custom models
1246

## Page 276

Amazon SageMaker AI
Developer Guide

500 rows. If you choose a large sample size, the dataset preview and summary statistics might take
a few moments to reload.

The Build page shows a preview of 100 rows from your dataset. If the sample size is the same size
as your dataset, then the preview uses the ﬁrst 100 rows of your dataset. Otherwise, the preview
uses the ﬁrst 100 rows of the random sample.

Build a model

The following sections show you how to build a model for each of the main types of custom
models.

• To build numeric prediction, 2 category prediction, or 3+ category prediction models, see Build a
custom numeric or categorical prediction model.

• To build single-label image prediction models, see Build a custom image prediction model.

• To build multi-category text prediction models, see Build a custom text prediction model.

• To build time series forecasting models, see Build a time series forecasting model.

Note

If you encounter an error during post-building analysis that tells you to increase your quota

for ml.m5.2xlarge instances, see Request a Quota Increase.

Build a custom numeric or categorical prediction model

Numeric and categorical prediction models support both Quick builds and Standard builds.

To build a numeric or categorical prediction model, use the following procedure:

1.
Open the SageMaker Canvas application.

2.
In the left navigation pane, choose My models.

3.
Choose New model.

4.
In the Create new model dialog box, do the following:

a.
Enter a name in the Model name ﬁeld.

b.
Select the Predictive analysis problem type.

Custom models
1247

## Page 277

Amazon SageMaker AI
Developer Guide

c.
Choose Create.

5.
For Select dataset, select your dataset from the list of datasets. If you haven’t already
imported your data, choose Import to be directed through the import data workﬂow.

6.
When you’re ready to begin building your model, choose Select dataset.

7.
On the Build tab, for the Target column dropdown list, select the target for your model that
you would like to predict.

8.
For Model type, Canvas automatically detects the problem type for you. If you want to change
the type or conﬁgure advanced model settings, choose Conﬁgure model.

When the Conﬁgure model dialog box opens, do the following:

a.
For Model type, choose the model type that you want to build.

b.
After you choose the model type, there are additional Advanced settings. For more
information about each of the advanced settings, see Advanced model building
conﬁgurations. To conﬁgure the advanced settings, do the following:

i.
(Optional) For the Objective metric dropdown menu, select the metric that you want
Canvas to optimize while building your model. If you don’t select a metric, Canvas
chooses one for you by default. For descriptions of the available metrics, see Metrics
reference.

ii.
For Training method, choose Auto, Ensemble, or Hyperparameter optimization
(HPO) mode.

iii.
For Algorithms, select the algorithms that you want to include for building model
candidates.

iv.
For Data split, specify in percentages how you want to split your data between the
Training set and the Validation set. The training set is used for building the model,
while the validation set is used for testing accuracy of model candidates.

v.
For Max candidates and runtime, do the following:

A.
Set the Max candidates value, or the maximum number of model candidates that
Canvas can generate. Note that Max candidates is only available in HPO mode.

B.
Set the hour and minute values for Max job runtime, or the maximum amount
of time that Canvas can spend building your model. After the maximum time,
Canvas stops building and selects the best model candidate.

c.
After conﬁguring the advanced settings, choose Save.

Custom models
1248

## Page 278

Amazon SageMaker AI
Developer Guide

9.
Select or deselect columns in your data to include or drop them from your build.

Note

If you make batch predictions with your model after building, Canvas adds dropped

columns to your prediction results. However, Canvas does not add the dropped
columns to your batch predictions for time series models.

10. (Optional) Use the visualization and analytics tools that Canvas provides to visualize your data

and determine which features you might want to include in your model. For more information,
see Explore and analyze your data.

11. (Optional) Use data transformations to clean, transform, and prepare your data for model

building. For more information, see  Prepare your data with advanced transformations. You
can view and remove your transforms by choosing Model recipe to open the Model recipe side
panel.

12. (Optional) For additional features such as previewing the accuracy of your model, validating

your dataset, and changing the size of the random sample that Canvas takes from your
dataset, see Preview your model.

13. After reviewing your data and making any changes to your dataset, choose Quick build or

Standard build to begin a build for your model. The following screenshot shows the Build
page and the Quick build and Standard build options.

![Page 278 Diagram 1](images/page-0278-img-01.png)

Custom models
1249

## Page 279

Amazon SageMaker AI
Developer Guide

After your model begins building, you can leave the page. When the model shows as Ready on the
My models page, it’s ready for analysis and predictions.

Build a custom image prediction model

Single-label image prediction models support both Quick builds and Standard builds.

To build a single-label image prediction model, use the following procedure:

1.
Open the SageMaker Canvas application.

2.
In the left navigation pane, choose My models.

3.
Choose New model.

4.
In the Create new model dialog box, do the following:

a.
Enter a name in the Model name ﬁeld.

b.
Select the Image analysis problem type.

c.
Choose Create.

5.
For Select dataset, select your dataset from the list of datasets. If you haven’t already
imported your data, choose Import to be directed through the import data workﬂow.

6.
When you’re ready to begin building your model, choose Select dataset.

7.
On the Build tab, you see the Label distribution for the images in your dataset. The Model
type is set to Single-label image prediction.

8.
On this page, you can preview your images and edit the dataset. If you have any unlabeled
images, choose Edit dataset and Assign labels to unlabeled images. You can also perform
other tasks when you Edit an image dataset, such as renaming labels and adding images to the
dataset.

9.
After reviewing your data and making any changes to your dataset, choose Quick build or
Standard build to begin a build for your model. The following screenshot shows the Build
page of an image prediction model that is ready to be built.

Custom models
1250

## Page 280

Amazon SageMaker AI
Developer Guide

![Page 280 Diagram 1](images/page-0280-img-01.png)

After your model begins building, you can leave the page. When the model shows as Ready on the
My models page, it’s ready for analysis and predictions.

Build a custom text prediction model

Multi-category text prediction models support both Quick builds and Standard builds.

To build a text prediction model, use the following procedure:

1.
Open the SageMaker Canvas application.

2.
In the left navigation pane, choose My models.

3.
Choose New model.

4.
In the Create new model dialog box, do the following:

a.
Enter a name in the Model name ﬁeld.

b.
Select the Text analysis problem type.

c.
Choose Create.

5.
For Select dataset, select your dataset from the list of datasets. If you haven’t already
imported your data, choose Import to be directed through the import data workﬂow.

6.
When you’re ready to begin building your model, choose Select dataset.

Custom models
1251

## Page 281

Amazon SageMaker AI
Developer Guide

7.
On the Build tab, for the Target column dropdown list, select the target for your model that
you would like to predict. The target column must have a binary or categorical data type, and
there must be at least 25 entries (or rows of data) for each unique label in the target column.

8.
For Model type, conﬁrm that the model type is automatically set to Multi-category text
prediction.

9.
For the training column, select your source column of text data. This should be the column
containing the text that you want to analyze.

10. Choose Quick build or Standard build to begin building your model. The following screenshot

shows the Build page of a text prediction model that is ready to be built.

![Page 281 Diagram 1](images/page-0281-img-01.png)

After your model begins building, you can leave the page. When the model shows as Ready on the
My models page, it’s ready for analysis and predictions.

Build a time series forecasting model

Time series forecasting models support both Quick builds and Standard builds.

To build a time series forecasting model, use the following procedure:

1.
Open the SageMaker Canvas application.

2.
In the left navigation pane, choose My models.

Custom models
1252

## Page 282

Amazon SageMaker AI
Developer Guide

3.
Choose New model.

4.
In the Create new model dialog box, do the following:

a.
Enter a name in the Model name ﬁeld.

b.
Select the Time series forecasting problem type.

c.
Choose Create.

5.
For Select dataset, select your dataset from the list of datasets. If you haven’t already
imported your data, choose Import to be directed through the import data workﬂow.

6.
When you’re ready to begin building your model, choose Select dataset.

7.
On the Build tab, for the Target column dropdown list, select the target for your model that
you would like to predict.

8.
In the Model type section, choose Conﬁgure model.

9.
The Conﬁgure model box opens. For the Time series conﬁguration section, ﬁll out the
following ﬁelds:

a.
For Item ID column, choose a column in your dataset that uniquely identiﬁes each row.

The column should have a data type of Text.

b.
(Optional) For Group column, choose one or more categorical columns (with a data type

of Text) that you want to use for grouping your forecasting values.

c.
For Time stamp column, select the column with timestamps (in datetime format). For
more information about the accepted datetime formats, see Time Series Forecasts in
Amazon SageMaker Canvas.

d.
For the Forecast length ﬁeld, enter the period of time for which you want to forecast
values. Canvas automatically detects the units of time in your data.

e.
(Optional) Turn on the Use holiday schedule toggle to select a holiday schedule from
various countries and make your forecasts with holiday data more accurate.

10. In the Conﬁgure model box, there are additional settings in the Advanced section. For more

information about each of the advanced settings, see Advanced model building conﬁgurations.
To conﬁgure the Advanced settings, do the following:

a.
For the Objective metric dropdown menu, select the metric that you want Canvas to
optimize while building your model. If you don’t select a metric, Canvas chooses one for
you by default. For descriptions of the available metrics, see Metrics reference.

b.
If you’re running a standard build, you’ll see the Algorithms section. This section is for
selecting the time series forecasting algorithms that you’d like to use for building your

Custom models
1253

## Page 283

Amazon SageMaker AI
Developer Guide

model. You can select a subset of the available algorithms, or you can select all of them if
you aren’t sure which ones to try.

When you run your standard build, Canvas builds an ensemble model that combines all of
the algorithms together to optimize prediction accuracy.

Note

If you’re running a quick build, Canvas uses a single tree-based learning algorithm
to train your model, and you don’t have to select any algorithms.

c.
For Forecast quantiles, enter up to 5 comma-separated quantile values to specify the
upper and lower bounds of your forecast.

d.
After conﬁguring the Advanced settings, choose Save.

11. Select or deselect columns in your data to include or drop them from your build.

Note

If you make batch predictions with your model after building, Canvas adds dropped
columns to your prediction results. However, Canvas does not add the dropped
columns to your batch predictions for time series models.

12. (Optional) Use the visualization and analytics tools that Canvas provides to visualize your data

and determine which features you might want to include in your model. For more information,
see Explore and analyze your data.

13. (Optional) Use data transformations to clean, transform, and prepare your data for model

building. For more information, see  Prepare your data with advanced transformations. You
can view and remove your transforms by choosing Model recipe to open the Model recipe side
panel.

14. (Optional) For additional features such as previewing the accuracy of your model, validating

your dataset, and changing the size of the random sample that Canvas takes from your
dataset, see Preview your model.

15. After reviewing your data and making any changes to your dataset, choose Quick build or

Standard build to begin a build for your model.

Custom models
1254

## Page 284

Amazon SageMaker AI
Developer Guide

After your model begins building, you can leave the page. When the model shows as Ready on the
My models page, it’s ready for analysis and predictions.

Advanced model building conﬁgurations

Amazon SageMaker Canvas supports various advanced settings that you can conﬁgure when
building a model. The following page lists all of the advanced settings along with additional
information about their options and conﬁgurations.

Note

The following advanced settings are currently only supported for numeric, categorical, and
time series forecasting model types.

Advanced numeric and categorical prediction model settings

Canvas supports the following advanced settings for numeric and categorical prediction model
types.

Objective metric

The objective metric is the metric that you want Canvas to optimize while building your model. If
you don’t select a metric, Canvas chooses one for you by default. For descriptions of the available
metrics, see the Metrics reference.

Training method

Canvas can automatically select the training method based on the dataset size, or you can select it
manually. The following training methods are available for you to choose from:

• Ensembling – SageMaker AI leverages the AutoGluon library to train several base models. To
ﬁnd the best combination for your dataset, ensemble mode runs 5–10 trials with diﬀerent model
and meta parameter settings. Then, these models are combined using a stacking ensemble
method to create an optimal predictive model. For a list of algorithms supported by ensemble
mode for tabular data, see the following Algorithms section.

• Hyperparameter optimization (HPO) – SageMaker AI ﬁnds the best version of a model by
tuning hyperparameters using Bayesian optimization or multi-ﬁdelity optimization while running
training jobs on your dataset. HPO mode selects the algorithms that are most relevant to your
dataset and selects the best range of hyperparameters to tune your models. To tune your

Custom models
1255

## Page 285

Amazon SageMaker AI
Developer Guide

models, HPO mode runs up to 100 trials (default) to ﬁnd the optimal hyperparameters settings
within the selected range. If your dataset size is less than 100 MB, SageMaker AI uses Bayesian
optimization. SageMaker AI chooses multi-ﬁdelity optimization if your dataset is larger than 100
MB.

For a list of algorithms supported by HPO mode for tabular data, see the following Algorithms
section.

• Auto – SageMaker AI automatically chooses either ensembling mode or HPO mode based on
your dataset size. If your dataset is larger than 100 MB, SageMaker AI chooses HPO mode.
Otherwise, it chooses ensembling mode.

Algorithms

In Ensembling mode, Canvas supports the following machine learning algorithms:

• LightGBM – An optimized framework that uses tree-based algorithms with gradient boosting.
This algorithm uses trees that grow in breadth, rather than depth, and is highly optimized for
speed.

• CatBoost – A framework that uses tree-based algorithms with gradient boosting. Optimized for
handling categorical variables.

• XGBoost – A framework that uses tree-based algorithms with gradient boosting that grows in
depth, rather than breadth.

• Random Forest – A tree-based algorithm that uses several decision trees on random sub-samples
of the data with replacement. The trees are split into optimal nodes at each level. The decisions
of each tree are averaged together to prevent overﬁtting and improve predictions.

• Extra Trees – A tree-based algorithm that uses several decision trees on the entire dataset.
The trees are split randomly at each level. The decisions of each tree are averaged to prevent
overﬁtting and to improve predictions. Extra trees add a degree of randomization in comparison
to the random forest algorithm.

• Linear Models – A framework that uses a linear equation to model the relationship between two
variables in observed data.

• Neural network torch – A neural network model that's implemented using Pytorch.

• Neural network fast.ai – A neural network model that's implemented using fast.ai.

In HPO mode, Canvas supports the following machine learning algorithms:

Custom models
1256

## Page 286

Amazon SageMaker AI
Developer Guide

• XGBoost – A supervised learning algorithm that attempts to accurately predict a target variable
by combining an ensemble of estimates from a set of simpler and weaker models.

• Deep learning algorithm – A multilayer perceptron (MLP) and feedforward artiﬁcial neural
network. This algorithm can handle data that is not linearly separable.

Data split

You have the option to specify how you want to split your dataset between the training set (the
portion of your dataset used for building the model) and the validation set, (the portion of your
dataset used for verifying the model’s accuracy). For example, a common split ratio is 80% training
and 20% validation, where 80% of your data is used to build the model while 20% is saved for
measuring model performance. If you don’t specify a custom ratio, then Canvas splits your dataset
automatically.

Max candidates

Note

This feature is only available in the HPO training mode.

You can specify the maximum number of model candidates that Canvas generates while building
your model. We recommend that you use the default number of candidates, which is 100, to build
the most accurate models. The maximum number you can specify is 250. Decreasing the number of
model candidates may impact your model’s accuracy.

Max job runtime

You can specify the maximum job runtime, or the maximum amount of time that Canvas spends
building your model. After the time limit, Canvas stops building and selects the best model
candidate.

The maximum time that you can specify is 720 hours. We highly recommend that you keep the
maximum job runtime greater than 30 minutes to ensure that Canvas has enough time to generate
model candidates and ﬁnish building your model.

Advanced time series forecasting model settings

For time series forecasting models, Canvas supports the Objective metric, which is listed in the
previous section.

Custom models
1257

## Page 287

Amazon SageMaker AI
Developer Guide

Time series forecasting models also support the following advanced setting:

Algorithm selection

When you build a time series forecasting model, Canvas uses an ensemble (or a combination)
of statistical and machine learning algorithms to deliver highly accurate time series forecasts.
By default, Canvas selects the optimal combination of all the available algorithms based on the
time series in your dataset. However, you have the option to specify one or more algorithms to
use for your forecasting model. In this case, Canvas determines the best blend using only your
selected algorithms. If you're uncertain about which algorithm to select for training your model, we
recommend that you choose all of the available algorithms.

Note

Algorithm selection is only supported for standard builds. If you don’t select any algorithms
in the advanced settings, then by default SageMaker AI runs a quick build and trains model
candidates using a single tree-based learning algorithm. For more information about the
diﬀerence between quick builds and standard builds, see How custom models work.

Canvas supports the following time series forecasting algorithms:

• Autoregressive Integrated Moving Average (ARIMA) – A simple stochastic time series model
that uses statistical analysis to interpret the data and make future predictions. This algorithm is
useful for simple datasets with fewer than 100 time series.

• Convolutional Neural Network - Quantile Regression (CNN-QR) – A proprietary, supervised
learning algorithm that trains one global model from a large collection of time series and uses
a quantile decoder to make predictions. CNN-QR works best with large datasets containing
hundreds of time series.

• DeepAR+ – A proprietary, supervised learning algorithm for forecasting scalar time series using
recurrent neural networks (RNNs) to train a single model jointly over all of the time series.
DeepAR+ works best with large datasets containing hundreds of feature time series.

• Non-Parametric Time Series (NPTS) – A scalable, probabilistic baseline forecaster that predicts
the future value distribution of a given time series by sampling from past observations. NPTS is
useful when working with sparse or intermittent time series (for example, forecasting demand
for individual items where the time series has many 0s or low counts).

• Exponential Smoothing (ETS) – A forecasting method that produces forecasts which are weighted
averages of past observations where the weights of older observations exponentially decrease.

Custom models
1258

## Page 288

Amazon SageMaker AI
Developer Guide

The algorithm is useful for simple datasets with fewer than 100 time series and datasets with
seasonality patterns.

• Prophet – An additive regression model that works best with time series that have strong
seasonal eﬀects and several seasons of historical data. The algorithm is useful for datasets with

non-linear growth trends that approach a limit.

Forecast quantiles

For time series forecasting, SageMaker AI trains 6 model candidates with your target time series.
Then, SageMaker AI combines these models using a stacking ensemble method to create an
optimal forecasting model for a given objective metric. Each forecasting model generates a
probabilistic forecast by producing forecasts at quantiles between P1 and P99. These quantiles

are used to account for forecast uncertainty. By default, forecasts are generated for 0.1 (p10), 0.5

(p50), and 0.9 (p90). You can choose to specify up to ﬁve of your own quantiles from 0.01 (p1) to

0.99 (p99), by increments of 0.01 or higher.

Edit an image dataset

In Amazon SageMaker Canvas, you can edit your image datasets and review your labels before
building a model. You might want to perform tasks such as assigning labels to unlabeled images
or adding more images to the dataset. These tasks can all be done in the Canvas application,
providing you with one place to modify your dataset and build a model.

Note

Before building a model, you must assign labels to all images in your dataset. Also, you
must have at least 25 images per label and a minimum of two labels. For more information
about assigning labels, see the section on this page called Assign labels to unlabeled
images. If you can’t determine a label for an image, you should delete it from your dataset.
For more information about deleting images, see the section on this page Add or delete
images from the dataset.

To begin editing your image dataset, you should be on the Build tab while building your single-
label image prediction model.

A new page opens that shows the images in your dataset along with their labels. This page
categorizes your image dataset into Total images, Labeled images, and Unlabeled images. You

Custom models
1259

## Page 289

Amazon SageMaker AI
Developer Guide

can also review the Dataset preparation guide for best practices on building a more accurate
image prediction model.

The following screenshot shows the page for editing your image dataset.

![Page 289 Diagram 1](images/page-0289-img-01.png)

From this page, you can do the following actions.

View the properties for each image (label, size, dimensions)

To view an individual image, you can search for it by ﬁle name in the search bar. Then, choose the
image to open the full view. You can view the image properties and reassign the image’s label.
Choose Save when you’re doing viewing the image.

Add, rename, or delete labels in the dataset

Canvas lists the labels for your dataset in the left navigation pane. You can add new labels to the
dataset by entering a label in the Add label text ﬁeld.

To rename or delete a label from your dataset, choose the More options icon
(

)
next to the label and select either Rename or Delete. If you rename the label, you can enter the
new label name and choose Conﬁrm. If you delete the label, the label is removed from all images
in your dataset that have that label. Any images with that label are left unlabeled.

Custom models
1260

## Page 290

Amazon SageMaker AI
Developer Guide

Assign labels to unlabeled images

To view the unlabeled images in your dataset, choose Unlabeled in the left navigation pane. For
each image, select it and open the label titled Unlabeled and select a label to assign to the image
from the dropdown list. You can also select more than one image and perform this action, and all
selected images are assigned the label you chose.

Reassign labels to images

You can reassign labels to images by selecting the image (or multiple images at a time) and
opening the dropdown titled with the current label. Select your desired label, and the image or
images are updated with the new label.

Sort your images by label

You can view all the images for a given label by choosing the label in the left navigation pane.

Add or delete images from the dataset

You can add more images to your dataset by choosing Add images in the top navigation pane.
You’ll be taken through the workﬂow to import more images. The images you import are added to
your existing dataset.

You can delete images from your dataset by selecting them and then choosing Delete in the top
navigation pane.

Note

After making any changes to your dataset, choose Save dataset to make sure that you
don’t lose your changes.

Data exploration and analysis

Note

You can only use SageMaker Canvas visualizations and analytics for models built on tabular
datasets. Multi-category text prediction models are also excluded.

Custom models
1261

## Page 291

Amazon SageMaker AI
Developer Guide

In Amazon SageMaker Canvas, you can explore the variables in your dataset using visualizations
and analytics and create in-application visualizations and analytics. You can use these explorations
to uncover relationships between your variables before building your model.

For more information about visualization techniques in Canvas, see Explore your data using

visualization techniques.

For more information about analytics in Canvas, see Explore your data using analytics.

Explore your data using visualization techniques

Note

You can only use SageMaker Canvas visualizations for models built on tabular datasets.
Multi-category text prediction models are also excluded.

With Amazon SageMaker Canvas, you can explore and visualize your data to gain advanced insights
into your data before building your ML models. You can visualize using scatter plots, bar charts,
and box plots, which can help you understand your data and discover the relationships between
features that could aﬀect the model accuracy.

In the Build tab of the SageMaker Canvas application, choose Data visualizer to begin creating
your visualizations.

You can change the visualization sample size to adjust the size of the random sample taken
from your dataset. A sample size that is too large might aﬀect the performance of your data
visualizations, so we recommend that you choose an appropriate sample size. To change the
sample size, use the following procedure.

1.
Choose Visualization sample.

2.
Use the slider to select your desired sample size.

3.
Choose Update to conﬁrm the change to your sample size.

Note

Certain visualization techniques require columns of a speciﬁc data type. For example, you
can only use numeric columns for the x and y-axes of scatter plots.

Custom models
1262

## Page 292

Amazon SageMaker AI
Developer Guide

Scatter plot

To create a scatter plot with your dataset, choose Scatter plot in the Visualization panel. Choose
the features you want to plot on the x and y-axes from the Columns section. You can drag and

drop the columns onto the axes or, once an axis has been dropped, you can choose a column from

the list of supported columns.

You can use Color by to color the data points on the plot with a third feature. You can also use
Group by to group the data into separate plots based on a fourth feature.

The following image shows a scatter plot that uses Color by and Group by. In this example, each

data point is colored by the MaritalStatus feature, and grouping by the Department feature
results in a scatter plot for the data points of each department.

![Page 292 Diagram 1](images/page-0292-img-01.png)

Bar chart

To create a bar chart with your dataset, choose Bar chart in the Visualization panel. Choose the
features you want to plot on the x and y-axes from the Columns section. You can drag and drop
the columns onto the axes or, once an axis has been dropped, you can choose a column from the
list of supported columns.

You can use Group by to group the bar chart by a third feature. You can use Stack by to vertically
shade each bar based on the unique values of a fourth feature.

Custom models
1263

## Page 293

Amazon SageMaker AI
Developer Guide

The following image shows a bar chart that uses Group by and Stack by. In this example, the bar

chart is grouped by the MaritalStatus feature and stacked by the JobLevel feature. For each

JobRole on the x axis, there is a separate bar for the unique categories in the MaritalStatus

feature, and every bar is vertically stacked by the JobLevel feature.

![Page 293 Diagram 1](images/page-0293-img-01.png)

Box plot

To create a box plot with your dataset, choose Box plot in the Visualization panel. Choose the
features you want to plot on the x and y-axes from the Columns section. You can drag and drop
the columns onto the axes or, once an axis has been dropped, you can choose a column from the
list of supported columns.

You can use Group by to group the box plots by a third feature.

The following image shows a box plot that uses Group by. In this example, the x and y-axes show

JobLevel and JobSatisfaction, respectively, and the colored box plots are grouped by the

Department feature.

Custom models
1264

## Page 294

Amazon SageMaker AI
Developer Guide

![Page 294 Diagram 1](images/page-0294-img-01.png)

Explore your data using analytics

Note

You can only use SageMaker Canvas analytics for models built on tabular datasets. Multi-
category text prediction models are also excluded.

With analytics in Amazon SageMaker Canvas, you can explore your dataset and gain insight on all
of your variables before building a model. You can determine the relationships between features
in your dataset using correlation matrices. You can use this technique to summarize your dataset
into a matrix that shows the correlations between two or more values. This helps you identify and
visualize patterns in a given dataset for advanced data analysis.

The matrix shows the correlation between each feature as positive, negative, or neutral. You might
want to include features that have a high correlation with each other when building your model.
Features that have little to no correlation might be irrelevant to your model, and you can drop
those features when building your model.

To get started with correlation matrices in SageMaker Canvas, see the following section.

Custom models
1265

## Page 295

Amazon SageMaker AI
Developer Guide

Create a correlation matrix

You can create a correlation matrix when you are preparing to build a model in the Build tab of the
SageMaker Canvas application.

For instructions on how to begin creating a model, see Build a model.

After you’ve started preparing a model in the SageMaker Canvas application, do the following:

1.
In the Build tab, choose Data visualizer.

2.
Choose Analytics.

3.
Choose Correlation matrix.

You should see a visualization similar to the following screenshot, which shows up to 15 columns
of the dataset organized into a correlation matrix.

![Page 295 Diagram 1](images/page-0295-img-01.png)

After you’ve created the correlation matrix, you can customize it by doing the following:

1. Choose your columns

For Columns, you can select the columns that you want to include in the matrix. You can compare
up to 15 columns from your dataset.

Custom models
1266

## Page 296

Amazon SageMaker AI
Developer Guide

Note

You can use numeric, categorical, or binary column types for a correlation matrix. The
correlation matrix doesn’t support datetime or text data column types.

To add or remove columns from the correlation matrix, select and deselect columns from the
Columns panel. You can also drag and drop columns from the panel directly onto the matrix. If
your dataset has a lot of columns, you can search for the columns you want in the Search columns
bar.

To ﬁlter the columns by data type, choose the dropdown list and select All, Numeric, or
Categorical. Selecting All shows you all of the columns from your dataset, whereas the Numeric
and Categorical ﬁlters only show you the numeric or categorical columns in your dataset. Note

that binary column types are included in the numeric or categorical ﬁlters.

For the best data insights, include your target column in the correlation matrix. When you include
your target column in the correlation matrix, it appears as the last feature on the matrix with a
target symbol.

2. Choose your correlation type

SageMaker Canvas supports diﬀerent correlation types, or methods for calculating the correlation
between your columns.

To change the correlation type, use the Columns ﬁlter mentioned in the preceding section to
ﬁlter for your desired column type and columns. You should see the Correlation type in the side
panel. For numeric comparisons, you have the option to select either Pearson or Spearman. For
categorical comparisons, the correlation type is set as MI. For categorical and mixed comparisons,
the correlation type is set as Spearman & MI.

For matrices that only compare numeric columns, the correlation type is either Pearson or
Spearman. The Pearson measure evaluates the linear relationship between two continuous
variables. The Spearman measure evaluates the monotonic relationship between two variables.
For both Pearson and Spearman, the scale of correlation ranges from -1 to 1, with either end of
the scale indicating a perfect correlation (a direct 1:1 relationship) and 0 indicating no correlation.
You might want to select Pearson if your data has more linear relationships (as revealed by a
scatter plot visualization). If your data is not linear, or contains a mixture of linear and monotonic
relationships, then you might want to select Spearman.

Custom models
1267

## Page 297

Amazon SageMaker AI
Developer Guide

For matrices that only compare categorical columns, the correlation type is set to Mutual
Information Classiﬁcation (MI). The MI value is a measure of the mutual dependence between two
random variables. The MI measure is on a scale of 0 to 1, with 0 indicating no correlation and 1
indicating a perfect correlation.

For matrices that compare a mix of numeric and categorical columns, the correlation type
Spearman & MI is a combination of the Spearman and MI correlation types. For correlations
between two numeric columns, the matrix shows the Spearman value. For correlations between a
numeric and categorical column or two categorical columns, the matrix shows the MI value.

Lastly, remember that correlation does not necessarily indicate causation. A strong correlation
value only indicates that there is a relationship between two variables, but the variables might not
have a causal relationship. Carefully review your columns of interest to avoid bias when building
your model.

3. Filter your correlations

In the side panel, you can use the Filter correlations feature to ﬁlter for the range of correlation
values that you want to include in the matrix. For example, if you want to ﬁlter for features that
only have positive or neutral correlation, you can set the Min to 0 and the Max to 1 (valid values
are -1 to 1).

For Spearman and Pearson comparisons, you can set the Filter correlations range anywhere from
-1 to 1, with 0 meaning that there is no correlation. -1 and 1 mean that the variables have a strong
negative or positive correlation, respectively.

For MI comparisons, the correlation range only goes from 0 to 1, with 0 meaning that there is no
correlation and 1 meaning that the variables have a strong correlation, either positive or negative.

Each feature has a perfect correlation (1) with itself. Therefore, you might notice that the top row
of the correlation matrix is always 1. If you want to exclude these values, you can use the ﬁlter to
set the Max less than 1.

Keep in mind that if your matrix compares a mix of numeric and categorical columns and uses
the Spearman & MI correlation type, then the categorical x numeric and categorical x categorical
correlations (which use the MI measure) are on a scale of 0 to 1, whereas the numeric x numeric
correlations (which use the Spearman measure) are on a scale of -1 to 1. Review your correlations
of interest carefully to ensure that you know the correlation type being used to calculate each
value.

Custom models
1268

## Page 298

Amazon SageMaker AI
Developer Guide

4. Choose the visualization method

In the side panel, you can use Visualize by to change the visualization method of the matrix.
Choose the Numeric visualization method to show the correlation (Pearson, Spearman, or MI)
value, or choose the Size visualization method to visualize the correlation with diﬀerently sized and
colored dots. If you choose Size, you can hover over a speciﬁc dot on the matrix to see the actual
correlation value.

5. Choose a color palette

In the side panel, you can use Color selection to change the color palette used for the scale of
negative to positive correlation in the matrix. Select one of the alternative color palettes to change
the colors used in the matrix.

Prepare data for model building

Note

You can now do advanced data preparation in SageMaker Canvas with Data
Wrangler, which provides you with a natural language interface and over 300 built-in
transformations. For more information, see Data preparation.

Your machine learning dataset might require data preparation before you build your model. You
might want to clean your data due to various issues, which might include missing values or outliers,
and perform feature engineering to improve the accuracy of your model. Amazon SageMaker
Canvas provides ML data transforms with which you can clean, transform, and prepare your data
for model building. You can use these transforms on your datasets without any code. SageMaker
Canvas adds the transforms you use to the Model recipe, which is a record of the data preparation
done on your data before building the model. Any data transforms you use only modify the input
data for model building and do not modify your original data source.

The preview of your dataset shows the ﬁrst 100 rows of the dataset. If your dataset has more than
20,000 rows, Canvas takes a random sample of 20,000 rows and previews the ﬁrst 100 rows from
that sample. You can only search for and specify values from the previewed rows, and the ﬁlter
functionality only ﬁlters the previewed rows and not the entire dataset.

The following transforms are available in SageMaker Canvas for you to prepare your data for
building.

Custom models
1269

## Page 299

Amazon SageMaker AI
Developer Guide

Note

You can only use advanced transformations for models built on tabular datasets. Multi-
category text prediction models are also excluded.

Drop columns

You can exclude a column from your model build by dropping it in the Build tab of the SageMaker
Canvas application. Deselect the column you want to drop, and it isn't included when building the
model.

Note

If you drop columns and then make batch predictions with your model, SageMaker Canvas
adds the dropped columns back to the ouput dataset available for you to download.
However, SageMaker Canvas does not add the dropped columns back for time series
models.

Filter rows

The ﬁlter functionality ﬁlters the previewed rows (the ﬁrst 100 rows of your dataset) according
to conditions that you specify. Filtering rows creates a temporary preview of the data and does
not impact the model building. You can ﬁlter to preview rows that have missing values, contain
outliers, or meet custom conditions in a column you choose.

Filter rows by missing values

Missing values are a common occurrence in machine learning datasets. If you have rows with null or
empty values in certain columns, you might want to ﬁlter for and preview those rows.

To ﬁlter missing values from your previewed data, do the following.

1.
In the Build tab of the SageMaker Canvas application, choose Filter by rows
(

).

2.
Choose the Column you want to check for missing values.

3.
For the Operation, choose Is missing.

Custom models
1270

## Page 300

Amazon SageMaker AI
Developer Guide

SageMaker Canvas ﬁlters for rows that contain missing values in the Column you selected and
provides a preview of the ﬁltered rows.

![Page 300 Diagram 1](images/page-0300-img-01.png)

Filter rows by outliers

Outliers, or rare values in the distribution and range of your data, can negatively impact model
accuracy and lead to longer building times. SageMaker Canvas enables you to detect and ﬁlter rows
that contain outliers in numeric columns. You can choose to deﬁne outliers with either standard
deviations or a custom range.

To ﬁlter for outliers in your data, do the following.

1.
In the Build tab of the SageMaker Canvas application, choose Filter by rows
(

).

2.
Choose the Column you want to check for outliers.

3.
For the Operation, choose Is outlier.

4.
Set the Outlier range to either Standard deviation or Custom range.

5.
If you choose Standard deviation, specify a SD (standard deviation) value from 1–3. If you
choose Custom range, select either Percentile or Number, and then specify the Min and Max
values.

The Standard deviation option detects and ﬁlters for outliers in numeric columns using the mean
and standard deviation. You specify the number of standard deviations a value must vary from the

Custom models
1271

## Page 301

Amazon SageMaker AI
Developer Guide

mean to be considered an outlier. For example, if you specify 3 for SD, a value must fall more than
3 standard deviations from the mean to be considered an outlier.

The Custom range option detects and ﬁlters for outliers in numeric columns using minimum and
maximum values. Use this method if you know your threshold values that delimit outliers. You can
set the Type of the range to either Percentile or Number. If you choose Percentile, the Min and
Max values should be the minimum and maximum of the percentile range (0-100) that you want
to allow. If you choose Number, the Min and Max values should be the minimum and maximum
numeric values that you want to ﬁlter in the data.

![Page 301 Diagram 1](images/page-0301-img-01.png)

Filter rows by custom values

You can ﬁlter for rows with values that meet custom conditions. For example, you might want
to preview rows that have a price value greater than 100 before removing them. With this
functionality, you can ﬁlter rows that exceed the threshold you set and preview the ﬁltered data.

To use the custom ﬁlter functionality, do the following.

1.
In the Build tab of the SageMaker Canvas application, choose Filter by rows
(

).

2.
Choose the Column you want to check.

3.
Select the type of Operation you want to use, and then specify the values for the selected
condition.

Custom models
1272

## Page 302

Amazon SageMaker AI
Developer Guide

For the Operation, you can choose one of the following options. Note that the available operations

depend on the data type of the column you choose. For example, you cannot create a is greater

than operation for a column containing text values.

Operation
Supported data
type

Supported
feature type

Function

Is equal to
Numeric, Text
Binary, Categoric
al

Filters rows where the value in
Column equals the values you
specify.

Is not equal to
Numeric, Text
Binary, Categoric
al

Filters rows where the value in
Column doesn't equal the values
you specify.

Is less than
Numeric
N/A
Filters rows where the value in
Column is less than the value you
specify.

Is less than or
equal to

Numeric
N/A
Filters rows where the value in
Column is less than or equal to the
value you specify.

Is greater than
Numeric
N/A
Filters rows where the value in
Column is greater than the value
you specify.

Is greater than
or equal to

Numeric
N/A
Filters rows where the value in
Column is greater than or equal to
the value you specify.

Is between
Numeric
N/A
Filters rows where the value in
Column is between or equal to two
values you specify.

Contains
Text
Categorical
Filters rows where the value in
Column contains a values you
specify.

Custom models
1273

## Page 303

Amazon SageMaker AI
Developer Guide

Operation
Supported data
type

Supported
feature type

Function

Starts with
Text
Categorical
Filters rows where the value in
Column begins with a value you
specify.

Ends with
Categorical
Categorical
Filters rows where the value in
Column ends with a value you
specify.

After you set the ﬁlter operation, SageMaker Canvas updates the preview of the dataset to show
you the ﬁltered data.

![Page 303 Diagram 1](images/page-0303-img-01.png)

Functions and operators

You can use mathematical functions and operators to explore and distribute your data. You can use
the SageMaker Canvas supported functions or create your own formula with your existing data and
create a new column with the result of the formula. For example, you can add the corresponding
values of two columns and save the result to a new column.

You can nest statements to create more complex functions. The following are some examples of
nested functions that you might use.

Custom models
1274

## Page 304

Amazon SageMaker AI
Developer Guide

• To calculate BMI, you could use the function weight / (height ^ 2).

• To classify ages, you could use the function Case(age < 18, 'child', age < 65,

'adult', 'senior').

You can specify functions in the data preparation stage before you build your model. To use a
function, do the following.

• In the Build tab of the SageMaker Canvas application, choose View all and then choose Custom
formula to open the Custom formula panel.

• In the Custom formula panel, you can choose a Formula to add to your Model Recipe. Each
formula is applied to all of the values in the columns you specify. For formulas that accept two or
more columns as arguments, use columns with matching data types; otherwise, you get an error

or null values in the new column.

• After you’ve speciﬁed a Formula, add a column name in the New Column Name ﬁeld.
SageMaker Canvas uses this name for the new column that is created.

• (Optional) Choose Preview to preview your transform.

• To add the function to your Model Recipe, choose Add.

SageMaker Canvas saves the result of your function to a new column using the name you speciﬁed
in New Column Name. You can view or remove functions from the Model Recipe panel.

SageMaker Canvas supports the following operators for functions. You can use either the text
format or the in-line format to specify your function.

Operator
Description
Supported
data types

Text format
In-line
format

Add
Returns the sum of the values
Numeric
Add(sales1,
sales2)

sales1 +
sales2

Subtract
Returns the diﬀerence
between the values

Numeric
Subtract(
sales1,
sales2)

sales1 ‐
sales2

Custom models
1275

## Page 305

Amazon SageMaker AI
Developer Guide

Operator
Description
Supported
data types

Text format
In-line
format

Multiply
Returns the product of the
values

Numeric
Multiply(
sales1,
sales2)

sales1 *
sales2

Divide
Returns the quotient of the
values

Numeric
Divide(sales1,
sales2)

sales1 /
sales2

Mod
Returns the result of the
modulo operator (the
remainder after dividing the
two values)

Numeric
Mod(sales1,
sales2)

sales1 %
sales2

Abs
Returns the absolute value of
the value

Numeric
Abs(sales1)
N/A

Negate
Returns the negative of the
value

Numeric
Negate(c1)
‐c1

Exp
Returns e (Euler's number)
raised to the power of the
value

Numeric
Exp(sales1)
N/A

Log
Returns the logarithm (base
10) of the value

Numeric
Log(sales1)
N/A

Ln
Returns the natural logarithm
(base e) of the value

Numeric
Ln(sales1)
N/A

Pow
Returns the value raised to a
power

Numeric
Pow(sales1,
2)

sales1 ^ 2

If
Returns a true or false label
based on a condition you
specify

Boolean,
Numeric,
Text

If(sales1
>7000,
'truelabel,
'falselabel')

N/A

Custom models
1276

## Page 306

Amazon SageMaker AI
Developer Guide

Operator
Description
Supported
data types

Text format
In-line
format

Or
Returns a Boolean value of
whether one of the speciﬁed
values or conditions is true or
not

Boolean
Or(fullprice,
discount)

fullprice ||
discount

And
Returns a Boolean value of
whether two of the speciﬁed
values or conditions are true
or not

Boolean
And(sales
1,sales2)

sales1 &&
sales2

Not
Returns a Boolean value
that is the opposite of the
speciﬁed value or conditions

Boolean
Not(sales1)
!sales1

Case
Returns a Boolean value
based on conditional
statements (returns c1 if
cond1 is true, returns c2 if
cond2 is true, else returns c3)

Boolean,
Numeric,
Text

Case(cond1,
c1, cond2, c2,
c3)

N/A

Equal
Returns a Boolean value of
whether two values are equal

Boolean,
Numeric,
Text

N/A
c1 = c2

c1 == c2

Not equal
Returns a Boolean value of
whether two values are not
equal

Boolean,
Numeric,
Text

N/A
c1 != c2

Less than
Returns a Boolean value of
whether c1 is less than c2

Boolean,
Numeric,
Text

N/A
c1 < c2

Greater than
Returns a Boolean value of
whether c1 is greater than c2

Boolean,
Numeric,
Text

N/A
c1 > c2

Custom models
1277

## Page 307

Amazon SageMaker AI
Developer Guide

Operator
Description
Supported
data types

Text format
In-line
format

Less than or
equal

Returns a Boolean value of
whether c1 is less than or
equal to c2

Boolean,
Numeric,
Text

N/A
c1 <= c2

Greater than
or equal

Returns a Boolean value of
whether c1 is greater than or
equal to c2

Boolean,
Numeric,
Text

N/A
c1 >= c2

SageMaker Canvas also supports aggregate operators, which can perform operations such as
calculating the sum of all the values or ﬁnding the minimum value in a column. You can use
aggregate operators in combination with standard operators in your functions. For example, to

calculate the diﬀerence of values from the mean, you could use the function Abs(height –

avg(height)). SageMaker Canvas supports the following aggregate operators.

Aggregate
operator

Description
Format
Example

sum
Returns the sum of all the values in
a column

sum
sum(c1)

minimum
Returns the minimum value of a
column

min
min(c2)

maximum
Returns the maximum value of a
column

max
max(c3)

average
Returns the average value of a
column

avg
avg(c4)

std
Returns the sample standard
deviation of a column

std
std(c1)

stddev
Returns the standard deviation of
the values in a column

stddev
stddev(c1)

Custom models
1278

## Page 308

Amazon SageMaker AI
Developer Guide

Aggregate
operator

Description
Format
Example

variance
Returns the unbiased variance of the
values in a column

variance
variance(c1)

approx_co
unt_distinct

Returns the approximate number of
distinct items in a column

approx_co
unt_distinct

approx_co
unt_distinct(c1)

count
Returns the number of items in a
column

count
count(c1)

ﬁrst
Returns the ﬁrst value of a column
ﬁrst
ﬁrst(c1)

last
Returns the last value of a column
last
last(c1)

stddev_pop
Returns the population standard
deviation of a column

stddev_pop
stddev_pop(c1)

variance_pop
Returns the population variance of
the values in a column

variance_pop
variance_pop(c1)

Manage rows

With the Manage rows transform, you can perform sort, random shuﬄe, and remove rows of data
from the dataset.

Sort rows

To sort the rows in a dataset by a given column, do the following.

1. In the Build tab of the SageMaker Canvas application, choose Manage rows and then choose

Sort rows.

2. For Sort Column, choose the column you want to sort by.

3. For Sort Order, choose either Ascending or Descending.

4. Choose Add to add the transform to the Model recipe.

Custom models
1279

## Page 309

Amazon SageMaker AI
Developer Guide

Shuﬄe rows

To randomly shuﬄe the rows in a dataset, do the following.

1. In the Build tab of the SageMaker Canvas application, choose Manage rows and then choose

Shuﬄe rows.

2. Choose Add to add the transform to the Model recipe.

Drop duplicate rows

To remove duplicate rows in a dataset, do the following.

1. In the Build tab of the SageMaker Canvas application, choose Manage rows and then choose

Drop duplicate rows.

2. Choose Add to add the transform to the Model recipe.

Remove rows by missing values

Missing values are a common occurrence in machine learning datasets and can impact model
accuracy. Use this transform if you want to drop rows with null or empty values in certain columns.

To remove rows that contain missing values in a speciﬁed column, do the following.

1.
In the Build tab of the SageMaker Canvas application, choose Manage rows.

2.
Choose Drop rows by missing values.

3.
Choose Add to add the transform to the Model recipe.

SageMaker Canvas drops rows that contain missing values in the Column you selected. After
removing the rows from the dataset, SageMaker Canvas adds the transform in the Model recipe
section. If you remove the transform from the Model recipe section, the rows return to your
dataset.

Custom models
1280

## Page 310

Amazon SageMaker AI
Developer Guide

![Page 310 Diagram 1](images/page-0310-img-01.png)

Remove rows by outliers

Outliers, or rare values in the distribution and range of your data, can negatively impact model
accuracy and lead to longer building times. With SageMaker Canvas, you can detect and remove
rows that contain outliers in numeric columns. You can choose to deﬁne outliers with either
standard deviations or a custom range.

To remove outliers from your data, do the following.

1.
In the Build tab of the SageMaker Canvas application, choose Manage rows.

2.
Choose Drop rows by outlier values.

3.
Choose the Column you want to check for outliers.

4.
Set the Operator to Standard deviation, Custom numeric range, or Custom quantile range.

5.
If you choose Standard deviation, specify a Standard deviations (standard deviation) value
from 1–3. If you choose Custom numeric range or Custom quantile range, specify the Min
and Max values (numbers for numeric ranges, or percentiles between 0–100% for quantile
ranges).

6.
Choose Add to add the transform to the Model recipe.

The Standard deviation option detects and removes outliers in numeric columns using the mean
and standard deviation. You specify the number of standard deviations a value must vary from the

Custom models
1281

## Page 311

Amazon SageMaker AI
Developer Guide

mean to be considered an outlier. For example, if you specify 3 for Standard deviations, a value
must fall more than 3 standard deviations from the mean to be considered an outlier.

The Custom numeric range and Custom quantile range options detect and remove outliers
in numeric columns using minimum and maximum values. Use this method if you know your
threshold values that delimit outliers. If you choose a numeric range, the Min and Max values
should be the minimum and maximum numeric values that you want to allow in the data. If you
choose a quantile range, the Min and Max values should be the minimum and maximum of the
percentile range (0–100) that you want to allow.

After removing the rows from the dataset, SageMaker Canvas adds the transform in the Model
recipe section. If you remove the transform from the Model recipe section, the rows return to your
dataset.

![Page 311 Diagram 1](images/page-0311-img-01.png)

Remove rows by custom values

You can remove rows with values that meet custom conditions. For example, you might want to
exclude all of the rows with a price value greater than 100 when building your model. With this
transform, you can create a rule that removes all rows that exceed the threshold you set.

To use the custom remove transform, do the following.

1.
In the Build tab of the SageMaker Canvas application, choose Manage rows.

2.
Choose Drop rows by formula.

Custom models
1282

## Page 312

Amazon SageMaker AI
Developer Guide

3.
Choose the Column you want to check.

4.
Select the type of Operation you want to use, and then specify the values for the selected
condition.

5.
Choose Add to add the transform to the Model recipe.

For the Operation, you can choose one of the following options. Note that the available operations

depend on the data type of the column you choose. For example, you cannot create a is greater

than operation for a column containing text values.

Operation
Supported data
type

Supported
feature type

Function

Is equal to
Numeric, Text
Binary, Categoric
al

Removes rows where the value
in Column equals the values you
specify.

Is not equal to
Numeric, Text
Binary, Categoric
al

Removes rows where the value in
Column doesn't equal the values
you specify.

Is less than
Numeric
N/A
Removes rows where the value in
Column is less than the value you
specify.

Is less than or

Numeric
N/A
Removes rows where the value in

Column is less than or equal to the
value you specify.

equal to

Is greater than
Numeric
N/A
Removes rows where the value in
Column is greater than the value
you specify.

Is greater than
or equal to

Numeric
N/A
Removes rows where the value in
Column is greater than or equal to
the value you specify.

Custom models
1283

## Page 313

Amazon SageMaker AI
Developer Guide

Operation
Supported data
type

Supported
feature type

Function

Is between
Numeric
N/A
Removes rows where the value in
Column is between or equal to two
values you specify.

Contains
Text
Categorical
Removes rows where the value
in Column contains a values you
specify.

Starts with
Text
Categorical
Removes rows where the value in
Column begins with a value you
specify.

Ends with
Text
Categorical
Removes rows where the value
in Column ends with a value you
specify.

After removing the rows from the dataset, SageMaker Canvas adds the transform in the Model
recipe section. If you remove the transform from the Model recipe section, the rows return to your
dataset.

![Page 313 Diagram 1](images/page-0313-img-01.png)

Custom models
1284

## Page 314

Amazon SageMaker AI
Developer Guide

Rename columns

With the rename columns transform, you can rename columns in your data. When you rename a
column, SageMaker Canvas changes the column name in the model input.

You can rename a column in your dataset by double-clicking on the column name
in the Build tab of the SageMaker Canvas application and entering a new name.
Pressing the Enter key submits the change, and clicking anywhere outside the input
cancels the change. You can also rename a column by clicking the More options icon
(

),
located at the end of the row in list view or at the end of the header cell in grid view, and choosing
Rename.

Your column name can’t be longer than 32 characters or have double underscores (__), and you
can’t rename a column to the same name as another column. You also can’t rename a dropped
column.

The following screenshot shows how to rename a column by double-clicking the column name.

![Page 314 Diagram 1](images/page-0314-img-01.png)

When you rename a column, SageMaker Canvas adds the transform in the Model recipe section. If
you remove the transform from the Model recipe section, the column reverts to its original name.

Custom models
1285

## Page 315

Amazon SageMaker AI
Developer Guide

Manage columns

With the following transforms, you can change the data type of columns and replace missing
values or outliers for speciﬁc columns. SageMaker Canvas uses the updated data types or values
when building your model but doesn’t change your original dataset. Note that if you've dropped
a column from your dataset using the Drop columns transform, you can't replace values in that
column.

Replace missing values

Missing values are a common occurrence in machine learning datasets and can impact model
accuracy. You can choose to drop rows that have missing values, but your model is more accurate
if you choose to replace the missing values instead. With this transform, you can replace missing
values in numeric columns with the mean or median of the data in a column, or you can also
specify a custom value with which to replace missing values. For non-numeric columns, you can
replace missing values with the mode (most common value) of the column or a custom value.

Use this transform if you want to replace the null or empty values in certain columns. To replace
missing values in a speciﬁed column, do the following.

1.
In the Build tab of the SageMaker Canvas application, choose Manage columns.

2.
Choose Replace missing values.

3.
Choose the Column in which you want to replace missing values.

4.
Set Mode to Manual to replace missing values with values that you specify. With the
Automatic (default) setting, SageMaker Canvas replaces missing values with imputed values
that best ﬁt your data. This imputation method is done automatically for each model build,
unless you specify the Manual mode.

5.
Set the Replace with value:

• If your column is numeric, then select Mean, Median, or Custom. Mean replaces missing
values with the mean for the column, and Median replaces missing values with the median
for the column. If you choose Custom, then you must specify a custom value that you want
to use to replace missing values.

• If your column is non-numeric, then select Mode or Custom. Mode replaces missing values
with the mode, or the most common value, for the column. For Custom, specify a custom
value. that you want to use to replace missing values.

6.
Choose Add to add the transform to the Model recipe.

Custom models
1286

## Page 316

Amazon SageMaker AI
Developer Guide

After replacing the missing values in the dataset, SageMaker Canvas adds the transform in the
Model recipe section. If you remove the transform from the Model recipe section, the missing
values return to the dataset.

![Page 316 Diagram 1](images/page-0316-img-01.png)

Replace outliers

Outliers, or rare values in the distribution and range of your data, can negatively impact model
accuracy and lead to longer building times. SageMaker Canvas enables you to detect outliers in
numeric columns and replace the outliers with values that lie within an accepted range in your
data. You can choose to deﬁne outliers with either standard deviations or a custom range, and you
can replace outliers with the minimum and maximum values in the accepted range.

To replace outliers in your data, do the following.

1.
In the Build tab of the SageMaker Canvas application, choose Manage columns.

2.
Choose Replace outlier values.

3.
Choose the Column in which you want to replace outliers.

4.
For Deﬁne outliers, choose Standard deviation, Custom numeric range, or Custom quantile
range.

5.
If you choose Standard deviation, specify a Standard deviations (standard deviation) value
from 1–3. If you choose Custom numeric range or Custom quantile range, specify the Min
and Max values (numbers for numeric ranges, or percentiles between 0–100% for quantile
ranges).

Custom models
1287

## Page 317

Amazon SageMaker AI
Developer Guide

6.
For Replace with, select Min/max range.

7.
Choose Add to add the transform to the Model recipe.

The Standard deviation option detects outliers in numeric columns using the mean and standard
deviation. You specify the number of standard deviations a value must vary from the mean to be
considered an outlier. For example, if you specify 3 for Standard deviations, a value must fall more
than 3 standard deviations from the mean to be considered an outlier. SageMaker Canvas replaces
outliers with the minimum value or maximum value in the accepted range. For example, if you
conﬁgure the standard deviations to only include values from 200–300, then SageMaker Canvas
changes a value of 198 to 200 (the minimum).

The Custom numeric range and Custom quantile range options detect outliers in numeric
columns using minimum and maximum values. Use this method if you know your threshold
values that delimit outliers. If you choose a numeric range, the Min and Max values should be the
minimum and maximum numeric values that you want to allow. SageMaker Canvas replaces any
values that fall outside of the minimum and maximum to the minimum and maximum values. For
example, if your range only allows values from 1–100, then SageMaker Canvas changes a value of
102 to 100 (the maximum). If you choose a quantile range, the Min and Max values should be the
minimum and maximum of the percentile range (0–100) that you want to allow.

After replacing the values in the dataset, SageMaker Canvas adds the transform in the Model
recipe section. If you remove the transform from the Model recipe section, the original values
return to the dataset.

Custom models
1288

## Page 318

Amazon SageMaker AI
Developer Guide

![Page 318 Diagram 1](images/page-0318-img-01.png)

Change data type

SageMaker Canvas provides you with the ability to change the data type of your columns between
numeric, text, and datetime, while also displaying the associated feature type for that data type.
A data type refers to the format of the data and how it is stored, while the feature type refers to
the characteristic of the data used in machine learning algorithms, such as binary or categorical.
This gives you the ﬂexibility to manually change the type of data in your columns based on the
features. The ability to choose the right data type ensures data integrity and accuracy prior to
building models. These data types are used when building models.

Note

Currently, changing the feature type (for example, from binary to categorical) is not
supported.

The following table lists all of the supported data types in Canvas.

Custom models
1289

## Page 319

Amazon SageMaker AI
Developer Guide

Data type
Description
Example

Numeric
Numeric data represents
numerical values

1, 2, 3

1.1, 1.2. 1.3

Text
Text data represents
sequences of characters, like
names or descriptions

A, B, C, D

apple, banana, orange

1A!, 2A!, 3A!

Datetime
Datetime data represents
dates and times in timestamp
format

2019-07-01 01:00:00,
2019-07-01 02:00:00,
2019-07-01 03:00:00

The following table lists all of the supported feature types in Canvas.

Feature type
Description
Example

Binary
Binary features represent two
possible values

0, 1, 0, 1, 0 (2 distinct values)

true, false, true (2 distinct
values)

Categorical
Categorical features represent
distinct categories or groups

apple, banana, orange, apple
(3 distinct values)

A, B, C, D, E, A, D, C (5 distinct
values)

To modify data type of a column in a dataset, do the following.

1.
In the Build tab of the SageMaker Canvas application, go to the Column view or Grid view and
select the Data type dropdown for the speciﬁc column.

2.
In the Data type dropdown, choose the data type to convert to. The following screenshot
shows the dropdown menu.

Custom models
1290

## Page 320

Amazon SageMaker AI
Developer Guide

![Page 320 Diagram 1](images/page-0320-img-01.png)

3.
For Column, choose or verify the column you want to change the data type for.

4.
For New data type, choose or verify the new data type you want to convert to.

5.
If the New data type is Datetime or Numeric, choose one of the following options under
Handle invalid values:

a.
Replace with empty value – Invalid values are substituted with an empty value

b.
Delete rows – Rows with an invalid value are removed from the dataset

c.
Replace with custom value – Invalid values are substituted with the Custom Value that
you specify.

6.
Choose Add to add the transform to the Model recipe.

The data type for your column should now be updated.

Prepare time series data

Use the following functionalities to prepare your time series data for building time series
forecasting models.

Resample time series data

By resampling time-series data, you can establish regular intervals for the observations in your
time series dataset. This is particularly useful when working with time series data containing
irregularly spaced observations. For instance, you can use resampling to transform a dataset with
observations recorded every one hour, two hour and three hour intervals into a regular one hour

Custom models
1291

## Page 321

Amazon SageMaker AI
Developer Guide

interval between observations. Forecasting algorithms require the observations to be taken at
regular intervals.

To resample time series data, do the following.

1.
In the Build tab of the SageMaker Canvas application, choose Time series.

2.
Choose Resample.

3.
For Timestamp column, choose the column you want to apply the transform to. You can only
select columns of the Datetime type.

4.
In the Frequency settings section, choose a Frequency and Rate. Frequency is the unit of
frequency and Rate is the interval of the unit of frequency to be applied to the column. For

example, choosing Calendar Day for Frequency value and 1 for Rate sets the interval to

increase every 1 calendar day, such as 2023-03-26 00:00:00, 2023-03-27 00:00:00,

2023-03-28 00:00:00. See the table after this procedure for a complete list of Frequency
value.

5.
Choose Add to add the transform to the Model recipe.

The following table lists all of the Frequency types you can select when resampling time series
data.

Frequency
Description
Example values (assuming
Rate is 1)

Business Day
Resample observations in
the datetime column to 5

2023-03-24 00:00:00

2023-03-27 00:00:00

business days of the week
(Monday, Tuesday, Wednesday
, Thursday, Friday)

2023-03-28 00:00:00

2023-03-29 00:00:00

2023-03-30 00:00:00

2023-03-31 00:00:00

2023-04-03 00:00:00

Calendar Day
Resample observations
in the datetime column

2023-03-26 00:00:00

Custom models
1292

## Page 322

Amazon SageMaker AI
Developer Guide

Frequency
Description
Example values (assuming
Rate is 1)

to all 7 days of the week
(Monday, Tuesday, Wednesday
, Thursday, Friday, Saturday,
Sunday)

2023-03-27 00:00:00

2023-03-28 00:00:00

2023-03-29 00:00:00

2023-03-30 00:00:00

2023-03-31 00:00:00

2023-04-01 00:00:00

Week
Resample observations in the
datetime column to the ﬁrst
day of each week

2023-03-13 00:00:00

2023-03-20 00:00:00

2023-03-27 00:00:00

2023-04-03 00:00:00

Month
Resample observations in the
datetime column to the ﬁrst
day of each month

2023-03-01 00:00:00

2023-04-01 00:00:00

2023-05-01 00:00:00

2023-06-01 00:00:00

Annual Quarter
Resample observations in the
datetime column to the last
day of each quarter

2023-03-31 00:00:00

2023-06-30 00:00:00

2023-09-30 00:00:00

2023-12-31 00:00:00

Custom models
1293

## Page 323

Amazon SageMaker AI
Developer Guide

Frequency
Description
Example values (assuming
Rate is 1)

Year
Resample observations in the
datetime column to the last
day of each year

2022-12-31 0:00:00

2023-12-31 00:00:00

2024-12-31 00:00:00

Hour
Resample observations in the
datetime column to each hour
of each day

2023-03-24 00:00:00

2023-03-24 01:00:00

2023-03-24 02:00:00

2023-03-24 03:00:00

Minute
Resample observations in
the datetime column to each
minute of each hour

2023-03-24 00:00:00

2023-03-24 00:01:00

2023-03-24 00:02:00

2023-03-24 00:03:00

Second
Resample observations in
the datetime column to each
second of each minute

2023-03-24 00:00:00

2023-03-24 00:00:01

2023-03-24 00:00:02

2023-03-24 00:00:03

When applying the resampling transform, you can use the Advanced option to specify how the
resulting values of the rest of the columns (other than the timestamp column) in your dataset are
modiﬁed. This can be achieved by specifying the resampling methodology, which can either be
downsampling or upsampling for both numeric and non-numeric columns.

Downsampling increases the interval between observations in the dataset. For example, if you
downsample observations that are taken either every hour or every two hours, each observation
in your dataset is taken every two hours. The values of other columns of the hourly observations

Custom models
1294

## Page 324

Amazon SageMaker AI
Developer Guide

are aggregated into a single value using a combination method. The following tables show an
example of downsampling time series data by using mean as the combination method. The data is
downsampled from every two hours to every hour.

The following table shows the hourly temperature readings over a day before downsampling.

Timestamp
Temperature (Celsius)

12:00 pm
30

1:00 am
32

2:00 am
35

3:00 am
32

4:00 am
30

The following table shows the temperature readings after downsampling to every two hours.

Timestamp
Temperature (Celsius)

12:00 pm
30

2:00 am
33.5

2:00 am
35

4:00 am
32.5

To downsample time series data, do the following:

1.
Expand the Advanced  section under the Resample transform.

2.
Choose Non-numeric combination to specify the combination method for non-numeric
columns. See the table below for a complete list of combination methods.

3.
Choose Numeric combination to specify the combination method for numeric columns. See
the table below for a complete list of combination methods.

Custom models
1295

## Page 325

Amazon SageMaker AI
Developer Guide

If you don’t specify combination methods, the default values are Most Common for Non-numeric

combination and Mean for Numeric combination. The following table lists the methods for

numeric and non-numeric combination.

Downsampling methodology
Combination method
Description

Non-numeric combination
Most Common
Aggregate values in the non-
numeric column by the most
commonly ocurring value

Non-numeric combination
Last
Aggregate values in the non-
numeric column by the last
value in the column

Non-numeric combination
First
Aggregate values in the non-
numeric column by the ﬁrst
value in the column

Numeric combination
Mean
Aggregate values in the
numeric column by the taking
the mean of all the values in
the column

Numeric combination
Median
Aggregate values in the
numeric column by the taking
the median of all the values in
the column

Numeric combination
Min
Aggregate values in the
numeric column by the taking
the minimum of all the values
in the column

Numeric combination
Max
Aggregate values in the
numeric column by the taking
the maximum of all the
values in the column

Custom models
1296

## Page 326

Amazon SageMaker AI
Developer Guide

Downsampling methodology
Combination method
Description

Numeric combination
Sum
Aggregate values in the
numeric column by adding all
the values in the column

Numeric combination
Quantile
Aggregate values in the
numeric column by the taking
the quantile of all the values
in the column

Upsampling reduces the interval between observations in the dataset. For example, if you
upsample observations that are taken every two hours into hourly observations, the values of
other columns of the hourly observations are interpolated from the ones that have been taken
every two hours.

To upsample time series data, do the following:

1.
Expand the Advanced section under the Resample transform.

2.
Choose Non-numeric estimation to specify the estimation method for non-numeric columns.
See the table after this procedure for a complete list of methods.

3.
Choose Numeric estimation to specify the estimation method for numeric columns. See the
table below for a complete list of methods.

4.
(Optional) Choose ID Column to specify the column that has the IDs of the observations of
the time series. Specify this option if your dataset has two time series. If you have a column
representing only one time series, don't specify a value for this ﬁeld. For example, you can

have a dataset that has the columns id and purchase. The id column has the following

values: [1, 2, 2, 1]. The purchase column has the following values [$2, $3, $4, $1].

Therefore, the dataset has two time series—one time series is: 1: [$2, $1], and the other

time series is 2: [$3, $4].

If you don’t specify estimation methods, the default values are Forward Fill for Non-numeric

estimation and Linear for Numeric estimation. The following table lists the methods for
estimation.

Custom models
1297

## Page 327

Amazon SageMaker AI
Developer Guide

Upsampling methodology
Estimation method
Description

Non-numeric estimation
Forward Fill
Interpolate values in the non-
numeric column by taking the

consecutive values after all
the values in the column

Non-numeric estimation
Backward Fill
Interpolate values in the non-
numeric column by taking the
consecutive values before all
the values in the column

Non-numeric estimation
Keep Missing
Interpolate values in the non-
numeric column by showing
empty values

Numeric estimation
Linear, Time, Index, Zero, S-
Linear, Nearest, Quadratic,
Cubic, Barycentric, Polynomia
l, Krogh, Piecewise Polynomia
l, Spline, P-chip, Akima, Cubic
Spline, From Derivatives

Interpolate values in the
numeric column by using
the specﬁed interpolator.
For information on interpola
tion methods, see pandas.Da
taFrame.interpolate in the
pandas documentation.

The following screenshot shows the Advanced settings with the ﬁelds for downsampling and
upsampling ﬁlled out.

Custom models
1298

## Page 328

Amazon SageMaker AI
Developer Guide

![Page 328 Diagram 1](images/page-0328-img-01.png)

Use datetime extraction

With the datetime extraction transform, you can extract values from a datetime column to a
separate column. For example, if you have a column containing dates of purchases, you can extract
the month value to a separate column and use the new column when building your model. You can
also extract multiple values to separate columns with a single transform.

Your datetime column must use a supported timestamp format. For a list of the formats that
SageMaker Canvas supports, see Time Series Forecasts in Amazon SageMaker Canvas. If your
dataset does not use one of the supported formats, update your dataset to use a supported
timestamp format and re-import it to Amazon SageMaker Canvas before building your model.

To perform a datetime extraction, do the following.

1.
In the Build tab of the SageMaker Canvas application, on the transforms bar, choose View all.

2.
Choose Extract features.

3.
Choose the Timestamp column from which you want to extract values.

Custom models
1299

## Page 329

Amazon SageMaker AI
Developer Guide

4.
For Values, select one or more values to extract from the column. The values you can extract
from a timestamp column are Year, Month, Day, Hour, Week of year, Day of year, and
Quarter.

5.
(Optional) Choose Preview to preview the transform results.

6.
Choose Add to add the transform to the Model recipe.

SageMaker Canvas creates a new column in the dataset for each of the values you extract. Except
for Year values, SageMaker Canvas uses a 0-based encoding for the extracted values. For example,
if you extract the Month value, January is extracted as 0, and February is extracted as 1.

![Page 329 Diagram 1](images/page-0329-img-01.png)

You can see the transform listed in the Model recipe section. If you remove the transform from the
Model recipe section, the new columns are removed from the dataset.

Model evaluation

After you’ve built your model, you can evaluate how well your model performed on your data
before using it to make predictions. You can use information, such as the model’s accuracy when
predicting labels and advanced metrics, to determine whether your model can make suﬃciently
accurate predictions for your data.

The section Evaluate your model's performance describes how to view and interpret the
information on your model's Analyze page. The section Use advanced metrics in your analyses

Custom models
1300

## Page 330

Amazon SageMaker AI
Developer Guide

contains more detailed information about the Advanced metrics used to quantify your model’s
accuracy.

You can also view more advanced information for speciﬁc model candidates, which are all of the
model iterations that Canvas runs through while building your model. Based on the advanced
metrics for a given model candidate, you can select a diﬀerent candidate to be the default, or the
version that is used for making predictions and deploying. For each model candidate, you can view
the Advanced metrics information to help you decide which model candidate you’d like to select
as the default. You can view this information by selecting the model candidate from the Model
leaderboard. For more information, see View model candidates in the model leaderboard.

Canvas also provides the option to download a Jupyter notebook so that you can view and run the
code used to build your model. This is useful if you’d like to make adjustments to the code or learn
more about how your model was built. For more information, see Download a model notebook.

Evaluate your model's performance

Amazon SageMaker Canvas provides overview and scoring information for the diﬀerent types
of model. Your model’s score can help you determine how accurate your model is when it makes
predictions. The additional scoring insights can help you quantify the diﬀerences between the
actual and predicted values.

To view the analysis of your model, do the following:

1. Open the SageMaker Canvas application.

2. In the left navigation pane, choose My models.

3. Choose the model that you built.

4. In the top navigation pane, choose the Analyze tab.

5. Within the Analyze tab, you can view the overview and scoring information for your model.

The following sections describe how to interpret the scoring for each model type.

Evaluate categorical prediction models

The Overview tab shows you the column impact for each column. Column impact is a percentage
score that indicates how much weight a column has in making predictions in relation to the other
columns. For a column impact of 25%, Canvas weighs the prediction as 25% for the column and
75% for the other columns.

Custom models
1301

## Page 331

Amazon SageMaker AI
Developer Guide

The following screenshot shows the Accuracy score for the model, along with the Optimization
metric, which is the metric that you choose to optimize when building the model. In this case, the
Optimization metric is Accuracy. You can specify a diﬀerent optimization metric if you build a new
version of your model.

![Page 331 Diagram 1](images/page-0331-img-01.png)

The Scoring tab for a categorical prediction model gives you the ability to visualize all the
predictions. Line segments extend from the left of the page, indicating all the predictions the
model has made. In the middle of the page, the line segments converge on a perpendicular
segment to indicate the proportion of each prediction to a single category. From the predicted
category, the segments branch out to the actual category. You can get a visual sense of how
accurate the predictions were by following each line segment from the predicted category to the
actual category.

The following image gives you an example Scoring section for a 3+ category prediction model.

Custom models
1302

## Page 332

Amazon SageMaker AI
Developer Guide

![Page 332 Diagram 1](images/page-0332-img-01.png)

You can also view the Advanced metrics tab for more detailed information about your model’s
performance, such as the advanced metrics, error density plots, or confusion matrices. To learn
more about the Advanced metrics tab, see Use advanced metrics in your analyses.

Evaluate numeric prediction models

The Overview tab shows you the column impact for each column. Column impact is a percentage
score that indicates how much weight a column has in making predictions in relation to the other
columns. For a column impact of 25%, Canvas weighs the prediction as 25% for the column and
75% for the other columns.

The following screenshot shows the RMSE score for the model on the Overview tab, which in
this case is the Optimization metric. The Optimization metric is the metric that you choose to
optimize when building the model. You can specify a diﬀerent optimization metric if you build a
new version of your model.

Custom models
1303

## Page 333

Amazon SageMaker AI
Developer Guide

The Scoring tab for numeric prediction shows a line to indicate the model's predicted value in
relation to the data used to make predictions. The values of the numeric prediction are often +/-
the RMSE (root mean squared error) value. The value that the model predicts is often within the
range of the RMSE. The width of the purple band around the line indicates the RMSE range. The
predicted values often fall within the range.

The following image shows the Scoring section for numeric prediction.

![Page 333 Diagram 1](images/page-0333-img-01.png)

You can also view the Advanced metrics tab for more detailed information about your model’s
performance, such as the advanced metrics, error density plots, or confusion matrices. To learn
more about the Advanced metrics tab, see Use advanced metrics in your analyses.

Custom models
1304

## Page 334

Amazon SageMaker AI
Developer Guide

Evaluate time series forecasting models

On the Analyze page for time series forecasting models, you can see an overview of the model’s
metrics. You can hover over each metric for more information, or you can see Use advanced metrics
in your analyses for more information about each metric.

In the Column impact section, you can see the score for each column. Column impact is a
percentage score that indicates how much weight a column has in making predictions in relation
to the other columns. For a column impact of 25%, Canvas weighs the prediction as 25% for the
column and 75% for the other columns.

The following screenshot shows the time series metrics scores for the model, along with the
Optimization metric, which is the metric that you choose to optimize when building the model. In
this case, the Optimization metric is RMSE. You can specify a diﬀerent optimization metric if you
build a new version of your model. These metrics scores are taken from your backtest results, which
are available for download in the Artifacts tab.

The Artifacts tab provides access to several key resources that you can use to dive deeper into your
model’s performance and continue iterating upon it:

• Shuﬄed training and validation splits – This section includes links to the artifacts generated
when your dataset was split into training and validation sets, enabling you to review the data
distribution and potential biases.

• Backtest results – This section includes a link to the forecasted values for your validation
dataset, which is used to generate accuracy metrics and evaluation data for your model.

• Accuracy metrics – This section lists the advanced metrics that evaluate your model's
performance, such as Root Mean Squared Error (RMSE). For more information about each metric,
see Metrics for time series forecasts.

• Explainability report – This section provides a link to download the explainability report, which
oﬀers insights into the model's decision-making process and the relative importance of input
columns. This report can help you identify potential areas for improvement.

Custom models
1305

## Page 335

Amazon SageMaker AI
Developer Guide

On the Analyze page, you can also choose the Download button to directly download the backtest
results, accuracy metrics, and explainability report artifacts to your local machine.

Evaluate image prediction models

The Overview tab shows you the Per label performance, which gives you an overall accuracy score
for the images predicted for each label. You can choose a label to see more speciﬁc details, such as
the Correctly predicted and Incorrectly predicted images for the label.

You can turn on the Heatmap toggle to see a heatmap for each image. The heatmap shows you
the areas of interest that have the most impact when your model is making predictions. For more
information about heatmaps and how to use them to improve your model, choose the More info
icon next to the Heatmap toggle.

The Scoring tab for single-label image prediction models shows you a comparison of what the
model predicted as the label versus what the actual label was. You can select up to 10 labels at a
time. You can change the labels in the visualization by choosing the labels dropdown menu and
selecting or deselecting labels.

You can also view insights for individual labels or groups of labels, such as the three labels with
the highest or lowest accuracy, by choosing the View scores for dropdown menu in the Model
accuracy insights section.

The following screenshot shows the Scoring information for a single-label image prediction model.

Custom models
1306

## Page 336

Amazon SageMaker AI
Developer Guide

![Page 336 Diagram 1](images/page-0336-img-01.png)

Evaluate text prediction models

The Overview tab shows you the Per label performance, which gives you an overall accuracy
score for the passages of text predicted for each label. You can choose a label to see more speciﬁc
details, such as the Correctly predicted and Incorrectly predicted passages for the label.

The Scoring tab for multi-category text prediction models shows you a comparison of what the
model predicted as the label versus what the actual label was.

In the Model accuracy insights section, you can see the Most frequent category, which tells you
the category that the model predicted most frequently and how accurate those predictions were. If
you model predicts a label of Positive correctly 99% of the time, then you can be fairly conﬁdent
that your model is good at predicting positive sentiment in text.

The following screenshot shows the Scoring information for a multi-category text prediction
model.

Custom models
1307

## Page 337

Amazon SageMaker AI
Developer Guide

![Page 337 Diagram 1](images/page-0337-img-01.png)

Use advanced metrics in your analyses

The following section describes how to ﬁnd and interpret the advanced metrics for your model in
Amazon SageMaker Canvas.

Note

Advanced metrics are only currently available for numeric and categorical prediction
models.

To ﬁnd the Advanced metrics tab, do the following:

1.
Open the SageMaker Canvas application.

2.
In the left navigation pane, choose My models.

3.
Choose the model that you built.

4.
In the top navigation pane, choose the Analyze tab.

5.
Within the Analyze tab, choose the Advanced metrics tab.

Custom models
1308

## Page 338

Amazon SageMaker AI
Developer Guide

In the Advanced metrics tab, you can ﬁnd the Performance tab. The page looks like the following
screenshot.

![Page 338 Diagram 1](images/page-0338-img-01.png)

At the top, you can see an overview of the metrics scores, including the Optimization metric,
which is the metric that you selected (or that Canvas selected by default) to optimize when
building the model.

The following sections describe more detailed information for the Performance tab within the
Advanced metrics.

Performance

In the Performance tab, you’ll see a Metrics table, along with visualizations that Canvas creates
based on your model type. For categorical prediction models, Canvas provides a confusion matrix,
whereas for numeric prediction models, Canvas provides you with residuals and error density charts.

In the Metrics table, you are provided with a full list of your model’s scores for each advanced
metric, which is more comprehensive than the scores overview at the top of the page. The metrics
shown here depend on your model type. For a reference to help you understand and interpret each
metric, see Metrics reference.

To understand the visualizations that might appear based on your model type, see the following
options:

Custom models
1309

## Page 339

Amazon SageMaker AI
Developer Guide

• Confusion matrix – Canvas uses confusion matrices to help you visualize when a model makes
predictions correctly. In a confusion matrix, your results are arranged to compare the predicted
values against the actual values. The following example explains how a confusion matrix works
for a 2 category prediction model that predicts positive and negative labels:

• True positive – The model correctly predicted positive when the true label was positive.

• True negative – The model correctly predicted negative when the true label was negative.

• False positive – The model incorrectly predicted positive when the true label was negative.

• False negative – The model incorrectly predicted negative when the true label was positive.

• Precision recall curve – The precision recall curve is a visualization of the model’s precision score
plotted against the model’s recall score. Generally, a model that can make perfect predictions
would have precision and recall scores that are both 1. The precision recall curve for a decently
accurate model is fairly high in both precision and recall.

• Residuals – Residuals are the diﬀerence between the actual values and the values predicted by
the model. A residuals chart plots the residuals against the corresponding values to visualize
their distribution and any patterns or outliers. A normal distribution of residuals around zero
indicates that the model is a good ﬁt for the data. However, if the residuals are signiﬁcantly
skewed or have outliers, it may indicate that the model is overﬁtting the data or that there are
other issues that need to be addressed.

• Error density – An error density plot is a representation of the distribution of errors made by a
model. It shows the probability density of the errors at each point, helping you to identify any
areas where the model may be overﬁtting or making systematic errors.

View model candidates in the model leaderboard

When you do a Standard build for tabular and time series forecasting models in Amazon
SageMaker Canvas, SageMaker AI trains multiple model candidates (diﬀerent iterations of the
model) and by default selects the one with the highest value for the optimization metric. For
tabular models, Canvas builds up to 250 diﬀerent model candidates using various algorithms and
hyperparameter settings. For time series forecasting models, Canvas builds 7 diﬀerent models—
one for each of the supported forecasting algorithms and one ensemble model that averages the
predictions of the other models to try to optimize accuracy.

The default model candidate is the only version that you can use in Canvas for actions like making
predictions, registering to the model registry, or deploying to an endpoint. However, you might
want to review all of the model candidates and select a diﬀerent candidate to be the default

Custom models
1310

## Page 340

Amazon SageMaker AI
Developer Guide

model. You can view all of the model candidates and more details about each candidate on the
Model leaderboard in Canvas.

To view the Model leaderboard, do the following:

1.
Open the SageMaker Canvas application.

2.
In the left navigation pane, choose My models.

3.
Choose the model that you built.

4.
In the top navigation pane, choose the Analyze tab.

5.
Within the Analyze tab, choose Model leaderboard.

The Model leaderboard page opens, which for tabular models looks like the following screenshot.

![Page 340 Diagram 1](images/page-0340-img-01.png)

For time series forecasting models, you see 7 models, which include one for each of the time series
forecasting algorithms supported by Canvas and one ensemble model. For more information about
the algorithms, see Advanced time series forecasting model settings.

In the preceding screenshot, you can see that the ﬁrst model candidate listed is marked as the
Default model. This is the model candidate with which you can make predictions or deploy to
endpoints.

Custom models
1311

## Page 341

Amazon SageMaker AI
Developer Guide

To view more detailed metrics information about the model candidates to compare them, you can
choose the More options icon
(

)
and choose View model details.

Important

Loading the model details for non-default model candidates may take a few minutes
(typically less than 10 minutes), and SageMaker AI Hosting charges apply. For more
information, see SageMaker AI Pricing.

The model candidate opens in the Analyze tab, and the metrics shown are speciﬁc to that model
candidate. When you’re done reviewing the model candidate’s metrics, you can go back or exit the
view to return to the Model leaderboard.

If you’d like to set the Default model to a diﬀerent candidate, you can choose the More options
icon
(

)
and choose Change to default model. Changing the default model for a model trained using HPO
mode might take several minutes.

Note

If your model is already deployed in production, registered to the model registry, or has
automations set up, you must delete your deployment, model registration, or automations
before changing the default model.

Metrics reference

The following sections describe the metrics that are available in Amazon SageMaker Canvas for
each model type.

Metrics for numeric prediction

The following list deﬁnes the metrics for numeric prediction in SageMaker Canvas and gives you
information about how you can use them.

Custom models
1312

## Page 342

Amazon SageMaker AI
Developer Guide

• InferenceLatency – The approximate amount of time between making a request for a model
prediction to receiving it from a real-time endpoint to which the model is deployed. This metric is
measured in seconds and is only available for models built with the Ensemblingmode.

• MAE – Mean absolute error. On average, the prediction for the target column is +/- {MAE} from
the actual value.

Measures how diﬀerent the predicted and actual values are when they're averaged over all
values. MAE is commonly used in numeric prediction to understand model prediction error. If the
predictions are linear, MAE represents the average distance from a predicted line to the actual
value. MAE is deﬁned as the sum of absolute errors divided by the number of observations.
Values range from 0 to inﬁnity, with smaller numbers indicating a better model ﬁt to the data.

• MAPE – Mean absolute percent error. On average, the prediction for the target column is +/-
{MAPE} % from the actual value.

MAPE is the mean of the absolute diﬀerences between the actual values and the predicted or
estimated values, divided by the actual values and expressed as a percentage. A lower MAPE
indicates better performance, as it means that the predicted or estimated values are closer to the
actual values.

• MSE – Mean squared error, or the average of the squared diﬀerences between the predicted and
actual values.

MSE values are always positive. The better a model is at predicting the actual values, the smaller
the MSE value is.

• R2 – The percentage of the diﬀerence in the target column that can be explained by the input
column.

Quantiﬁes how much a model can explain the variance of a dependent variable. Values range
from one (1) to negative one (-1). Higher numbers indicate a higher fraction of explained
variability. Values close to zero (0) indicate that very little of the dependent variable can be
explained by the model. Negative values indicate a poor ﬁt and that the model is outperformed
by a constant function (or a horizontal line).

• RMSE – Root mean squared error, or the standard deviation of the errors.

Measures the square root of the squared diﬀerence between predicted and actual values, and is
averaged over all values. It is used to understand model prediction error, and it's an important
metric to indicate the presence of large model errors and outliers. Values range from zero (0) to

Custom models
1313

## Page 343

Amazon SageMaker AI
Developer Guide

inﬁnity, with smaller numbers indicating a better model ﬁt to the data. RMSE is dependent on
scale, and should not be used to compare datasets of diﬀerent types.

Metrics for categorical prediction

This section deﬁnes the metrics for categorical prediction in SageMaker Canvas and gives you
information about how you can use them.

The following is a list of available metrics for 2-category prediction:

• Accuracy – The percentage of correct predictions.

Or, the ratio of the number of correctly predicted items to the total number of predictions.
Accuracy measures how close the predicted class values are to the actual values. Values for
accuracy metrics vary between zero (0) and one (1). A value of 1 indicates perfect accuracy, and 0
indicates complete inaccuracy.

• AUC – A value between 0 and 1 that indicates how well your model is able to separate the
categories in your dataset. A value of 1 indicates that it was able to separate the categories
perfectly.

• BalancedAccuracy – Measures the ratio of accurate predictions to all predictions.

This ratio is calculated after normalizing true positives (TP) and true negatives (TN) by the total

number of positive (P) and negative (N) values. It is deﬁned as follows: 0.5*((TP/P)+(TN/N)),
with values ranging from 0 to 1. The balanced accuracy metric gives a better measure of accuracy
when the number of positives or negatives diﬀer greatly from each other in an imbalanced
dataset, such as when only 1% of email is spam.

• F1 – A balanced measure of accuracy that takes class balance into account.

It is the harmonic mean of the precision and recall scores, deﬁned as follows: F1 = 2 *

(precision * recall) / (precision + recall). F1 scores vary between 0 and 1. A
score of 1 indicates the best possible performance, and 0 indicates the worst.

• InferenceLatency – The approximate amount of time between making a request for a model
prediction to receiving it from a real-time endpoint to which the model is deployed. This metric is
measured in seconds and is only available for models built with the Ensemblingmode.

• LogLoss – Log loss, also known as cross-entropy loss, is a metric used to evaluate the quality of
the probability outputs, rather than the outputs themselves. Log loss is an important metric to

Custom models
1314

## Page 344

Amazon SageMaker AI
Developer Guide

indicate when a model makes incorrect predictions with high probabilities. Values range from 0
to inﬁnity. A value of 0 represents a model that perfectly predicts the data.

• Precision – Of all the times that {category x} was predicted, the prediction was correct
{precision}% of the time.

Precision measures how well an algorithm predicts the true positives (TP) out of all of the

positives that it identiﬁes. It is deﬁned as follows: Precision = TP/(TP+FP), with values
ranging from zero (0) to one (1). Precision is an important metric when the cost of a false
positive is high. For example, the cost of a false positive is very high if an airplane safety system
is falsely deemed safe to ﬂy. A false positive (FP) reﬂects a positive prediction that is actually
negative in the data.

• Recall – The model correctly predicted {recall}% to be {category x} when {target_column} was
actually {category x}.

Recall measures how well an algorithm correctly predicts all of the true positives (TP) in a
dataset. A true positive is a positive prediction that is also an actual positive value in the data.

Recall is deﬁned as follows: Recall = TP/(TP+FN), with values ranging from 0 to 1. Higher
scores reﬂect a better ability of the model to predict true positives (TP) in the data. Note that it is
often insuﬃcient to measure only recall, because predicting every output as a true positive yields
a perfect recall score.

The following is a list of available metrics for 3+ category prediction:

• Accuracy – The percentage of correct predictions.

Or, the ratio of the number of correctly predicted items to the total number of predictions.
Accuracy measures how close the predicted class values are to the actual values. Values for
accuracy metrics vary between zero (0) and one (1). A value of 1 indicates perfect accuracy, and 0
indicates complete inaccuracy.

• BalancedAccuracy – Measures the ratio of accurate predictions to all predictions.

This ratio is calculated after normalizing true positives (TP) and true negatives (TN) by the total

number of positive (P) and negative (N) values. It is deﬁned as follows: 0.5*((TP/P)+(TN/N)),
with values ranging from 0 to 1. The balanced accuracy metric gives a better measure of accuracy
when the number of positives or negatives diﬀer greatly from each other in an imbalanced
dataset, such as when only 1% of email is spam.

Custom models
1315

## Page 345

Amazon SageMaker AI
Developer Guide

• F1macro – The F1macro score applies F1 scoring by calculating the precision and recall, and then
taking their harmonic mean to calculate the F1 score for each class. Then, the F1macro averages
the individual scores to obtain the F1macro score. F1macro scores vary between 0 and 1. A score
of 1 indicates the best possible performance, and 0 indicates the worst.

• InferenceLatency – The approximate amount of time between making a request for a model
prediction to receiving it from a real-time endpoint to which the model is deployed. This metric is
measured in seconds and is only available for models built with the Ensemblingmode.

• LogLoss – Log loss, also known as cross-entropy loss, is a metric used to evaluate the quality of
the probability outputs, rather than the outputs themselves. Log loss is an important metric to
indicate when a model makes incorrect predictions with high probabilities. Values range from 0
to inﬁnity. A value of 0 represents a model that perfectly predicts the data.

• PrecisionMacro – Measures precision by calculating precision for each class and averaging scores
to obtain precision for several classes. Scores range from zero (0) to one (1). Higher scores reﬂect
the model's ability to predict true positives (TP) out of all of the positives that it identiﬁes,
averaged across multiple classes.

• RecallMacro – Measures recall by calculating recall for each class and averaging scores to obtain
recall for several classes. Scores range from 0 to 1. Higher scores reﬂect the model's ability to
predict true positives (TP) in a dataset, whereas a true positive reﬂects a positive prediction that
is also an actual positive value in the data. It is often insuﬃcient to measure only recall, because
predicting every output as a true positive will yield a perfect recall score.

Note that for 3+ category prediction, you also receive the average F1, Accuracy, Precision, and
Recall metrics. The scores for these metrics are just the metric scores averaged for all categories.

Metrics for image and text prediction

The following is a list of available metrics for image prediction and text prediction.

• Accuracy – The percentage of correct predictions.

Or, the ratio of the number of correctly predicted items to the total number of predictions.
Accuracy measures how close the predicted class values are to the actual values. Values for
accuracy metrics vary between zero (0) and one (1). A value of 1 indicates perfect accuracy, and 0
indicates complete inaccuracy.

• F1 – A balanced measure of accuracy that takes class balance into account.

Custom models
1316

## Page 346

Amazon SageMaker AI
Developer Guide

It is the harmonic mean of the precision and recall scores, deﬁned as follows: F1 = 2 *

(precision * recall) / (precision + recall). F1 scores vary between 0 and 1. A

score of 1 indicates the best possible performance, and 0 indicates the worst.

• Precision – Of all the times that {category x} was predicted, the prediction was correct
{precision}% of the time.

Precision measures how well an algorithm predicts the true positives (TP) out of all of the

positives that it identiﬁes. It is deﬁned as follows: Precision = TP/(TP+FP), with values
ranging from zero (0) to one (1). Precision is an important metric when the cost of a false
positive is high. For example, the cost of a false positive is very high if an airplane safety system
is falsely deemed safe to ﬂy. A false positive (FP) reﬂects a positive prediction that is actually
negative in the data.

• Recall – The model correctly predicted {recall}% to be {category x} when {target_column} was
actually {category x}.

Recall measures how well an algorithm correctly predicts all of the true positives (TP) in a
dataset. A true positive is a positive prediction that is also an actual positive value in the data.

Recall is deﬁned as follows: Recall = TP/(TP+FN), with values ranging from 0 to 1. Higher
scores reﬂect a better ability of the model to predict true positives (TP) in the data. Note that it is
often insuﬃcient to measure only recall, because predicting every output as a true positive yields
a perfect recall score.

Note that for image and text prediction models where you are predicting 3 or more categories, you
also receive the average F1, Accuracy, Precision, and Recall metrics. The scores for these metrics are
just the metric scores average for all categories.

Metrics for time series forecasts

The following deﬁnes the advanced metrics for time series forecasts in Amazon SageMaker Canvas
and gives you information about how you can use them.

• Average Weighted Quantile Loss (wQL) – Evaluates the forecast by averaging the accuracy at the
P10, P50, and P90 quantiles. A lower value indicates a more accurate model.

• Weighted Absolute Percent Error (WAPE) – The sum of the absolute error normalized by the
sum of the absolute target, which measures the overall deviation of forecasted values from
observed values. A lower value indicates a more accurate model, where WAPE = 0 is a model with
no errors.

Custom models
1317

## Page 347

Amazon SageMaker AI
Developer Guide

• Root Mean Square Error (RMSE) – The square root of the average squared errors. A lower RMSE
indicates a more accurate model, where RMSE = 0 is a model with no errors.

• Mean Absolute Percent Error (MAPE) – The percentage error (percent diﬀerence of the mean
forecasted value versus the actual value) averaged over all time points. A lower value indicates a
more accurate model, where MAPE = 0 is a model with no errors.

• Mean Absolute Scaled Error (MASE) – The mean absolute error of the forecast normalized by the
mean absolute error of a simple baseline forecasting method. A lower value indicates a more
accurate model, where MASE < 1 is estimated to be better than the baseline and MASE > 1 is
estimated to be worse than the baseline.

Predictions with custom models

Use the custom model that you've built in SageMaker Canvas to make predictions for your data.
The following sections show you how to make predictions for numeric and categorical prediction
models, time series forecasts, image prediction models, and text prediction models.

Numeric and categorical prediction, image prediction, and text prediction custom models support
making the following types of predictions for your data:

• Single predictions — A Single prediction is when you only need to make one prediction. For
example, you have one image or passage of text that you want to classify.

• Batch predictions — A Batch prediction is when you’d like to make predictions for an entire
dataset. You can make batch predictions for datasets that are 1 TB+. For example, you have a
CSV ﬁle of customer reviews for which you’d like to predict the customer sentiment, or you have
a folder of image ﬁles that you'd like to classify. You should make predictions with a dataset that
matches your input dataset. Canvas provides you with the ability to do manual batch predictions,
or you can conﬁgure automatic batch predictions that run whenever you update a dataset.

For each prediction or set of predictions, SageMaker Canvas returns the following:

• The predicted values

• The probability of the predicted value being correct

Get started

Choose one of the following workﬂows to make predictions with your custom model:

Custom models
1318

## Page 348

Amazon SageMaker AI
Developer Guide

• Batch predictions in SageMaker Canvas

• Make single predictions

After generating predictions with your model, you can also do the following:

• Update your model by adding versions. If you want to try to improve the prediction accuracy of
your model, you can build new versions of your model. You can choose to clone your original
model building conﬁguration and dataset, or you can change your conﬁguration and select a
diﬀerent dataset. After adding a new version, you can review and compare versions to choose the
best one.

• Register a model version in the SageMaker AI model registry. You can register versions of your
model to the SageMaker Model Registry, which is a feature for tracking and managing the status
of model versions and machine learning pipelines. A data scientist or MLOps team user with
access to the SageMaker Model Registry can review your model versions and approve or reject
them before deploying them to production.

• Send your batch predictions to Quick. In Quick, you can build and publish dashboards with your
batch prediction datasets. This can help you analyze and share results generated by your custom
model.

Make single predictions

Note

This section describes how to get single predictions from your model inside the Canvas
application. For information about making real-time invocations in a production
environment by deploying your model to an endpoint, see Deploy your models to an
endpoint.

Make single predictions if you want to get a prediction for a single data point. You can use this
feature to get real-time predictions or to experiment with changing individual values to see
how they impact the prediction outcome. Note that single predictions rely on an Asynchronous
Inference endpoint, which shuts down after being idle (or not receiving any prediction requests) for
two hours.

Choose one of the following procedures based on your model type.

Custom models
1319

## Page 349

Amazon SageMaker AI
Developer Guide

Make single predictions with numeric and categorical prediction models

To make a single prediction for a numeric or categorical prediction model, do the following:

1.
In the left navigation pane of the Canvas application, choose My models.

2.
On the My models page, choose your model.

3.
After opening your model, choose the Predict tab.

4.
On the Run predictions page, choose Single prediction.

5.
For each Column ﬁeld, which represents the columns of your input data, you can change the
Value. Select the dropdown menu for the Value you want to change. For numeric ﬁelds, you
can enter a new number. For ﬁelds with labels, you can select a diﬀerent label.

6.
When you’re ready to generate the prediction, in the right Prediction pane, choose Update.

In the right Prediction pane, you’ll see the prediction result. You can Copy the prediction result
chart, or you can also choose Download to either download the prediction result chart as an image
or to download the values and prediction as a CSV ﬁle.

Make single predictions with time series forecasting models

To make a single prediction for a time series forecasting model, do the following:

1.
In the left navigation pane of the Canvas application, choose My models.

2.
On the My models page, choose your model.

3.
After opening your model, choose the Predict tab.

4.
Choose Single prediction.

5.
For Item, select the item for which you want to forecast values.

6.
If you used a group by column to train the model, then select the group by category for the
item.

The prediction result loads in the pane below, showing you a chart with the forecast for each
quantile. Choose Schema view to see the numeric predicted values. You can also choose Download
to download the prediction results as either an image or a CSV ﬁle.

Make single predictions with image prediction models

To make a single prediction for a single-label image prediction model, do the following:

Custom models
1320

## Page 350

Amazon SageMaker AI
Developer Guide

1.
In the left navigation pane of the Canvas application, choose My models.

2.
On the My models page, choose your model.

3.
After opening your model, choose the Predict tab.

4.
On the Run predictions page, choose Single prediction.

5.
Choose Import image.

6.
You’ll be prompted to upload an image. You can upload an image from your local computer or
from an Amazon S3 bucket.

7.
Choose Import to import your image and generate the prediction.

In the right Prediction results pane, the model lists the possible labels for the image along with a
Conﬁdence score for each label. For example, the model might predict the label Sea for an image,
with a conﬁdence score of 96%. The model may have predicted the image as a Glacier with only
a conﬁdence score of 4%. Therefore, you can determine that your model is fairly conﬁdent in
predicting images of the sea.

Make single predictions with text prediction models

To make a single prediction for a multi-category text prediction model, do the following:

1.
In the left navigation pane of the Canvas application, choose My models.

2.
On the My models page, choose your model.

3.
After opening your model, choose the Predict tab.

4.
On the Run predictions page, choose Single prediction.

5.
For the Text ﬁeld, enter the text for which you’d like to get a prediction.

6.
Choose Generate prediction results to get your prediction.

In the right Prediction results pane, you receive an analysis of your text in addition to a
Conﬁdence score for each possible label. For example, if you entered a good review for a product,
you might get Positive with a conﬁdence score of 85%, while the conﬁdence score for Neutral
might be 10% and the conﬁdence score for Negative only 5%.

Batch predictions in SageMaker Canvas

Make batch predictions when you have an entire dataset for which you’d like to generate
predictions. Amazon SageMaker Canvas supports batch predictions for datasets up to PBs in size.

Custom models
1321

## Page 351

Amazon SageMaker AI
Developer Guide

There are two types of batch predictions you can make:

• Manual batch predictions are when you have a dataset for which you want to make one-time
predictions.

• Automatic batch predictions are when you set up a conﬁguration that runs whenever a speciﬁc
dataset is updated. For example, if you’ve conﬁgured weekly updates to a SageMaker Canvas
dataset of inventory data, you can set up automatic batch predictions that run whenever
you update the dataset. After setting up an automated batch predictions workﬂow, see How
to manage automations for more information about viewing and editing the details of your
conﬁguration. For more information about setting up automatic dataset updates, see Conﬁgure
automatic updates for a dataset.

Note

Time series forecasting models don't support automatic batch predictions.
You can only set up automatic batch predictions for datasets imported through local
upload or Amazon S3. Additionally, automatic batch predictions can only run while
you’re logged in to the Canvas application. If you log out of Canvas, the automatic batch
prediction job resumes when you log back in.

To get started, review the Batch prediction dataset requirements, and then choose one of the
following manual or automatic batch prediction workﬂows.

Topics

• Batch prediction dataset requirements

• Make manual batch predictions

• Make automatic batch predictions

• Edit your automatic batch prediction conﬁguration

• Delete your automatic batch prediction conﬁguration

• View your batch prediction jobs

Batch prediction dataset requirements

For batch predictions, make sure that your datasets meet the requirements outlined in Create a
dataset. If your dataset is larger than 5 GB, then Canvas uses Amazon EMR Serverless to process

Custom models
1322

## Page 352

Amazon SageMaker AI
Developer Guide

your data and split it into smaller batches. After your data has been split, Canvas uses SageMaker
AI Batch Transform to make predictions. You may see charges from both of these services after
running batch predictions. For more information, see Canvas pricing.

You might not be able to make predictions on some datasets if they have incompatible schemas.
A schema is an organizational structure. For a tabular dataset, the schema is the names of the
columns and the data type of the data in the columns. An incompatible schema might happen for

one of the following reasons:

• The dataset that you're using to make predictions has fewer columns than the dataset that you're
using to build the model.

• The data types in the columns you used to build the dataset might be diﬀerent from the data
types in dataset that you're using to make predictions.

• The dataset that you're using to make predictions and the dataset that you've used to build the

model have column names that don't match. The column names are case sensitive. Column1 is

not the same as column1.

To ensure that you can successfully generate batch predictions, match the schema of your batch
predictions dataset to the dataset you used to train the model.

Note

For batch predictions, if you dropped any columns when building your model, Canvas adds
the dropped columns back to the prediction results. However, Canvas does not add the
dropped columns to your batch predictions for time series models.

Make manual batch predictions

Choose one of the following procedures to make manual batch predictions based on your model
type.

Make manual batch predictions with numeric, categorical, and time series forecasting models

To make manual batch predictions for numeric, categorical, and time series forecasting model
types, do the following:

1.
In the left navigation pane of the Canvas application, choose My models.

2.
On the My models page, choose your model.

Custom models
1323

## Page 353

Amazon SageMaker AI
Developer Guide

3.
After opening your model, choose the Predict tab.

4.
On the Run predictions page, choose Batch prediction.

5.
Choose Select dataset to pick a dataset for generating predictions.

6.
From the list of available datasets, select your dataset, and then choose Start Predictions to

get your predictions.

After the prediction job ﬁnishes running, there is an output dataset listed on the same page in the
Predictions section. This dataset contains your results, and if you select the More options icon
(

),
you can choose Preview to preview the output data. You can see the input data matched to the
prediction and the probability that the prediction is correct. Then, you can choose Download
prediction to download the results as a ﬁle.

Make manual batch predictions with image prediction models

To make manual batch predictions for a single-label image prediction model, do the following:

1.
In the left navigation pane of the Canvas application, choose My models.

2.
On the My models page, choose your model.

3.
After opening your model, choose the Predict tab.

4.
On the Run predictions page, choose Batch prediction.

5.
Choose Select dataset if you’ve already imported your dataset. If not, choose Import new
dataset, and then you’ll be directed through the import data workﬂow.

6.
From the list of available datasets, select your dataset and choose Generate predictions to get
your predictions.

After the prediction job ﬁnishes running, on the Run predictions page, you see an output dataset
listed under Predictions. This dataset contains your results, and if you select the More options icon
(

),
you can choose View prediction results to see the output data. You can see the images along
with their predicted labels and conﬁdence scores. Then, you can choose Download prediction to
download the results as a CSV or a ZIP ﬁle.

Make manual batch predictions with text prediction models

To make manual batch predictions for a multi-category text prediction model, do the following:

Custom models
1324

## Page 354

Amazon SageMaker AI
Developer Guide

1.
In the left navigation pane of the Canvas application, choose My models.

2.
On the My models page, choose your model.

3.
After opening your model, choose the Predict tab.

4.
On the Run predictions page, choose Batch prediction.

5.
Choose Select dataset if you’ve already imported your dataset. If not, choose Import new
dataset, and then you’ll be directed through the import data workﬂow. The dataset you
choose must have the same source column as the dataset with which you built the model.

6.
From the list of available datasets, select your dataset and choose Generate predictions to get
your predictions.

After the prediction job ﬁnishes running, on the Run predictions page, you see an output dataset
listed under Predictions. This dataset contains your results, and if you select the More options icon

(

),
you can choose Preview to see the output data. You can see the images along with their predicted
labels and conﬁdence scores. Then, you can choose Download prediction to download the results.

Make automatic batch predictions

Note

Time series forecasting models don't support automatic batch predictions.

To set up a schedule for automatic batch predictions, do the following:

1. In the left navigation pane of Canvas, choose My models.

2. Choose your model.

3. Choose the Predict tab.

4. Choose Batch prediction.

5. For Generate predictions, choose Automatic.

6. The Automate batch predictions dialog box pops up. Choose Select dataset and choose the

dataset for which you want to automate predictions. Note that you can only select a dataset
that was imported through local upload or Amazon S3.

7. After selecting a dataset, choose Set up.

Custom models
1325

## Page 355

Amazon SageMaker AI
Developer Guide

Canvas runs a batch predictions job for the dataset after you set up the conﬁguration. Then, every
time you Update a dataset, either manually or automatically, another batch predictions job runs.

After the prediction job ﬁnishes running, on the Run predictions page, you see an output dataset

listed under Predictions. This dataset contains your results, and if you select the More options icon
(

),
you can choose Preview to preview the output data. You can see the input data matched to the
prediction and the probability that the prediction is correct. Then, you can choose Download to
download the results.

The following sections describe how to view, update, and delete your automatic batch prediction
conﬁguration through the Datasets page in the Canvas application. You can only set up a
maximum of 20 automatic conﬁgurations in Canvas. For more information about viewing your
automated batch predictions job history or making changes to your automatic conﬁguration
through the Automations page, see How to manage automations.

Edit your automatic batch prediction conﬁguration

You might want to make changes to your auto update conﬁguration for a dataset, such as changing
the frequency of the updates. You might also want to turn oﬀ your automatic update conﬁguration
to pause the updates to your dataset.

When you edit a batch prediction conﬁguration, you can change the target dataset but not the
frequency (since automatic batch predictions occur whenever the dataset is updated).

To edit your auto update conﬁguration, do the following:

1. Go to the Predict tab of your model.

2. Under Predictions, choose the Conﬁguration tab.

3. Find your conﬁguration and choose the More options icon

(

).

4. From the dropdown menu, choose Update conﬁguration.

5. The Automate batch prediction dialog box opens. You can select another dataset and choose

Set up to save your changes.

Your automatic batch predictions conﬁguration is now updated.

To pause your automatic batch predictions, turn oﬀ your automatic conﬁguration by doing the
following:

Custom models
1326

## Page 356

Amazon SageMaker AI
Developer Guide

1. Go to the Predict tab of your model.

2. Under Predictions, choose the Conﬁguration tab.

3. Find your conﬁguration from the list and turn oﬀ the Auto update toggle.

Automatic batch predictions are now paused. You can turn the toggle back on at any time to
resume the update schedule.

Delete your automatic batch prediction conﬁguration

To learn how to delete your automatic batch prediction conﬁguration, see Delete an automatic
conﬁguration.

You can also delete your conﬁguration by doing the following:

1. Go to the Predict tab of your model.

2. Under Predictions, choose the Conﬁguration tab.

3. Find your conﬁguration from the list and choose the More options icon

(

).

4. From the dropdown menu, choose Delete conﬁguration.

Your conﬁguration should now be deleted.

View your batch prediction jobs

To view the statuses and history of your batch prediction jobs, go to the Predict tab of your model.

Each batch prediction job shows up in the Predict tab of your model. Under Predictions, you can
see the All jobs tab and the Conﬁguration tabs:

• All jobs – In this tab, you can see all of the manual and automatic batch prediction jobs for this
model. You can ﬁlter the jobs by conﬁguration name. For each job, you can see the following
ﬁelds:

• Status – The current status of your batch prediction job. If the status is Failed or Partially
failed, you can hover over the status to view a more detailed error message to help you
troubleshoot.

• Input dataset – The name of your Canvas input dataset, including the dataset version.

• Prediction type – Whether the prediction job was automatic or manual.

Custom models
1327

## Page 357

Amazon SageMaker AI
Developer Guide

• Rows – The number of rows predicted.

• Conﬁguration name – The name of the batch prediction job conﬁguration.

• QuickSight – Describes whether you've sent the batch predictions to Quick.

• Created – The creation time of the batch prediction job.

If you choose the More options icon
(

),
you can choose View details, Preview prediction, Download prediction, or Send to Quick. If
you choose View details, a page opens that shows you the full details of the batch prediction
job, including the status, the input and output data conﬁgurations, information about the
instances used to complete the job and access to the Amazon CloudWatch logs. The page looks
like the following screenshot.

Custom models
1328

## Page 358

Amazon SageMaker AI
Developer Guide

![Page 358 Diagram 1](images/page-0358-img-01.png)

• Conﬁguration – In this tab, you can see all of the automatic batch prediction
conﬁgurations you’ve created for this model. For each conﬁguration, you can
see ﬁelds such as the timestamp for when it was Created, the Input dataset it
tracks for updates, and the Next job scheduled, which is the time when the next
automatic prediction job is scheduled to start. If you choose the More options icon
(

),
you can choose View all jobs to see the job history and in progress jobs for the conﬁguration.

Custom models
1329

## Page 359

Amazon SageMaker AI
Developer Guide

Send predictions to Quick

Note

You can send batch predictions to Quick for numeric and categorical prediction and time
series forecasting models. Single-label image prediction and multi-category text prediction
models are excluded.

Once you generate batch predictions with custom tabular models in SageMaker Canvas, you can
send those predictions as CSV ﬁles to Quick, which is a business intelligence (BI) service to build
and publish predictive dashboards.

For example, if you built a 2 category prediction model to determine whether a customer will
churn, you can create a visual, predictive dashboard in Quick to show the percentage of customers
that are expected to churn. To learn more about Quick, see the Quick User Guide.

The following sections show you how to send your batch predictions to Quick for analysis.

Before you begin

Your user must have the necessary AWS Identity and Access Management (IAM) permissions to
send your predictions to Quick. Your administrator can set up the IAM permissions for your user.
For more information, see Grant Your Users Permissions to Send Predictions to Quick.

Your Quick account must contain the default namespace, which is set up when you ﬁrst create
your Quick account. Contact your administrator to help you get access to Quick. For more
information, see Setting up for Quick in the Quick User Guide.

Your Quick account must be created in the same Region as your Canvas application. If your Quick
account’s home Region diﬀers from your Canvas application’s Region, you must either close and
recreate your Quick account, or set up a Canvas application in the same Region as your Quick
account. You can check your Quick home Region by doing the following (assuming you already
have an Quick account):

1.
Open your Quick console.

2.
When the page loads, your Quick home Region is appended to the URL in the following

format: https://<your-home-region>.quicksight.aws.amazon.com/.

Custom models
1330

## Page 360

Amazon SageMaker AI
Developer Guide

You must know the usernames of the Quick users to whom you want to send your predictions.
You can send predictions to yourself or other users who have the right permissions. Any users to

whom you send predictions must be in the default namespace of your Quick account and have

the Author or Admin role in Quick.

Additionally, Quick must have access to the SageMaker AI default Amazon S3 bucket for your

domain, which is named with the following format: sagemaker-{REGION}-{ACCOUNT_ID}. The
Region should be the same as your Quick account's home Region and your Canvas application’s
Region. To learn how to give Quick access to the batch predictions stored in your Amazon S3
bucket, see the topic I can’t connect to Amazon S3 in the Quick User Guide.

Supported data formats

Before sending your predictions, check that the data format of your batch predictions is compatible
with Quick.

• To learn more about the accepted data formats for timeseries data, see Supported date formats
in the Quick User Guide.

• To learn more about data values that might prevent you from sending to Quick, see Unsupported
values in data in the Quick User Guide.

Also note that Quick uses the character " as a text qualiﬁer, so if your Canvas data contains any "
characters, make sure that you close all matching quotes. Any mismatching quotes can cause issues
with sending your dataset to Quick.

Send your batch predictions to Quick

Use the following procedure to send your predictions to Quick:

1.
Open the SageMaker Canvas application.

2.
In the left navigation pane, choose My models.

3.
On the My models page, choose your model.

4.
Choose the Predict tab.

5.
Under Predictions, select the dataset (or datasets) of batch predictions that you’d like to share.
You can share up to 5 datasets of batch predictions at a time.

6.
After you select your dataset, choose Send to Quick.

Custom models
1331

## Page 361

Amazon SageMaker AI
Developer Guide

Note

The Send to Quick button doesn’t activate unless you select one or more datasets.

Alternatively, you can preview your predictions by choosing the More options icon
(

)
and then View prediction results. From the dataset preview, you can choose Send to Quick.
The following screenshot shows you the Send to Quick button in a dataset preview.

![Page 361 Diagram 1](images/page-0361-img-01.png)

7.
In the Send to Quick dialog box, do the following:

a.
For QuickSight users, enter the name of the Quick users to whom you want to send your
predictions. If you want to send them to yourself, enter your own username. You can only

send predictions to users in the default namespace of the Quick account, and the user

must have the Author or Admin role in Quick.

b.
Choose Send.

Custom models
1332

## Page 362

Amazon SageMaker AI
Developer Guide

The following screenshot shows the Send to Quick dialog box:

![Page 362 Diagram 1](images/page-0362-img-01.png)

After you send your batch predictions, the QuickSight ﬁeld for the datasets you sent shows as

Sent. In the conﬁrmation box that conﬁrms your predictions were sent, you can choose Open
Quick to open your Quick application. If you’re done using Canvas, you should log out of the
Canvas application.

The Quick users that you’ve sent datasets to can open their Quick application and view the Canvas
datasets that have been shared with them. Then, they can create predictive dashboards with the
data. For more information, see Getting started with Quick data analysis in the Quick User Guide.

By default, all of the users to whom you send predictions have owner permissions for the dataset
in Quick. Owners are able to create analyses, refresh, edit, delete, and re-share datasets. The
changes that owners make to a dataset change the dataset for all users with access. To change the
permissions, go to the dataset in Quick and manage its permissions. For more information, see
Viewing and editing the permissions users that a dataset is shared with in the Quick User Guide.

Custom models
1333

## Page 363

Amazon SageMaker AI
Developer Guide

Download a model notebook

Note

The model notebook feature is available for quick build and standard build tabular
models, and ﬁne-tuned foundation models. Model notebooks aren't supported for image
prediction, text prediction, or time series forecasting models.
If you'd like to generate a model notebook for a tabular model built before this feature was
launched, you must rebuild the model to generate a notebook.

For eligible models that you successfully build in Amazon SageMaker Canvas, a Jupyter notebook
containing a report of all the model building steps is generated. This Jupyter notebook contains
Python code that you can run locally or run in an environment like Amazon SageMaker Studio

Classic to replicate the steps necessary to build your model. The notebook can be useful if you’d
like to experiment with the code or see the backend details of how Canvas builds models.

To access the model notebook, do the following:

1.
Open the SageMaker Canvas application.

2.
In the left navigation pane, choose My models.

3.
Choose the model and version that you built.

4.
On the model version’s page, choose the More options icon
(

)
in the header.

5.
From the dropdown menu, choose View Notebook.

6.
A popup appears with the notebook content. You can choose Download and then do one of
the following:

a.
Choose Download to save the notebook content to your local device.

b.
Choose Copy S3 URI to copy the Amazon S3 location where the notebook is stored.
The notebook is stored in the Amazon S3 bucket speciﬁed in your Canvas storage
conﬁguration, which is conﬁgured in the Prerequisites for setting up Amazon SageMaker
Canvas section.

Custom models
1334

## Page 364

Amazon SageMaker AI
Developer Guide

You should now be able to view the notebook either locally or as an object in Amazon S3. You can
upload the notebook to an IDE to edit and run the code, or you can share the notebook with others
in your organization to review.

Send your model to Quick

If you use Quick and want to leverage SageMaker Canvas in your Quick visualizations, you can
build an Amazon SageMaker Canvas model and use it as a predictive ﬁeld in your Quick dataset. A
predictive ﬁeld is a ﬁeld in your Quick dataset that can make predictions for a given column in your
dataset, similar to how Canvas users make single or batch predictions with a model. To learn more
about how to integrate Canvas predictive abilities into your Quick datasets, see SageMaker Canvas
integration in the Quick User Guide.

The following steps explain how you can add a predictive ﬁeld to your Quick dataset using a
Canvas model:

1.
Open the Canvas application and build a model with your dataset.

2.
After building the model in Canvas, send the model to Quick. A schema ﬁle automatically
downloads to your local machine when you send the model to Quick. You upload this schema
ﬁle to Quick in the next step.

3.
Open Quick and choose a dataset with the same schema as the dataset you used to build your
model. Add a predictive ﬁeld to the dataset and do the following:

a.
Specify the model sent from Canvas.

b.
Upload the schema ﬁle that was downloaded in Step 2.

4.
Save and publish your changes, and then generate predictions for the new dataset. Quick uses
the model to ﬁll in the target column with predictions.

In order to send a model from Canvas to Quick, you must meet the following prerequisites:

• You must have both Canvas and Quick set up. Your Quick account must be created in the same
AWS Region as your Canvas application. If your Quick account’s home Region diﬀers from your
Canvas application’s Region, you must either close and recreate your Quick account, or set up
a Canvas application in the same Region as your Quick account. Your Quick account must also
contain the default namespace, which you set up when you ﬁrst create your Quick account.
Contact your administrator to help you get access to Quick. For more information, see Setting up
for Quick in the Quick User Guide.

Custom models
1335

## Page 365

Amazon SageMaker AI
Developer Guide

• Your user must have the necessary AWS Identity and Access Management (IAM) permissions to
send your predictions to Quick. Your administrator can set up the IAM permissions for your user.
For more information, see Grant Your Users Permissions to Send Predictions to Quick.

• Quick must have access to the Amazon S3 bucket that you’ve speciﬁed for Canvas application

storage. For more information, see Conﬁgure your Amazon S3 storage.

Time Series Forecasts in Amazon SageMaker Canvas

Note

Time series forecasting models are only supported for tabular datasets.

Amazon SageMaker Canvas gives you the ability to use machine learning time series forecasts.

Time series forecasts give you the ability to make predictions that can vary with time.

You can make a time series forecast for the following examples:

• Forecasting your inventory in the coming months.

• The number of items sold in the next four months.

• The eﬀect of reducing the price on sales during the holiday season.

• Item inventory in the next 12 months.

• The number of customers entering a store in the next several hours.

• Forecasting how a 10% reduction in the price of a product aﬀects sales over a time period.

To make a time series forecast, your dataset must have the following:

• A timestamp column with all values having the datetime type.

• A target column that has the values that you're using to forecast future values.

• An item ID column that contains unique identiﬁers for each item in your dataset, such as SKU
numbers.

The datetime values in the timestamp column must use one of the following formats:

• YYYY-MM-DD HH:MM:SS

• YYYY-MM-DDTHH:MM:SSZ

Custom models
1336

## Page 366

Amazon SageMaker AI
Developer Guide

• YYYY-MM-DD

• MM/DD/YY

• MM/DD/YY HH:MM

• MM/DD/YYYY

• YYYY/MM/DD HH:MM:SS

• YYYY/MM/DD

• DD/MM/YYYY

• DD/MM/YY

• DD-MM-YY

• DD-MM-YYYY

You can make forecasts for the following intervals:

• 1 min

• 5 min

• 15 min

• 30 min

• 1 hour

• 1 day

• 1 week

• 1 month

• 1 year

Future values in your input dataset

Canvas automatically detects columns in your dataset that might potentially contain future values.
If present, these values can enhance the accuracy of predictions. Canvas marks these speciﬁc

columns with a Future values label. Canvas infers the relationship between the data in these
columns and the target column that you are trying to predict, and utilizes that relationship to
generate more accurate forecasts.

For example, you can forecast the amount of ice cream sold by a grocery store. To make a forecast,
you must have a timestamp column and a column that indicates how much ice cream the grocery

Custom models
1337

## Page 367

Amazon SageMaker AI
Developer Guide

store sold. For a more accurate forecast, your dataset can also include the price, the ambient
temperature, the ﬂavor of the ice cream, or a unique identiﬁer for the ice cream.

Ice cream sales might increase when the weather is warmer. A decrease in the price of the ice cream
might result in more units sold. Having a column with ambient temperature data and a column

with pricing data can improve your ability to forecast the number of units of ice cream the grocery
store sells.

While providing future values is optional, it helps you to perform what-if analyses directly in the
Canvas application, showing you how changes in future values could alter your predictions.

Handling missing values

You might have missing data for diﬀerent reasons. The reason for your missing data might inform
how you want Canvas to impute it. For example, your organization might use an automatic system
that only tracks when a sale happens. If you're using a dataset that comes from this type of
automatic system, you have missing values in the target column.

Important

If you have missing values in the target column, we recommend using a dataset that
doesn't have them. SageMaker Canvas uses the target column to forecast future values.
Missing values in the target column can greatly reduce the accuracy of the forecast.

For missing values in the dataset, Canvas automatically imputes the missing values for you by

ﬁlling the target column with 0 and other numeric columns with the median value of the column.

However, you can select your own ﬁlling logic for the target column and other numeric columns in
your datasets. Target columns have diﬀerent ﬁlling guidelines and restrictions than the rest of the
numeric columns. Target columns are ﬁlled up to the end of the historical period, whereas numeric
columns are ﬁlled across both historical and future periods all the way to the end of the forecast
horizon. Canvas only ﬁlls future values in a numeric column if your data has at least one record
with a future timestamp and a value for that speciﬁc column.

You can choose one of the following ﬁlling logic options to impute missing values in your data:

• zero – Fill with 0.

• NaN – Fill with NaN, or not a number. This is only supported for the target column.

Custom models
1338

## Page 368

Amazon SageMaker AI
Developer Guide

• mean – Fill with the mean value from the data series.

• median – Fill with the median value from the data series.

• min – Fill with the minimum value from the data series.

• max – Fill with the maximum value from the data series.

When choosing a ﬁlling logic, you should consider how your model interprets the logic. For
example, in a retail scenario, recording zero sales of an available item is diﬀerent from recording
zero sales of an unavailable item, as the latter scenario doesn’t necessarily imply a lack of customer

interest in the unavailable item. In this case, ﬁlling with 0 in the target column of the dataset
might cause the model to be under-biased in its predictions and infer a lack of customer interest in

unavailable items. Conversely, ﬁlling with NaN might cause the model to ignore true occurrences of
zero items being sold of available items.

Types of forecasts

You can make one of the following types of forecasts:

• Single item

• All items

For a forecast on all the items in your dataset, SageMaker Canvas returns a forecast for the future
values for each item in your dataset.

For a single item forecast, you specify the item and SageMaker Canvas returns a forecast for the
future values. The forecast includes a line graph that plots the predicted values over time.

Topics

• Additional options for forecasting insights

Additional options for forecasting insights

In Amazon SageMaker Canvas, you can use the following optional methods to get more insights
from your forecast:

• Group column

• Holiday schedule

• What-if scenario

Custom models
1339

## Page 369

Amazon SageMaker AI
Developer Guide

You can specify a column in your dataset as a Group column. Amazon SageMaker Canvas groups
the forecast by each value in the column. For example, you can group the forecast on columns
containing price data or unique item identiﬁers. Grouping a forecast by a column lets you make
more speciﬁc forecasts. For example, if you group a forecast on a column containing item
identiﬁers, you can see the forecast for each item.

Overall sales of items might be impacted by the presence of holidays. For example, in the United
States, the number of items sold in both November and December might diﬀer greatly from the
number of items sold in January. If you use the data from November and December to forecast the
sales in January, your results might be inaccurate. Using a holiday schedule prevents you getting
inaccurate results. You can use a holiday schedule for 251 countries.

For a forecast on a single item in your dataset, you can use what-if scenarios. A what-if scenario
gives you the ability to change values in your data and change the forecast. For example, you can
answer the following questions by using a what-if scenario, "What if I lowered prices? How would

that aﬀect the number of items sold?"

Adding model versions in Amazon SageMaker Canvas

In Amazon SageMaker Canvas, you can update the models that you’ve built by adding versions.

Each model that you build has a version number. The ﬁrst model is version 1 or V1. You can use
model versions to see changes in prediction accuracy when you update your data or use advanced
transformations.

When viewing your model, SageMaker Canvas shows you the model history so that you can
compare all of the model versions that you built. You can also delete versions that are no longer
useful to you. By creating multiple model versions and evaluating their accuracy, you can iteratively
improve your model performance.

Note

Text prediction and image prediction models only support one model version.

To add a model version, you can either clone an existing version or create a new version.

Cloning an existing version copies over the current model conﬁguration, including the model recipe
and the input dataset. Alternatively, you can create a new version if you want to conﬁgure a new
model recipe or choose a diﬀerent dataset.

Custom models
1340

## Page 370

Amazon SageMaker AI
Developer Guide

If you create a new version and select a diﬀerent dataset, you must choose a dataset with the same
target column and schema as the dataset from version 1.

Before you can add a new version, you must successfully build at least one model version. Then,
you can  register a model version in the SageMaker Model Registry. Use the registry for tracking
model versions and for collaborating with Studio Classic users on production model approvals.

If you did a quick build for your ﬁrst model version, you have the option to run a standard build
when you add a version. Standard builds generally have higher accuracy. Therefore, if you feel
conﬁdent in your quick build conﬁguration, you can run a standard build to create a ﬁnal version
of your model. To learn more about the diﬀerences between quick builds and standard builds, see
How custom models work.

The following procedures show you how to add model versions; the procedure is diﬀerent
depending on whether you are adding a version of the same build type or a diﬀerent build type
(quick versus standard). Use the procedure To add a new model version to add versions of the
same build type. To add a standard build model version after running a quick build, follow the
procedure To run a standard build.

To add a new model version

1.
Open your SageMaker Canvas application. For more information, see Getting started with
using Amazon SageMaker Canvas.

2.
In the left navigation pane, choose My models.

3.
On the My models page, choose your model. To ﬁnd your model, you can choose Filter by
problem type.

4.
After your model opens, choose the Add version button in the top panel.

5.
From the dropdown menu, select one of the following options:

a.
Add a new version from scratch – When you select this option, the Build tab opens
with the draft for a new model version. You can select a diﬀerent dataset (as long as the
schema matches the schema of the ﬁrst model version’s dataset) and conﬁgure a new
model recipe. For more information about building a model version, see Build a model.

b.
Clone an existing version with conﬁgurations – A dialog box prompts you to select
the version that you want to clone. After you've selected your desired version, choose
Clone. The Build tab opens with the draft for a new model version. Any model recipe
conﬁgurations are copied over from the cloned version. For more information about
building a model version, see Build a model.

Custom models
1341

## Page 371

Amazon SageMaker AI
Developer Guide

To run a standard build

1.
Open your SageMaker Canvas application. For more information, see Getting started with
using Amazon SageMaker Canvas.

2.
In the left navigation pane, choose My models.

3.
On the My models page, choose your model. You can choose Filter by problem type to ﬁnd
your model more easily.

4.
After your model opens, choose the Analyze tab.

5.
Choose Standard build.

![Page 371 Diagram 1](images/page-0371-img-01.png)

On the model draft page that opens to the Build tab, you can modify your model
conﬁguration and start a build. For more information about building a model version, see Build
a model.

Custom models
1342

## Page 372

Amazon SageMaker AI
Developer Guide

You should now have a new model version build in progress. For more information about building a
model, see How custom models work.

After building a model version, you can return to your model details page at any time to view all of
the versions or add more versions. The following image shows the Versions page for a model.

On the Versions page, you can view the following information for each of your model versions:

• Status – This ﬁeld tells you whether your model is currently building (In building), done

building (Ready), failed to build (Failed), or still being edited (In draft).

• Model score, F1, Precision, Recall, and AUC – If you turn on the Show advanced metrics
toggle on this page, you can see these model metrics. These metrics indicate the accuracy and
performance of your model. For more information, see Evaluate your model.

• Shared – This ﬁeld states whether you shared the model version with SageMaker Studio Classic
users.

• Model registry – This ﬁeld states whether you registered the version to a model registry. For
more information, see Register a model version in the SageMaker AI model registry.

MLOps

After building a model in SageMaker Canvas that you feel conﬁdent about, you might want
to integrate your model with the machine learning operations (MLOps) processes in your
organization. MLOps includes common tasks such as deploying a model for use in production or
setting up continuous integration and continuous deployment (CI/CD) pipelines.

The following topics describe how you can use features within Canvas to use a Canvas-built model
in production.

Topics

• Register a model version in the SageMaker AI model registry

Custom models
1343

## Page 373

Amazon SageMaker AI
Developer Guide

• Deploy your models to an endpoint

• View your deployments

• Update a deployment conﬁguration

• Test your deployment

• Invoke your endpoint

• Delete a model deployment

Register a model version in the SageMaker AI model registry

With SageMaker Canvas, you can build multiple iterations, or versions, of your model to improve it
over time. You might want to build a new version of your model if you acquire better training data
or if you want to attempt to improve the model’s accuracy. For more information about adding
versions to your model, see Update a model.

After you’ve built a model that you feel conﬁdent about, you might want to evaluate its
performance and have it reviewed by a data scientist or MLOps engineer in your organization
before using it in production. To do this, you can register your model versions to the SageMaker
Model Registry. The SageMaker Model Registry is a repository that data scientists or engineers
can use to catalog machine learning (ML) models and manage model versions and their associated
metadata, such as training metrics. They can also manage and log the approval status of a model.

After you register your model versions to the SageMaker Model Registry, a data scientist or your
MLOps team can access the SageMaker Model Registry through SageMaker Studio Classic, which
is a web-based integrated development environment (IDE) for working with machine learning
models. In the SageMaker Model Registry interface in Studio Classic, the data scientist or MLOps
team can evaluate your model and update its approval status. If the model doesn’t perform to their

requirements, the data scientist or MLOps team can update the status to Rejected. If the model
does perform to their requirements, then the data scientist or MLOps team can update the status

to Approved. Then, they can deploy your model to an endpoint or automate model deployment
with CI/CD pipelines. You can use the SageMaker AI model registry feature to seamlessly integrate
models built in Canvas with the MLOps processes in your organization.

The following diagram summarizes an example of registering a model version built in Canvas to
the SageMaker Model Registry for integration into an MLOps workﬂow.

Custom models
1344

## Page 374

Amazon SageMaker AI
Developer Guide

![Page 374 Diagram 1](images/page-0374-img-01.png)

You can register tabular, image, and text model versions to the SageMaker Model Registry. This
includes time series forecasting models and JumpStart based ﬁne-tuned foundation models.

Note

Currently, you can't register Amazon Bedrock based ﬁne-tuned foundation models built in
Canvas to the SageMaker Model Registry.

The following sections show you how to register a model version to the SageMaker Model Registry
from Canvas.

Permissions management

By default, you have permissions to register model versions to the SageMaker Model Registry.
SageMaker AI grants these permissions for all new and existing Canvas user proﬁles through the
AmazonSageMakerCanvasFullAccess policy, which is attached to the AWS IAM execution role for
the SageMaker AI domain that hosts your Canvas application.

If your Canvas administrator is setting up a new domain or user proﬁle, when they're setting up
the domain and following the prerequisite instructions in the Getting started guide, SageMaker
AI turns on the model registration permissions through the ML Ops permissions conﬁguration
option, which is enabled by default.

The Canvas administrator can manage model registration permissions at the user proﬁle level as
well. For example, if the administrator wants to grant model registration permissions to some
user proﬁles but remove permissions for others, they can edit the permissions for a speciﬁc user.

Custom models
1345

## Page 375

Amazon SageMaker AI
Developer Guide

The following procedure shows how to turn oﬀ model registration permissions for a speciﬁc user
proﬁle:

1.
Open the SageMaker AI console at https://console.aws.amazon.com/sagemaker/.

2.
On the left navigation pane, choose Admin conﬁgurations.

3.
Under Admin conﬁgurations, choose domains.

4.
From the list of domains, select the user proﬁle’s domain.

5.
On the domain details page, choose the User proﬁle whose permissions you want to edit.

6.
On the User Details page, choose Edit.

7.
In the left navigation pane, choose Canvas settings.

8.
In the ML Ops permissions conﬁguration section, turn oﬀ the Enable Model Registry
registration permissions toggle.

9.
Choose Submit to save the changes to your domain settings.

The user proﬁle should no longer have model registration permissions.

Register a model version to the SageMaker AI model registry

SageMaker Model Registry tracks all of the model versions that you build to solve a particular
problem in a model group. When you build a SageMaker Canvas model and register it to SageMaker
Model Registry, it gets added to a model group as a new model version. For example, if you build
and register four versions of your model, then a data scientist or MLOps team working in the
SageMaker Model Registry interface can view the model group and review all four versions of the
model in one place.

When registering a Canvas model to the SageMaker Model Registry, a model group is automatically
created and named after your Canvas model. Optionally, you can rename it to a name of your
choice, or use an existing model group in the SageMaker Model Registry. For more information
about creating a model group, see Create a Model Group.

Note

Currently, you can only register models built in Canvas to the SageMaker Model Registry in
the same account.

Custom models
1346

## Page 376

Amazon SageMaker AI
Developer Guide

To register a model version to the SageMaker Model Registry from the Canvas application, use the
following procedure:

1.
Open the SageMaker Canvas application.

2.
In the left navigation pane, choose My models.

3.
On the My models page, choose your model. You can Filter by problem type to ﬁnd your
model more easily.

4.
After choosing your model, the Versions page opens, listing all of the versions of your model.
You can turn on the Show advanced metrics toggle to view the advanced metrics, such as
Recall and Precision, to compare your model versions and determine which one you’d like to
register.

5.
From the list of model versions, for the the version that you want to register, choose the More
options icon
(

).
Alternatively, you can double click on the version that you need to register,
and then on the version details page, choose the More options icon
(

).

6.
In the dropdown list, choose Add to Model Registry. The Add to Model Registry dialog box
opens.

7.
In the Add to Model Registry dialog box, do the following:

a.
(Optional) In the SageMaker Studio Classic model group section, for the Model group
name ﬁeld, enter the name of the model group to which you want to register your
version. You can specify the name for a new model group that SageMaker AI creates for
you, or you can specify an existing model group. If you don’t specify this ﬁeld, Canvas
registers your version to a default model group with the same name as your model.

b.
Choose Add.

Your model version should now be registered to the model group in the SageMaker Model
Registry. When you register a model version to a model group in the SageMaker Model Registry, all
subsequent versions of the Canvas model are registered to the same model group (if you choose
to register them). If you register your versions to a diﬀerent model group, you need to go to the
SageMaker Model Registry and delete the model group. Then, you can re-register your model
versions to the new model group.

Custom models
1347

## Page 377

Amazon SageMaker AI
Developer Guide

To view the status of your models, you can return to the Versions page for your model in the
Canvas application. This page shows you the Model Registry status of each version. If the status is

Registered, then the model has been successfully registered.

If you want to view the details of your registered model version, for the Model Registry status, you
can hover over the Registered ﬁeld to see the Model registry details pop-up box. These details
contain more info, such as the following:

• The Model package group name is the model group that your version is registered to in the
SageMaker Model Registry.

• The Approval status, which can be Pending Approval, Approved, or Rejected. If a Studio
Classic user approves or rejects your version in the SageMaker Model Registry, then this status is
updated on your model versions page when you refresh the page.

The following screenshot shows the Model registry details box, along with an Approval status of

Approved for this particular model version.

![Page 377 Diagram 1](images/page-0377-img-01.png)

Deploy your models to an endpoint

In Amazon SageMaker Canvas, you can deploy your models to an endpoint to make predictions.
SageMaker AI provides the ML infrastructure for you to host your model on an endpoint with the
compute instances that you choose. Then, you can invoke the endpoint (send a prediction request)
and get a real-time prediction from your model. With this functionality, you can use your model
in production to respond to incoming requests, and you can integrate your model with existing
applications and workﬂows.

Custom models
1348

## Page 378

Amazon SageMaker AI
Developer Guide

To get started, you should have a model that you'd like to deploy. You can deploy custom model
versions that you've built, Amazon SageMaker JumpStart foundation models, and ﬁne-tuned
JumpStart foundation models. For more information about building a model in Canvas, see How
custom models work. For more information about JumpStart foundation models in Canvas, see
Generative AI foundation models in SageMaker Canvas.

Review the following Permissions management section, and then begin creating new deployments
in the Deploy a model section.

Permissions management

By default, you have permissions to deploy models to SageMaker AI Hosting endpoints.
SageMaker AI grants these permissions for all new and existing Canvas user proﬁles through the
AmazonSageMakerCanvasFullAccess policy, which is attached to the AWS IAM execution role for
the SageMaker AI domain that hosts your Canvas application.

If your Canvas administrator is setting up a new domain or user proﬁle, when they're setting up
the domain and following the prerequisite instructions in the Prerequisites for setting up Amazon
SageMaker Canvas, SageMaker AI turns on the model deployment permissions through the Enable
direct deployment of Canvas models option, which is enabled by default.

The Canvas administrator can manage model deployment permissions at the user proﬁle level as
well. For example, if the administrator doesn't want to grant model deployment permissions to all
user proﬁles when setting up a domain, they can grant permissions to speciﬁc users after creating
the domain.

The following procedure shows how to modify the model deployment permissions for a speciﬁc
user proﬁle:

1.
Open the SageMaker AI console at https://console.aws.amazon.com/sagemaker/.

2.
On the left navigation pane, choose Admin conﬁgurations.

3.
Under Admin conﬁgurations, choose Domains.

4.
From the list of domains, select the user proﬁle’s domain.

5.
On the Domain details page, select the User proﬁles tab.

6.
Choose your User proﬁle.

7.
On the user proﬁle's page, select the App Conﬁgurations tab.

8.
In the Canvas section, choose Edit.

Custom models
1349

## Page 379

Amazon SageMaker AI
Developer Guide

9.
In the ML Ops conﬁguration section, turn on the Enable direct deployment of Canvas
models toggle to enable deployment permissions.

10. Choose Submit to save the changes to your domain settings.

The user proﬁle should now have model deployment permissions.

After granting permissions to the domain or user proﬁle, make sure that the user logs out of their
Canvas application and logs back in to apply the permission changes.

Deploy a model

To get started with deploying your model, you create a new deployment in Canvas and specify
the model version that you want to deploy along with the ML infrastructure, such as the type and
number of compute instances that you would like to use for hosting the model.

Canvas suggests a default type and number of instances based on your model type, or you can
learn more about the various SageMaker AI instance types on the Amazon SageMaker pricing page.
You are charged based on the SageMaker AI instance pricing while your endpoint is active.

When deploying JumpStart foundation models, you also have the option to specify the length of
the deployment time. You can deploy the model to an endpoint indeﬁnitely (meaning the endpoint
is active until you delete the deployment). Or, if you only need the endpoint for a short period
of time and would like to reduce costs, you can deploy the model to an endpoint for a speciﬁed
amount of time, after which SageMaker AI shuts down the endpoint for you.

Note

If you deploy a model for a speciﬁed amount of time, stay logged in to the Canvas
application for the duration of the endpoint. If you log out of or delete the application,
then Canvas is unable to shut down the endpoint at the speciﬁed time.

After your model is deployed to a SageMaker AI Hosting real-time inference endpoint, you can
begin making predictions by invoking the endpoint.

There are several diﬀerent ways for you to deploy a model from the Canvas application. You can
access the model deployment option through any of the following methods:

• On the My models page of the Canvas application, choose the model that you want
to deploy. Then, from the model’s Versions page, choose the More options icon

Custom models
1350

## Page 380

Amazon SageMaker AI
Developer Guide

(

)
next to a model version and select Deploy.

• When on the details page for a model version, on the Analyze tab, choose the Deploy option.

• When on the details page for a model version, on the Predict tab, choose the More options icon
(

)
at the top of the page and select Deploy.

• On the ML Ops page of the Canvas application, choose the Deployments tab and then choose
Create deployment.

• For JumpStart foundation models and ﬁne-tuned foundation models, go to the Ready-to-use
models page of the Canvas application. Choose Generate, extract and summarize content.
Then, ﬁnd the JumpStart foundation model or ﬁne-tuned foundation model that you want to
deploy. Choose the model, and on the model's chat page, choose the Deploy button.

All of these methods open the Deploy model side panel, where you specify the deployment
conﬁguration for your model. To deploy the model from this panel, do the following:

1.
(Optional) If you’re creating a deployment from the ML Ops page, you’ll have the option to
Select model and version. Use the dropdown menus to select the model and model version
that you want to deploy.

2.
Enter a name in the Deployment name ﬁeld.

3.
(For JumpStart foundation models and ﬁne-tuned foundation models only) Choose a
Deployment length. Select Indeﬁnite to leave the endpoint active until you shut it down, or
select Specify length and then enter the period of time for which you want the endpoint to
remain active.

4.
For Instance type, SageMaker AI detects a default instance type and number that is suitable
for your model. However, you can change the instance type that you would like to use for
hosting your model.

Note

If you run out of the instance quota for the chosen instance type on your AWS account,
you can request a quota increase. For more information about the default quotas and
how to request an increase, see Amazon SageMaker AI endpoints and quotas in the
AWS General Reference guide.

Custom models
1351

## Page 381

Amazon SageMaker AI
Developer Guide

5.
For Instance count, you can set the number of active instances that are used for your
endpoint. SageMaker AI detects a default number that is suitable for your model, but you can
change this number.

6.
When you’re ready to deploy your model, choose Deploy.

Your model should now be deployed to an endpoint.

View your deployments

You might want to check the status or details of a model deployment in Amazon SageMaker
Canvas. For example, if your deployment failed, you might want to check the details to
troubleshoot.

You can view your Canvas model deployments from the Canvas application or from the Amazon
SageMaker AI console.

To view deployment details from Canvas, choose one of the following procedures:

To view your deployment details from the ML Ops page, do the following:

1.
Open the SageMaker Canvas application.

2.
In the left navigation pane, choose ML Ops.

3.
Choose the Deployments tab.

4.
Choose your deployment by name from the list.

To view your deployment details from a model version’s page, do the following:

1.
In the SageMaker Canvas application, go to your model version’s details page.

2.
Choose the Deploy tab.

3.
On the Deployments  section that lists all of the deployment conﬁgurations associated with
that model version, ﬁnd your deployment.

4.
Choose the More options icon
(

),
and then select View details to open the details page.

Custom models
1352

## Page 382

Amazon SageMaker AI
Developer Guide

The details page for your deployment opens, and you can view information such as the time of
the most recent prediction, the endpoint’s status and conﬁguration, and the model version that is
currently deployed to the endpoint.

You can also view your currently active Canvas workspace instances and active endpoints from
the SageMaker AI dashboard in the SageMaker AI console. Your Canvas endpoints are listed
alongside any other SageMaker AI Hosting endpoints that you’ve created, and you can ﬁlter them
by searching for endpoints with the Canvas tag.

The following screenshot shows the SageMaker AI dashboard. In the Canvas section, you can see
that one workspace instance is in service and four endpoints are active.

![Page 382 Diagram 1](images/page-0382-img-01.png)

Update a deployment conﬁguration

You can update the deployment conﬁguration for models that you've deployed to endpoints
in Amazon SageMaker Canvas. For example, you can deploy an updated model version to the

Custom models
1353

## Page 383

Amazon SageMaker AI
Developer Guide

endpoint, or you can update the instance type or number of instances behind the endpoint based
on your capacity needs.

There are several diﬀerent ways for you to update your deployment from the Canvas application.
You can use any of the following methods:

• On the ML Ops page of the Canvas application, you can choose the Deployments tab and select
the deployment that you want to update. Then, choose Update conﬁguration.

• When on the details page for a model version, on the Deploy tab, you can view the
deployments for that version. Next to the deployment, choose the More options icon
(

)
and then choose Update conﬁguration.

Both of the preceding methods open the Update conﬁguration side panel, where you can make
changes to your deployment conﬁguration. To update the conﬁguration, do the following:

1.
For the Select version dropdown menu, you can select a diﬀerent model version to deploy to
the endpoint.

Note

When updating a deployment conﬁguration, you can only choose a diﬀerent model
version to deploy. To deploy a diﬀerent model, create a new deployment.

2.
For Instance type, you can select a diﬀerent instance type for hosting your model.

3.
For Instance count, you can change the number of active instances that are used for your
endpoint.

4.
Choose Save.

Your deployment conﬁguration should now be updated.

Test your deployment

You can test a model deployment by invoking the endpoint, or making single prediction requests,
through the Amazon SageMaker Canvas application. You can use this functionality to conﬁrm
that your endpoint responds to requests before invoking your endpoint programmatically in a
production environment.

Custom models
1354

## Page 384

Amazon SageMaker AI
Developer Guide

Test a custom model deployment

You can test a custom model deployment by accessing it through the ML Ops page and making
a single invocation, which returns a prediction along with the probability that the prediction is
correct.

Note

Execution length is an estimate of the time taken to invoke and get a response from the
endpoint in Canvas. For detailed latency metrics, see SageMaker AI Endpoint Invocation
Metrics.

To test your endpoint through the Canvas application, do the following:

1.
Open the SageMaker Canvas application.

2.
In the left navigation panel, choose ML Ops.

3.
Choose the Deployments tab.

4.
From the list of deployments, choose the one with the endpoint that you want to invoke.

5.
On the deployment’s details page, choose the Test deployment tab.

6.
On the deployment testing page, you can modify the Value ﬁelds to specify a new data point.
For time series forecasting models, you specify the Item ID for which you want to make a
forecast.

7.
After modifying the values, choose Update to get the prediction result.

The prediction loads, along with the Invocation result ﬁelds which indicate whether or not the
invocation was successful and how long the request took to process.

The following screenshot shows a prediction performed in the Canvas application on the Test
deployment tab.

Custom models
1355

## Page 385

Amazon SageMaker AI
Developer Guide

![Page 385 Diagram 1](images/page-0385-img-01.png)

For all model types except numeric prediction and time series forecasting, the prediction returns
the following ﬁelds:

• predicted_label – the predicted output

• probability – the probability that the predicted label is correct

• labels – the list of all the possible labels

• probabilities – the probabilities corresponding to each label (the order of this list matches the
order of the labels)

For numeric prediction models, the prediction only contains the score ﬁeld, which is the predicted
output of the model, such as the predicted price of a house.

For time series forecasting models, the prediction is a graph showing the forecasts by quantile. You
can choose Schema view to see the forecasted numeric values for each quantile.

You can continue making single predictions through the deployment testing page, or you can see
the following section Invoke your endpoint to learn how to invoke your endpoint programmatically
from applications.

Custom models
1356

## Page 386

Amazon SageMaker AI
Developer Guide

Test a JumpStart foundation model deployment

You can chat with a deployed JumpStart foundation model through the Canvas application to test
its functionality before invoking it through code.

To chat with a deployed JumpStart foundation model, do the following:

1.
Open the SageMaker Canvas application.

2.
In the left navigation panel, choose ML Ops.

3.
Choose the Deployments tab.

4.
From the list of deployments, ﬁnd the one that you want to invoke and choose its More
options icon
(

).

5.
From the context menu, choose Test deployment.

6.
A new Generate, extract and summarize content chat opens with the JumpStart foundation
model, and you can begin typing prompts. Note that prompts from this chat are sent as
requests to your SageMaker AI Hosting endpoint.

Invoke your endpoint

Note

We recommend that you test your model deployment in Amazon SageMaker Canvas before
invoking a SageMaker AI endpoint programmatically.

You can use your Amazon SageMaker Canvas models that you've deployed to a SageMaker
AI endpoint in production with your applications. Invoke the endpoint programmatically the
same way that you invoke any other SageMaker AI real-time endpoint. Invoking an endpoint
programmatically returns a response object which contains the same ﬁelds described in Test your
deployment.

For more detailed information about how to programmatically invoke endpoints, see Invoke
models for real-time inference.

The following Python examples show you how to invoke your endpoint based on the model type.

Custom models
1357

## Page 387

Amazon SageMaker AI
Developer Guide

JumpStart foundation models

The following example shows you how to invoke a JumpStart foundation model that you've
deployed to an endpoint.

import boto3
import pandas as pd

client = boto3.client("runtime.sagemaker")
body = pd.DataFrame(
[['feature_column1', 'feature_column2'],
['feature_column1', 'feature_column2']]
).to_csv(header=False, index=False).encode("utf-8")
response = client.invoke_endpoint(
EndpointName="endpoint_name",
ContentType="text/csv",
Body=body,
Accept="application/json"
)

Numeric and categorical prediction models

The following example shows you how to invoke numeric or categorical prediction models.

import boto3
import pandas as pd

client = boto3.client("runtime.sagemaker")
body = pd.DataFrame(['feature_column1', 'feature_column2'], ['feature_column1',
'feature_column2']).to_csv(header=False, index=False).encode("utf-8")
response = client.invoke_endpoint(
EndpointName="endpoint_name",
ContentType="text/csv",
Body=body,
Accept="application/json"
)

Custom models
1358

## Page 388

Amazon SageMaker AI
Developer Guide

Time series forecasting models

The following example shows you how to invoke time series forecasting models. For a complete
example of how to test invoke a time series forecasting model, see  Time-Series Forecasting with
Amazon SageMaker Autopilot.

import boto3
import pandas as pd

csv_path = './real-time-payload.csv'
data = pd.read_csv(csv_path)

client = boto3.client("runtime.sagemaker")

body = data.to_csv(index=False).encode("utf-8")

response = client.invoke_endpoint(
EndpointName="endpoint_name",
ContentType="text/csv",
Body=body,
Accept="application/json"
)

Image prediction models

The following example shows you how to invoke image prediction models.

import boto3
client = boto3.client("runtime.sagemaker")
with open("example_image.jpg", "rb") as file:
body = file.read()
response = client.invoke_endpoint(
EndpointName="endpoint_name",
ContentType="application/x-image",
Body=body,
Accept="application/json"
)

Text prediction models

The following example shows you how to invoke text prediction models.

import boto3

Custom models
1359

## Page 389

Amazon SageMaker AI
Developer Guide

import pandas as pd

client = boto3.client("runtime.sagemaker")
body = pd.DataFrame([["Example text 1"], ["Example text 2"]]).to_csv(header=False,
index=False).encode("utf-8")
response = client.invoke_endpoint(
EndpointName="endpoint_name",
ContentType="text/csv",
Body=body,
Accept="application/json"
)

Delete a model deployment

You can delete your model deployments from the Amazon SageMaker Canvas application. This
action also deletes the endpoint from the SageMaker AI console and shuts down any endpoint-
related resources.

Note

Optionally, you can delete your endpoint through the SageMaker AI console or using the

SageMaker AI DeleteEndpoint API. For more information, see Delete Endpoints and
Resources. However, when you delete the endpoint through the SageMaker AI console or
APIs instead of the Canvas application, the list of deployments in Canvas isn’t automatically
updated. You must also delete the deployment from the Canvas application to remove it
from the list.

To delete a deployment in Canvas, do the following:

1.
Open the SageMaker Canvas application.

2.
In the left navigation panel, choose ML Ops.

3.
Choose the Deployments tab.

4.
From the list of deployments, choose the one that you want to delete.

5.
At the top of the deployment details page, choose the More options icon
(

).

6.
Choose Delete deployment.

7.
In the  Delete deployment dialog box, choose Delete.

Custom models
1360

## Page 390

Amazon SageMaker AI
Developer Guide

Your deployment and SageMaker AI Hosting endpoint should now be deleted from both Canvas
and the SageMaker AI console.

How to manage automations

In SageMaker Canvas, you can create automations that update your dataset or generate predictions
from your model on a schedule. For example, you might receive new shipping data on a daily basis.
You can set up an automatic update for your dataset and automatic batch predictions that run
whenever the dataset is updated. Using these features, you can set up an automated workﬂow and
reduce the amount of time you spend manually updating datasets and making predictions.

Note

You can only set up a maximum of 20 automatic conﬁgurations in your Canvas application.
Automations are only active while you’re logged in to the Canvas application. If you log out

of Canvas, your automatic jobs pause until you log back in.

The following sections describe how to view, edit, and delete conﬁgurations for existing
automations. To learn how to set up automations, see the following topics:

• To set up automatic dataset updates, see Update a dataset.

• To set up automatic batch predictions, see Batch predictions in SageMaker Canvas.

Topics

• View your automations

• Edit your automatic conﬁgurations

• Delete an automatic conﬁguration

View your automations

You can also view all of your auto update jobs by going to the left navigation pane of Canvas and
choosing ML Ops. The ML Operations page combines automations for both automatic dataset
updates and automatic batch predictions. On the Automations tab, you can see the following sub-
tabs:

• All jobs – You can see every instance of a Dataset update or Batch prediction job that
Canvas has done. For each job, you can see ﬁelds such as the associated Input dataset, the

Custom models
1361

## Page 391

Amazon SageMaker AI
Developer Guide

Conﬁguration name of the associated auto update conﬁguration, and the Status showing
whether the job was successful or not. You can ﬁlter the jobs by conﬁguration name:

• For dataset update jobs, you can choose the latest version of the dataset, or the most recent

job, to preview the dataset.

• For batch prediction jobs, you can choose the More options icon
(

)
to preview or download the predictions for that job. You can also choose View details to
see more details about your prediction job. For more information about batch prediction job
details, see View your batch prediction jobs.

• Conﬁguration – You can see all of the Dataset update and Batch prediction conﬁgurations
you’ve created. For each conﬁguration, you can see ﬁelds such as the associated Input
dataset and the Frequency of the jobs. You can also turn oﬀ or turn on the Auto update
toggle to pause or resume automatic updates. If you choose the More options icon

(

)
for a speciﬁc conﬁguration, you can choose to View all jobs for the conﬁguration, Update
conﬁguration, or Delete conﬁguration.

Edit your automatic conﬁgurations

After setting up a conﬁguration, you might want to make changes to it. For automatic dataset
updates, you can update the Amazon S3 location for Canvas to import data, the frequency of the
updates, and the starting time. For automatic batch predictions, you can change the dataset that
the conﬁguration tracks for updates. You can also turn oﬀ the automation to temporarily pause
updates until you choose to resume them.

The following sections show you how to update each type of conﬁguration.

Note

You can’t change the frequency for automatic batch predictions because automatic batch
predictions run every time the target dataset is updated.

Topics

• Edit your automatic dataset update conﬁguration

• Edit your automatic batch prediction conﬁguration

Custom models
1362

## Page 392

Amazon SageMaker AI
Developer Guide

Edit your automatic dataset update conﬁguration

You might want to make changes to your auto update conﬁguration for a dataset, such as changing
the frequency of the updates. You might also want to turn oﬀ your automatic update conﬁguration
to pause the updates to your dataset.

To make changes to your auto update conﬁguration for a dataset, do the following:

1. In the left navigation pane of Canvas, choose ML Ops.

2. Choose the Automations tab.

3. Choose the Conﬁguration tab.

4. For your auto update conﬁguration, choose the More options icon

(

).

5. In the dropdown menu, choose Update conﬁguration. You are taken to the Auto updates tab of

the dataset.

6. Make your changes to the conﬁguration. When you’re done making changes, choose Save.

To pause your dataset updates, turn oﬀ your automatic conﬁguration. One way to turn oﬀ auto
updates is by doing the following:

1. In the left navigation pane of Canvas, choose ML Ops.

2. Choose the Automations tab.

3. Choose the  Conﬁguration tab.

4. Find your conﬁguration from the list and turn oﬀ the Auto update toggle.

Automatic updates for your dataset are now paused. You can turn this toggle back on at any time
to resume the update schedule.

Edit your automatic batch prediction conﬁguration

When you edit a batch prediction conﬁguration, you can change the target dataset but not the
frequency (since automatic batch predictions occur whenever the dataset is updated).

To make changes to your automatic batch predictions conﬁguration, do the following:

1. In the left navigation pane of Canvas, choose ML Ops.

2. Choose the Automations tab.

Custom models
1363

## Page 393

Amazon SageMaker AI
Developer Guide

3. Choose the Conﬁguration tab.

4. For your auto update conﬁguration, choose the More options icon

(

).

5. In the dropdown menu, choose Update conﬁguration. You are taken to the Auto updates tab of

the dataset.

6. The Automate batch prediction dialog box opens. You can select another dataset and choose

Set up to save your changes.

Your automatic batch predictions conﬁguration is now updated.

To pause your automatic batch predictions, turn oﬀ your automatic conﬁguration. Use the
following procedure to turn oﬀ your conﬁguration:

1. In the left navigation pane of Canvas, choose ML Ops.

2. Choose the Automations tab.

3. Choose the  Conﬁguration tab.

4. Find your conﬁguration from the list and turn oﬀ the Auto update toggle.

Automatic batch predictions for your dataset are now paused. You can turn this toggle back on at
any time to resume the update schedule.

Delete an automatic conﬁguration

You might want to delete a conﬁguration to stop your automated workﬂow in SageMaker Canvas.

To delete a conﬁguration for automatic dataset updates or automatic batch predictions, do the
following:

1. In the left navigation pane of Canvas, choose ML Ops.

2. Choose the Automations tab.

3. Choose the Conﬁguration tab.

4. Find your auto update conﬁguration, and choose the More options icon

(

).

5. Choose Delete conﬁguration.

6. In the dialog box that pops up, choose Delete.

Custom models
1364

## Page 394

Amazon SageMaker AI
Developer Guide

Your auto update conﬁguration is now deleted.

Logging out of Amazon SageMaker Canvas

After completing your work in Amazon SageMaker Canvas, you can log out or conﬁgure your

application to automatically terminate the workspace instance. A workspace instance is dedicated
for your use every time you launch a Canvas application, and you are billed for as long as the
instance runs. Logging out or terminating the workspace instance stops the workspace instance
billing. For more information, see SageMaker Pricing.

The following sections describe how to log out of your Canvas application and how to conﬁgure
your application to automatically shut down on a schedule.

Log out of Canvas

When you log out of Canvas, your models and datasets aren't aﬀected. Any quick or standard
model builds or large data processing jobs continue running even if you log out.

To log out, choose the Log out button

(

)
on the left panel of the SageMaker Canvas application.

You can also log out from the SageMaker Canvas application by closing your browser tab and then
deleting the application in the console.

After you log out, SageMaker Canvas tells you to relaunch in a diﬀerent tab. Logging in takes
around 1 minute. If you have an administrator who set up SageMaker Canvas for you, use the
instructions they gave you to log back in. If don't have an administrator, see the procedure for
accessing SageMaker Canvas in Prerequisites for setting up Amazon SageMaker Canvas.

Automatically shut down Canvas

If you’re a Canvas administrator, you might want to regularly shut down applications to reduce
costs. You can either create a schedule to shut down active Canvas applications, or you can create
an automation to shut down Canvas applications as soon as they’re idle (meaning the user hasn’t
been active for 2 hours).

You can create these solutions using AWS Lambda functions that call the DeleteApp API and
delete Canvas applications given certain conditions. For more information about these solutions
and access to CloudFormation templates that you can use, see the blog Optimizing costs for
Amazon SageMaker Canvas with automatic shutdown of idle apps .

Logging out
1365

## Page 395

Amazon SageMaker AI
Developer Guide

Note

You might experience missing Amazon CloudWatch metrics if there was an error when
setting up your idle shut down schedule or a CloudWatch error. We recommend that you
add a CloudWatch alarm that monitors for missing metrics. If you encounter this issue,
reach out to Support for help.

Limitations and troubleshooting

The following section outlines troubleshooting help and limitations that apply when using Amazon
SageMaker Canvas. You can use these this topic to help troubleshoot any issues you encounter.

Troubleshooting issues with granting permissions through the SageMaker AI
console

If you’re having trouble granting Canvas base permissions or Ready-to-use models permissions to
your user, your user might have an AWS IAM execution role with more than one trust relationship
to other AWS services. A trust relationship is a policy attached to your role that deﬁnes which
principals (users, roles, accounts, or services) can assume the role. For example, you might
encounter an issue granting additional Canvas permissions to your user if their execution role has a
trust relationship to both Amazon SageMaker AI and Amazon Forecast.

You can ﬁx this problem by choosing one of the following options.

1. Remove all but one trusted service from the role.

This solution requires you to edit the trust relationship for your user proﬁle’s IAM role and remove
all AWS services except SageMaker AI.

To edit the trust relationship for your IAM execution role, do the following:

1.
Go to the IAM console at https://console.aws.amazon.com/iam/.

2.
In the navigation pane of the IAM console, choose Roles. The console displays the roles for
your account.

3.
Choose the name of the role that you want to modify, and select the Trust relationships tab
on the details page.

4.
Choose Edit trust policy.

Limitations and troubleshooting
1366

## Page 396

Amazon SageMaker AI
Developer Guide

5.
In the Edit trust policy editor, paste the following, and then choose Update Policy.

JSON

{
"Version":"2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Principal": {
"Service": [
"sagemaker.amazonaws.com"
]
},
"Action": "sts:AssumeRole"
}

]
}

You can also update this policy document using the IAM CLI. For more information, see update-
trust in the IAM Command Line Reference.

You can now retry granting the Canvas base permissions or the Ready-to-use models permissions
to your user.

2. Use a diﬀerent role with one or fewer trusted services.

This solution requires you to specify a diﬀerent IAM role for your user proﬁle. Use this option if you
already have an IAM role that you can substitute.

To specify a diﬀerent execution role for your user, do the following:

1.
Open the Amazon SageMaker AI console at https://console.aws.amazon.com/sagemaker/.

2.
On the left navigation pane, choose Admin conﬁgurations.

3.
Under Admin conﬁgurations, choose domains.

4.
From the list of domains, select the domain that you want to view a list of user proﬁles for.

5.
On the domain details page, choose the User proﬁles tab.

6.
Choose the user whose permissions you want to edit. On the User details page, choose Edit.

Limitations and troubleshooting
1367

## Page 397

Amazon SageMaker AI
Developer Guide

7.
On the General settings page, choose the Execution role dropdown list and select the role
that you want to use.

8.
Choose Submit to save your changes to the user proﬁle.

Your user should now be using an execution role with only one trusted service (SageMaker AI).

You can retry granting the Canvas base permissions or the Ready-to-use models permissions to
your user.

3. Manually attach the AWS managed policy to the execution role instead of using the toggle in
the SageMaker AI domain settings.

Instead of using the toggle in the domain or user proﬁle settings, you can manually attach the AWS
managed policies that grant a user the correct permissions.

To grant a user Canvas base permissions, attach the AmazonSageMakerCanvasFullAccess
policy. To grant a user Ready-to-use models permissions, attach the
AmazonSageMakerCanvasAIServicesAccess policy.

Use the following procedure to attach an AWS managed policy to your role:

1.
Go to the IAM console at https://console.aws.amazon.com/iam/.

2.
Choose Roles.

3.
In the search box, search for the user's IAM role by name and select it.

4.
On the page for the user's role, under Permissions, choose Add permissions.

5.
From the dropdown menu, choose Attach policies.

6.
Search for and select the policy or policies that you want to attach to the user’s execution role:

a.
To grant the Canvas base permissions, search for and select the
AmazonSageMakerCanvasFullAccess policy.

b.
To grant the Ready-to-use models permissions, search for and select the
AmazonSageMakerCanvasAIServicesAccess policy.

7.
Choose Add permissions to attach the policy to the role.

After attaching an AWS managed policy to the user’s role through the IAM console, your user
should now have the Canvas base permissions or Ready-to-use models permissions.

Limitations and troubleshooting
1368

## Page 398

Amazon SageMaker AI
Developer Guide

Troubleshooting issues with creating a Canvas application due to space failure

When creating a new Canvas application, if you encounter an error stating Unable to create

app <app-arn> because space <space-arn> is not in InService state, this
indicates that the underlying Amazon SageMaker Studio space creation has failed. A Studio
space is the underlying storage that hosts your Canvas application data. For more general
information about Studio spaces, see Amazon SageMaker Studio spaces. For more information
about conﬁguring spaces in Canvas, see Store SageMaker Canvas application data in your own
SageMaker AI space.

To determine the root cause of your why space creation failed, you can use the DescribeSpace API

to check the FailureReason ﬁeld. For more information about the possible statuses of spaces
and what they mean, see Amazon SageMaker AI domain entities and statuses.

To resolve this issue, ﬁnd your domain in the SageMaker AI console and delete the failed space
listed in the error message you received. For detailed steps on how to ﬁnd and delete a space, see
the page Stop and delete your Studio running applications and spaces and follow the instructions
to Delete a Studio space. Deleting the space also deletes any applications associated with the
space. After deleting the space, you can try to create your Canvas application again. The space
should now provision successfully, allowing Canvas to launch.

Billing and cost in SageMaker Canvas

To track the costs associated with your SageMaker Canvas application, you can use the AWS
Billing and Cost Management service. Billing and Cost Management provides tools to help you
gather information related to your cost and usage, analyze your cost drivers and usage trends,
and take action to budget your spending. For more information, see What is AWS Billing and Cost
Management?

Billing in SageMaker Canvas consists of the following components:

• Workspace instance charges – You are charged for the number of hours that you are logged in to
or using SageMaker Canvas. We recommend that you log out or create a schedule to shut down
any Canvas applications that you’re not actively using to reduce costs. For more information, see
Logging out of Amazon SageMaker Canvas.

• AWS service charges – You are charged for building and making predictions with custom models,
or for making predictions with Ready-to-use models:

Billing and cost in SageMaker Canvas
1369

## Page 399

Amazon SageMaker AI
Developer Guide

• Training charges – For all model types, you are charged based on your resource usage while the
model builds. These resources include any compute instances that Canvas spins up. You may
see these charges on your account as Hosting, Training, Processing, or Batch Transform jobs.

• Prediction charges – You are charged for the resources used to generate predictions,
depending on the type of custom model that you built or the type of Ready-to-use model you
used.

The Ready-to-use models in Canvas leverage other AWS services to generate predictions. When you
use a Ready-to-use model, you are charged by the respective service, and their pricing conditions
apply:

• For sentiment analysis, entities extraction, language detection, and personal information
detection, you’re charged with Amazon Comprehend pricing.

• For object detection in images and text detection in images, you’re charged with Amazon
Rekognition pricing.

• For expense analysis, identity document analysis, and document analysis, you’re charged with
Amazon Textract pricing.

For more information, see SageMaker Canvas pricing.

To help you track your costs in Billing and Cost Management, you can assign custom tags to
your SageMaker Canvas app and users. You can track the costs your apps incur, and by tagging
individual user proﬁles, you can track costs based on the user proﬁle. For more information about
tags, see Using Cost Allocation Tags.

You can add tags to your SageMaker Canvas app and users by doing the following:

• If you are setting up your Amazon SageMaker AI domain and SageMaker Canvas for the ﬁrst
time, follow the Getting Started instructions and add tags when creating your domain or users.
You can add tags either through the General settings in the domain console setup, or through
the APIs (CreateDomain or CreateUserProﬁle). SageMaker AI adds the tags speciﬁed in your
domain or UserProﬁle to any SageMaker Canvas apps or users you create after you create the
domain.

• If you want to add tags to apps in an existing domain, you must add tags to either the domain or
the UserProﬁle. You can adds tags through either the console or the AddTags API. If you add tags
through the console, then you must delete and relaunch your SageMaker Canvas app in order for

Billing and cost in SageMaker Canvas
1370

## Page 400

Amazon SageMaker AI
Developer Guide

the tags to propagate to the app. If you use the API, the tags are added directly to the app. For
more information about deleting and relaunching a SageMaker Canvas app, see Manage apps.

After you add tags to your domain, it might take up to 24 hours for the tags to appear in the AWS
Billing and Cost Management console for activation. After they appear in the console, it takes
another 24 hours for the tags to activate.

On the Cost explorer page, you can group and ﬁlter your costs by tags and usage types to separate
your Workspace instance charges from your Training charges. The charges for each are listed as the
following:

• Workspace instance charges: Charges show up under the usage type REGION-

Canvas:Session-Hrs (Hrs).

• Training charges: Charges show up under the usage types for SageMaker AI Hosting, Training,
Processing, or Batch Transform jobs.

Amazon SageMaker geospatial capabilities

Important

As of November 30, 2023, the previous Amazon SageMaker Studio experience is now
named Amazon SageMaker Studio Classic. If prior to November 30, 2023 you created a
Amazon SageMaker AI domain, Studio Classic remains the default experience. domains
created after November 30, 2023 default to the new Studio experience.
Amazon SageMaker geospatial features and resources are only available in Studio Classic.
To learn more about setting up a domain and getting started with Studio, see Getting
started with Amazon SageMaker geospatial.

Amazon SageMaker geospatial capabilities makes it easier for data scientists and machine learning
(ML) engineers to build, train, and deploy ML models faster using geospatial data. You have access
to open-source and third-party data, processing, and visualization tools to make it more eﬃcient
to prepare geospatial data for ML. You can increase your productivity by using purpose-built
algorithms and pre-trained ML models to speed up model building and training, and use built-in
visualization tools to explore prediction outputs on an interactive map and then collaborate across
teams on insights and results.

Geospatial capabilities
1371

## Page 401

Amazon SageMaker AI
Developer Guide

Note

Currently, SageMaker geospatial capabilities are only supported in the US West (Oregon)
Region.
If you don't see the SageMaker geospatial UI available in your current Studio Classic
instance check to make sure you are currently in the US West (Oregon) Region.

Why use SageMaker geospatial capabilities?

You can use SageMaker geospatial capabilities to make predictions on geospatial data faster than
do-it-yourself solutions. SageMaker geospatial capabilities make it easier to access geospatial
data from your existing customer data lakes, open-source datasets, and other SageMaker
geospatial data providers. SageMaker geospatial capabilities minimize the need for building

custom infrastructure and data preprocessing functions by oﬀering purpose-built algorithms for
eﬃcient data preparation, model training, and inference. You can also create and share custom
visualizations and data with your company from Amazon SageMaker Studio Classic. SageMaker
geospatial capabilities oﬀer pre-trained models for common uses in agriculture, real estate,
insurance, and ﬁnancial services.

How can I use SageMaker geospatial capabilities?

You can use SageMaker geospatial capabilities in two ways.

• Through the SageMaker geospatial UI, as a part of Amazon SageMaker Studio Classic UI.

• Through a Studio Classic notebook instance that uses the Geospatial 1.0 image.

SageMaker AI has the following geospatial capabilities

• Use a purpose built SageMaker geospatial image that supports both CPU and GPU-based
notebook instances, and also includes commonly used open-source libraries found in geospatial
machine learning workﬂows.

• Use the Amazon SageMaker Processing and the SageMaker geospatial container to run large-
scale workloads with your own datasets, including soil, weather, climate, LiDAR, and commercial
aerial and satellite imagery.

• Run an Earth Observation job for raster data processing.

How can I use SageMaker geospatial capabilities?
1372

## Page 402

Amazon SageMaker AI
Developer Guide

• Run a Vector Enrichment job to convert latitude and longitude into human readable addresses,
and match noisy GPS traces to speciﬁc roads.

• Use built-in visualization tools right in Studio Classic to interactively view geospatial data or
model predictions on a map.

You can also use data from a collection of geospatial data providers. Currently, the data collections
available include:

• USGS Landsat

• Sentinel-1

• Sentinel-2

• Copernicus DEM

• National Agriculture Imagery Program

Are you a ﬁrst-time user of SageMaker geospatial?

As of November 30, 2023, the previous Amazon SageMaker Studio experience is now named
Amazon SageMaker Studio Classic. New domains created after November 30, 2023 default to the
Studio experience. Access to SageMaker geospatial is limited to Studio Classic, to learn more see
Accessing SageMaker geospatial.

If you're a ﬁrst-time user of AWS or Amazon SageMaker AI, we recommend that you do the
following:

1. Create an AWS account.

To learn about setting up an AWS account and getting started with SageMaker AI, see Complete
Amazon SageMaker AI prerequisites.

2. Create a user role and execution role that work with SageMaker geospatial.

As a managed service, Amazon SageMaker geospatial capabilities performs operations on your
behalf on the AWS hardware that SageMaker AI manages. A SageMaker AI execution role an
perform only the operations that users grant. To work with SageMaker geospatial capabilities,
you must set up a user role and an execution role. For more information, see SageMaker
geospatial capabilities roles.

3. Update your trust policy to include SageMaker geospatial.

First-time user?
1373

## Page 403

Amazon SageMaker AI
Developer Guide

SageMaker geospatial deﬁnes an additional service principal. To learn how to create or update
your SageMaker AI execution role's trust policy, see Adding the SageMaker geospatial service
principal to an existing SageMaker AI execution role.

4. Set up an Amazon SageMaker AI domain to access Amazon SageMaker Studio Classic.

To use SageMaker geospatial, a domain is required. For domains created before November
30, 2023 the default experience is Studio Classic. domains created after November 30, 2023
default to the Studio experience. To learn more about accessing Studio Classic from Studio, see
Accessing SageMaker geospatial.

5. Remember, shut down resources.

When you have ﬁnished using SageMaker geospatial capabilities, shut down the instance it runs
on to avoid incurring additional charges. For more information, see Shut Down Resources from
Amazon SageMaker Studio Classic.

Topics

• Getting started with Amazon SageMaker geospatial

• Using a processing jobs for custom geospatial workloads

• Earth Observation Jobs

• Vector Enrichment Jobs

• Visualization Using SageMaker geospatial capabilities

• Amazon SageMaker geospatial Map SDK

• SageMaker geospatial capabilities FAQ

• SageMaker geospatial Security and Permissions

• Types of compute instances

• Data collections

Getting started with Amazon SageMaker geospatial

SageMaker geospatial provides a purpose built Image and Instance type for Amazon SageMaker
Studio Classic notebooks. You can use either CPU or GPU enabled notebooks with the SageMaker
geospatial Image. You can also visualize your geospatial data using a purpose built visualizer.
Furthermore, SageMaker geospatial also provides APIs that allow you to query raster data

Getting started
1374

## Page 404

Amazon SageMaker AI
Developer Guide

collections.You can also use pre-trained models to analyze geospatial data, reverse geocoding, and
map matching.

Important

As of November 30, 2023, the previous Amazon SageMaker Studio experience is now
named Amazon SageMaker Studio Classic. If prior to November 30, 2023 you created a
Amazon SageMaker AI domain, Studio Classic remains the default experience. domains
created after November 30, 2023 default to the new Studio experience.

To access and get started using Amazon SageMaker geospatial, do the following:

Topics

• Accessing SageMaker geospatial

• Create an Amazon SageMaker Studio Classic notebook using the geospatial image

• Access the Sentinel-2 raster data collection and create an earth observation job to perform land
segmentation

Accessing SageMaker geospatial

Note

Currently, SageMaker geospatial capabilities are only supported in the US West (Oregon)
Region and in Studio Classic.
If you don't see the SageMaker geospatial UI available in your current Studio Classic
instance check to make sure you are currently in the US West (Oregon) Region.

A domain is required to access SageMaker geospatial. If you created a domain prior to November
30, 2023 the default experience is Studio Classic.

If you created a domain after November 30, 2023 or if you have migrated to Studio, then you
can use the following procedure to activate Studio Classic from within Studio to use SageMaker
geospatial features.

To learn more about creating a domain, see Onboard to Amazon SageMaker AI domain.

Getting started
1375

## Page 405

Amazon SageMaker AI
Developer Guide

To access Studio Classic from Studio

1.
Launch Amazon SageMaker Studio.

2.
Under Applications, choose Studio Classic.

3.
Then, choose Create Studio Classic space.

4.
On the Create Studio Classic space page, enter a Name.

5.
Disable the Share with my domain option. SageMaker geospatial is not available in shared
domains.

6.
Then choose Create space.

When successful the Status changes to Updating. When your Studio Classic application is ready to
be used the status changes to Stopped.

To start your Studio Classic application, choose Run.

Create an Amazon SageMaker Studio Classic notebook using the geospatial image

Important

As of November 30, 2023, the previous Amazon SageMaker Studio experience is now
named Amazon SageMaker Studio Classic. The following section is speciﬁc to using the
Studio Classic application. For information about using the updated Studio experience, see
Amazon SageMaker Studio.
Studio Classic is still maintained for existing workloads but is no longer available for
onboarding. You can only stop or delete existing Studio Classic applications and cannot
create new ones. We recommend that you migrate your workload to the new Studio
experience.

Note

Currently, SageMaker geospatial is only supported in the US West (Oregon) Region.
If you don't see SageMaker geospatial available in your current domain or notebook
instance, make sure that you're currently in the US West (Oregon) Region.

Getting started
1376

## Page 406

Amazon SageMaker AI
Developer Guide

Use the following procedure to create Studio Classic notebook with the SageMaker geospatial
image. If your default studio experience is Studio, see Accessing SageMaker geospatial to learn
about starting a Studio Classic application.

To create a Studio Classic notebook with the SageMaker geospatial image

1.
Launch Studio Classic

2.
Choose Home in the menu bar.

3.
Under Quick actions, choose Open Launcher.

4.
When the Launcher dialog box opens. Choose Change environment under Notebooks and
compute resources.

5.
When, the Change environment dialog box opens. Choose the Image dropdown and choose or
type Geospatial 1.0.

![Page 406 Diagram 1](images/page-0406-img-01.png)

6.
Next, choose an Instance type from the dropdown.

SageMaker geospatial supports two types of notebook instances: CPU and GPU. The supported
CPU instance is called ml.geospatial.interactive. Any of the G5-family of GPU instances can be
used with the Geospatial 1.0 image.

Getting started
1377

## Page 407

Amazon SageMaker AI
Developer Guide

Note

If you receive a ResourceLimitExceeded error when attempting to start a GPU based
instance, you need to request a quota increase. To get started on a Service Quotas
quota increase request, see Requesting a quota increase in the Service Quotas User
Guide

7.
Choose Select.

8.
Choose Create notebook.

After creating a notebook, to learn more about SageMaker geospatial, try the SageMaker
geospatial tutorial. It shows you how to process Sentinel-2 image data and perform land
segmentation on it using the earth observation jobs API.

Access the Sentinel-2 raster data collection and create an earth observation job to
perform land segmentation

This Python-based tutorial uses the SDK for Python (Boto3) and an Amazon SageMaker Studio
Classic notebook. To complete this demo successfully, make sure that you have the required AWS
Identity and Access Management (IAM) permissions to use SageMaker geospatial and Studio
Classic. SageMaker geospatial requires that you have a user, group, or role which can access Studio
Classic. You must also have a SageMaker AI execution role that speciﬁes the SageMaker geospatial

service principal, sagemaker-geospatial.amazonaws.com in its trust policy.

To learn more about these requirements, see SageMaker geospatial IAM roles.

This tutorial shows you how to use SageMaker geospatial API to complete the following tasks:

• Find the available raster data collections with list_raster_data_collections.

• Search a speciﬁed raster data collection by using search_raster_data_collection.

• Create an earth observation job (EOJ) by using start_earth_observation_job.

Using list_raster_data_collections to ﬁnd available data collections

SageMaker geospatial supports multiple raster data collections. To learn more about the available
data collections, see Data collections.

Getting started
1378

## Page 408

Amazon SageMaker AI
Developer Guide

This demo uses satellite data that's collected from Sentinel-2 Cloud-Optimized GeoTIFF satellites.
These satellites provide global coverage of Earth's land surface every ﬁve days. In addition to
collecting surface images of Earth, the Sentinel-2 satellites also collect data across a variety of
spectralbands.

To search an area of interest (AOI), you need the ARN that's associated with the Sentinel-2 satellite
data. To ﬁnd the available data collections and their associated ARNs in your AWS Region, use the

list_raster_data_collections API operation.

Because the response can be paginated, you must use the get_paginator operation to return all
of the relevant data:

import boto3
import sagemaker
import sagemaker_geospatial_map
import json

## SageMaker Geospatial  is currently only avaialable in US-WEST-2
session = boto3.Session(region_name='us-west-2')
execution_role = sagemaker.get_execution_role()

## Creates a SageMaker Geospatial client instance
geospatial_client = session.client(service_name="sagemaker-geospatial")

# Creates a resusable Paginator for the list_raster_data_collections API operation
paginator = geospatial_client.get_paginator("list_raster_data_collections")

# Create a PageIterator from the paginator class
page_iterator = paginator.paginate()

# Use the iterator to iterate throught the results of list_raster_data_collections
results = []
for page in page_iterator:
results.append(page['RasterDataCollectionSummaries'])

print(results)

This is a sample JSON response from the list_raster_data_collections API operation. It's
truncated to include only the data collection (Sentinel-2) that's used in this code example. For more

details about a speciﬁc raster data collection, use get_raster_data_collection:

{

Getting started
1379

## Page 409

Amazon SageMaker AI
Developer Guide

"Arn": "arn:aws:sagemaker-geospatial:us-west-2:378778860802:raster-data-collection/
public/nmqj48dcu3g7ayw8",
"Description": "Sentinel-2a and Sentinel-2b imagery, processed to Level 2A (Surface
Reflectance) and converted to Cloud-Optimized GeoTIFFs",
"DescriptionPageUrl": "https://registry.opendata.aws/sentinel-2-l2a-cogs",
"Name": "Sentinel 2 L2A COGs",
"SupportedFilters": [
{
"Maximum": 100,
"Minimum": 0,
"Name": "EoCloudCover",
"Type": "number"
},
{
"Maximum": 90,
"Minimum": 0,
"Name": "ViewOffNadir",

"Type": "number"
},
{
"Name": "Platform",
"Type": "string"
}
],
"Tags": {},
"Type": "PUBLIC"
}

After running the previous code sample, you get the ARN of the Sentinel-2 raster data collection,

arn:aws:sagemaker-geospatial:us-west-2:378778860802:raster-data-collection/

public/nmqj48dcu3g7ayw8. In the next section, you can query the Sentinel-2 data collection

using the search_raster_data_collection API.

Searching the Sentinel-2 raster data collection using search_raster_data_collection

In the preceding section, you used list_raster_data_collections to get the ARN for the
Sentinel-2 data collection. Now you can use that ARN to search the data collection over a given
area of interest (AOI), time range, properties, and the available UV bands.

To call the search_raster_data_collection API you must pass in a Python dictionary

to the RasterDataCollectionQuery parameter. This example uses AreaOfInterest,

Getting started
1380

## Page 410

Amazon SageMaker AI
Developer Guide

TimeRangeFilter, PropertyFilters, and BandFilter. For ease, you can specify the Python

dictionary using the variable search_rdc_query to hold the search query parameters:

search_rdc_query = {
"AreaOfInterest": {
"AreaOfInterestGeometry": {
"PolygonGeometry": {
"Coordinates": [
[
# coordinates are input as longitute followed by latitude
[-114.529, 36.142],
[-114.373, 36.142],
[-114.373, 36.411],
[-114.529, 36.411],
[-114.529, 36.142],
]
]
}
}
},
"TimeRangeFilter": {
"StartTime": "2022-01-01T00:00:00Z",
"EndTime": "2022-07-10T23:59:59Z"
},
"PropertyFilters": {
"Properties": [
{
"Property": {
"EoCloudCover": {
"LowerBound": 0,
"UpperBound": 1
}
}
}
],
"LogicalOperator": "AND"
},
"BandFilter": [
"visual"
]
}

Getting started
1381

## Page 411

Amazon SageMaker AI
Developer Guide

In this example, you query an AreaOfInterest that includes Lake Mead in Utah. Furthermore,
Sentinel-2 supports multiple types of image bands. To measure the change in the surface of the

water, you only need the visual band.

After you create the query parameters, you can use the search_raster_data_collection API
to make the request.

The following code sample implements a search_raster_data_collection API request. This

API does not support pagination using the get_paginator API. To make sure that the full API

response has been gathered the code sample uses a while loop to check that NextToken exists.

The code sample then uses .extend() to append the satellite image URLs and other response

metadata to the items_list.

To learn more about the search_raster_data_collection, see SearchRasterDataCollection in
the Amazon SageMaker AI API Reference.

search_rdc_response = sm_geo_client.search_raster_data_collection(
Arn='arn:aws:sagemaker-geospatial:us-west-2:378778860802:raster-data-collection/
public/nmqj48dcu3g7ayw8',
RasterDataCollectionQuery=search_rdc_query
)

## items_list is the response from the API request.
items_list = []

## Use the python .get() method to check that the 'NextToken' exists, if null returns
None breaking the while loop
while search_rdc_response.get('NextToken'):
items_list.extend(search_rdc_response['Items'])
search_rdc_response = sm_geo_client.search_raster_data_collection(
Arn='arn:aws:sagemaker-geospatial:us-west-2:378778860802:raster-data-
collection/public/nmqj48dcu3g7ayw8',
RasterDataCollectionQuery=search_rdc_query,
NextToken=search_rdc_response['NextToken']
)

## Print the number of observation return based on the query
print (len(items_list))

The following is a JSON response from your query. It has been truncated for clarity. Only the

"BandFilter": ["visual"] speciﬁed in the request is returned in the Assets key-value pair:

Getting started
1382

## Page 412

Amazon SageMaker AI
Developer Guide

{
'Assets': {
'visual': {
'Href': 'https://sentinel-cogs.s3.us-west-2.amazonaws.com/sentinel-s2-l2a-
cogs/15/T/UH/2022/6/S2A_15TUH_20220623_0_L2A/TCI.tif'
}
},
'DateTime': datetime.datetime(2022, 6, 23, 17, 22, 5, 926000, tzinfo = tzlocal()),
'Geometry': {
'Coordinates': [
[
[-114.529, 36.142],
[-114.373, 36.142],
[-114.373, 36.411],
[-114.529, 36.411],
[-114.529, 36.142],

]
],
'Type': 'Polygon'
},
'Id': 'S2A_15TUH_20220623_0_L2A',
'Properties': {
'EoCloudCover': 0.046519,
'Platform': 'sentinel-2a'
}
}

Now that you have your query results, in the next section you can visualize the results by using

matplotlib. This is to verify that results are from the correct geographical region.

Visualizing your search_raster_data_collection using matplotlib

Before you start the earth observation job (EOJ), you can visualize a result from our query

withmatplotlib. The following code sample takes the ﬁrst item, items_list[0]["Assets"]

["visual"]["Href"], from the items_list variable created in the previous code sample and

prints an image using matplotlib.

# Visualize an example image.
import os
from urllib import request
import tifffile
import matplotlib.pyplot as plt

Getting started
1383

## Page 413

Amazon SageMaker AI
Developer Guide

image_dir = "./images/lake_mead"
os.makedirs(image_dir, exist_ok=True)

image_dir = "./images/lake_mead"
os.makedirs(image_dir, exist_ok=True)

image_url = items_list[0]["Assets"]["visual"]["Href"]
img_id = image_url.split("/")[-2]
path_to_image = image_dir + "/" + img_id + "_TCI.tif"
response = request.urlretrieve(image_url, path_to_image)
print("Downloaded image: " + img_id)

tci = tifffile.imread(path_to_image)
plt.figure(figsize=(6, 6))
plt.imshow(tci)
plt.show()

After checking that the results are in the correct geographical region, you can start the Earth
Observation Job (EOJ) in the next step. You use the EOJ to identify the water bodies from the
satellite images by using a process called land segmentation.

Starting an earth observation job (EOJ) that performs land segmentation on a series of
Satellite images

SageMaker geospatial provides multiple pre-trained models that you can use to process geospatial
data from raster data collections. To learn more about the available pre-trained models and
custom operations, see Types of Operations.

To calculate the change in the water surface area, you need to identify which pixels in the images
correspond to water. Land cover segmentation is a semantic segmentation model supported by

the start_earth_observation_job API. Semantic segmentation models associate a label with
every pixel in each image. In the results, each pixel is assigned a label that's based on the class map
for the model. The following is the class map for the land segmentation model:

{
0: "No_data",
1: "Saturated_or_defective",
2: "Dark_area_pixels",
3: "Cloud_shadows",
4: "Vegetation",
5: "Not_vegetated",

Getting started
1384

## Page 414

Amazon SageMaker AI
Developer Guide

6: "Water",
7: "Unclassified",
8: "Cloud_medium_probability",
9: "Cloud_high_probability",
10: "Thin_cirrus",
11: "Snow_ice"
}

To start an earth observation job, use the start_earth_observation_job API. When you
submit your request, you must specify the following:

• InputConfig (dict) – Used to specify the coordinates of the area that you want to search, and
other metadata that's associated with your search.

• JobConfig (dict) – Used to specify the type of EOJ operation that you performed on the data.

This example uses LandCoverSegmentationConfig.

• ExecutionRoleArn (string) – The ARN of the SageMaker AI execution role with the necessary
permissions to run the job.

• Name (string) –A name for the earth observation job.

The InputConfig is a Python dictionary. Use the following variable eoj_input_config
to hold the search query parameters. Use this variable when you make the

start_earth_observation_job API request. w.

# Perform land cover segmentation on images returned from the Sentinel-2 dataset.
eoj_input_config = {
"RasterDataCollectionQuery": {
"RasterDataCollectionArn": "arn:aws:sagemaker-geospatial:us-
west-2:378778860802:raster-data-collection/public/nmqj48dcu3g7ayw8",
"AreaOfInterest": {
"AreaOfInterestGeometry": {
"PolygonGeometry": {
"Coordinates":[
[
[-114.529, 36.142],
[-114.373, 36.142],
[-114.373, 36.411],
[-114.529, 36.411],
[-114.529, 36.142],
]
]

Getting started
1385

## Page 415

Amazon SageMaker AI
Developer Guide

}
}
},
"TimeRangeFilter": {
"StartTime": "2021-01-01T00:00:00Z",
"EndTime": "2022-07-10T23:59:59Z",
},
"PropertyFilters": {
"Properties": [{"Property": {"EoCloudCover": {"LowerBound": 0,
"UpperBound": 1}}}],
"LogicalOperator": "AND",
},
}
}

The JobConfig is a Python dictionary that is used to specify the EOJ operation that you want
performed on your data:

eoj_config = {"LandCoverSegmentationConfig": {}}

With the dictionary elements now speciﬁed, you can submit your

start_earth_observation_job API request using the following code sample:

# Gets the execution role arn associated with current notebook instance
execution_role_arn = sagemaker.get_execution_role()

# Starts an earth observation job
response = sm_geo_client.start_earth_observation_job(
Name="lake-mead-landcover",
InputConfig=eoj_input_config,
JobConfig=eoj_config,
ExecutionRoleArn=execution_role_arn,
)
print(response)

The start an earth observation job returns an ARN along with other metadata.

To get a list of all ongoing and current earth observation jobs use the

list_earth_observation_jobs API. To monitor the status of a single earth observation

job use the get_earth_observation_job API. To make this request, use the ARN created

Getting started
1386

## Page 416

Amazon SageMaker AI
Developer Guide

after submitting your EOJ request. To learn more, see GetEarthObservationJob in the Amazon
SageMaker AI API Reference.

To ﬁnd the ARNs associated with your EOJs use the list_earth_observation_jobs API
operation. To learn more, see ListEarthObservationJobs in the Amazon SageMaker AI API Reference.

# List all jobs in the account
sg_client.list_earth_observation_jobs()["EarthObservationJobSummaries"]

The following is an example JSON response:

{
'Arn': 'arn:aws:sagemaker-geospatial:us-west-2:111122223333:earth-observation-job/
futg3vuq935t',
'CreationTime': datetime.datetime(2023, 10, 19, 4, 33, 54, 21481, tzinfo =
tzlocal()),
'DurationInSeconds': 3493,
'Name': 'lake-mead-landcover',
'OperationType': 'LAND_COVER_SEGMENTATION',
'Status': 'COMPLETED',
'Tags': {}
}, {
'Arn': 'arn:aws:sagemaker-geospatial:us-west-2:111122223333:earth-observation-job/
wu8j9x42zw3d',
'CreationTime': datetime.datetime(2023, 10, 20, 0, 3, 27, 270920, tzinfo =
tzlocal()),
'DurationInSeconds': 1,
'Name': 'mt-shasta-landcover',
'OperationType': 'LAND_COVER_SEGMENTATION',
'Status': 'INITIALIZING',
'Tags': {}
}

After the status of your EOJ job changes to COMPLETED, proceed to the next section to calculate
the change in Lake Mead's surface area.

Calculating the change in the Lake Mead surface area

To calculate the change in Lake Mead's surface area, ﬁrst export the results of the EOJ to Amazon

S3 by using export_earth_observation_job:

sagemaker_session = sagemaker.Session()

Getting started
1387

## Page 417

Amazon SageMaker AI
Developer Guide

s3_bucket_name = sagemaker_session.default_bucket()  # Replace with your own bucket if
needed
s3_bucket = session.resource("s3").Bucket(s3_bucket_name)
prefix = "export-lake-mead-eoj"  # Replace with the S3 prefix desired
export_bucket_and_key = f"s3://{s3_bucket_name}/{prefix}/"

eoj_output_config = {"S3Data": {"S3Uri": export_bucket_and_key}}
export_response = sm_geo_client.export_earth_observation_job(
Arn="arn:aws:sagemaker-geospatial:us-west-2:111122223333:earth-observation-
job/7xgwzijebynp",
ExecutionRoleArn=execution_role_arn,
OutputConfig=eoj_output_config,
ExportSourceImages=False,
)

To see the status of your export job, use get_earth_observation_job:

export_job_details =
sm_geo_client.get_earth_observation_job(Arn=export_response["Arn"])

To calculate the changes in Lake Mead's water level, download the land cover masks to the local
SageMaker notebook instance and download the source images from our previous query. In the
class map for the land segmentation model, the water’s class index is 6.

To extract the water mask from a Sentinel-2 image, follow these steps. First, count the number of
pixels marked as water (class index 6) in the image. Second, multiply the count by the area that
each pixel covers. Bands can diﬀer in their spatial resolution. For the land cover segmentation
model all bands are down sampled to a spatial resolution equal to 60 meters.

import os
from glob import glob
import cv2
import numpy as np
import tifffile
import matplotlib.pyplot as plt
from urllib.parse import urlparse
from botocore import UNSIGNED
from botocore.config import Config

# Download land cover masks
mask_dir = "./masks/lake_mead"
os.makedirs(mask_dir, exist_ok=True)

Getting started
1388

## Page 418

Amazon SageMaker AI
Developer Guide

image_paths = []
for s3_object in s3_bucket.objects.filter(Prefix=prefix).all():
path, filename = os.path.split(s3_object.key)
if "output" in path:
mask_name = mask_dir + "/" + filename
s3_bucket.download_file(s3_object.key, mask_name)
print("Downloaded mask: " + mask_name)

# Download source images for visualization
for tci_url in tci_urls:
url_parts = urlparse(tci_url)
img_id = url_parts.path.split("/")[-2]
tci_download_path = image_dir + "/" + img_id + "_TCI.tif"
cogs_bucket = session.resource(
"s3", config=Config(signature_version=UNSIGNED, region_name="us-west-2")
).Bucket(url_parts.hostname.split(".")[0])
cogs_bucket.download_file(url_parts.path[1:], tci_download_path)

print("Downloaded image: " + img_id)

print("Downloads complete.")

image_files = glob("images/lake_mead/*.tif")
mask_files = glob("masks/lake_mead/*.tif")
image_files.sort(key=lambda x: x.split("SQA_")[1])
mask_files.sort(key=lambda x: x.split("SQA_")[1])
overlay_dir = "./masks/lake_mead_overlay"
os.makedirs(overlay_dir, exist_ok=True)
lake_areas = []
mask_dates = []

for image_file, mask_file in zip(image_files, mask_files):
image_id = image_file.split("/")[-1].split("_TCI")[0]
mask_id = mask_file.split("/")[-1].split(".tif")[0]
mask_date = mask_id.split("_")[2]
mask_dates.append(mask_date)
assert image_id == mask_id
image = tifffile.imread(image_file)
image_ds = cv2.resize(image, (1830, 1830), interpolation=cv2.INTER_LINEAR)
mask = tifffile.imread(mask_file)
water_mask = np.isin(mask, [6]).astype(np.uint8)  # water has a class index 6
lake_mask = water_mask[1000:, :1100]
lake_area = lake_mask.sum() * 60 * 60 / (1000 * 1000)  # calculate the surface area
lake_areas.append(lake_area)
contour, _ = cv2.findContours(water_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)

Getting started
1389

## Page 419

Amazon SageMaker AI
Developer Guide

combined = cv2.drawContours(image_ds, contour, -1, (255, 0, 0), 4)
lake_crop = combined[1000:, :1100]
cv2.putText(lake_crop, f"{mask_date}", (10,50), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,
0, 0), 3, cv2.LINE_AA)
cv2.putText(lake_crop, f"{lake_area} [sq km]", (10,100), cv2.FONT_HERSHEY_SIMPLEX,
1.5, (0, 0, 0), 3, cv2.LINE_AA)
overlay_file = overlay_dir + '/' + mask_date + '.png'
cv2.imwrite(overlay_file, cv2.cvtColor(lake_crop, cv2.COLOR_RGB2BGR))

# Plot water surface area vs. time.
plt.figure(figsize=(20,10))
plt.title('Lake Mead surface area for the 2021.02 - 2022.07 period.', fontsize=20)
plt.xticks(rotation=45)
plt.ylabel('Water surface area [sq km]', fontsize=14)
plt.plot(mask_dates, lake_areas, marker='o')
plt.grid('on')
plt.ylim(240, 320)

for i, v in enumerate(lake_areas):
plt.text(i, v+2, "%d" %v, ha='center')
plt.show()

Using matplotlib, you can visualize the results with a graph. The graph shows that the surface
area of Lake Mead decreased from January 2021–July 2022.

![Page 419 Diagram 1](images/page-0419-img-01.png)

Getting started
1390

## Page 420

Amazon SageMaker AI
Developer Guide

Using a processing jobs for custom geospatial workloads

With Amazon SageMaker Processing, you can use a simpliﬁed, managed experience on SageMaker
AI to run your data processing workloads with the purpose-built geospatial container.

The underlying infrastructure for a Amazon SageMaker Processing job is fully managed by
SageMaker AI. During a processing job, cluster resources are provisioned for the duration of your
job, and cleaned up when a job completes.

![Page 420 Diagram 1](images/page-0420-img-01.png)

The preceding diagram shows how SageMaker AI spins up a geospatial processing job. SageMaker
AI takes your geospatial workload script, copies your geospatial data from Amazon Simple
Storage Service(Amazon S3), and then pulls the speciﬁed geospatial container. The underlying
infrastructure for the processing job is fully managed by SageMaker AI. Cluster resources are
provisioned for the duration of your job, and cleaned up when a job completes. The output of the
processing job is stored in the bucket you speciﬁed.

Path naming constraints

The local paths inside a Processing jobs container must begin with /opt/ml/

processing/.

SageMaker geospatial provides a purpose-built container, 081189585635.dkr.ecr.us-

west-2.amazonaws.com/sagemaker-geospatial-v1-0:latest that can be speciﬁed when
running a processing job.

Topics

Geospatial processing job
1391

## Page 421

Amazon SageMaker AI
Developer Guide

• Overview: Run processing jobs using ScriptProcessor and a SageMaker geospatial container

• Using ScriptProcessor to calculate the Normalized Diﬀerence Vegetation Index (NDVI) using
Sentinel-2 satellite data

Overview: Run processing jobs using ScriptProcessor and a SageMaker
geospatial container

SageMaker geospatial provides a purpose-built processing container,

081189585635.dkr.ecr.us-west-2.amazonaws.com/sagemaker-geospatial-

v1-0:latest. You can use this container when running a job with Amazon SageMaker Processing.

When you create an instance of the ScriptProcessor class that is available through the Amazon

SageMaker Python SDK for Processing, specify this image_uri.

Note

If you receive a ResourceLimitExceeded error when attempting to start a processing job,
you need to request a quota increase. To get started on a Service Quotas quota increase
request, see Requesting a quota increase in the Service Quotas User Guide

Prerequisites for using ScriptProcessor

1. You have created a Python script that speciﬁes your geospatial ML workload.

2. You have granted the SageMaker AI execution role access to any Amazon S3 buckets that are

needed.

3. Prepare your data for import into the container. Amazon SageMaker Processing jobs support

either setting the s3_data_type equal to "ManifestFile" or to "S3Prefix".

The following procedure show you how to create an instance of ScriptProcessor and submit a
Amazon SageMaker Processing job using the SageMaker geospatial container.

To create a ScriptProcessor instance and submit a Amazon SageMaker Processing job using
a SageMaker geospatial container

1. Instantiate an instance of the ScriptProcessor class using the SageMaker geospatial image:

from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput

Geospatial processing job
1392

## Page 422

Amazon SageMaker AI
Developer Guide

sm_session = sagemaker.session.Session()
execution_role_arn = sagemaker.get_execution_role()

# purpose-built geospatial container
image_uri = '081189585635.dkr.ecr.us-west-2.amazonaws.com/sagemaker-geospatial-
v1-0:latest'

script_processor = ScriptProcessor(
command=['python3'],
image_uri=image_uri,
role=execution_role_arn,
instance_count=4,
instance_type='ml.m5.4xlarge',
sagemaker_session=sm_session
)

Replace execution_role_arn with the ARN of the SageMaker AI execution role that has
access to the input data stored in Amazon S3 and any other AWS services that you want to

call in your processing job. You can update the instance_count and the instance_type to
match the requirements of your processing job.

2. To start a processing job, use the .run() method:

# Can be replaced with any S3 compliant string for the name of the folder.
s3_folder = geospatial-data-analysis

# Use .default_bucket() to get the name of the S3 bucket associated with your current
SageMaker session
s3_bucket = sm_session.default_bucket()
s3_manifest_uri = f's3://{s3_bucket}/{s3_folder}/manifest.json'
s3_prefix_uri =  f's3://{s3_bucket}/{s3_folder}/image-prefix

script_processor.run(
code='preprocessing.py',
inputs=[
ProcessingInput(
source=s3_manifest_uri | s3_prefix_uri ,
destination='/opt/ml/processing/input_data/',
s3_data_type= "ManifestFile" | "S3Prefix",
s3_data_distribution_type= "ShardedByS3Key" | "FullyReplicated"
)

Geospatial processing job
1393

## Page 423

Amazon SageMaker AI
Developer Guide

],
outputs=[
ProcessingOutput(
source='/opt/ml/processing/output_data/',
destination=s3_output_prefix_url
)
]
)

• Replace preprocessing.py with the name of your own Python data processing script.

• A processing job supports two methods for formatting your input data. You can either
create a manifest ﬁle that points to all of the input data for your processing job, or
you can use a common preﬁx on each individual data input. If you created a manifest

ﬁle set s3_manifest_uri equal to "ManifestFile". If you used a ﬁle preﬁx set

s3_manifest_uri equal to "S3Prefix". You specify the path to your data using source.

• You can distribute your processing job data two ways:

• Distribute your data to all processing instances by setting s3_data_distribution_type

equal to FullyReplicated.

• Distribute your data in shards based on the Amazon S3 key by setting

s3_data_distribution_type equal to ShardedByS3Key. When you use

ShardedByS3Key one shard of data is sent to each processing instance.

You can use a script to process SageMaker geospatial data. That script can be found in Step 3:

Writing a script that can calculate the NDVI. To learn more about the .run() API operation, see

run in the Amazon SageMaker Python SDK for Processing.

To monitor the progress of your processing job, the ProcessingJobs class supports a describe

method. This method returns a response from the DescribeProcessingJob API call. To learn

more, see DescribeProcessingJob in the Amazon SageMaker AI API Reference.

The next topic show you how to create an instance of the ScriptProcessor class using the
SageMaker geospatial container, and then how to use it to calculate the Normalized Diﬀerence
Vegetation Index (NDVI) with Sentinel-2 images.

Geospatial processing job
1394

## Page 424

Amazon SageMaker AI
Developer Guide

Using ScriptProcessor to calculate the Normalized Diﬀerence Vegetation Index
(NDVI) using Sentinel-2 satellite data

The following code samples show you how to calculate the normalized diﬀerence vegetation

index of a speciﬁc geographical area using the purpose-built geospatial image within a Studio
Classic notebook and run a large-scale workload with Amazon SageMaker Processing using

ScriptProcessor from the SageMaker AI Python SDK.

This demo also uses an Amazon SageMaker Studio Classic notebook instance that uses the
geospatial kernel and instance type. To learn how to create a Studio Classic geospatial notebook
instance, see Create an Amazon SageMaker Studio Classic notebook using the geospatial image.

You can follow along with this demo in your own notebook instance by copying and pasting the
following code snippets:

1. Use search_raster_data_collection to query a speciﬁc area of interest (AOI) over a given

a time range using a speciﬁc raster data collection, Sentinel-2.

2. Create a manifest ﬁle that speciﬁes what data will be processed during the processing job.

3. Write a data processing Python script calculating the NDVI.

4. Create a ScriptProcessor instance and start the Amazon SageMaker Processing job.

5. Visualizing the results of your processing job.

Query the Sentinel-2 raster data collection using SearchRasterDataCollection

With search_raster_data_collection you can query supported raster data collections. This

example uses data that's pulled from Sentinel-2 satellites. The area of interest (AreaOfInterest)

speciﬁed is rural northern Iowa, and the time range (TimeRangeFilter) is January 1,
2022 to December 30, 2022. To see the available raster data collections in your AWS

Region use list_raster_data_collections. To see a code example using this API, see

ListRasterDataCollections in the Amazon SageMaker AI Developer Guide.

In following code examples you use the ARN associated with Sentinel-2 raster data collection,

arn:aws:sagemaker-geospatial:us-west-2:378778860802:raster-data-collection/

public/nmqj48dcu3g7ayw8.

A search_raster_data_collection API request requires two parameters:

Geospatial processing job
1395

## Page 425

Amazon SageMaker AI
Developer Guide

• You need to specify an Arn parameter that corresponds to the raster data collection that you
want to query.

• You also need to specify a RasterDataCollectionQuery parameter, which takes in a Python
dictionary.

The following code example contains the necessary key-value pairs needed for the

RasterDataCollectionQuery parameter saved to the search_rdc_query variable.

search_rdc_query = {
"AreaOfInterest": {
"AreaOfInterestGeometry": {
"PolygonGeometry": {
"Coordinates": [[
[
-94.50938680498298,
43.22487436936203
],
[
-94.50938680498298,
42.843474642037194
],
[
-93.86520004156142,
42.843474642037194
],
[
-93.86520004156142,
43.22487436936203
],
[
-94.50938680498298,
43.22487436936203
]
]]
}
}
},
"TimeRangeFilter": {"StartTime": "2022-01-01T00:00:00Z", "EndTime":
"2022-12-30T23:59:59Z"}
}

Geospatial processing job
1396

## Page 426

Amazon SageMaker AI
Developer Guide

To make the search_raster_data_collection request, you must specify the ARN

of the Sentinel-2 raster data collection: arn:aws:sagemaker-geospatial:us-

west-2:378778860802:raster-data-collection/public/nmqj48dcu3g7ayw8. You also
must need to pass in the Python dictionary that was deﬁned previously, which speciﬁes query

parameters.

## Creates a SageMaker Geospatial client instance
sm_geo_client= session.create_client(service_name="sagemaker-geospatial")

search_rdc_response1 = sm_geo_client.search_raster_data_collection(
Arn='arn:aws:sagemaker-geospatial:us-west-2:378778860802:raster-data-collection/
public/nmqj48dcu3g7ayw8',
RasterDataCollectionQuery=search_rdc_query
)

The results of this API can not be paginated. To collect all the satellite images returned by the

search_raster_data_collection operation, you can implement a while loop. This checks

forNextToken in the API response:

## Holds the list of API responses from search_raster_data_collection
items_list = []
while search_rdc_response1.get('NextToken') and search_rdc_response1['NextToken'] !=
None:
items_list.extend(search_rdc_response1['Items'])
search_rdc_response1 = sm_geo_client.search_raster_data_collection(
Arn='arn:aws:sagemaker-geospatial:us-west-2:378778860802:raster-data-collection/
public/nmqj48dcu3g7ayw8',
RasterDataCollectionQuery=search_rdc_query,
NextToken=search_rdc_response1['NextToken']
)

The API response returns a list of URLs under the Assets key corresponding to speciﬁc image
bands. The following is a truncated version of the API response. Some of the image bands were
removed for clarity.

{
'Assets': {
'aot': {
'Href': 'https://sentinel-cogs.s3.us-west-2.amazonaws.com/sentinel-s2-l2a-
cogs/15/T/UH/2022/12/S2A_15TUH_20221230_0_L2A/AOT.tif'

Geospatial processing job
1397

## Page 427

Amazon SageMaker AI
Developer Guide

},
'blue': {
'Href': 'https://sentinel-cogs.s3.us-west-2.amazonaws.com/sentinel-s2-l2a-
cogs/15/T/UH/2022/12/S2A_15TUH_20221230_0_L2A/B02.tif'
},
'swir22-jp2': {
'Href': 's3://sentinel-s2-l2a/tiles/15/T/UH/2022/12/30/0/B12.jp2'
},
'visual-jp2': {
'Href': 's3://sentinel-s2-l2a/tiles/15/T/UH/2022/12/30/0/TCI.jp2'
},
'wvp-jp2': {
'Href': 's3://sentinel-s2-l2a/tiles/15/T/UH/2022/12/30/0/WVP.jp2'
}
},
'DateTime': datetime.datetime(2022, 12, 30, 17, 21, 52, 469000, tzinfo =
tzlocal()),

'Geometry': {
'Coordinates': [
[
[-95.46676936182894, 43.32623760511659],
[-94.11293433656887, 43.347431265475954],
[-94.09532154452742, 42.35884880571144],
[-95.42776890002203, 42.3383710796791],
[-95.46676936182894, 43.32623760511659]
]
],
'Type': 'Polygon'
},
'Id': 'S2A_15TUH_20221230_0_L2A',
'Properties': {
'EoCloudCover': 62.384969,
'Platform': 'sentinel-2a'
}
}

In the next section, you create a manifest ﬁle using the 'Id' key from the API response.

Create an input manifest ﬁle using the Id key from the search_raster_data_collection
API response

When you run a processing job, you must specify a data input from Amazon S3. The input data
type can either be a manifest ﬁle, which then points to the individual data ﬁles. You can also add a

Geospatial processing job
1398

## Page 428

Amazon SageMaker AI
Developer Guide

preﬁx to each ﬁle that you want processed. The following code example deﬁnes the folder where
your manifest ﬁles will be generated.

Use SDK for Python (Boto3) to get the default bucket and the ARN of the execution role that is
associated with your Studio Classic notebook instance:

sm_session = sagemaker.session.Session()

s3 = boto3.resource('s3')
# Gets the default excution role associated with the notebook
execution_role_arn = sagemaker.get_execution_role()

# Gets the default bucket associated with the notebook
s3_bucket = sm_session.default_bucket()

# Can be replaced with any name
s3_folder = "script-processor-input-manifest"

Next, you create a manifest ﬁle. It will hold the URLs of the satellite images that you wanted
processed when you run your processing job later in step 4.

# Format of a manifest file
manifest_prefix = {}
manifest_prefix['prefix'] = 's3://' + s3_bucket + '/' + s3_folder + '/'
manifest = [manifest_prefix]

print(manifest)

The following code sample returns the S3 URI where your manifest ﬁles will be created.

[{'prefix': 's3://sagemaker-us-west-2-111122223333/script-processor-input-manifest/'}]

All the response elements from the search_raster_data_collection response are not needed to run
the processing job.

The following code snippet removes the unnecessary elements 'Properties', 'Geometry',

and 'DateTime'. The 'Id' key-value pair, 'Id': 'S2A_15TUH_20221230_0_L2A', contains
the year and the month. The following code example parses that data to create new keys in the

Python dictionary dict_month_items. The values are the assets that are returned from the

SearchRasterDataCollection query.

Geospatial processing job
1399

## Page 429

Amazon SageMaker AI
Developer Guide

# For each response get the month and year, and then remove the metadata not related to
the satelite images.
dict_month_items = {}
for item in items_list:
# Example ID being split: 'S2A_15TUH_20221230_0_L2A'
yyyymm = item['Id'].split("_")[2][:6]
if yyyymm not in dict_month_items:
dict_month_items[yyyymm] = []
# Removes uneeded metadata elements for this demo
item.pop('Properties', None)
item.pop('Geometry', None)
item.pop('DateTime', None)

# Appends the response from search_raster_data_collection to newly created key
above

dict_month_items[yyyymm].append(item)

This code example uploads the dict_month_items to Amazon S3 as a JSON object using the

.upload_file() API operation:

## key_ is the yyyymm timestamp formatted above
## value_ is the reference to all the satellite images collected via our searchRDC
query
for key_, value_ in dict_month_items.items():
filename = f'manifest_{key_}.json'
with open(filename, 'w') as fp:
json.dump(value_, fp)
s3.meta.client.upload_file(filename, s3_bucket, s3_folder + '/' + filename)
manifest.append(filename)
os.remove(filename)

This code example uploads a parent manifest.json ﬁle that points to all the other manifests

uploaded to Amazon S3. It also saves the path to a local variable: s3_manifest_uri. You'll use
that variable again to specify the source of the input data when you run the processing job in step
4.

with open('manifest.json', 'w') as fp:
json.dump(manifest, fp)
s3.meta.client.upload_file('manifest.json', s3_bucket, s3_folder + '/' +
'manifest.json')
os.remove('manifest.json')

Geospatial processing job
1400

## Page 430

Amazon SageMaker AI
Developer Guide

s3_manifest_uri = f's3://{s3_bucket}/{s3_folder}/manifest.json'

Now that you created the input manifest ﬁles and uploaded them, you can write a script that
processes your data in the processing job. It processes the data from the satellite images, calculates
the NDVI, and then returns the results to a diﬀerent Amazon S3 location.

Write a script that calculates the NDVI

Amazon SageMaker Studio Classic supports the use of the %%writefile cell magic command.
After running a cell with this command, its contents will be saved to your local Studio Classic
directory. This is code speciﬁc to calculating NDVI. However, the following can be useful when you
write your own script for a processing job:

• In your processing job container, the local paths inside the container must begin with /

opt/ml/processing/. In this example, input_data_path = '/opt/ml/processing/

input_data/'  and processed_data_path = '/opt/ml/processing/output_data/'
are speciﬁed in that way.

• With Amazon SageMaker Processing, a script that a processing job runs can upload your
processed data directly to Amazon S3. To do so, make sure that the execution role associated

with your ScriptProcessor instance has the necessary requirements to access the S3 bucket.
You can also specify an outputs parameter when you run your processing job. To learn more,

see the .run() API operation  in the Amazon SageMaker Python SDK. In this code example, the
results of the data processing are uploaded directly to Amazon S3.

• To manage the size of the Amazon EBScontainer attached to your processing jobuse the

volume_size_in_gb parameter. The containers's default size is 30 GB. You can aslo optionally
use the Python library Garbage Collector to manage storage in your Amazon EBS container.

The following code example loads the arrays into the processing job container. When arrays
build up and ﬁll in the memory, the processing job crashes. To prevent this crash, the following
example contains commands that remove the arrays from the processing job’s container.

%%writefile compute_ndvi.py

import os
import pickle
import sys
import subprocess

Geospatial processing job
1401

## Page 431

Amazon SageMaker AI
Developer Guide

import json
import rioxarray

if __name__ == "__main__":
print("Starting processing")
input_data_path = '/opt/ml/processing/input_data/'
input_files = []
for current_path, sub_dirs, files in os.walk(input_data_path):
for file in files:
if file.endswith(".json"):
input_files.append(os.path.join(current_path, file))
print("Received {} input_files: {}".format(len(input_files), input_files))

items = []

for input_file in input_files:
full_file_path = os.path.join(input_data_path, input_file)
print(full_file_path)
with open(full_file_path, 'r') as f:
items.append(json.load(f))
items = [item for sub_items in items for item in sub_items]

for item in items:
red_uri = item["Assets"]["red"]["Href"]
nir_uri = item["Assets"]["nir"]["Href"]

red = rioxarray.open_rasterio(red_uri, masked=True)
nir = rioxarray.open_rasterio(nir_uri, masked=True)

ndvi = (nir - red)/ (nir + red)
file_name = 'ndvi_' + item["Id"] + '.tif'
output_path = '/opt/ml/processing/output_data'
output_file_path = f"{output_path}/{file_name}"
ndvi.rio.to_raster(output_file_path)
print("Written output:", output_file_path)

You now have a script that can calculate the NDVI. Next, you can create an instance of the
ScriptProcessor and run your Processing job.

Geospatial processing job
1402

## Page 432

Amazon SageMaker AI
Developer Guide

Creating an instance of the ScriptProcessor class

This demo uses the ScriptProcessor class that is available via the Amazon SageMaker Python SDK.
First, you need to create an instance of the class, and then you can start your Processing job by

using the .run() method.

from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput

image_uri = '081189585635.dkr.ecr.us-west-2.amazonaws.com/sagemaker-geospatial-
v1-0:latest'

processor = ScriptProcessor(
command=['python3'],
image_uri=image_uri,
role=execution_role_arn,
instance_count=4,
instance_type='ml.m5.4xlarge',
sagemaker_session=sm_session
)

print('Starting processing job.')

When you start your Processing job, you need to specify a ProcessingInput object. In that
object, you specify the following:

• The path to the manifest ﬁle that you created in step 2, s3_manifest_uri. This is the source of
the input data to the container.

• The path to where you want the input data to be saved in the container. This must match the
path that you speciﬁed in your script.

• Use the s3_data_type parameter to specify the input as "ManifestFile".

s3_output_prefix_url = f"s3://{s3_bucket}/{s3_folder}/output"

processor.run(
code='compute_ndvi.py',
inputs=[
ProcessingInput(
source=s3_manifest_uri,
destination='/opt/ml/processing/input_data/',
s3_data_type="ManifestFile",

Geospatial processing job
1403

## Page 433

Amazon SageMaker AI
Developer Guide

s3_data_distribution_type="ShardedByS3Key"
),
],
outputs=[
ProcessingOutput(
source='/opt/ml/processing/output_data/',
destination=s3_output_prefix_url,
s3_upload_mode="Continuous"
)
]
)

The following code example uses the .describe() method to get details of your Processing job.

preprocessing_job_descriptor = processor.jobs[-1].describe()
s3_output_uri = preprocessing_job_descriptor["ProcessingOutputConfig"]["Outputs"][0]

["S3Output"]["S3Uri"]
print(s3_output_uri)

Visualizing your results using matplotlib

With the Matplotlib Python library, you can plot raster data. Before you plot the data, you need to
calculate the NDVI using sample images from the Sentinel-2 satellites. The following code example

opens the image arrays using the .open_rasterio() API operation, and then calculates the NDVI

using the nir and red image bands from the Sentinel-2 satellite data.

# Opens the python arrays
import rioxarray

red_uri = items[25]["Assets"]["red"]["Href"]
nir_uri = items[25]["Assets"]["nir"]["Href"]

red = rioxarray.open_rasterio(red_uri, masked=True)
nir = rioxarray.open_rasterio(nir_uri, masked=True)

# Calculates the NDVI
ndvi = (nir - red)/ (nir + red)

# Common plotting library in Python
import matplotlib.pyplot as plt

f, ax = plt.subplots(figsize=(18, 18))

Geospatial processing job
1404

## Page 434

Amazon SageMaker AI
Developer Guide

ndvi.plot(cmap='viridis', ax=ax)
ax.set_title("NDVI for {}".format(items[25]["Id"]))
ax.set_axis_off()
plt.show()

The output of the preceding code example is a satellite image with the NDVI values overlaid on
it. An NDVI value near 1 indicates lots of vegetation is present, and values near 0 indicate no
vegetation is presentation.

Geospatial processing job
1405

## Page 435

Amazon SageMaker AI
Developer Guide

![Page 435 Diagram 1](images/page-0435-img-01.png)

This completes the demo of using ScriptProcessor.

Geospatial processing job
1406

## Page 436

Amazon SageMaker AI
Developer Guide

Earth Observation Jobs

Using an Earth Observation job (EOJ), you can acquire, transform, and visualize geospatial data
to make predictions. You can choose an operation based on your use case from a wide range of
operations and models. You get the ﬂexibility of choosing your area of interest, selecting the
data providers, and setting time-range based and cloud-cover-percentage-based ﬁlters. After
SageMaker AI creates an EOJ for you, you can visualize the inputs and outputs of the job using the
visualization functionality. An EOJ has various use cases that include comparing deforestation over
time and diagnosing plant health. You can create an EOJ by using a SageMaker notebook with a
SageMaker geospatial image. You can also access the SageMaker geospatial UI as a part of Amazon
SageMaker Studio Classic UI to view the list of all your jobs. You can also use the UI to pause or
stop an ongoing job. You can choose a job from the list of available EOJ to view the Job summary,
the Job details as well as visualize the Job output.

Topics

• Create an Earth Observation Job Using a Amazon SageMaker Studio Classic Notebook with a
SageMaker geospatial Image

• Types of Operations

Create an Earth Observation Job Using a Amazon SageMaker Studio Classic
Notebook with a SageMaker geospatial Image

To use a SageMaker Studio Classic notebook with a SageMaker geospatial image:

1.
From the Launcher, choose Change environment under Notebooks and compute resources.

2.
Next, the Change environment dialog opens.

3.
Select the Image dropdown and choose Geospatial 1.0. The Instance type should be
ml.geospatial.interactive. Do not change the default values for other settings.

4.
Choose Select.

5.
Choose Create notebook.

You can initiate an EOJ using a Amazon SageMaker Studio Classic notebook with a SageMaker
geospatial image using the code provided below.

import boto3
import sagemaker

Earth Observation Jobs
1407

## Page 437

Amazon SageMaker AI
Developer Guide

import sagemaker_geospatial_map

session = boto3.Session()
execution_role = sagemaker.get_execution_role()
sg_client = session.client(service_name="sagemaker-geospatial")

The following is an example showing how to create an EOJ in the in the US West (Oregon) Region.

#Query and Access Data
search_rdc_args = {
"Arn": "arn:aws:sagemaker-geospatial:us-west-2:378778860802:raster-data-collection/
public/nmqj48dcu3g7ayw8",  # sentinel-2 L2A COG
"RasterDataCollectionQuery": {
"AreaOfInterest": {
"AreaOfInterestGeometry": {
"PolygonGeometry": {
"Coordinates": [
[
[-114.529, 36.142],
[-114.373, 36.142],
[-114.373, 36.411],
[-114.529, 36.411],
[-114.529, 36.142],
]
]
}
}
},
"TimeRangeFilter": {
"StartTime": "2021-01-01T00:00:00Z",
"EndTime": "2022-07-10T23:59:59Z",
},
"PropertyFilters": {
"Properties": [{"Property": {"EoCloudCover": {"LowerBound": 0,
"UpperBound": 1}}}],
"LogicalOperator": "AND",
},
"BandFilter": ["visual"],
},
}

tci_urls = []
data_manifests = []

Earth Observation Jobs
1408

## Page 438

Amazon SageMaker AI
Developer Guide

while search_rdc_args.get("NextToken", True):
search_result = sg_client.search_raster_data_collection(**search_rdc_args)
if search_result.get("NextToken"):
data_manifests.append(search_result)
for item in search_result["Items"]:
tci_url = item["Assets"]["visual"]["Href"]
print(tci_url)
tci_urls.append(tci_url)

search_rdc_args["NextToken"] = search_result.get("NextToken")
# Perform land cover segmentation on images returned from the sentinel dataset.
eoj_input_config = {
"RasterDataCollectionQuery": {
"RasterDataCollectionArn": "arn:aws:sagemaker-geospatial:us-
west-2:378778860802:raster-data-collection/public/nmqj48dcu3g7ayw8",
"AreaOfInterest": {

"AreaOfInterestGeometry": {
"PolygonGeometry": {
"Coordinates": [
[
[-114.529, 36.142],
[-114.373, 36.142],
[-114.373, 36.411],
[-114.529, 36.411],
[-114.529, 36.142],
]
]
}
}
},
"TimeRangeFilter": {
"StartTime": "2021-01-01T00:00:00Z",
"EndTime": "2022-07-10T23:59:59Z",
},
"PropertyFilters": {
"Properties": [{"Property": {"EoCloudCover": {"LowerBound": 0,
"UpperBound": 1}}}],
"LogicalOperator": "AND",
},
}
}
eoj_config = {"LandCoverSegmentationConfig": {}}

Earth Observation Jobs
1409

## Page 439

Amazon SageMaker AI
Developer Guide

response = sg_client.start_earth_observation_job(
Name="lake-mead-landcover",
InputConfig=eoj_input_config,
JobConfig=eoj_config,
ExecutionRoleArn=execution_role,
)

After your EOJ is created, the Arn is returned to you. You use the Arn to identify
a job and perform further operations. To get the status of a job, you can run

sg_client.get_earth_observation_job(Arn = response['Arn']).

The following example shows how to query the status of an EOJ until it is completed.

eoj_arn = response["Arn"]
job_details = sg_client.get_earth_observation_job(Arn=eoj_arn)
{k: v for k, v in job_details.items() if k in ["Arn", "Status", "DurationInSeconds"]}
# List all jobs in the account
sg_client.list_earth_observation_jobs()["EarthObservationJobSummaries"]

After the EOJ is completed, you can visualize the EOJ outputs directly in the notebook. The
following example shows you how an interactive map can be rendered.

map = sagemaker_geospatial_map.create_map({
'is_raster': True
})
map.set_sagemaker_geospatial_client(sg_client)
# render the map
map.render()

The following example shows how the map can be centered on an area of interest and the input
and output of the EOJ can be rendered as separate layers within the map.

# visualize the area of interest
config = {"label": "Lake Mead AOI"}
aoi_layer = map.visualize_eoj_aoi(Arn=eoj_arn, config=config)

# Visualize input.
time_range_filter = {
"start_date": "2022-07-01T00:00:00Z",
"end_date": "2022-07-10T23:59:59Z",
}

Earth Observation Jobs
1410

## Page 440

Amazon SageMaker AI
Developer Guide

config = {"label": "Input"}

input_layer = map.visualize_eoj_input(
Arn=eoj_arn, config=config, time_range_filter=time_range_filter
)
# Visualize output, EOJ needs to be in completed status.
time_range_filter = {
"start_date": "2022-07-01T00:00:00Z",
"end_date": "2022-07-10T23:59:59Z",
}
config = {"preset": "singleBand", "band_name": "mask"}
output_layer = map.visualize_eoj_output(
Arn=eoj_arn, config=config, time_range_filter=time_range_filter
)

You can use the export_earth_observation_job function to export the EOJ results to
your Amazon S3 bucket. The export function makes it convenient to share results across teams.
SageMaker AI also simpliﬁes dataset management. We can simply share the EOJ results using the
job ARN, instead of crawling thousands of ﬁles in the S3 bucket. Each EOJ becomes an asset in the
data catalog, as results can be grouped by the job ARN. The following example shows how you can
export the results of an EOJ.

sagemaker_session = sagemaker.Session()
s3_bucket_name = sagemaker_session.default_bucket()  # Replace with your own bucket if
needed
s3_bucket = session.resource("s3").Bucket(s3_bucket_name)
prefix = "eoj_lakemead"  # Replace with the S3 prefix desired
export_bucket_and_key = f"s3://{s3_bucket_name}/{prefix}/"

eoj_output_config = {"S3Data": {"S3Uri": export_bucket_and_key}}
export_response = sg_client.export_earth_observation_job(
Arn=eoj_arn,
ExecutionRoleArn=execution_role,
OutputConfig=eoj_output_config,
ExportSourceImages=False,
)

You can monitor the status of your export job using the following snippet.

# Monitor the export job status
export_job_details = sg_client.get_earth_observation_job(Arn=export_response["Arn"])

Earth Observation Jobs
1411

## Page 441

Amazon SageMaker AI
Developer Guide

{k: v for k, v in export_job_details.items() if k in ["Arn", "Status",
"DurationInSeconds"]}

You are not charged the storage fees after you delete the EOJ.

For an example that showcases how to run an EOJ, see this blog post.

For more example notebooks on SageMaker geospatial capabilities, see this GitHub repository.

Types of Operations

When you create an EOJ, you select an operation based on your use case. Amazon SageMaker
geospatial capabilities provide a combination of purpose-built operations and pre-trained models.
You can use these operations to understand the impact of environmental changes and human
activities over time or identify cloud and cloud-free pixels.

Cloud Masking

Identify clouds in satellite images is an essential pre-processing step in producing high-quality
geospatial data. Ignoring cloud pixels can lead to errors in analysis, and over-detection of cloud
pixels can decrease the number of valid observations. Cloud masking has the ability to identify
cloudy and cloud-free pixels in satellite images. An accurate cloud mask helps get satellite images
for processing and improves data generation. The following is the class map for cloud masking.

{
0: "No_cloud",
1: "cloud"
}

Cloud Removal

Cloud removal for Sentinel-2 data uses an ML-based semantic segmentation model to identify
clouds in the image. Cloudy pixels can be replaced by with pixels from other timestamps. USGS
Landsat data contains landsat metadata that is used for cloud removal.

Temporal Statistics

Temporal statistics calculate statistics for geospatial data through time. The temporal statistics
currently supported include mean, median, and standard deviation. You can calculate these

Earth Observation Jobs
1412

## Page 442

Amazon SageMaker AI
Developer Guide

statistics by using GROUPBY and set it to either all or yearly. You can also mention the

TargetBands.

Zonal Statistics

Zonal statistics performs statistical operations over a speciﬁed area on the image.

Resampling

Resampling is used to upscale and downscale the resolution of a geospatial image. The value
attribute in resampling represents the length of a side of the pixel.

Geomosaic

Geomosaic allows you to stitch smaller images into a large image.

Band Stacking

Band stacking takes more than one image band as input and stacks them into a single GeoTIFF.

The OutputResolution attribute determines the resolution of the output image. Based on the

resolutions of the input images, you can set it to lowest, highest or average.

Band Math

Band Math, also known as Spectral Index, is a process of transforming the observations from
multiple spectral bands to a single band, indicating the relative abundance of features of interests.
For instance, Normalized Diﬀerence Vegetation Index (NDVI) and Enhanced Vegetation Index (EVI)
are helpful for observing the presence of green vegetation features.

Land Cover Segmentation

Land Cover segmentation is a semantic segmentation model that has the capability to identify
the physical material, such as vegetation, water, and bare ground, at the earth surface. Having an
accurate way to map the land cover patterns helps you understand the impact of environmental
change and human activities over time. Land Cover segmentation is often used for region planning,
disaster response, ecological management, and environmental impact assessment. The following is
the class map for Land Cover segmentation.

{
0: "No_data",
1: "Saturated_or_defective",

Earth Observation Jobs
1413

## Page 443

Amazon SageMaker AI
Developer Guide

2: "Dark_area_pixels",
3: "Cloud_shadows",
4: "Vegetation",
5: "Not_vegetated",
6: "Water",
7: "Unclassified",
8: "Cloud_medium_probability",
9: "Cloud_high_probability",
10: "Thin_cirrus",
11: "Snow_ice"
}

Availability of EOJ Operations

The availability of operations depends on whether you are using the SageMaker geospatial UI or
the Amazon SageMaker Studio Classic notebooks with a SageMaker geospatial image. Currently,
notebooks support all functionalities. To summarize, the following geospatial operations are
supported by SageMaker AI:

Operations
Description
Availability

Cloud Masking
Identify cloud and cloud-fre
e pixels to get improved and
accurate satellite imagery.

UI, Notebook

Cloud Removal
Remove pixels containing
parts of a cloud from satellite
imagery.

Notebook

Temporal Statistics
Calculate statistics through
time for a given GeoTIFF.

Notebook

Zonal Statistics
Calculate statistics on user-
deﬁned regions.

Notebook

Resampling
Scale images to diﬀerent
resolutions.

Notebook

Geomosaic
Combine multiple images for
greater ﬁdelity.

Notebook

Earth Observation Jobs
1414

## Page 444

Amazon SageMaker AI
Developer Guide

Operations
Description
Availability

Band Stacking
Combine multiple spectral
bands to create a single
image.

Notebook

Band Math / Spectral Index
Obtain a combination of
spectral bands that indicate
the abundance of features of
interest.

UI, Notebook

Land Cover Segmentation
Identify land cover types such
as vegetation and water in
satellite imagery.

UI, Notebook

Vector Enrichment Jobs

A Vector Enrichment Job (VEJ) performs operations on your vector data. Currently, you can use a
VEJ to do reverse geocoding or map matching.

Reverse Geocoding

With a reverse geocoding VEJ, you can convert geographic coordinates (latitude, longitude) to
human-readable addresses powered by Amazon Location Service. When you upload a CSV ﬁle
containing the longitude and latitude coordinates, a it returns the address number, country, label,
municipality, neighborhood, postal code and region of that location. The output ﬁle consists of
your input data along with columns containing these the values appended at the end. These jobs
are optimized to accept tens of thousands of GPS traces.

Map Matching

Map matching allows you to snap GPS coordinates to road segments. The input should be a CSV
ﬁle containing the trace ID (route), longitude, latitude and the timestamp attributes. There can
be multiple GPS co-ordinates per route. The input can contain multiple routes too. The output
is a GeoJSON ﬁle that contains links of the predicted route. It also has the snap points provided
in the input. These jobs are optimized to accept tens of thousands of drives in one request. Map
matching is supported by OpenStreetMap. Map matching fails if the names in the input source

ﬁeld don't match the ones in MapMatchingConfig. The error message you receive contains

Vector Enrichment Jobs
1415

## Page 445

Amazon SageMaker AI
Developer Guide

the the ﬁeld names present in the input ﬁle and the expected ﬁeld name that is not found in

MapMatchingConfig.

The input CSV ﬁle for a VEJ must contain the following:

• A header row

• Latitude and longitude in separate columns

• The ID and Timestamp columns can be in numeric or string format. All other column data must
be in numeric format only

• No miss matching quotes

For the timestamp column, SageMaker geospatial capabilities supports epoch time in seconds and
milliseconds (long integer). The string formats supported are as follows:

• "dd.MM.yyyy HH:mm:ss z"

• "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"

• "yyyy-MM-dd'T'HH:mm:ss"

• "yyyy-MM-dd hh:mm:ss a"

• "yyyy-MM-dd HH:mm:ss"

• "yyyyMMddHHmmss"

While you need to use an Amazon SageMaker Studio Classic notebook to execute a VEJ, you can
view all the jobs you create using the UI. To use the visualization in the notebook, you ﬁrst need to
export your output to your S3 bucket. The VEJ actions you can perform are as follows.

• StartVectorEnrichmentJob

• GetVectorEnrichmentJob

• ListVectorEnrichmentJobs

• StopVectorEnrichmentJob

• DeleteVectorEnrichmentJob

Visualization Using SageMaker geospatial capabilities

Using the visualization functionalities provided by Amazon SageMaker geospatial you can visualize
geospatial data, the inputs to your EOJ or VEJ jobs as well as the outputs exported from your

Visualization Using SageMaker geospatial capabilities
1416

## Page 446

Amazon SageMaker AI
Developer Guide

Amazon S3 bucket. The visualization tool is powered by Foursquare Studio. The following image
depicts the visualization tool supported by SageMaker geospatial capabilities.

![Page 446 Diagram 1](images/page-0446-img-01.png)

You can use the left navigation panel to add data, layers, ﬁlters, and columns. You can also make
modiﬁcations to how you interact with the map.

Dataset

The source of data used for visualization is called a Dataset. To add data for visualization, choose
Add Data in the left navigation panel. You can either upload the data from your Amazon S3 bucket
or your local machine. The data formats supported are CSV, JSON and GeoJSON. You can add
multiple datasets to your map. After you upload the dataset, you can see it loaded on the map
screen.

Layers

In the layer panel, a layer is created and populated automatically when you add a dataset. If your
map consists of more than one dataset, you can select which dataset belongs to a layer. You can
create new layers and group them. SageMaker SageMaker geospatial capabilities support various
layer types, including point, arc, icon, and polygon.

You can choose any data point in a layer to have an Outline. You can also further customize the
data points. For example, you can choose the layer type as Point and then Fill Color based on any
column of your dataset. You can also change the radius of the points.

Visualization Using SageMaker geospatial capabilities
1417

## Page 447

Amazon SageMaker AI
Developer Guide

The following image shows the layers panel supported by SageMaker geospatial capabilities.

![Page 447 Diagram 1](images/page-0447-img-01.png)

Columns

You can view the columns present in your dataset by using the Columns tab in the left navigation
panel.

Filters

You can use ﬁlters to limit the data points that display on the map.

Interactions

In the Interactions panel, you can customize how you interact with the map. For example, you can
choose what metrics to display when you hover the tooltip over a data point.

Base map

Currently, SageMaker AI only supports the Amazon Dark base map.

Split Map Modes

Visualization Using SageMaker geospatial capabilities
1418

## Page 448

Amazon SageMaker AI
Developer Guide

You can have a Single Map, Dual Maps or Swipe Maps. With Dual Maps, you can compare the
same map side-by-side using diﬀerent layers. Use Swipe Maps to overlay two maps on each other
and use the sliding separator to compare them. You can choose the split map mode by choosing
the Split Mode button on the top right corner of your map.

Legends for EOJ in the SageMaker geospatial UI

The output visualization of an EOJ depends on the operation you choose to create it. The legend is
based on the default color scale. You can view the legend by choosing the Show legend button on
the top right corner of your map.

Spectral Index

When you visualize the output for an EOJ that uses the spectral index operation, you can map the
category based on the color from the legend as shown.

![Page 448 Diagram 1](images/page-0448-img-01.png)

Cloud Masking

When you visualize the output for an EOJ that uses the cloud masking operation, you can map the
category based on the color from the legend as shown.

Visualization Using SageMaker geospatial capabilities
1419

## Page 449

Amazon SageMaker AI
Developer Guide

Land Cover Segmentation

When you visualize the output for an EOJ that uses the Land Cover Segmentation operation, you
can map the category based on the color from the legend as shown.

![Page 449 Diagram 1](images/page-0449-img-01.png)

Amazon SageMaker geospatial Map SDK

You can use Amazon SageMaker geospatial capabilities to visualize maps within the SageMaker
geospatial UI as well as SageMaker notebooks with a geospatial image. These visualizations are
supported by the map visualization library called Foursquare Studio

You can use the APIs provided by the SageMaker geospatial map SDK to visualize your geospatial
data, including the input, output, and AoI for EOJ.

Topics

Amazon SageMaker geospatial Map SDK
1420

## Page 450

Amazon SageMaker AI
Developer Guide

• add_dataset API

• update_dataset API

• add_layer API

• update_layer API

• visualize_eoj_aoi API

• visualize_eoj_input API

• visualize_eoj_output API

add_dataset API

Adds a raster or vector dataset object to the map.

Request syntax

Request =
add_dataset(
self,
dataset: Union[Dataset, Dict, None] = None,
*,
auto_create_layers: bool = True,
center_map: bool = True,
**kwargs: Any,
) -> Optional[Dataset]

Request parameters

The request accepts the following parameters.

Positional arguments

Argument
Type
Description

dataset
Union[Dataset, Dict, None]
Data used to create a dataset,
in CSV, JSON, or GeoJSON
format (for local datasets) or
a UUID string.

Amazon SageMaker geospatial Map SDK
1421

## Page 451

Amazon SageMaker AI
Developer Guide

Keyword arguments

Argument
Type
Description

auto_create_layers
Boolean
Whether to attempt to create
new layers when adding
a dataset. Default value is

False.

center_map
Boolean
Whether to center the map
on the created dataset.

Default value is True.

id
String
Unique identiﬁer of the
dataset. If you do not provide
it, a random ID is generated.

label
String
Dataset label which is
displayed.

color
Tuple[ﬂoat, ﬂoat, ﬂoat]
Color label of the dataset.

metadata
Dictionary
Object containing tileset
metadata (for tiled datasets).

Response

This API returns the Dataset object that was added to the map.

update_dataset API

Updates an existing dataset's settings.

Request syntax

Request =
update_dataset(
self,
dataset_id: str,
values: Union[_DatasetUpdateProps, dict, None] = None,

Amazon SageMaker geospatial Map SDK
1422

## Page 452

Amazon SageMaker AI
Developer Guide

**kwargs: Any,
) -> Dataset

Request parameters

The request accepts the following parameters.

Positional arguments

Argument
Type
Description

dataset_id
String
The identiﬁer of the dataset
to be updated.

values
Union[_DatasetUpdateProps,
dict, None]

The values to update.

Keyword arguments

Argument
Type
Description

label
String
Dataset label which is
displayed.

color
RGBColor
Color label of the dataset.

Response

This API returns the updated dataset object for interactive maps, or None for non-interactive HTML
environments.

add_layer API

Adds a new layer to the map. This function requires at least one valid layer conﬁguration.

Request syntax

Request =

Amazon SageMaker geospatial Map SDK
1423

## Page 453

Amazon SageMaker AI
Developer Guide

add_layer(
self,
layer: Union[LayerCreationProps, dict, None] = None,
**kwargs: Any
) -> Layer

Request parameters

The request accepts the following parameters.

Arguments

Argument
Type
Description

layer
Union[LayerCreationProps,
dict, None]

A set of properties used to
create a layer.

Response

The layer object that was added to the map.

update_layer API

Update an existing layer with given values.

Request syntax

Request =
update_layer(
self,
layer_id: str,
values: Union[LayerUpdateProps, dict, None],
**kwargs: Any
) -> Layer

Request parameters

The request accepts the following parameters.

Arguments

Amazon SageMaker geospatial Map SDK
1424

## Page 454

Amazon SageMaker AI
Developer Guide

Positional argument
Type
Description

layer_id
String
The ID of the layer to be
updated.

values
Union[LayerUpdateProps,
dict, None]

The values to update.

Keyword arguments

Argument
Type
Description

type
LayerType
The type of layer.

data_id
String
Unique identiﬁer of the
dataset this layer visualizes.

fields
Dict [string, Optional[string]]
Dictionary that maps ﬁelds
that the layer requires for
visualization to appropriate
dataset ﬁelds.

label
String
Canonical label of this layer.

is_visible
Boolean
Whether the layer is visible or
not.

config
LayerConfig
Layer conﬁguration speciﬁc to
its type.

Response

Returns the updated layer object.

visualize_eoj_aoi API

Visualize the AoI of the given job ARN.

Amazon SageMaker geospatial Map SDK
1425

## Page 455

Amazon SageMaker AI
Developer Guide

Request parameters

The request accepts the following parameters.

Arguments

Argument
Type
Description

Arn
String
The ARN of the job.

config
Dictionary

An option to pass layer
properties.

conﬁg = { label: <string>
custom label of the added AoI
layer, default AoI }

Response

Reference of the added input layer object.

visualize_eoj_input API

Visualize the input of the given EOJ ARN.

Request parameters

The request accepts the following parameters.

Arguments

Argument
Type
Description

Arn
String
The ARN of the job.

time_range_filter
Dictionary

An option to provide the start
and end time. Defaults to the
raster data collection search
start and end date.

time_range_ﬁlter = {

start_date: <string> date in
ISO format

Amazon SageMaker geospatial Map SDK
1426

## Page 456

Amazon SageMaker AI
Developer Guide

Argument
Type
Description

end_date: <string> date in
ISO format

}

config
Dictionary

An option to pass layer
properties.

conﬁg = { label: <string>
custom label of the added
output layer, default Input }

Response

Reference of the added input layer object.

visualize_eoj_output API

Visualize the output of the given EOJ ARN.

Request parameters

The request accepts the following parameters.

Arguments

Argument
Type
Description

Arn
String
The ARN of the job.

time_range_filter
Dictionary

An option to provide the start
and end time. Defaults to the
raster data collection search
start and end date.

time_range_ﬁlter = {

start_date: <string> date in
ISO format

end_date: <string> date in
ISO format

Amazon SageMaker geospatial Map SDK
1427

## Page 457

Amazon SageMaker AI
Developer Guide

Argument
Type
Description

}

config
Dictionary

An option to pass layer
properties.

conﬁg = {

label: <string> custom label
of the added output layer,
default Output

preset: <string> singleBand or
trueColor,

band_name: <string>, only
required for 'singleBand'
preset. Allowed bands for a
EOJ

}

Response

Reference of the added output Layer object.

To learn more about visualizing your geospatial data, refer to Visualization Using Amazon
SageMaker geospatial.

SageMaker geospatial capabilities FAQ

Use the following FAQ items to ﬁnd answers to commonly asked questions about SageMaker
geospatial capabilities.

1. What regions are Amazon SageMaker geospatial capabilities available in?

Currently, SageMaker geospatial capabilities are only supported in the US West (Oregon)
Region. To view SageMaker geospatial, choose the name of the currently displayed Region in the
navigation bar of the console. Then choose the US West (Oregon) Region.

SageMaker geospatial capabilities FAQ
1428

## Page 458

Amazon SageMaker AI
Developer Guide

2. What AWS Identity and Access Management permissions and policies are required to use

SageMaker geospatial?

To use SageMaker geospatial you need a user, group, or role that can access SageMaker AI. You
also need to create a SageMaker AI execution role so that SageMaker geospatial can perform
operations on your behalf. To learn more, see SageMaker geospatial capabilities roles.

3. I have an existing SageMaker AI execution role. Do I need to update it?

Yes. To use SageMaker geospatial you must specify an additional service principal in your IAM

trust policy: sagemaker-geospatial.amazonaws.com. To learn about specifying a service
principal in a trust relationship, see Adding the SageMaker geospatial service principal to an
existing SageMaker AI execution role in the Amazon SageMaker AI Developer Guide.

4. Can I use SageMaker geospatial capabilities through my VPC environment?

Yes, you can use SageMaker geospatial through a VPN. To learn more, see Use Amazon
SageMaker geospatial capabilities in Your Amazon Virtual Private Cloud.

5. Why can't I see the SageMaker geospatial map visualizer, image or instance type when I

navigate to Amazon SageMaker Studio Classic?

Verify that you are launching Amazon SageMaker Studio Classic in the US West (Oregon) Region
and that you are not using a shared space.

6. Why can't I see the SageMaker geospatial image or instance type when I try to create a

notebook instance in Studio Classic?

Verify that you are launching Amazon SageMaker Studio Classic in the US West (Oregon) Region
and that you are not using a shared space. To learn more, see Create an Amazon SageMaker
Studio Classic notebook using the geospatial image.

7. What bands supported for various raster data collections?

Use the GetRasterDataCollection API response and refer to the ImageSourceBands ﬁeld
to ﬁnd the bands supported for that particular data collection.

SageMaker geospatial Security and Permissions

Use the topics on this page to learn about SageMaker geospatial capabilities security features.
Additionally, learn how to use SageMaker geospatial capabilities in an Amazon Virtual Private
Cloud as well as protect your data at rest using encryption.

Security and Permissions
1429

## Page 459

Amazon SageMaker AI
Developer Guide

For more information about IAM users and roles, see Identities (Users, Groups, and Roles) in the
IAM User Guide.

To learn more about using IAM with SageMaker AI, see AWS Identity and Access Management for
Amazon SageMaker AI.

Topics

• Conﬁguration and Vulnerability Analysis in SageMaker geospatial

• Security Best Practices for SageMaker geospatial capabilities

• Use Amazon SageMaker geospatial capabilities in Your Amazon Virtual Private Cloud

• Use AWS KMS Permissions for Amazon SageMaker geospatial capabilities

Conﬁguration and Vulnerability Analysis in SageMaker geospatial

Conﬁguration and IT controls are a shared responsibility between AWS and you, our customer.
AWS handles basic security tasks like guest operating system (OS) and database patching, ﬁrewall
conﬁguration, and disaster recovery. These procedures have been reviewed and certiﬁed by the
appropriate third parties. For more details, see the following resources:

• Shared Responsibility Model.

• Amazon Web Services: Overview of Security Processes.

Security Best Practices for SageMaker geospatial capabilities

Amazon SageMaker geospatial capabilities provide a number of security features to consider as
you develop and implement your own security policies. The following best practices are general
guidelines and don't represent a complete security solution. Because these best practices might not
be appropriate or suﬃcient for your environment, treat them as helpful considerations rather than
prescriptions.

Apply principle of least privilege

Amazon SageMaker geospatial capabilities provide granular access policy for applications using
IAM roles. We recommend that the roles be granted only the minimum set of privileges required
by the job. We also recommend auditing the jobs for permissions on a regular basis and upon any
change to your application.

Role-based access control (RBAC) permissions

Security and Permissions
1430

## Page 460

Amazon SageMaker AI
Developer Guide

Administrators should strictly control Role-based access control (RBAC) permissions for Amazon
SageMaker geospatial capabilities.

Use temporary credentials whenever possible

Where possible, use temporary credentials instead of long-term credentials, such as access keys.
For scenarios in which you need IAM users with programmatic access and long-term credentials,
we recommend that you rotate access keys. Regularly rotating long-term credentials helps you
familiarize yourself with the process. This is useful in case you are ever in a situation where you
must rotate credentials, such as when an employee leaves your company. We recommend that
you use IAM access last used information to rotate and remove access keys safely. For more
information, see Rotating access keys and Security best practices in IAM.

Use AWS CloudTrail to view and log API calls

AWS CloudTrail tracks anyone making API calls in your AWS account. API calls are logged whenever
anyone uses the Amazon SageMaker geospatial capabilities API, the Amazon SageMaker geospatial
capabilities console or Amazon SageMaker geospatial capabilities AWS CLI commands. Enable
logging and specify an Amazon S3 bucket to store the logs.

Your trust, privacy, and the security of your content are our highest priorities. We implement
responsible and sophisticated technical and physical controls designed to prevent unauthorized
access to, or disclosure of, your content and ensure that our use complies with our commitments to
you. For more information, see AWS Data Privacy FAQ.

Use Amazon SageMaker geospatial capabilities in Your Amazon Virtual Private
Cloud

The following topic gives information on how to use SageMaker notebooks with a SageMaker
geospatial image in a Amazon SageMaker AI domain with VPC only mode. For more information on
VPCs in Amazon SageMaker Studio Classic see Choose an Amazon VPC.

VPC only communication with the internet

By default, SageMaker AI domain uses two Amazon VPC. One of the Amazon VPC is managed
by Amazon SageMaker AI and provides direct internet access. You specify the other Amazon
VPC, which provides encrypted traﬃc between the domain and your Amazon Elastic File System
(Amazon EFS) volume.

You can change this behavior so that SageMaker AI sends all traﬃc over your speciﬁed Amazon

VPC. If VPC only has been choosen as the network access mode during the SageMaker AI domain

Security and Permissions
1431

## Page 461

Amazon SageMaker AI
Developer Guide

creation, the following requirements need to be considered to still allow usage of SageMaker
Studio Classic notebooks within the created SageMaker AI domain.

Requirements to use VPC only mode

Note

In order to use the visualization components of SageMaker geospatial capabilities, the
browser you use to access the SageMaker Studio Classic UI needs to be connected to the
internet.

When you choose VpcOnly, follow these steps:

1.
You must use private subnets only. You cannot use public subnets in VpcOnly mode.

2.
Ensure your subnets have the required number of IP addresses needed. The expected number
of IP addresses needed per user can vary based on use case. We recommend between 2 and 4
IP addresses per user. The total IP address capacity for a Studio Classic domain is the sum of
available IP addresses for each subnet provided when the domain is created. Ensure that your
estimated IP address usage does not exceed the capacity supported by the number of subnets
you provide. Additionally, using subnets distributed across many availability zones can aid in IP
address availability. For more information, see VPC and subnet sizing for IPv4.

Note

You can conﬁgure only subnets with a default tenancy VPC in which your instance
runs on shared hardware. For more information on the tenancy attribute for VPCs, see
Dedicated Instances.

3.
Set up one or more security groups with inbound and outbound rules that together allow the
following traﬃc:

• NFS traﬃc over TCP on port 2049 between the domain and the Amazon EFS volume.

• TCP traﬃc within the security group. This is required for connectivity between the
JupyterServer app and the KernelGateway apps. You must allow access to at least ports in

the range 8192-65535.

4.
If you want to allow internet access, you must use a NAT gateway with access to the internet,
for example through an internet gateway.

Security and Permissions
1432

## Page 462

Amazon SageMaker AI
Developer Guide

5.
If you don't want to allow internet access, create interface VPC endpoints (AWS PrivateLink) to
allow Studio Classic to access the following services with the corresponding service names. You
must also associate the security groups for your VPC with these endpoints.

Note

Currently, SageMaker geospatial capabilities are only supported in the US West
(Oregon) Region.

• SageMaker API : com.amazonaws.us-west-2.sagemaker.api

• SageMaker AI runtime: com.amazonaws.us-west-2.sagemaker.runtime. This is
required to run Studio Classic notebooks with a SageMaker geospatial image.

• Amazon S3: com.amazonaws.us-west-2.s3.

• To use SageMaker Projects: com.amazonaws.us-west-2.servicecatalog.

• SageMaker geospatial capabilities: com.amazonaws.us-west-2.sagemaker-

geospatial

If you use the SageMaker Python SDK to run remote training jobs, you must also create the
following Amazon VPC endpoints.

• AWS Security Token Service: com.amazonaws.region.sts

• Amazon CloudWatch: com.amazonaws.region.logs. This is required to allow SageMaker
Python SDK to get the remote training job status from Amazon CloudWatch.

Note

For a customer working within VPC mode, company ﬁrewalls can cause connection issues
with SageMaker Studio Classic or between JupyterServer and the KernelGateway. Make the
following checks if you encounter one of these issues when using SageMaker Studio Classic
from behind a ﬁrewall.

• Check that the Studio Classic URL is in your networks allowlist.

• Check that the websocket connections are not blocked. Jupyter uses websocket under
the hood. If the KernelGateway application is InService, JupyterServer may not be able

Security and Permissions
1433

## Page 463

Amazon SageMaker AI
Developer Guide

to connect to the KernelGateway. You should see this problem when opening System
Terminal as well.

Use AWS KMS Permissions for Amazon SageMaker geospatial capabilities

You can protect your data at rest using encryption for SageMaker geospatial capabilities. By
default, it uses server-side encryption with an Amazon SageMaker geospatial owned key.
SageMaker geospatial capabilities also supports an option for server-side encryption with a
customer managed KMS key.

Server-Side Encryption with Amazon SageMaker geospatial managed key (Default)

SageMaker geospatial capabilities encrypts all your data, including computational results from
your Earth Observation jobs (EOJ) and Vector Enrichment jobs (VEJ) along with all your service
metadata. There is no data that is stored within SageMaker geospatial capabilities unencrypted. It
uses a default AWS owned key to encrypt all your data.

Server-Side Encryption with customer managed KMS key (Optional)

SageMaker geospatial capabilities supports the use of a symmetric customer managed key that
you create, own, and manage to add a second layer of encryption over the existing AWS owned
encryption. Because you have full control of this layer of encryption, you can perform such tasks as:

• Establishing and maintaining key policies

• Establishing and maintaining IAM policies and grants

• Enabling and disabling key policies

• Rotating key cryptographic material

• Adding tags

• Creating key aliases

• Scheduling keys for deletion

For more information, see Customer managed keys in the AWS Key Management Service Developer
Guide.

Security and Permissions
1434

## Page 464

Amazon SageMaker AI
Developer Guide

How SageMaker geospatial capabilities uses grants in AWS KMS

SageMaker geospatial capabilities requires a grant to use your customer managed key. When
you create an EOJ or an VEJ encrypted with a customer managed key, SageMaker geospatial

capabilities creates a grant on your behalf by sending a CreateGrant request to AWS KMS.

Grants in AWS KMS are used to give SageMaker geospatial capabilities access to a KMS key in
a customer account. You can revoke access to the grant, or remove the service's access to the
customer managed key at any time. If you do, SageMaker geospatial capabilities won't be able to
access any of the data encrypted by the customer managed key, which aﬀects operations that are
dependent on that data.

Create a customer managed key

You can create a symmetric customer managed key by using the AWS Management Console, or the
AWS KMS APIs.

To create a symmetric customer managed key

Follow the steps for Creating symmetric encryption KMS keys in the AWS Key Management Service
Developer Guide.

Key policy

Key policies control access to your customer managed key. Every customer managed key must have
exactly one key policy, which contains statements that determine who can use the key and how
they can use it. When you create your customer managed key, you can specify a key policy. For
more information, see Determining access to AWS KMS keys in the AWS Key Management Service
Developer Guide.

To use your customer managed key with your SageMaker geospatial capabilities resources, the
following API operations must be permitted in the key policy. The principal for these operations
should be the Execution Role you provide in the SageMaker geospatial capabilities request.
SageMaker geospatial capabilities assumes the provided Execution Role in the request to perform
these KMS operations.

• kms:CreateGrant

• kms:GenerateDataKey

• kms:Decrypt

• kms:GenerateDataKeyWithoutPlaintext

Security and Permissions
1435

## Page 465

Amazon SageMaker AI
Developer Guide

The following are policy statement examples you can add for SageMaker geospatial capabilities:

CreateGrant

"Statement" : [

{
"Sid" : "Allow access to Amazon SageMaker geospatial capabilities",
"Effect" : "Allow",
"Principal" : {
"AWS" : "<Customer provided Execution Role ARN>"
},
"Action" : [
"kms:CreateGrant",
"kms:Decrypt",
"kms:GenerateDataKey",
"kms:GenerateDataKeyWithoutPlaintext"
],
"Resource" : "*",
},
]

For more information about specifying permissions in a policy, see AWS KMS permissions in the
AWS Key Management Service Developer Guide. For more information about troubleshooting, see
Troubleshooting key access in the AWS Key Management Service Developer Guide.

If your key policy does not have your account root as key administrator, you need to add the same
KMS permissions on your execution role ARN. Here is a sample policy you can add to the execution
role:

JSON

{
"Version":"2012-10-17",
"Statement": [
{
"Action": [
"kms:CreateGrant",
"kms:Decrypt",
"kms:GenerateDataKey",
"kms:GenerateDataKeyWithoutPlaintext"
],
"Resource": [

Security and Permissions
1436

## Page 466

Amazon SageMaker AI
Developer Guide

"arn:aws:kms:us-east-1:111122223333:key/key-id"
],
"Effect": "Allow"
}
]
}

Monitoring your encryption keys for SageMaker geospatial capabilities

When you use an AWS KMS customer managed key with your SageMaker geospatial capabilities
resources, you can use AWS CloudTrail or Amazon CloudWatch Logs to track requests that
SageMaker geospatial sends to AWS KMS.

Select a tab in the following table to see examples of AWS CloudTrail events to monitor KMS
operations called by SageMaker geospatial capabilities to access data encrypted by your customer

managed key.

CreateGrant

{
"eventVersion": "1.08",
"userIdentity": {
"type": "AssumedRole",
"principalId": "AROAIGDTESTANDEXAMPLE:SageMaker-Geospatial-StartEOJ-
KMSAccess",
"arn": "arn:aws:sts::111122223333:assumed-role/
SageMakerGeospatialCustomerRole/SageMaker-Geospatial-StartEOJ-KMSAccess",
"accountId": "111122223333",
"accessKeyId": "AKIAIOSFODNN7EXAMPLE3",
"sessionContext": {
"sessionIssuer": {
"type": "Role",
"principalId": "AKIAIOSFODNN7EXAMPLE3",
"arn": "arn:aws:sts::111122223333:assumed-role/
SageMakerGeospatialCustomerRole",
"accountId": "111122223333",
"userName": "SageMakerGeospatialCustomerRole"
},
"webIdFederationData": {},
"attributes": {
"creationDate": "2023-03-17T18:02:06Z",
"mfaAuthenticated": "false"

Security and Permissions
1437

## Page 467

Amazon SageMaker AI
Developer Guide

}
},
"invokedBy": "arn:aws:iam::111122223333:root"
},
"eventTime": "2023-03-17T18:02:06Z",
"eventSource": "kms.amazonaws.com",
"eventName": "CreateGrant",
"awsRegion": "us-west-2",
"sourceIPAddress": "172.12.34.56",
"userAgent": "ExampleDesktop/1.0 (V1; OS)",
"requestParameters": {
"retiringPrincipal": "sagemaker-geospatial.us-west-2.amazonaws.com",
"keyId": "arn:aws:kms:us-
west-2:111122223333:key/1234abcd-12ab-34cd-56ef-123456SAMPLE",
"operations": [
"Decrypt"
],

"granteePrincipal": "sagemaker-geospatial.us-west-2.amazonaws.com"
},
"responseElements": {
"grantId":
"0ab0ac0d0b000f00ea00cc0a0e00fc00bce000c000f0000000c0bc0a0000aaafSAMPLE",
"keyId": "arn:aws:kms:us-
west-2:111122223333:key/1234abcd-12ab-34cd-56ef-123456SAMPLE"
},
"requestID": "ff000af-00eb-00ce-0e00-ea000fb0fba0SAMPLE",
"eventID": "ff000af-00eb-00ce-0e00-ea000fb0fba0SAMPLE",
"readOnly": false,
"resources": [
{
"accountId": "111122223333",
"type": "AWS::KMS::Key",
"ARN": "arn:aws:kms:us-
west-2:111122223333:key/1234abcd-12ab-34cd-56ef-123456SAMPLE"
}
],
"eventType": "AwsApiCall",
"managementEvent": true,
"recipientAccountId": "111122223333",
"eventCategory": "Management"
}

Security and Permissions
1438

## Page 468

Amazon SageMaker AI
Developer Guide

GenerateDataKey

{
"eventVersion": "1.08",
"userIdentity": {
"type": "AWSService",
"invokedBy": "sagemaker-geospatial.amazonaws.com"
},
"eventTime": "2023-03-24T00:29:45Z",
"eventSource": "kms.amazonaws.com",
"eventName": "GenerateDataKey",
"awsRegion": "us-west-2",
"sourceIPAddress": "sagemaker-geospatial.amazonaws.com",
"userAgent": "sagemaker-geospatial.amazonaws.com",
"requestParameters": {
"encryptionContext": {

"aws:s3:arn": "arn:aws:s3:::axis-earth-observation-
job-378778860802/111122223333/napy9eintp64/output/
consolidated/32PPR/2022-01-04T09:58:03Z/S2B_32PPR_20220104_0_L2A_msavi.tif"
},
"keyId": "arn:aws:kms:us-
west-2:111122223333:key/1234abcd-12ab-34cd-56ef-123456SAMPLE",
"keySpec": "AES_256"
},
"responseElements": null,
"requestID": "ff000af-00eb-00ce-0e00-ea000fb0fba0SAMPLE",
"eventID": "ff000af-00eb-00ce-0e00-ea000fb0fba0SAMPLE",
"readOnly": true,
"resources": [
{
"accountId": "111122223333",
"type": "AWS::KMS::Key",
"ARN": "arn:aws:kms:us-
west-2:111122223333:key/1234abcd-12ab-34cd-56ef-123456SAMPLE"
}
],
"eventType": "AwsApiCall",
"managementEvent": true,
"recipientAccountId": "111122223333",
"eventCategory": "Management"
}

Security and Permissions
1439

## Page 469

Amazon SageMaker AI
Developer Guide

Decrypt

{
"eventVersion": "1.08",
"userIdentity": {

"type": "AWSService",
"invokedBy": "sagemaker-geospatial.amazonaws.com"
},
"eventTime": "2023-03-28T22:04:24Z",
"eventSource": "kms.amazonaws.com",
"eventName": "Decrypt",
"awsRegion": "us-west-2",
"sourceIPAddress": "sagemaker-geospatial.amazonaws.com",
"userAgent": "sagemaker-geospatial.amazonaws.com",
"requestParameters": {
"encryptionAlgorithm": "SYMMETRIC_DEFAULT",
"encryptionContext": {
"aws:s3:arn": "arn:aws:s3:::axis-earth-observation-
job-378778860802/111122223333/napy9eintp64/output/
consolidated/32PPR/2022-01-04T09:58:03Z/S2B_32PPR_20220104_0_L2A_msavi.tif"
},
},
"responseElements": null,
"requestID": "ff000af-00eb-00ce-0e00-ea000fb0fba0SAMPLE",
"eventID": "ff000af-00eb-00ce-0e00-ea000fb0fba0SAMPLE",
"readOnly": true,
"resources": [
{
"accountId": "111122223333",
"type": "AWS::KMS::Key",
"ARN": "arn:aws:kms:us-
west-2:111122223333:key/1234abcd-12ab-34cd-56ef-123456SAMPLE"
}
],
"eventType": "AwsApiCall",
"managementEvent": true,
"recipientAccountId": "111122223333",
"eventCategory": "Management"
}

GenerateDataKeyWithoutPlainText

{

Security and Permissions
1440

## Page 470

Amazon SageMaker AI
Developer Guide

"eventVersion": "1.08",
"userIdentity": {
"type": "AssumedRole",
"principalId": "AROAIGDTESTANDEXAMPLE:SageMaker-Geospatial-StartEOJ-
KMSAccess",
"arn": "arn:aws:sts::111122223333:assumed-role/
SageMakerGeospatialCustomerRole/SageMaker-Geospatial-StartEOJ-KMSAccess",
"accountId": "111122223333",
"accessKeyId": "AKIAIOSFODNN7EXAMPLE3",
"sessionContext": {
"sessionIssuer": {
"type": "Role",
"principalId": "AKIAIOSFODNN7EXAMPLE3",
"arn": "arn:aws:sts::111122223333:assumed-role/
SageMakerGeospatialCustomerRole",
"accountId": "111122223333",
"userName": "SageMakerGeospatialCustomerRole"

},
"webIdFederationData": {},
"attributes": {
"creationDate": "2023-03-17T18:02:06Z",
"mfaAuthenticated": "false"
}
},
"invokedBy": "arn:aws:iam::111122223333:root"
},
"eventTime": "2023-03-28T22:09:16Z",
"eventSource": "kms.amazonaws.com",
"eventName": "GenerateDataKeyWithoutPlaintext",
"awsRegion": "us-west-2",
"sourceIPAddress": "172.12.34.56",
"userAgent": "ExampleDesktop/1.0 (V1; OS)",
"requestParameters": {
"keySpec": "AES_256",
"keyId": "arn:aws:kms:us-
west-2:111122223333:key/1234abcd-12ab-34cd-56ef-123456SAMPLE"
},
"responseElements": null,
"requestID": "ff000af-00eb-00ce-0e00-ea000fb0fba0SAMPLE",
"eventID": "ff000af-00eb-00ce-0e00-ea000fb0fba0SAMPLE",
"readOnly": true,
"resources": [
{
"accountId": "111122223333",

Security and Permissions
1441

## Page 471

Amazon SageMaker AI
Developer Guide

"type": "AWS::KMS::Key",
"ARN": "arn:aws:kms:us-
west-2:111122223333:key/1234abcd-12ab-34cd-56ef-123456SAMPLE"
}
],
"eventType": "AwsApiCall",
"managementEvent": true,
"recipientAccountId": "111122223333",
"eventCategory": "Management"
}

Types of compute instances

SageMaker geospatial capabilities oﬀer three types of compute instances.

• SageMaker Studio Classic geospatial notebook instances – SageMaker geospatial supports
both CPU and GPU-based notebook instances in Studio Classic. Notebook instances are used to
build, train, and deploy ML models. For a list of available notebook instance types that work with
the geospatial image, see Supported notebook instance types.

• SageMaker geospatial jobs instances – Run processing jobs to transform satellite image data.

• SageMaker geospatial model inference types – Make predictions by using pre-trained ML
models on satellite imagery.

The instance type is determined by the operations that you run.

The following table shows the available SageMaker geospatial speciﬁc operations and instance
types that you can use.

Operations
Instance

Temporal Statistics
ml.geospatial.jobs

Zonal Statistics
ml.geospatial.jobs

Resampling
ml.geospatial.jobs

Geomosaic
ml.geospatial.jobs

Band Stacking
ml.geospatial.jobs

Types of compute instances
1442

## Page 472

Amazon SageMaker AI
Developer Guide

Operations
Instance

Band Math
ml.geospatial.jobs

Cloud Removal with Landsat8
ml.geospatial.jobs

Cloud Removal with Sentinel-2
ml.geospatial.models

Cloud Masking
ml.geospatial.models

Land Cover Segmentation
ml.geospatial.models

SageMaker geospatial supported notebook instance types

SageMaker geospatial supports both CPU and GPU-based notebook instances in Studio Classic. If

when starting a GPU enabled notebook instance you receive a ResourceLimitExceeded error, you
need to request a quota increase. To get started on a Service Quotas quota increase request, see
Requesting a quota increase in the Service Quotas User Guide.

Supported Studio Classic notebook instance types

Name
Instance type

ml.geospatial.interactive
CPU

ml.g5.xlarge
GPU

ml.g5.2xlarge
GPU

ml.g5.4xlarge
GPU

ml.g5.8xlarge
GPU

ml.g5.16xlarge
GPU

ml.g5.12xlarge
GPU

ml.g5.24xlarge
GPU

ml.g5.48xlarge
GPU

Types of compute instances
1443

## Page 473

Amazon SageMaker AI
Developer Guide

You are charged diﬀerent rates for each type of compute instance that you use. For more
information about pricing, see Geospatial ML with Amazon SageMaker AI.

SageMaker geospatial libraries

The SageMaker geospatial speciﬁc Instance type, ml.geospatial.interactive contains the
following Python libraries.

Geospatial libraries available on the geospatial instance type

Library name
Version available

numpy
1.23.4

scipy
1.11.2

pandas
1.4.4

gdal
3.2.2

ﬁona
1.8.22

geopandas
0.11.1

shapley
1.8.4

seaborn
0.11.2

notebook
1.8.22

scikit-image
0.11.2

rasterio
6.4.12

scikit-learn
0.19.2

ipyleaﬂet
1.0.1

rtree
0.17.2

opencv
4.6.0.66

Types of compute instances
1444

## Page 474

Amazon SageMaker AI
Developer Guide

Library name
Version available

supy
2022.4.7

SNAP toolbox
9.0

cdsapi
0.6.1

arosics
1.8.1

rasterstats
0.18.0

rioxarray
0.14.1

pyroSAR
0.20.0

eo-learn
1.4.1

deepforest
1.2.7

scrapy
2.8.0

netCDF4
1.6.3

xarray[complete]
0.20.1

Orfeotoolbox
OTB-8.1.1

pytorch
2.0.1

pytorch-cuda
11.8

torchvision
0.15.2

torchaudio
2.0.2

pytorch-lightning
2.0.6

tensorﬂow
2.13.0

Types of compute instances
1445

## Page 475

Amazon SageMaker AI
Developer Guide

Data collections

Amazon SageMaker geospatial supports the following raster data collections. Of the following
data collections, you can use the USGS Landsat and the Sentinel-2 Cloud-Optimized GeoTIFF data

collections when starting an Earth Observation Job (EOJ). To learn more about the EOJs, see Earth
Observation Jobs.

• Copernicus Digital Elevation Model (DEM) – GLO-30

• Copernicus Digital Elevation Model (DEM) – GLO-90

• Sentinel-2 Cloud-Optimized GeoTIFFs

• Sentinel-1

• National Agriculture Imagery Program (NAIP) on AWS

• USGS Landsat 8

To ﬁnd the list of available raster data collections in your AWS Regions, use

ListRasterDataCollections. In the ListRasterDataCollections response, you get a

RasterDataCollectionMetadata object that contains details about the available raster data
collections.

Example – Calling the ListRasterDataCollections API using the AWS SDK for Python (Boto3)

When you use the SDK for Python (Boto3) and SageMaker geospatial, you must create a

geospatial client, geospatial_client. Use the following Python snippet to make a call to the

list_raster_data_collections API:

import boto3
import sagemaker
import sagemaker_geospatial_map
import json

## SageMaker Geospatial Capabilities is currently only avaialable in US-WEST-2
session = boto3.Session(region_name='us-west-2')
execution_role = sagemaker.get_execution_role()

## Creates a SageMaker Geospatial client instance
geospatial_client = session.client(service_name="sagemaker-geospatial")

# Creates a resusable Paginator for the list_raster_data_collections API operation
paginator = geospatial_client.get_paginator("list_raster_data_collections")

Data collections
1446

## Page 476

Amazon SageMaker AI
Developer Guide

# Create a PageIterator from the Paginator
page_iterator = paginator.paginate()

# Use the iterator to iterate throught the results of list_raster_data_collections
results = []
for page in page_iterator:
results.append(page['RasterDataCollectionSummaries'])

print (results)

In the JSON response, you will receive the following, which has been truncated for clarity:

{
"Arn": "arn:aws:sagemaker-geospatial:us-west-2:555555555555:raster-data-collection/
public/dxxbpqwvu9041ny8",
"Description": "Copernicus DEM is a Digital Surface Model which represents the
surface of the Earth including buildings, infrastructure, and vegetation. GLO-30 is
instance of Copernicus DEM that provides limited worldwide coverage at 30 meters.",
"DescriptionPageUrl": "https://registry.opendata.aws/copernicus-dem/",
"Name": "Copernicus DEM GLO-30",
"Tags": {},
"Type": "PUBLIC"
}

Image band information from the USGS Landsat and Sentinel-2 data collections

Image band information from the USGS Landsat 8 and Sentinel-2 data collections are provided in
the following table.

USGS Landsat

Band name
Wave length
range (nm)

Units
Valid range
Fill value
Spatial
resolution

coastal
435 - 451
Unitless
1 - 65455
0 (No Data)
30m

blue
452 - 512
Unitless
1 - 65455
0 (No Data)
30m

green
533 - 590
Unitless
1 - 65455
0 (No Data)
30m

red
636 - 673
Unitless
1 - 65455
0 (No Data)
30m

Data collections
1447

## Page 477

Amazon SageMaker AI
Developer Guide

Band name
Wave length
range (nm)

Units
Valid range
Fill value
Spatial
resolution

nir
851 - 879
Unitless
1 - 65455
0 (No Data)
30m

swir16
1566 - 1651
Unitless
1 - 65455
0 (No Data)
30m

swir22
2107 - 2294
Unitless
1 - 65455
0 (No Data)
30m

qa_aerosol
NA
Bit Index
0 - 255
1
30m

qa_pixel
NA
Bit Index
1 - 65455
1 (bit 0)
30m

qa_radsat
NA
Bit Index
1 - 65455
NA
30m

t
10600 -
11190

Scaled Kelvin
1 - 65455
0 (No Data)
30m (scaled
from 100m)

atran
NA
Unitless
0 - 10000
-9999 (No
Data)

30m

cdist
NA
Kilometers
0 - 24000
-9999 (No
Data)

30m

drad
NA
W/(m^2 sr
µm)/DN

0 - 28000
-9999 (No
Data)

30m

urad
NA
W/(m^2 sr
µm)/DN

0 - 28000
-9999 (No
Data)

30m

trad
NA
W/(m^2 sr
µm)/DN

0 - 28000
-9999 (No
Data)

30m

emis
NA
Emissivity
coeﬃcient

1 - 10000
-9999 (No
Data)

30m

emsd
NA
Emissivity
coeﬃcient

1 - 10000
-9999 (No
Data)

30m

Sentinel-2

Data collections
1448

## Page 478

Amazon SageMaker AI
Developer Guide

Band name
Wave length
range (nm)

Scale
Valid range
Fill value
Spatial
resolution

coastal
443
0.0001
NA
0 (No Data)
60m

blue
490
0.0001
NA
0 (No Data)
10m

green
560
0.0001
NA
0 (No Data)
10m

red
665
0.0001
NA
0 (No Data)
10m

rededge1
705
0.0001
NA
0 (No Data)
20m

rededge2
740
0.0001
NA
0 (No Data)
20m

rededge3
783
0.0001
NA
0 (No Data)
20m

nir
842
0.0001
NA
0 (No Data)
10m

nir08
865
0.0001
NA
0 (No Data)
20m

nir08
865
0.0001
NA
0 (No Data)
20m

nir09
940
0.0001
NA
0 (No Data)
60m

swir16
1610
0.0001
NA
0 (No Data)
20m

swir22
2190
0.0001
NA
0 (No Data)
20m

aot
Aerosol
optical
thickness

0.001
NA
0 (No Data)
10m

wvp
Scene-ave
rage water
vapor

0.001
NA
0 (No Data)
10m

scl
Scene
classiﬁcation
data

NA
1 - 11
0 (No Data)
20m

Data collections
1449

## Page 479

Amazon SageMaker AI
Developer Guide

RStudio on Amazon SageMaker AI

RStudio is an integrated development environment for R, with a console, syntax-highlighting editor
that supports direct code execution, and tools for plotting, history, debugging and workspace
management. Amazon SageMaker AI supports RStudio as a fully-managed integrated development
environment (IDE) integrated with Amazon SageMaker AI domain through Posit Workbench.
RStudio allows customers to create data science insights using an R environment. With RStudio
integration, you can launch an RStudio environment in the domain to run your RStudio workﬂows
on SageMaker AI resources. For more information about Posit Workbench, see the Posit website.
This page gives information about important RStudio concepts.

SageMaker AI integrates RStudio through the creation of a RStudioServerPro app.

The following are supported by RStudio on SageMaker AI.

• R developers use the RStudio IDE interface with popular developer tools from the R ecosystem.
Users can launch new RStudio sessions, write R code, install dependencies from RStudio Package
Manager, and publish Shiny apps using RStudio Connect.

• R developers can quickly scale underlying compute resources to run large scale data processing
and statistical analysis.

• Platform administrators can set up user identities, authorization, networking, storage, and
security for their data science teams through AWS IAM Identity Center and AWS Identity and
Access Management integration. This includes connection to private Amazon Virtual Private
Cloud (Amazon VPC) resources and internet-free mode with AWS PrivateLink.

• Integration with AWS License Manager.

For information on the onboarding steps to create a domain with RStudio enabled, see Amazon
SageMaker AI domain overview.

Region availability

The following table gives information about the AWS Regions that RStudio on SageMaker AI is
supported in.

Region name
Region

US East (Ohio)
us-east-2

RStudio
1450

## Page 480

Amazon SageMaker AI
Developer Guide

Region name
Region

US East (N. Virginia)
us-east-1

US West (N. California)
us-west-1

US West (Oregon)
us-west-2

Asia Paciﬁc (Mumbai)
ap-south-1

Asia Paciﬁc (Seoul)
ap-northeast-2

Asia Paciﬁc (Singapore)
ap-southeast-1

Asia Paciﬁc (Sydney)
ap-southeast-2

Asia Paciﬁc (Tokyo)
ap-northeast-1

Canada (Central)
ca-central-1

Europe (Frankfurt)
eu-central-1

Europe (Ireland)
eu-west-1

Europe (London)
eu-west-2

Europe (Paris)
eu-west-3

Europe (Stockholm)
eu-north-1

South America (São Paulo)
sa-east-1

RStudio components

• RStudioServerPro: The RStudioServerPro app is a multiuser app that is a shared resource among
all user proﬁles in the domain. Once an RStudio app is created in a domain, the admin can give
permissions to users in the domain.

• RStudio user: RStudio users are users within the domain that are authorized to use the RStudio
license.

RStudio components
1451

## Page 481

Amazon SageMaker AI
Developer Guide

• RStudio admin: An RStudio on Amazon SageMaker AI admin can access the RStudio
administrative dashboard. RStudio on Amazon SageMaker AI admins diﬀer from "stock"
Posit Workbench admins because they do not have root access to the instance running the
RStudioServerPro app and can't modify the RStudio conﬁguration ﬁle.

• RStudio Server: The RStudio Server instance is responsible for serving the RStudio UI to all
authorized Users. This instance is launched on an Amazon SageMaker AI instance.

• RSession: An RSession is a browser-based interface to the RStudio IDE running on an Amazon
SageMaker AI instance. Users can create and interact with their RStudio projects through the
RSession.

• RSessionGateway: The RSessionGateway app is used to support an RSession.

• RStudio administrative dashboard: This dashboard gives information on the RStudio users in the
Amazon SageMaker AI domain and their sessions. This dashboard can only be accessed by users
that have RStudio admin authorization.

Diﬀerences from Posit Workbench

RStudio on Amazon SageMaker AI has some signiﬁcant diﬀerences from Posit Workbench.

• When using RStudio on SageMaker AI, users don’t have access to the RStudio conﬁguration
ﬁles. Amazon SageMaker AI manages the conﬁguration ﬁle and sets defaults. You can modify
the RStudio Connect and RStudio Package Manager URLs when creating your RStudio-enabled
Amazon SageMaker AI domain.

• Project sharing, realtime collaboration, and Job Launcher are not currently supported when
using RStudio on Amazon SageMaker AI.

• When using RStudio on SageMaker AI, the RStudio IDE runs on Amazon SageMaker AI instances
for on-demand containerized compute resources.

• RStudio on SageMaker AI only supports the RStudio IDE and does not support other IDEs
supported by a Posit Workbench installation.

• RStudio on SageMaker AI only supports the RStudio version speciﬁed in RStudio Versioning.

RStudio on Amazon SageMaker AI management

The following topics give information on managing RStudio on Amazon SageMaker AI. This
includes information about your RStudio environment conﬁguration, user sessions, and necessary

Diﬀerences from Posit Workbench
1452

## Page 482

Amazon SageMaker AI
Developer Guide

resources. For information on how to use RStudio on SageMaker AI, see RStudio on Amazon
SageMaker AI user guide.

For information about creating a Amazon SageMaker AI domain with RStudio enabled, see Amazon
SageMaker AI domain overview.

For information about the AWS Regions that RStudio on SageMaker AI is supported in,
see Supported Regions and Quotas.

Topics

• Get an RStudio license

• RStudio Versioning

• Network and Storage

• RStudioServerPro instance type

• Add an RStudio Connect URL

• Update the RStudio Package Manager URL

• Create an Amazon SageMaker AI domain with RStudio using the AWS CLI

• Add RStudio support to an existing domain

• Custom images with RStudio on SageMaker AI

• Create a user to use RStudio

• Log in to RStudio as another user

• Terminate sessions for another user

• Use the RStudio administrative dashboard

• Shut down RStudio

• Billing and cost

• Diagnose issues and get support

Get an RStudio license

RStudio on Amazon SageMaker AI is a paid product and requires that each user is appropriately
licensed. Licenses for RStudio on Amazon SageMaker AI may be obtained from RStudio PBC
directly, or by purchasing a subscription to Posit Workbench on AWS Marketplace. For existing
customers of Posit Workbench Enterprise, licenses are issued at no additional cost. To use an

RStudio on SageMaker AI management
1453

## Page 483

Amazon SageMaker AI
Developer Guide

RStudio license with Amazon SageMaker AI, you must ﬁrst have a valid RStudio license registered
with AWS License Manager. For licenses purchased directly through Rstudio PBC, a licenses grant
for your AWS Account must be created. Contact RStudio for direct license purchases or to enable
existing licenses in AWS License Manager. For more information about registering a license with
AWS License Manager, see Seller issued licenses in AWS License Manager.

The following topics show how to acquire and validate a license granted by RStudio PBC.

Get an RStudio license

1.
If you don't have an RStudio license, you may purchase one from the AWS Marketplace or from
RStudio PBC directly.

• To purchase a subscription from the AWS Marketplace, complete the steps to subscribe
with a SaaS contract by searching for Posit Platform (RStudio on SageMaker). To fulﬁll the

license, you will be redirected to an external form outside the AWS Marketplace. You must
provide additional information, including your company name and email address. If you
can’t access that form to provide a company name and a contact email, create a ticket with
Posit Support at https://support.posit.co/hc/en-us/requests/new with details about your
purchase.

• To purchase from RStudio PBC directly, navigate to RStudio Pricing or contact
sales@rstudio.com. When buying or updating an RStudio license, you must provide the AWS
Account that will host your Amazon SageMaker AI domain.

If you have an existing RStudio license, contact your RStudio Sales representative or
sales@rstudio.com to add RStudio on Amazon SageMaker AI to your existing Posit Workbench
Enterprise license, or to convert your Posit Workbench Standard license. The RStudio Sales
representative will send you the appropriate electronic order form.

2.
RStudio grants a Posit Workbench license to your AWS Account through AWS License Manager
in the US East (N. Virginia) Region. Although the RStudio license is granted in the US East (N.
Virginia) Region, your license can be consumed in any AWS Region that RStudio on Amazon
SageMaker AI is supported in. You can expect the license grant process to complete within
three business days after you share your AWS account ID with RStudio.

3.
When this license is granted, you receive an email from your RStudio Sales representative with
instructions to accept your license grant.

Validate your RStudio license to be used with Amazon SageMaker AI

RStudio on SageMaker AI management
1454

## Page 484

Amazon SageMaker AI
Developer Guide

1.
Log into the AWS License Manager console in the same region as your Amazon SageMaker
AI domain. If you are using AWS License Manager for the ﬁrst time, AWS License Manager
prompts you to grant permission to use AWS License Manager.

2.
Select Start using AWS License manager.

3.
Select I grant AWS License Manager the required permissions and select Grant
Permissions.

4.
Navigate to Granted Licenses on the left panel.

5.
Select the license grant with RSW-SageMaker as the Product name and select View.

6.
From the license detail page, select Accept & activate license.

RStudio administrative dashboard

You can use the RStudio administrative dashboard to see the number of users on the license
following the steps in Use the RStudio administrative dashboard.

RStudio Versioning

Important

Custom IAM policies that allow Amazon SageMaker Studio or Amazon SageMaker Studio
Classic to create Amazon SageMaker resources must also grant permissions to add tags to
those resources. The permission to add tags to resources is required because Studio and
Studio Classic automatically tag any resources they create. If an IAM policy allows Studio
and Studio Classic to create resources but does not allow tagging, "AccessDenied" errors can
occur when trying to create resources. For more information, see Provide permissions for
tagging SageMaker AI resources.
AWS managed policies for Amazon SageMaker AI that give permissions to create
SageMaker resources already include permissions to add tags while creating those
resources.

This guide provides information about the 2025.05.1+513.pro3 version update for RStudio
on SageMaker AI. Starting October 31, 2025, new domains with RStudio support are created

with Posit Workbench version 2025.05.1+513.pro3. This applies to the RStudioServerPro

applications and default RSessionGateway applications.

The following sections provide information about the 2025.05.1+513.pro3 release.

RStudio on SageMaker AI management
1455

## Page 485

Amazon SageMaker AI
Developer Guide

Latest version updates

The latest RStudio version is 2025.05.1+513.pro3.

• R versions supported:

• 4.5.1

• 4.4.3

• 4.4.0

• 4.3.3

• 4.2.3

• 4.2.1

• 4.1.3

• 4.0.2

For more information about the changes in this release, see https://docs.posit.co/ide/news/.

Note

To ensure compatibility, we recommend using RSessions with a preﬁx that matches the
current Posit Workbench version.

If you see the following warning, there is a version mismatch between the RSession and
the Posit Workbench version used in RStudio on SageMaker AI. To resolve this issue, update
the RStudio version for the domain. For information about updating the RStudio version,
see Upgrade to the new version.

Session version 2024.04.2+764.pro1 does not match server version
2025.05.1+513.pro3 - this is an unsupported configuration, and you may
experience unexpected issues as a result.

Versioning

There are currently two versions of Posit Workbench supported by SageMaker AI.

• Latest version: 2025.05.1+513.pro3

Deprecation Date: December 5, 2026

RStudio on SageMaker AI management
1456

## Page 486

Amazon SageMaker AI
Developer Guide

• Previous version: 2024.04.2+764.pro1

Deprecation Date: April 30, 2026

Note

While you can continue creating new domains with the older version

2024.04.2+764.pro1 until 04/30/2026 by explicitly pinning the version when you
create the domain using CLI, we strongly recommend customers to begin using the

2025.05 version in all domains. POSIT has ceased providing vulnerability ﬁxes for

2024.04.2+764.pro1.

Versions 2023.03.2-547.pro5 and 2022.02.2-485.pro2 are deprecated and are no
longer supported. We recommend updating to the latest version.

The default Posit Workbench version that SageMaker AI selects depends on the creation date of
the domain.

• For domains created after October 31, 2025, version 2025.05.1+513.pro3 is the default
selected version.

• For domains created after September 04, 2024 and before October 31, 2025, version

2024.04.2+764.pro1 is the default selected version. You can update your domains to the

latest version (2025.05.1+513.pro3) by setting it as the default version for the domain. For
more information, see Upgrade to the new version.

Note

The default RSessionGateway application version matches the current version of the

RStudioServerPro application.

The following table lists the image ARNs for both versions for each AWS Region. These ARNs are

passed as part of an update-domain command to set the desired version.

RStudio on SageMaker AI management
1457

## Page 487

Amazon SageMaker AI
Developer Guide

Region
2024.04.2+764.pro1
Image ARN

2025.05.1+513.pro3
Image ARN

us-east-1
arn:aws:sagemaker:us-east-1

arn:aws:sagemaker:us-east-1

:081325390199:image/
rstudio-workbench-2024.04-
sagemaker-1.1

:081325390199:image/
rstudio-workbench-2025.05-
sagemaker-1.0

us-east-2
arn:aws:sagemaker:us-east-2
:429704687514:image/
rstudio-workbench-2024.04-
sagemaker-1.1

arn:aws:sagemaker:us-east-2
:429704687514:image/
rstudio-workbench-2025.05-
sagemaker-1.0

us-west-1
arn:aws:sagemaker:us-west-1
:742091327244:image/
rstudio-workbench-2024.04-
sagemaker-1.1

arn:aws:sagemaker:us-west-1
:742091327244:image/
rstudio-workbench-2025.05-
sagemaker-1.0

us-west-2
arn:aws:sagemaker:us-west-2
:236514542706:image/
rstudio-workbench-2024.04-
sagemaker-1.1

arn:aws:sagemaker:us-west-2
:236514542706:image/
rstudio-workbench-2025.05-
sagemaker-1.0

af-south-1
arn:aws:sagemaker:af-south-
1:559312083959:image/
rstudio-workbench-2024.04-

arn:aws:sagemaker:af-south-
1:559312083959:image/
rstudio-workbench-2025.05-

sagemaker-1.1

sagemaker-1.0

ap-east-1
arn:aws:sagemaker:ap-east-1
:493642496378:image/
rstudio-workbench-2024.04-
sagemaker-1.1

arn:aws:sagemaker:ap-east-1
:493642496378:image/
rstudio-workbench-2025.05-
sagemaker-1.0

ap-south-1
arn:aws:sagemaker:ap-south-
1:394103062818:image/
rstudio-workbench-2024.04-
sagemaker-1.1

arn:aws:sagemaker:ap-south-
1:394103062818:image/
rstudio-workbench-2025.05-
sagemaker-1.0

RStudio on SageMaker AI management
1458

## Page 488

Amazon SageMaker AI
Developer Guide

Region
2024.04.2+764.pro1
Image ARN

2025.05.1+513.pro3
Image ARN

ap-northeast-2
arn:aws:sagemaker:ap-northe

arn:aws:sagemaker:ap-northe

ast-2:806072073708:image/
rstudio-workbench-2024.04-
sagemaker-1.1

ast-2:806072073708:image/
rstudio-workbench-2025.05-
sagemaker-1.0

ap-southeast-1
arn:aws:sagemaker:ap-southe
ast-1:492261229750:image/
rstudio-workbench-2024.04-
sagemaker-1.1

arn:aws:sagemaker:ap-southe
ast-1:492261229750:image/
rstudio-workbench-2025.05-
sagemaker-1.0

ap-southeast-2
arn:aws:sagemaker:ap-southe
ast-2:452832661640:image/
rstudio-workbench-2024.04-
sagemaker-1.1

arn:aws:sagemaker:ap-southe
ast-2:452832661640:image/
rstudio-workbench-2025.05-
sagemaker-1.0

ap-northeast-1
arn:aws:sagemaker:ap-northe
ast-1:102112518831:image/
rstudio-workbench-2024.04-
sagemaker-1.1

arn:aws:sagemaker:ap-northe
ast-1:102112518831:image/
rstudio-workbench-2025.05-
sagemaker-1.0

ca-central-1
arn:aws:sagemaker:ca-centra
l-1:310906938811:image/
rstudio-workbench-2024.04-
sagemaker-1.1

arn:aws:sagemaker:ca-centra
l-1:310906938811:image/
rstudio-workbench-2025.05-
sagemaker-1.0

eu-central-1
arn:aws:sagemaker:eu-centra
l-1:936697816551:image/
rstudio-workbench-2024.04-
sagemaker-1.1

arn:aws:sagemaker:eu-centra
l-1:936697816551:image/
rstudio-workbench-2025.05-
sagemaker-1.0

eu-west-1
arn:aws:sagemaker:eu-west-1
:470317259841:image/
rstudio-workbench-2024.04-
sagemaker-1.1

arn:aws:sagemaker:eu-west-1
:470317259841:image/
rstudio-workbench-2025.05-
sagemaker-1.0

RStudio on SageMaker AI management
1459

## Page 489

Amazon SageMaker AI
Developer Guide

Region
2024.04.2+764.pro1
Image ARN

2025.05.1+513.pro3
Image ARN

eu-west-2
arn:aws:sagemaker:eu-west-2

arn:aws:sagemaker:eu-west-2

:712779665605:image/
rstudio-workbench-2024.04-
sagemaker-1.1

:712779665605:image/
rstudio-workbench-2025.05-
sagemaker-1.0

eu-west-3
arn:aws:sagemaker:eu-west-3
:615547856133:image/
rstudio-workbench-2024.04-
sagemaker-1.1

arn:aws:sagemaker:eu-west-3
:615547856133:image/
rstudio-workbench-2025.05-
sagemaker-1.0

eu-north-1
arn:aws:sagemaker:eu-north-
1:243637512696:image/
rstudio-workbench-2024.04-
sagemaker-1.1

arn:aws:sagemaker:eu-north-
1:243637512696:image/
rstudio-workbench-2025.05-
sagemaker-1.0

eu-south-1
arn:aws:sagemaker:eu-south-
1:592751261982:image/
rstudio-workbench-2024.04-
sagemaker-1.1

arn:aws:sagemaker:eu-south-
1:592751261982:image/
rstudio-workbench-2025.05-
sagemaker-1.0

sa-east-1
arn:aws:sagemaker:sa-east-1
:782484402741:image/
rstudio-workbench-2024.04-
sagemaker-1.1

arn:aws:sagemaker:sa-east-1
:782484402741:image/
rstudio-workbench-2025.05-
sagemaker-1.0

Changes to BYOI Images

If you use a BYOI image with RStudio and update your RStudioServerPro version to

2025.05.1+513.pro3, you must upgrade your custom images to use the 2025.05.1+513.pro3
release and redeploy your existing RSessions. If you attempt to load a non-compatible image in

an RSession of a domain using the 2025.05.1+513.pro3 version, the RSession fails because it
cannot parse parameters that it receives. To prevent failure, update all of the deployed custom

images in your existing RStudioServerPro application.

RStudio on SageMaker AI management
1460

## Page 490

Amazon SageMaker AI
Developer Guide

The RSW_VERSION in the Dockerﬁle must be consistent with the Posit Workbench version used in
RStudio on SageMaker AI. You can validate the current version in Posit Workbench. To do so, use
the version name that's located in the lower left corner of the Posit Workbench launcher page.

ARG RSW_VERSION=2025.05.1+513.pro3
ENV RSTUDIO_FORCE_NON_ZERO_EXIT_CODE="1"
ARG RSW_NAME=rstudio-workbench
ARG OS_CODE_NAME=jammy
ARG RSW_DOWNLOAD_URL=https://s3.amazonaws.com/rstudio-ide-build/server/${OS_CODE_NAME}/
amd64
RUN RSW_VERSION_URL=`echo -n "${RSW_VERSION}" | sed 's/+/-/g'` \
&& curl -o rstudio-workbench.deb ${RSW_DOWNLOAD_URL}/${RSW_NAME}-
${RSW_VERSION_URL}-amd64.deb \
&& gdebi -n ./rstudio-workbench.deb

Upgrade to the new version

Existing domains using version 2024.04.2+764.pro1 can upgrade to 2025.05.1+513.pro3
version in one of two ways:

• Create a new domain from the AWS CLI with RStudio enabled.

• Update an existing domain to use the 2025.05.1+513.pro3 version.

The following procedure shows how to delete the RStudio application for an existing domain, set

the default version to 2025.05.1+513.pro3, and then create an RStudio application.

1.
Delete the RStudioServerPro application and all RSessionGateway applications
associated with your existing domain. For information about how to ﬁnd your domain ID, see
View domains. For more information about deleting applications, see Shut down RStudio.

aws sagemaker delete-app \
--region region \
--domain-id domainId \
--user-profile-name domain-shared \
--app-type RStudioServerPro \
--app-name default

2.
If your domain is using RStudio version 2024.04.2+764.pro1, update the

domain to set 2025.05.1+513.pro3 as the default Posit Workbench version. The

SageMakerImageArn value in the following update-domain command speciﬁes the RStudio

RStudio on SageMaker AI management
1461

## Page 491

Amazon SageMaker AI
Developer Guide

2025.05.1+513.pro3 version as the default. This ARN must match the Region that your
domain is in. For a list of all available ARNs, see Versioning.

Pass an execution role ARN for the domain that provides permissions to update the domain.

aws sagemaker update-domain \
--region region \
--domain-id domainId \
--domain-settings-for-update "{\"RStudioServerProDomainSettingsForUpdate\":
{\"DefaultResourceSpec\": {\"SageMakerImageArn\": \"arn-for-2025.05.1+513.pro3-
version\", \"InstanceType\": \"system\"}, \"DomainExecutionRoleArn\": \"execution-
role-arn\"}}"

3.
Create a new RStudioServerPro application in the existing domain.

aws sagemaker create-app \
--region region
--domain-id domainId \
--user-profile-name domain-shared \
--app-type RStudioServerPro \
--app-name default

Your RStudioServerPro application is now updated to version 2025.05.1+513.pro3. You can

now relaunch your RSessionGateway applications.

Downgrade to a previous version

You can manually downgrade the version of your existing RStudio application to the

2024.04.2+764.pro1 version.

To downgrade to a previous version

1.
Delete the RStudioServerPro application that's associated with your existing domain. For
information about how to ﬁnd your domain ID, see View domains.

aws sagemaker delete-app \
--domain-id domainId \
--user-profile-name domain-shared \
--app-type RStudioServerPro \
--app-name default

RStudio on SageMaker AI management
1462

## Page 492

Amazon SageMaker AI
Developer Guide

2.
Pass the corresponding 2024.04.2+764.pro1 ARN for your Region as part of the update-

domain command. For a list of all available ARNs, see Versioning. You must also pass an

execution role ARN for the domain that provides permissions to update the domain.

aws sagemaker update-domain \
--region region \
--domain-id domainId \
--domain-settings-for-update "{\"RStudioServerProDomainSettingsForUpdate\":
{\"DefaultResourceSpec\": {\"SageMakerImageArn\": \"arn-for-2024.04.2+764.pro1-
version\", \"InstanceType\": \"system\"}, \"DomainExecutionRoleArn\": \"execution-
role-arn\"}}"

3.
Create a new RStudioServerPro application in the existing domain. The RStudio version

defaults to 2024.04.2+764.pro1.

aws sagemaker create-app \
--domain-id domainId \
--user-profile-name domain-shared \
--app-type RStudioServerPro \
--app-name default

Your RStudioServerPro application is now downgraded to version 2024.04.2+764.pro1.

Network and Storage

The following topic describes network access and data storage considerations for your RStudio
instance. For general information about network access and data storage when using Amazon
SageMaker AI, see Data Protection in Amazon SageMaker AI.

Amazon EFS volume

RStudio on Amazon SageMaker AI shares an Amazon EFS volume with the Amazon SageMaker
Studio Classic application in the domain. When the RStudio application is added to a domain,

SageMaker AI creates a folder named shared in the Amazon EFS directory. If this shared folder
is deleted or changed manually, then the RStudio application may no longer function. For more
information about the Amazon EFS volume, see Manage Your Amazon EFS Storage Volume in
Amazon SageMaker Studio Classic.

Installed packages and scripts

RStudio on SageMaker AI management
1463

## Page 493

Amazon SageMaker AI
Developer Guide

Packages that you install from within RStudio are scoped to the user proﬁle level. This means that
the installed package persists through RSession shut down, restarts, and across RSessions for each
user proﬁle that they are installed in. R Scripts that are saved in RSessions behave the same way.
Both packages and R Scripts are saved in the user's Amazon EFS volume.

Encryption

RStudio on Amazon SageMaker AI supports encryption at rest.

Use RStudio in VPC-only mode

RStudio on Amazon SageMaker AI supports AWS PrivateLink integration. With this integration, you
can use RStudio on SageMaker AI in VPC-only mode without direct access to the internet. When
you use RStudio in VPC-only mode, your security groups are automatically managed by the service.
This includes connectivity between your RServer and your RSessions.

The following are required to use RStudio in VPC-only mode. For more information on selecting a
VPC, see Choose an Amazon VPC.

• A private subnet with either access the internet to make a call to Amazon SageMaker AI &
License Manager, or Amazon Virtual Private Cloud (Amazon VPC) endpoints for both Amazon
SageMaker AI & License Manager.

• The domain cannot have any more than two associated Security Groups.

• A Security Group ID for use with the domain in domain Settings. This must allow all outbound
access.

• A Security Group ID for use with the Amazon VPC endpoint. This security group must allow
inbound traﬃc from the domain Security Group ID.

• Amazon VPC Endpoint for sagemaker.api and AWS License Manager. This must be in the same
Amazon VPC as the private subnet.

RStudioServerPro instance type

When deciding which Amazon EC2 instance type to use for your RStudioServerPro app, the main
factor to consider is bandwidth. Bandwidth is important because the RStudioServerPro instance
is responsible for serving the RStudio UI to all users. This includes UI heavy workﬂows, such as
generating ﬁgures, animations, and displaying many data rows. Therefore, there may be some
UI performance degradation depending on the workload across all users. The following are the

RStudio on SageMaker AI management
1464

## Page 494

Amazon SageMaker AI
Developer Guide

available instance types to use for your RStudioServerPro. For pricing information about these
instances, see Amazon SageMaker Pricing.

• system: This instance type is recommended for Domains with low UI use.

Note

The system value is translated to ml.t3.medium.

• ml.c5.4xlarge: This instance type is recommended for Domains with moderate UI use.

• ml.c5.9xlarge: This instance type is recommended for Domains with heavy UI use.

Changing RStudio instance type

To change the instance type of your RStudioServerPro, pass the new instance type as part of a call

to the update-domain CLI command. You then need to delete the existing RStudioServerPro app

using the delete-app CLI command and create a new RStudioServerPro app using the create-

app CLI command.

Add an RStudio Connect URL

RStudio Connect is a publishing platform for Shiny applications, R Markdown reports, dashboards,
plots, and more. RStudio Connect makes it easy to surface machine learning and data science
insights by making hosting content simple and scalable. If you have an RStudio Connect server,
then you can set the server as the default place where apps are published. For more information
about RStudio Connect, see RStudio Connect.

When you onboard to RStudio on Amazon SageMaker AI domain, an RStudio Connect server is
not created. You can create an RStudio Connect server on an Amazon EC2 instance to use Connect
with Amazon SageMaker AI domain. For information about how to set up your RStudio Connect
server, see Host RStudio Connect and Package Manager for ML development in RStudio on Amazon
SageMaker AI.

Add an RStudio Connect URL

If you have an RStudio Connect URL, you can update the default URL so that your RStudio Users
can publish to it.

1.
Navigate to the domains page.

RStudio on SageMaker AI management
1465

## Page 495

Amazon SageMaker AI
Developer Guide

2.
Select the desired domain.

3.
Choose domain Settings.

4.
Under General Settings, select Edit.

5.
From the new page, select RStudio Settings on the left side.

6.
Under RStudio Connect URL, enter the RStudio Connect URL to add.

7.
Select Submit.

CLI

You can set a default RStudio Connect URL when you create your domain. The only way to update
your RStudio Connect URL from the AWS CLI is to delete your domain and create a new one with
the updated RStudio Connect URL.

Update the RStudio Package Manager URL

RStudio Package Manager is a repository management server used to organize and centralize
packages across your organization. For more information on RStudio Package Manager, see
RStudio Package Manager. If you don't supply your own Package Manager URL, Amazon
SageMaker AI domain uses the default Package Manager repository when you onboard RStudio
following the steps in Amazon SageMaker AI domain overview. For more information, see Host
RStudio Connect and Package Manager for ML development in RStudio on Amazon SageMaker AI.
The following procedure shows how to update the Package Manager URL.

Update Package Manager URL

You can update the Package Manager URL used for your RStudio-enabled domain as follows.

1.
Navigate to the domains page.

2.
Select the desired domain.

3.
Choose domain Settings.

4.
Under General Settings, select Edit.

5.
From the new page, select RStudio Settings on the left side.

6.
Under RStudio Package Manager, enter your RStudio Package Manager URL.

7.
Select Submit.

CLI

RStudio on SageMaker AI management
1466

## Page 496

Amazon SageMaker AI
Developer Guide

The only way to update your Package Manager URL from the AWS CLI is to delete your domain and
create a new one with the updated Package Manager URL.

Create an Amazon SageMaker AI domain with RStudio using the AWS CLI

Important

Custom IAM policies that allow Amazon SageMaker Studio or Amazon SageMaker Studio
Classic to create Amazon SageMaker resources must also grant permissions to add tags to
those resources. The permission to add tags to resources is required because Studio and
Studio Classic automatically tag any resources they create. If an IAM policy allows Studio
and Studio Classic to create resources but does not allow tagging, "AccessDenied" errors can
occur when trying to create resources. For more information, see Provide permissions for
tagging SageMaker AI resources.

AWS managed policies for Amazon SageMaker AI that give permissions to create
SageMaker resources already include permissions to add tags while creating those
resources.

The following topic shows how to onboard to Amazon SageMaker AI domain with RStudio enabled
using the AWS CLI. To onboard using the AWS Management Console, see Amazon SageMaker AI
domain overview.

Prerequisites

• Install and conﬁgure AWS CLI version 2

• Conﬁgure the AWS CLI with IAM credentials

Create DomainExecution role

To launch the RStudio App, you must provide a DomainExecution role. This role is used to
determine whether RStudio needs to be launched as part of Amazon SageMaker AI domain
creation. This role is also used by Amazon SageMaker AI to access the RStudio License and push
RStudio logs.

RStudio on SageMaker AI management
1467

## Page 497

Amazon SageMaker AI
Developer Guide

Note

The DomainExecution role should have at least AWS License Manager permissions to
access RStudio License, and CloudWatch permissions to push logs in your account.

The following procedure shows how to create the DomainExecution role with the AWS CLI.

1.
Create a ﬁle named assume-role-policy.json with the following content.

JSON

{
"Version":"2012-10-17",
"Statement": [
{
"Action": "sts:AssumeRole",
"Effect": "Allow",
"Principal": {
"Service": [
"sagemaker.amazonaws.com"
]
}
}
]
}

2.
Create the DomainExecution role. <REGION> should be the AWS Region to launch your
domain in.

aws iam create-role --region <REGION> --role-name DomainExecution --assume-role-
policy-document file://assume-role-policy.json

3.
Create a ﬁle named domain-setting-policy.json with the following content. This policy
allows the RStudioServerPro app to access necessary resources and allows Amazon SageMaker
AI to automatically launch an RStudioServerPro app when the existing RStudioServerPro app is

in a Deleted or Failed status.

RStudio on SageMaker AI management
1468

## Page 498

Amazon SageMaker AI
Developer Guide

JSON

{
"Version":"2012-10-17",
"Statement": [
{
"Sid": "VisualEditor0",
"Effect": "Allow",
"Action": [
"license-manager:ExtendLicenseConsumption",
"license-manager:ListReceivedLicenses",
"license-manager:GetLicense",
"license-manager:CheckoutLicense",
"license-manager:CheckInLicense",
"logs:CreateLogDelivery",
"logs:CreateLogGroup",

"logs:CreateLogStream",
"logs:DeleteLogDelivery",
"logs:Describe*",
"logs:GetLogDelivery",
"logs:GetLogEvents",
"logs:ListLogDeliveries",
"logs:PutLogEvents",
"logs:PutResourcePolicy",
"logs:UpdateLogDelivery",
"sagemaker:CreateApp"
],
"Resource": "*"
}
]
}

4.
Create the domain setting policy that is attached to the DomainExecution role. Be aware of

the PolicyArn from the response, you will need to enter that ARN in the following steps.

aws iam create-policy --region <REGION> --policy-name domain-setting-policy --
policy-document file://domain-setting-policy.json

5.
Attach domain-setting-policy to the DomainExecution role. Use the PolicyArn
returned in the previous step.

RStudio on SageMaker AI management
1469

## Page 499

Amazon SageMaker AI
Developer Guide

aws iam attach-role-policy --role-name DomainExecution --policy-arn <POLICY_ARN>

Create Amazon SageMaker AI domain with RStudio App

The RStudioServerPro app is launched automatically when you create a Amazon SageMaker AI

domain using the create-domain CLI command with the RStudioServerProDomainSettings

parameter speciﬁed. When launching the RStudioServerPro App, Amazon SageMaker AI checks for
a valid RStudio license in the account and fails domain creation if the license is not found.

The creation of a Amazon SageMaker AI domain diﬀers based on the authentication method and
the network type. These options must be used together, with one authentication method and one
network connection type selected. For more information about the requirements to create a new
domain, see CreateDomain.

The following authentication methods are supported.

• IAM Auth

• SSO Auth

The following network connection types are supported:

• PublicInternet

• VPCOnly

Authentication methods

IAM Auth Mode

The following shows how to create a Amazon SageMaker AI domain with RStudio enabled and an

IAM Auth Network Type. For more information about AWS Identity and Access Management, see
What is IAM?.

• DomainExecutionRoleArn should be the ARN for the role created in the previous step.

• ExecutionRole is the ARN of the role given to users in the Amazon SageMaker AI domain.

• vpc-id should be the ID of your Amazon Virtual Private Cloud. subnet-ids should be a space-

separated list of subnet IDs. For information about vpc-id and subnet-ids, see VPCs and
subnets.

RStudio on SageMaker AI management
1470

## Page 500

Amazon SageMaker AI
Developer Guide

• RStudioPackageManagerUrl and RStudioConnectUrl are optional and should be set to the
URLs of your RStudio Package Manager and RStudio Connect server, respectively.

• app-network-access-type should be either PublicInternetOnly or VPCOnly.

aws sagemaker create-domain --region <REGION> --domain-name <DOMAIN_NAME> \
--auth-mode IAM \

--default-user-settings ExecutionRole=<DEFAULT_USER_EXECUTIONROLE> \
--domain-settings
RStudioServerProDomainSettings={RStudioPackageManagerUrl=<<PACKAGE_MANAGER_URL>,RStudioConnect
\
--vpc-id <VPC_ID> \
--subnet-ids <SUBNET_IDS> \
--app-network-access-type <NETWORK_ACCESS_TYPE>

Authentication using IAM Identity Center

The following shows how to create a Amazon SageMaker AI domain with RStudio enabled and an

SSO Auth Network Type. AWS IAM Identity Center must be enabled for the region that the domain
is launched on. For more information about IAM Identity Center, see What is AWS IAM Identity
Center?.

• DomainExecutionRoleArn should be the ARN for the role created in the previous step.

• ExecutionRole is the ARN of the role given to users in the Amazon SageMaker AI domain.

• vpc-id should be the ID of your Amazon Virtual Private Cloud. subnet-ids should be a space-

separated list of subnet IDs. For information about vpc-id and subnet-ids, see VPCs and
subnets.

• RStudioPackageManagerUrl and RStudioConnectUrl are optional and should be set to the
URLs of your RStudio Package Manager and RStudio Connect server, respectively.

• app-network-access-type should be either PublicInternetOnly or VPCOnly.

aws sagemaker create-domain --region <REGION> --domain-name <DOMAIN_NAME> \
--auth-mode SSO \
--default-user-settings ExecutionRole=<DEFAULT_USER_EXECUTIONROLE> \
--domain-settings
RStudioServerProDomainSettings={RStudioPackageManagerUrl=<<PACKAGE_MANAGER_URL>,RStudioConnect
\
--vpc-id <VPC_ID> \

RStudio on SageMaker AI management
1471

## Page 501

Amazon SageMaker AI
Developer Guide

--subnet-ids <SUBNET_IDS> \
--app-network-access-type <NETWORK_ACCESS_TYPE>

Connection types

PublicInternet/Direct Internet network type

The following shows how to create a Amazon SageMaker AI domain with RStudio enabled and a

PublicInternet Network Type.

• DomainExecutionRoleArn should be the ARN for the role created in the previous step.

• ExecutionRole is the ARN of the role given to users in the Amazon SageMaker AI domain.

• vpc-id should be the ID of your Amazon Virtual Private Cloud. subnet-ids should be a space-

separated list of subnet IDs. For information about vpc-id and subnet-ids, see VPCs and
subnets.

• RStudioPackageManagerUrl and RStudioConnectUrl are optional and should be set to the
URLs of your RStudio Package Manager and RStudio Connect server, respectively.

• auth-mode should be either SSO or IAM.

aws sagemaker create-domain --region <REGION> --domain-name <DOMAIN_NAME> \
--auth-mode <AUTH_MODE> \
--default-user-settings ExecutionRole=<DEFAULT_USER_EXECUTIONROLE> \
--domain-settings
RStudioServerProDomainSettings={RStudioPackageManagerUrl=<<PACKAGE_MANAGER_URL>,RStudioConnect
\
--vpc-id <VPC_ID> \
--subnet-ids <SUBNET_IDS> \
--app-network-access-type PublicInternetOnly

VPCOnly mode

The following shows how to launch a Amazon SageMaker AI domain with RStudio enabled and

a VPCOnly Network Type. For more information about using the VPCOnly network access type,
see Connect Studio notebooks in a VPC to external resources.

• DomainExecutionRoleArn should be the ARN for the role created in the previous step.

• ExecutionRole is the ARN of the role given to users in the Amazon SageMaker AI domain.

RStudio on SageMaker AI management
1472

## Page 502

Amazon SageMaker AI
Developer Guide

• vpc-id should be the ID of your Amazon Virtual Private Cloud. subnet-ids should be a space-
separated list of subnet IDs. Your private subnet must be able to either access the internet to
make a call to Amazon SageMaker AI, and AWS License Manager or have Amazon VPC endpoints
for both Amazon SageMaker AI and AWS License Manager. For information about Amazon VPC

endpoints, see Interface Amazon VPC endpoints For information about vpc-id and subnet-

ids, see VPCs and subnets.

• SecurityGroups must allow outbound access to the Amazon SageMaker AI and AWS License

Manager endpoints.

• auth-mode should be either SSO or IAM.

Note

When using Amazon Virtual Private Cloud endpoints, the security group attached to your

Amazon Virtual Private Cloud endpoints must allow inbound traﬃc from the security group

you pass as part of the domain-setting parameter of the create-domain CLI call.

With RStudio, Amazon SageMaker AI manages security groups for you. This means that Amazon
SageMaker AI manages security group rules to ensure RSessions can access RStudioServerPro Apps.
Amazon SageMaker AI creates one security group rule per user proﬁle.

aws sagemaker create-domain --region <REGION> --domain-name <DOMAIN_NAME> \
--auth-mode <AUTH_MODE> \
--default-user-settings
SecurityGroups=<USER_SECURITY_GROUP>,ExecutionRole=<DEFAULT_USER_EXECUTIONROLE> \
--domain-settings
SecurityGroupIds=<DOMAIN_SECURITY_GROUP>,RStudioServerProDomainSettings={DomainExecutionRoleAr
\
--vpc-id <VPC_ID> \
--subnet-ids "<SUBNET_IDS>" \
--app-network-access-type VPCOnly --app-security-group-management Service

Note: The RStudioServerPro app is launched by a special user proﬁle named domain-shared. As a

result, this app is not returned as part of list-app API calls by any other user proﬁles.

You may have to increase the Amazon VPC quota in your account to increase the number of users.
For more information, see Amazon VPC quotas.

RStudio on SageMaker AI management
1473

## Page 503

Amazon SageMaker AI
Developer Guide

Verify domain creation

Use the following command to verify that your domain has been created with a Status

of InService. Your domain-id is appended to the domains ARN. For example,

arn:aws:sagemaker:<REGION>:<ACCOUNT_ID>:domain/<DOMAIN_ID>.

aws sagemaker describe-domain --domain-id <DOMAIN_ID> --region <REGION>

Add RStudio support to an existing domain

Important

Custom IAM policies that allow Amazon SageMaker Studio or Amazon SageMaker Studio
Classic to create Amazon SageMaker resources must also grant permissions to add tags to
those resources. The permission to add tags to resources is required because Studio and
Studio Classic automatically tag any resources they create. If an IAM policy allows Studio
and Studio Classic to create resources but does not allow tagging, "AccessDenied" errors can
occur when trying to create resources. For more information, see Provide permissions for
tagging SageMaker AI resources.
AWS managed policies for Amazon SageMaker AI that give permissions to create
SageMaker resources already include permissions to add tags while creating those
resources.

If you have added an RStudio License through AWS License Manager, you can create a new Amazon
SageMaker AI domain with support for RStudio on SageMaker AI. If you have an existing domain
that does not support RStudio, you can add RStudio support to that domain without having to
delete and recreate the domain.

The following topic outlines how to add this support.

Prerequisites

You must complete the following steps before you update your current domain to add support for
RStudio on SageMaker AI.

• Install and conﬁgure AWS CLI version 2

• Conﬁgure the AWS CLI with IAM credentials

RStudio on SageMaker AI management
1474

## Page 504

Amazon SageMaker AI
Developer Guide

• Create a domain execution role following the steps in Create a SageMaker AI Domain with
RStudio using the AWS CLI. This domain-level IAM role is required by the RStudioServerPro app.
The role requires access to AWS License Manager for verifying a valid Posit Workbench license
and Amazon CloudWatch Logs for publishing server logs.

• Bring your RStudio license to AWS License Manager following the steps in RStudio license.

• (Optional) If you want to use RStudio in VPCOnly mode, complete the steps in RStudio in VPC-
Only.

• Ensure that the security groups you have conﬁgured for each UserProﬁle in your domain
meet the account-level quotas. When conﬁguring the default user proﬁle during domain

creation, you can use the DefaultUserSettings parameter of the CreateDomain API to

add SecurityGroups that are inherited by all the user proﬁles created in the domain. You

can also provide additional security groups for a speciﬁc user as part of the UserSettings
parameter of the CreateUserProﬁle API. If you have added security groups this way, you must

ensure that the total number of security groups per user proﬁle doesn’t exceed the maximum

quota of 2 in VPCOnly mode and 4 in PublicInternetOnly mode. If the resulting total
number of security groups for any user proﬁle exceeds the quota, you can combine multiple
security groups’ rules into one security group.

Add RStudio support to an existing domain

After you have completed the prerequisites, you can add RStudio support to your existing domain.
The following steps outline how to update your existing domain to add support for RStudio.

Step 1: Delete all apps in the domain

To add support for RStudio in your domain, SageMaker AI must update the underlying security
groups for all existing user proﬁles. To complete this, you must delete and recreate all existing apps
in the domain. The following procedure shows how to delete all of the apps.

1.
List all of the apps in the domain.

aws sagemaker \
list-apps \
--domain-id-equals <DOMAIN_ID>

2.
Delete each app for each user proﬁle in the domain.

// JupyterServer apps

RStudio on SageMaker AI management
1475

## Page 505

Amazon SageMaker AI
Developer Guide

aws sagemaker \
delete-app \
--domain-id <DOMAIN_ID> \
--user-profile-name <USER_PROFILE> \
--app-type JupyterServer \
--app-name <APP_NAME>

// KernelGateway apps
aws sagemaker \
delete-app \
--domain-id <DOMAIN_ID> \
--user-profile-name <USER_PROFILE> \
--app-type KernelGateway \
--app-name <APP_NAME>

Step 2 - Update all user proﬁles with the new list of security groups

This is a one-time action that you must complete for all of the existing user proﬁles in your domain
when you have refactored your existing security groups. This prevents you from hitting the quota

for the maximum number of security groups. The UpdateUserProfile API call fails if the user

has any apps that are in InService status. Delete all apps, then call UpdateUserProfile API to
update the security groups.

Note

The following requirement for VPCOnly mode outlined in Connect Amazon SageMaker
Studio Classic Notebooks in a VPC to External Resources is no longer needed when adding

RStudio support because AppSecurityGroupManagement is managed by the SageMaker
AI service:
“TCP traﬃc within the security group. This is required for connectivity between the
JupyterServer app and the KernelGateway apps. You must allow access to at least ports in

the range 8192-65535.”

aws sagemaker \
update-user-profile \
--domain-id <DOMAIN_ID>\
--user-profile-name <USER_PROFILE> \

RStudio on SageMaker AI management
1476

## Page 506

Amazon SageMaker AI
Developer Guide

--user-settings "{\"SecurityGroups\": [\"<SECURITY_GROUP>\",
\"<SECURITY_GROUP>\"]}"

Step 3 - Activate RStudio by calling the UpdateDomain API

1.
Call the UpdateDomain API to add support for RStudio on SageMaker AI. The

defaultusersettings parameter is only needed if you have refactored the default security
groups for your user proﬁles.

• For VPCOnly mode:

aws sagemaker \
update-domain \
--domain-id <DOMAIN_ID> \
--app-security-group-management Service \

--domain-settings-for-update
RStudioServerProDomainSettingsForUpdate={DomainExecutionRoleArn=<DOMAIN_EXECUTION_ROLE_A
\
--default-user-settings "{\"SecurityGroups\": [\"<SECURITY_GROUP>\",
\"<SECURITY_GROUP>\"]}"

• For PublicInternetOnly mode:

aws sagemaker \
update-domain \
--domain-id <DOMAIN_ID> \
--domain-settings-for-update
RStudioServerProDomainSettingsForUpdate={DomainExecutionRoleArn=<DOMAIN_EXECUTION_ROLE_A
--default-user-settings "{\"SecurityGroups\": [\"<SECURITY_GROUP>\",
\"<SECURITY_GROUP>\"]}"

2.
Verify that the domain status is InService. After the domain status is InService, support
for RStudio on SageMaker AI is added.

aws sagemaker \
describe-domain \
--domain-id <DOMAIN_ID>

3.
Verify that the RStudioServerPro app’s status is InService using the following command.

aws sagemaker list-apps --user-profile-name domain-shared

RStudio on SageMaker AI management
1477

## Page 507

Amazon SageMaker AI
Developer Guide

Step 4 - Add RStudio access for existing users

As part of the update in Step 3, SageMaker AI marks the RStudio AccessStatus of all

existing user proﬁles in the domain as DISABLED by default. This prevents exceeding the

number of users allowed by your current license. To add access for existing users, there is
a one-time opt-in step. Perform the opt-in by calling the UpdateUserProﬁle API with the
following RStudioServerProAppSettings:

• AccessStatus = ENABLED

• Optional - UserGroup = R_STUDIO_USER or R_STUDIO_ADMIN

aws sagemaker \
update-user-profile \
--domain-id <DOMAIN_ID>\
--user-profile-name <USER_PROFILE> \
--user-settings "{\"RStudioServerProAppSettings\": {\"AccessStatus\": \"ENABLED
\"}}"

Note

By default, the number of users that can have access to RStudio is 60.

Step 5 – Deactivate RStudio access for new users

Unless otherwise speciﬁed when calling UpdateDomain, RStudio support is added by default
for all new user proﬁles created after you have added support for RStudio on SageMaker AI. To

deactivate access for a new user proﬁle, you must explicitly set the AccessStatus parameter to

DISABLED as part of the CreateUserProfile API call. If the AccessStatus parameter is not

speciﬁed as part of the CreateUserProfile API, the default access status is ENABLED.

aws sagemaker \
create-user-profile \
--domain-id <DOMAIN_ID>\
--user-profile-name <USER_PROFILE> \
--user-settings "{\"RStudioServerProAppSettings\": {\"AccessStatus\": \"DISABLED
\"}}"

RStudio on SageMaker AI management
1478

## Page 508

Amazon SageMaker AI
Developer Guide

Custom images with RStudio on SageMaker AI

A SageMaker image is a ﬁle that identiﬁes language packages and other dependencies that are
required to run RStudio on Amazon SageMaker AI. SageMaker AI uses these images to create an
environment where you run RStudio. Amazon SageMaker AI provides a built-in RStudio image for

you to use. If you need diﬀerent functionality, you can bring your own custom images. This page
gives information about key concepts for using custom images with RStudio on SageMaker AI. The
process to bring your own image to use with RStudio on SageMaker AI takes three steps:

1.
Build a custom image from a Dockerﬁle and push it to a repository in Amazon Elastic
Container Registry (Amazon ECR).

2.
Create a SageMaker image that points to a container image in Amazon ECR and attach it to
your Amazon SageMaker AI domain.

3.
Launch a new session in RStudio with your custom image.

You can create images and image versions, and attach image versions to your domain, using
the SageMaker AI control panel, the AWS SDK for Python (Boto3), and the AWS Command Line
Interface (AWS CLI). You can also create images and image versions using the SageMaker AI
console, even if you haven't onboarded to a domain.

The following topics show how to bring your own image to RStudio on SageMaker AI by creating,
attaching, and launching a custom image.

Key terminology

The following section deﬁnes key terms for bringing your own image to use with RStudio on
SageMaker AI.

• Dockerﬁle: A Dockerﬁle is a ﬁle that identiﬁes the language packages and other dependencies
for your Docker image.

• Docker image: The Docker image is a built Dockerﬁle. This image is checked into Amazon ECR
and serves as the basis of the SageMaker AI image.

• SageMaker image: A SageMaker image is a holder for a set of SageMaker image versions based
on Docker images.

• Image version: An image version of a SageMaker image represents a Docker image that is
compatible with RStudio and stored in an Amazon ECR repository. Each image version is
immutable. These image versions can be attached to a domain and used with RStudio on
SageMaker AI.

RStudio on SageMaker AI management
1479

## Page 509

Amazon SageMaker AI
Developer Guide

Complete prerequisites

You must complete the following prerequisites before bringing your own image to use with
RStudio on Amazon SageMaker AI.

• If you have an existing domain with RStudio that was created before April 7, 2022, you must
delete your RStudioServerPro application and recreate it. For information about how to delete an
application, see Shut Down and Update Amazon SageMaker Studio Classic.

• Install the Docker application. For information about setting up Docker, see Orientation and
setup.

• Create a local copy of an RStudio-compatible Dockerﬁle that works with SageMaker AI. For
information about creating a sample RStudio dockerﬁle, see Use a custom image to bring your
own development environment to RStudio on Amazon SageMaker AI.

• Use an AWS Identity and Access Management execution role that has the
AmazonSageMakerFullAccess policy attached. If you have onboarded to domain, you can get the
role from the domain Summary section of the SageMaker AI control panel.

Add the following permissions to access the Amazon Elastic Container Registry (Amazon ECR)
service to your execution role.

JSON

{
"Version":"2012-10-17",
"Statement":[
{
"Sid": "VisualEditor0",
"Effect":"Allow",
"Action":[
"ecr:CreateRepository",
"ecr:BatchGetImage",
"ecr:CompleteLayerUpload",
"ecr:DescribeImages",
"ecr:DescribeRepositories",
"ecr:UploadLayerPart",
"ecr:ListImages",
"ecr:InitiateLayerUpload",
"ecr:BatchCheckLayerAvailability",
"ecr:PutImage"
],
"Resource": "*"

RStudio on SageMaker AI management
1480

## Page 510

Amazon SageMaker AI
Developer Guide

}
]
}

• Install and conﬁgure AWS CLI with the following (or higher) version. For information about
installing the AWS CLI, see Installing or updating the latest version of the AWS CLI.

AWS CLI v1 >= 1.23.6
AWS CLI v2 >= 2.6.2

Custom RStudio image speciﬁcations

In this guide, you'll learn custom RStudio image speciﬁcations to use when you bring your own
image. There are two sets of requirements that you must satisfy with your custom RStudio image

to use it with Amazon SageMaker AI. These requirements are imposed by RStudio PBC and the
Amazon SageMaker Studio Classic platform. If either of these sets of requirements aren't satisﬁed,
then your custom image won't function properly.

RStudio PBC requirements

RStudio PBC requirements are laid out in the Using Docker images with RStudio Workbench /
RStudio Server Pro, Launcher, and Kubernetes article. Follow the instructions in this article to
create the base of your custom RStudio image.

For instructions about how to install multiple R versions in your custom image, see Installing
multiple versions of R on Linux.

Amazon SageMaker Studio Classic requirements

Amazon SageMaker Studio Classic imposes the following set of installation requirements for your
RStudio image.

• You must use an RStudio base image of at least 2025.05.1+513.pro3. For more information,
see RStudio Versioning.

• You must install the following packages:

yum install -y sudo \
openjdk-11-jdk \
libpng-dev \
&& yum clean all \

RStudio on SageMaker AI management
1481

## Page 511

Amazon SageMaker AI
Developer Guide

&& /opt/R/${R_VERSION}/bin/R -e "install.packages('reticulate', repos='https://
packagemanager.rstudio.com/cran/__linux__/centos7/latest')" \
&& /opt/python/${PYTHON_VERSION}/bin/pip install --upgrade \
'boto3>1.0<2.0' \
'awscli>1.0<2.0' \
'sagemaker[local]<3'

• You must provide default values for the RSTUDIO_CONNECT_URL and

RSTUDIO_PACKAGE_MANAGER_URL environment values.

ENV RSTUDIO_CONNECT_URL "YOUR_CONNECT_URL"
ENV RSTUDIO_PACKAGE_MANAGER_URL "YOUR_PACKAGE_MANAGER_URL"
ENV RSTUDIO_FORCE_NON_ZERO_EXIT_CODE 1

The following general speciﬁcations apply to the image that is represented by an RStudio image
version.

Running the image

ENTRYPOINT and CMD instructions are overridden so that the image is run as an RSession
application.

Stopping the image

The DeleteApp API issues the equivalent of a docker stop command. Other processes in the
container won’t get the SIGKILL/SIGTERM signals.

File system

The /opt/.sagemakerinternal and /opt/ml directories are reserved. Any data in these
directories might not be visible at runtime.

User data

Each user in a SageMaker AI domain gets a user directory on a shared Amazon Elastic File
System volume in the image. The location of the current user’s directory on the Amazon Elastic

File System volume is /home/sagemaker-user.

Metadata

A metadata ﬁle is located at /opt/ml/metadata/resource-metadata.json. No additional
environment variables are added to the variables deﬁned in the image. For more information,
see Get App Metadata.

RStudio on SageMaker AI management
1482

## Page 512

Amazon SageMaker AI
Developer Guide

GPU

On a GPU instance, the image is run with the --gpus option. Only the CUDA toolkit should be
included in the image, not the NVIDIA drivers. For more information, see NVIDIA User Guide.

Metrics and logging

Logs from the RSession process are sent to Amazon CloudWatch in the customer’s account.

The name of the log group is /aws/sagemaker/studio. The name of the log stream is

$domainID/$userProfileName/RSession/$appName.

Image size

Image size is limited to 25 GB. To view the size of your image, run docker image ls.

Create a custom RStudio image

Important

Custom IAM policies that allow Amazon SageMaker Studio or Amazon SageMaker Studio
Classic to create Amazon SageMaker resources must also grant permissions to add tags to
those resources. The permission to add tags to resources is required because Studio and
Studio Classic automatically tag any resources they create. If an IAM policy allows Studio
and Studio Classic to create resources but does not allow tagging, "AccessDenied" errors can
occur when trying to create resources. For more information, see Provide permissions for
tagging SageMaker AI resources.
AWS managed policies for Amazon SageMaker AI that give permissions to create
SageMaker resources already include permissions to add tags while creating those
resources.

This topic describes how you can create a custom RStudio image using the SageMaker AI console
and the AWS CLI. If you use the AWS CLI, you must run the steps from your local machine. The
following steps do not work from within Amazon SageMaker Studio Classic.

When you create an image, SageMaker AI also creates an initial image version. The image version
represents a container image in Amazon Elastic Container Registry (ECR). The container image must
satisfy the requirements to be used in RStudio. For more information, see Custom RStudio image
speciﬁcations.

RStudio on SageMaker AI management
1483

## Page 513

Amazon SageMaker AI
Developer Guide

For information about testing your image locally and resolving common issues, see the SageMaker
Studio Custom Image Samples repo.

Topics

• Add a SageMaker AI-compatible RStudio Docker container image to Amazon ECR

• Create a SageMaker image from the console

• Create an image from the AWS CLI

Add a SageMaker AI-compatible RStudio Docker container image to Amazon ECR

Use the following steps to add a Docker container image to Amazon ECR:

• Create an Amazon ECR repository.

• Authenticate to Amazon ECR.

• Build a SageMaker AI-compatible RStudio Docker image.

• Push the image to the Amazon ECR repository.

Note

The Amazon ECR repository must be in the same AWS Region as your domain.

To build and add a Docker image to Amazon ECR

1.
Create an Amazon ECR repository using the AWS CLI. To create the repository using the
Amazon ECR console, see Creating a repository.

aws ecr create-repository \
--repository-name rstudio-custom \
--image-scanning-configuration scanOnPush=true

Response:

{
"repository": {
"repositoryArn": "arn:aws:ecr:us-east-2:acct-id:repository/rstudio-custom",
"registryId": "acct-id",

RStudio on SageMaker AI management
1484

## Page 514

Amazon SageMaker AI
Developer Guide

"repositoryName": "rstudio-custom",
"repositoryUri": "acct-id.dkr.ecr.us-east-2.amazonaws.com/rstudio-custom",
...
}
}

2.
Authenticate to Amazon ECR using the repository URI returned as a response from the

create-repository command. Make sure that the Docker application is running. For more
information, see Registry Authentication.

aws ecr get-login-password | \
docker login --username AWS --password-stdin <repository-uri>

Response:

Login Succeeded

3.
Build the Docker image. Run the following command from the directory that includes your
Dockerﬁle.

docker build .

4.
Tag your built image with a unique tag.

docker tag <image-id> "<repository-uri>:<tag>"

5.
Push the container image to the Amazon ECR repository. For more information, see ImagePush
and Pushing an image.

docker push <repository-uri>:<tag>

Response:

The push refers to repository [<account-id>.dkr.ecr.us-east-2.amazonaws.com/
rstudio-custom]
r: digest: <digest> size: 3066

RStudio on SageMaker AI management
1485

## Page 515

Amazon SageMaker AI
Developer Guide

Create a SageMaker image from the console

To create an image

1.
Open the Amazon SageMaker AI console at https://console.aws.amazon.com/sagemaker/.

2.
On the left navigation pane, choose Admin conﬁgurations.

3.
Under Admin conﬁgurations, choose Images.

4.
On the Custom images page, choose Create image.

5.
For Image source, enter the registry path to the container image in Amazon ECR. The path is in
the following format:

acct-id.dkr.ecr.region.amazonaws.com/repo-name[:tag] or [@digest]

6.
Choose Next.

7.
Under Image properties, enter the following:

• Image name – The name must be unique to your account in the current AWS Region.

• (Optional) Image display name – The name displayed in the domain user interface. When not

provided, Image name is displayed.

• (Optional) Description – A description of the image.

• IAM role – The role must have the AmazonSageMakerFullAccess policy attached. Use the
dropdown menu to choose one of the following options:

• Create a new role – Specify any additional Amazon Simple Storage Service (Amazon S3)
buckets that you want your notebooks users to access. If you don't want to allow access to
additional buckets, choose None.

SageMaker AI attaches the AmazonSageMakerFullAccess policy to the role. The role
allows your notebook users to access the Amazon S3 buckets listed next to the check
marks.

• Enter a custom IAM role ARN – Enter the Amazon Resource Name (ARN) of your IAM role.

• Use existing role – Choose one of your existing roles from the list.

• (Optional) Image tags – Choose Add new tag. You can add up to 50 tags. Tags are searchable

using the SageMaker AI console or the SageMaker AI Search API.

8.
Under Image type, select RStudio image.

9.
Choose Submit.

RStudio on SageMaker AI management
1486

## Page 516

Amazon SageMaker AI
Developer Guide

The new image is displayed in the Custom images list and brieﬂy highlighted. After the image has
been successfully created, you can choose the image name to view its properties or choose Create
version to create another version.

To create another image version

1.
Choose Create version on the same row as the image.

2.
For Image source, enter the registry path to the Amazon ECR image. The image shouldn't be
the same image as used in a previous version of the SageMaker AI image.

To use the custom image in RStudio, you must attach it to your domain. For more information, see
Attach a custom SageMaker image.

Create an image from the AWS CLI

This section shows how to create a custom Amazon SageMaker image using the AWS CLI.

Use the following steps to create a SageMaker image:

• Create an Image.

• Create an ImageVersion.

• Create a conﬁguration ﬁle.

• Create an AppImageConfig.

To create the SageMaker image entities

1.
Create a SageMaker image. The role ARN must have at least the

AmazonSageMakerFullAccessPolicy policy attached.

aws sagemaker create-image \
--image-name rstudio-custom-image \
--role-arn arn:aws:iam::<acct-id>:role/service-role/<execution-role>

Response:

{
"ImageArn": "arn:aws:sagemaker:us-east-2:acct-id:image/rstudio-custom-image"
}

RStudio on SageMaker AI management
1487

## Page 517

Amazon SageMaker AI
Developer Guide

2.
Create a SageMaker image version from the image. Pass the unique tag value that you chose
when you pushed the image to Amazon ECR.

aws sagemaker create-image-version \
--image-name rstudio-custom-image \
--base-image <repository-uri>:<tag>

Response:

{
"ImageVersionArn": "arn:aws:sagemaker:us-east-2:acct-id:image-version/rstudio-
image/1"
}

3.
Check that the image version was successfully created.

aws sagemaker describe-image-version \
--image-name rstudio-custom-image \
--version 1

Response:

{
"ImageVersionArn": "arn:aws:sagemaker:us-east-2:acct-id:image-version/rstudio-
custom-image/1",
"ImageVersionStatus": "CREATED"
}

Note

If the response is "ImageVersionStatus": "CREATED_FAILED", the response
also includes the failure reason. A permissions issue is a common cause of failure.
You also can check your Amazon CloudWatch Logs. The name of the log group

is /aws/sagemaker/studio. The name of the log stream is $domainID/

$userProfileName/KernelGateway/$appName.

4.
Create a conﬁguration ﬁle, named app-image-config-input.json. The app image conﬁg
is used to conﬁguration for running a SageMaker image as a Kernel Gateway application.

RStudio on SageMaker AI management
1488

## Page 518

Amazon SageMaker AI
Developer Guide

{
"AppImageConfigName": "rstudio-custom-config"
}

5.
Create the AppImageConﬁg using the ﬁle that you created in the previous step.

aws sagemaker create-app-image-config \
--cli-input-json file://app-image-config-input.json

Response:

{
"AppImageConfigArn": "arn:aws:sagemaker:us-east-2:acct-id:app-image-config/r-
image-config"
}

Attach a custom SageMaker image

Important

Custom IAM policies that allow Amazon SageMaker Studio or Amazon SageMaker Studio
Classic to create Amazon SageMaker resources must also grant permissions to add tags to
those resources. The permission to add tags to resources is required because Studio and
Studio Classic automatically tag any resources they create. If an IAM policy allows Studio
and Studio Classic to create resources but does not allow tagging, "AccessDenied" errors can
occur when trying to create resources. For more information, see Provide permissions for
tagging SageMaker AI resources.
AWS managed policies for Amazon SageMaker AI that give permissions to create
SageMaker resources already include permissions to add tags while creating those
resources.

This guide shows how to attach a custom RStudio image to your Amazon SageMaker AI domain
using the SageMaker AI console or the AWS Command Line Interface (AWS CLI).

RStudio on SageMaker AI management
1489

## Page 519

Amazon SageMaker AI
Developer Guide

To use a custom SageMaker image, you must attach a custom RStudio image to your domain.
When you attach an image version, it appears in the RStudio Launcher and is available in the Select
image dropdown list. You use the dropdown to change the image used by RStudio.

There is a limit to the number of image versions that you can attach. After you reach the limit, you
must ﬁrst detach a version so that you can attach a diﬀerent version of the image.

Topics

• Attach an image version to your domain using the console

• Attach an existing image version to your domain using the AWS CLI

Attach an image version to your domain using the console

You can attach a custom SageMaker image version to your domain using the SageMaker AI
console's control panel. You can also create a custom SageMaker image, and an image version, and
then attach that version to your domain.

To attach an existing image

1.
Open the Amazon SageMaker AI console at https://console.aws.amazon.com/sagemaker/.

2.
On the left navigation pane, choose Admin conﬁgurations.

3.
Under Admin conﬁgurations, choose domains.

4.
Select the desired domain.

5.
Choose Environment.

6.
Under Custom SageMaker Studio Classic images attached to domain, choose Attach image.

7.
For Image source, choose Existing image or New image.

If you select Existing image, choose an image from the Amazon SageMaker image store.

If you select New image, provide the Amazon ECR registry path for your Docker image. The
path must be in the same AWS Region as the domain. The Amazon ECR repo must be in
the same account as your domain, or cross-account permissions for SageMaker AI must be
enabled.

8.
Choose an existing image from the list.

9.
Choose a version of the image from the list.

10. Choose Next.

RStudio on SageMaker AI management
1490

## Page 520

Amazon SageMaker AI
Developer Guide

11. Enter values for Image name, Image display name, and Description.

12. Choose the IAM role. For more information, see Create a custom RStudio image.

13. (Optional) Add tags for the image.

14. (Optional) Choose Add new tag, then add a conﬁguration tag.

15. For Image type, select RStudio Image.

16. Choose Submit.

Wait for the image version to be attached to the domain. After the version is attached, it appears in
the Custom images list and is brieﬂy highlighted.

Attach an existing image version to your domain using the AWS CLI

Two methods are presented to attach the image version to your domain using the AWS CLI. In the
ﬁrst method, you create a new domain with the version attached. This method is simpler but you
must specify the Amazon Virtual Private Cloud (Amazon VPC) information and execution role that's
required to create the domain.

If you have already onboarded to the domain, you can use the second method to attach the image
version to your current domain. In this case, you don't need to specify the Amazon VPC information
and execution role. After you attach the version, delete all of the applications in your domain and
relaunch RStudio.

Attach the SageMaker image to a new domain

To use this method, you must specify an execution role that has the AmazonSageMakerFullAccess
policy attached.

Use the following steps to create the domain and attach the custom SageMaker AI image:

• Get your default VPC ID and subnet IDs.

• Create the conﬁguration ﬁle for the domain, which speciﬁes the image.

• Create the domain with the conﬁguration ﬁle.

To add the custom SageMaker image to your domain

1.
Get your default VPC ID.

RStudio on SageMaker AI management
1491

## Page 521

Amazon SageMaker AI
Developer Guide

aws ec2 describe-vpcs \
--filters Name=isDefault,Values=true \
--query "Vpcs[0].VpcId" --output text

Response:

vpc-xxxxxxxx

2.
Get your default subnet IDs using the VPC ID from the previous step.

aws ec2 describe-subnets \
--filters Name=vpc-id,Values=<vpc-id> \
--query "Subnets[*].SubnetId" --output json

Response:

[
"subnet-b55171dd",
"subnet-8a5f99c6",
"subnet-e88d1392"
]

3.
Create a conﬁguration ﬁle named create-domain-input.json. Insert the VPC ID,

subnet IDs, ImageName, and AppImageConfigName from the previous steps. Because

ImageVersionNumber isn't speciﬁed, the latest version of the image is used, which is the
only version in this case. Your execution role must satisfy the requirements in Complete
prerequisites.

{
"DomainName": "domain-with-custom-r-image",
"VpcId": "<vpc-id>",
"SubnetIds": [
"<subnet-ids>"
],
"DomainSettings": {
"RStudioServerProDomainSettings": {
"DomainExecutionRoleArn": "<execution-role>"
}
},
"DefaultUserSettings": {

RStudio on SageMaker AI management
1492

## Page 522

Amazon SageMaker AI
Developer Guide

"ExecutionRole": "<execution-role>",
"RSessionAppSettings": {
"CustomImages": [
{
"AppImageConfigName": "rstudio-custom-config",
"ImageName": "rstudio-custom-image"
}
]
}
},
"AuthMode": "IAM"
}

4.
Create the domain with the attached custom SageMaker image.

aws sagemaker create-domain \
--cli-input-json file://create-domain-input.json

Response:

{
"DomainArn": "arn:aws:sagemaker:region:acct-id:domain/domain-id",
"Url": "https://domain-id.studio.region.sagemaker.aws/..."
}

Attach the SageMaker image to an existing domain

This method assumes that you've already onboarded to domain. For more information, see Amazon
SageMaker AI domain overview.

Note

You must delete all of the applications in your domain to update the domain with the new
image version. For information about deleting these applications, see Delete an Amazon
SageMaker AI domain.

Use the following steps to add the SageMaker image to your current domain.

• Get your DomainID from the SageMaker AI console.

RStudio on SageMaker AI management
1493

## Page 523

Amazon SageMaker AI
Developer Guide

• Use the DomainID to get the DefaultUserSettings for the domain.

• Add the ImageName and AppImageConfig as a CustomImage to the DefaultUserSettings.

• Update your domain to include the custom image.

To add the custom SageMaker image to your domain

1.
Open the Amazon SageMaker AI console at https://console.aws.amazon.com/sagemaker/.

2.
On the left navigation pane, choose Admin conﬁgurations.

3.
Under Admin conﬁgurations, choose domains.

4.
Select the desired domain.

5.
Choose domain settings.

6.
Under General Settings, ﬁnd the domain ID. The ID is in the following format: d-

xxxxxxxxxxxx.

7.
Use the domain ID to get the description of the domain.

aws sagemaker describe-domain \
--domain-id <d-xxxxxxxxxxxx>

Response:

{
"DomainId": "d-xxxxxxxxxxxx",
"DefaultUserSettings": {
"KernelGatewayAppSettings": {
"CustomImages": [
],
...
}
}
}

8.
Save the DefaultUserSettings section of the response to a ﬁle named update-domain-

input.json.

9.
Insert the ImageName and AppImageConfigName from the previous steps as a custom image.

Because ImageVersionNumber isn't speciﬁed, the latest version of the image is used, which is
the only version in this case.

RStudio on SageMaker AI management
1494

## Page 524

Amazon SageMaker AI
Developer Guide

{
"DefaultUserSettings": {
"RSessionAppSettings": {
"CustomImages": [
{
"ImageName": "rstudio-custom-image",
"AppImageConfigName": "rstudio-custom-config"
}
]
}
}
}

10. Use the domain ID and default user settings ﬁle to update your domain.

aws sagemaker update-domain \

--domain-id <d-xxxxxxxxxxxx> \
--cli-input-json file://update-domain-input.json

Response:

{
"DomainArn": "arn:aws:sagemaker:region:acct-id:domain/domain-id"
}

11. Delete the RStudioServerPro application. You must restart the RStudioServerPro

domain-shared application for the RStudio Launcher UI to pick up the latest changes.

aws sagemaker delete-app \
--domain-id <d-xxxxxxxxxxxx> --user-profile-name domain-shared \
--app-type RStudioServerPro --app-name default

12. Create a new RStudioServerPro application. You must create this application using the AWS

CLI.

aws sagemaker create-app \
--domain-id <d-xxxxxxxxxxxx> --user-profile-name domain-shared \
--app-type RStudioServerPro --app-name default

RStudio on SageMaker AI management
1495

## Page 525

Amazon SageMaker AI
Developer Guide

Launch a custom SageMaker image in RStudio

You can use your custom image when launching an RStudio applicaton from the console. After
you create your custom SageMaker image and attach it to your domain, the image appears in the
image selector dialog box of the RStudio Launcher. To launch a new RStudio app, follow the steps
in Launch RSessions from the RStudio Launcher and select your custom image as shown in the
following image.

![Page 525 Diagram 1](images/page-0525-img-01.png)

Clean up image resources

This guide shows how to clean up RStudio image resources that you created in the previous
sections. To delete an image, complete the following steps using either the SageMaker AI console
or the AWS CLI, as shown in this guide.

• Detach the image and image versions from your Amazon SageMaker AI domain.

• Delete the image, image version, and app image conﬁg.

RStudio on SageMaker AI management
1496

## Page 526

Amazon SageMaker AI
Developer Guide

After you've completed these steps, you can delete the container image and repository from
Amazon ECR. For more information about how to delete the container image and repository, see
Deleting a repository.

Clean up resources from the SageMaker AI console

When you detach an image from a domain, all versions of the image are detached. When an image
is detached, all users of the domain lose access to the image versions.

To detach an image

1.
Open the Amazon SageMaker AI console at https://console.aws.amazon.com/sagemaker/.

2.
On the left navigation pane, choose Admin conﬁgurations.

3.
Under Admin conﬁgurations, choose domains.

4.
Select the desired domain.

5.
Choose Environment.

6.
Under Custom images attached to domain, choose the image and then choose Detach.

7.
(Optional) To delete the image and all versions from SageMaker AI, select Also delete the
selected images .... This does not delete the associated images from Amazon ECR.

8.
Choose Detach.

Clean up resources from the AWS CLI

To clean up resources

1.
Detach the image and image versions from your domain by passing an empty custom image

list to the domain. Open the update-domain-input.json ﬁle that you created in Attach the
SageMaker image to your current domain.

2.
Delete the RSessionAppSettings custom images and then save the ﬁle. Do not modify the

KernelGatewayAppSettings custom images.

{
"DomainId": "d-xxxxxxxxxxxx",
"DefaultUserSettings": {
"KernelGatewayAppSettings": {
"CustomImages": [
],
...

RStudio on SageMaker AI management
1497

## Page 527

Amazon SageMaker AI
Developer Guide

},
"RSessionAppSettings": {
"CustomImages": [
],
"DefaultResourceSpec": {
}
...
}
}
}

3.
Use the domain ID and default user settings ﬁle to update your domain.

aws sagemaker update-domain \
--domain-id <d-xxxxxxxxxxxx> \
--cli-input-json file://update-domain-input.json

Response:

{
"DomainArn": "arn:aws:sagemaker:us-east-2:acct-id:domain/d-xxxxxxxxxxxx"
}

4.
Delete the app image conﬁg.

aws sagemaker delete-app-image-config \
--app-image-config-name rstudio-image-config

5.
Delete the SageMaker image, which also deletes all image versions. The container images in
Amazon ECR that are represented by the image versions are not deleted.

aws sagemaker delete-image \
--image-name rstudio-image

Create a user to use RStudio

Important

Custom IAM policies that allow Amazon SageMaker Studio or Amazon SageMaker Studio
Classic to create Amazon SageMaker resources must also grant permissions to add tags to

RStudio on SageMaker AI management
1498

## Page 528

Amazon SageMaker AI
Developer Guide

those resources. The permission to add tags to resources is required because Studio and
Studio Classic automatically tag any resources they create. If an IAM policy allows Studio
and Studio Classic to create resources but does not allow tagging, "AccessDenied" errors can
occur when trying to create resources. For more information, see Provide permissions for
tagging SageMaker AI resources.
AWS managed policies for Amazon SageMaker AI that give permissions to create
SageMaker resources already include permissions to add tags while creating those
resources.

After your RStudio-enabled Amazon SageMaker AI domain is running, you can add user proﬁles
(UserProﬁles) to the domain. The following topics show how to create user proﬁles that are
authorized to use RStudio, as well as update an existing user proﬁle. For information on how to
delete an RStudio App, UserProﬁle, or domain, follow the steps in Delete an Amazon SageMaker AI
domain.

Note

The limit for the total number of UserProﬁles in a Amazon SageMaker AI domain is 60.

There are two types of users:

• Unauthorized: This user cannot access the RStudio app.

• Authorized: This user can access the RStudio app and use one of the RStudio license seats. By

default, a new user is Authorized if the domain is enabled for RStudio.

Changing a user's authorization status is only valid from Unauthorized to Authorized. If a user
is authorized, they can be given one of the following levels of access to RStudio.

• RStudio User: This is a standard RStudio user and can access RStudio.

• RStudio Admin: The admin of your Amazon SageMaker AI domain has the ability to create users,
add existing users, and update the permissions of existing users. Admins can also access the
RStudio Administrative dashboard. However, this admin is not able to update parameters that
are managed by Amazon SageMaker AI.

RStudio on SageMaker AI management
1499

## Page 529

Amazon SageMaker AI
Developer Guide

Methods to create a user

The following topics show how to create a user in your RStudio-enabled Amazon SageMaker AI
domain.

Create user console

To create a user in your RStudio-enabled Amazon SageMaker AI domain from the console,
complete the steps in Add user proﬁles.

Create user CLI

The following command shows how to add users to a Amazon SageMaker AI domain with IAM

authentication. A User can belong to either the R_STUDIO_USER or R_STUDIO_ADMIN User group.

aws sagemaker create-user-profile --region <REGION> \
--domain-id <DOMAIN-ID> \
--user-profile-name <USER_PROFILE_NAME-ID> \
--user-settings RStudioServerProAppSettings={UserGroup=<USER-GROUP>}

The following command shows how to add users to a Amazon SageMaker AI domain with

authentication using IAM Identity Center. A user can belong to either the R_STUDIO_USER or

R_STUDIO_ADMIN User group.

aws sagemaker create-user-profile --region <REGION> \
--domain-id <DOMAIN-ID> \
--user-profile-name <USER_PROFILE_NAME-ID> \
--user-settings RStudioServerProAppSettings={UserGroup=<USER-GROUP>} \
--single-sign-on-user-identifier UserName \
--single-sign-on-user-value <USER-NAME>

Log in to RStudio as another user

The following topic demonstrates how to log in to RStudio on Amazon SageMaker AI as another
user.

1.
Open the Amazon SageMaker AI console at https://console.aws.amazon.com/sagemaker/.

2.
On the left navigation pane, choose Admin conﬁgurations.

3.
Under Admin conﬁgurations, choose domains.

RStudio on SageMaker AI management
1500

## Page 530

Amazon SageMaker AI
Developer Guide

4.
Select the domain containing the user proﬁle.

5.
Select a user name from the list of users. This opens a new page with details about the user
proﬁle and the apps that are running.

6.
Select Launch.

7.
From the dropdown, select RStudio to launch an RStudio instance.

Terminate sessions for another user

The following topic demonstrates how to terminate sessions for another user in RStudio on
Amazon SageMaker AI.

1.
From the list of running apps, identify the app you want to delete.

2.
Click the respective Delete app button for the app you are deleting.

Use the RStudio administrative dashboard

This topic shows how to access and use the RStudio administrative dashboard. With the RStudio
administrative dashboard, admins can manage users and RSessions, as well as view information
about RStudio Server instance utilization and Amazon CloudWatch Logs.

Launch the RStudio administrative dashboard

The R_STUDIO_ADMIN authorization allows the user to access the RStudio administrative

dashboard. An R_STUDIO_ADMIN user can access the RStudio administrative dashboard by

replacing workspaces with admin in their RStudio URL manually. The following shows how to
modify the URL to access the RStudio administrative dashboard.

For example, the following RStudio URL:

https://<DOMAIN-ID>.studio.us-east-2.sagemaker.aws/rstudio/default/s/<SESSION-ID>/
workspaces

Can be converted to:

https://<DOMAIN-ID>.studio.us-east-2.sagemaker.aws/rstudio/default/s/<SESSION-ID>/admin

RStudio on SageMaker AI management
1501

## Page 531

Amazon SageMaker AI
Developer Guide

Dashboard tab

This tab gives an overview of your RStudio Server instance utilization, as well as information on the
number of active RSessions.

Sessions tab

This tab gives information on the active RSessions, such as the user that launched the RSessions,
the time that the RSessions have been running, and their resource utilization.

Users tab

This tab gives information on the RStudio authorized users in the domain, such as the time that the
last RSession was launched and their resource utilization.

Stats tab

This tab gives information on the utilization of your RStudio Server instance.

Logs tab

This tab displays Amazon CloudWatch Logs for the RStudio Server instance. For more information
about logging events with Amazon CloudWatch Logs, see What is Amazon CloudWatch Logs?.

Shut down RStudio

Important

Custom IAM policies that allow Amazon SageMaker Studio or Amazon SageMaker Studio
Classic to create Amazon SageMaker resources must also grant permissions to add tags to
those resources. The permission to add tags to resources is required because Studio and
Studio Classic automatically tag any resources they create. If an IAM policy allows Studio
and Studio Classic to create resources but does not allow tagging, "AccessDenied" errors can
occur when trying to create resources. For more information, see Provide permissions for
tagging SageMaker AI resources.
AWS managed policies for Amazon SageMaker AI that give permissions to create
SageMaker resources already include permissions to add tags while creating those
resources.

To shut down and restart your Posit Workbench and the associated RStudioServerPro app, you
must ﬁrst shut down all of your existing RSessions. You can shut down the RSessionGateway apps

RStudio on SageMaker AI management
1502

## Page 532

Amazon SageMaker AI
Developer Guide

from within RStudio. You can then shut down the RStudioServerPro app using the AWS CLI. After
the RStudioServerPro app is shut down, you must reopen RStudio through the SageMaker AI
console.

Any unsaved notebook information is lost in the process. The user data in the Amazon EFS volume
isn't impacted.

Note

If you are using a custom image with RStudio, ensure that your docker image is using an
RStudio version that is compatible with the version of Posit Workbench being used by
SageMaker AI after you restart your RStudioServerPro app.

The following topics show how to shut down the RSessionGateway and RStudioServerPro apps and
restart them.

Suspend your RSessions

Complete the following procedure to suspend all of your RSessions.

1.
From the RStudio Launcher, identify the RSession that you want to suspend.

2.
Select Suspend for the session.

3.
Repeat this for all RSessions.

Delete your RSessions

Complete the following procedure to shut down all of your RSessions.

1.
From the RStudio Launcher, identify the RSession that you want to delete.

2.
Select Quit for the session. This opens a new Quit Session window.

3.
From the Quit Session window, select Force Quit, to end all child processes in the session.

4.
Select Quit Session to conﬁrm deletion of the session.

5.
Repeat this for all RSessions.

Delete your RStudioServerPro app

Run the following commands from the AWS CLI to delete and restart your RStudioServerPro app.

RStudio on SageMaker AI management
1503

## Page 533

Amazon SageMaker AI
Developer Guide

1.
Delete the RStudioServerPro application by using your current domain id.

aws sagemaker delete-app \
--domain-id <domainId> \
--user-profile-name domain-shared \
--app-type RStudioServerPro \
--app-name default

2.
Re-create the RStudioServerPro application.

aws sagemaker create-app \
--domain-id <domainId> \
--user-profile-name domain-shared \
--app-type RStudioServerPro \
--app-name default

Billing and cost

To track the costs associated with your RStudio environment, you can use the AWS Billing and
Cost Management service. AWS Billing and Cost Management provides useful tools to help you
gather information related to your cost and usage, analyze your cost drivers and usage trends,
and take action to budget your spending. For more information, see What is AWS Billing and
Cost Management?. The following describes components required to run RStudio on Amazon
SageMaker AI and how each component factors into billing for your RStudio instance.

• RStudio License –You must purchase an RStudio license. There is no additional charge for using
your RStudio license with Amazon SageMaker AI. For more information about your RStudio
license, see Get an RStudio license.

• RSession - These are RStudio working sessions launched by end users. You are charged while the
RSession is running.

• RStudio Server - A multi-tenant server manages all the RSessions. You can choose the instance
type to run RStudio Server on, and pay the related costs. The default instance, "system", is free,
but you can choose to pay for higher tiers. For more information about the available instance
types for your RStudio Server, see RStudioServerPro instance type.

Tracking billing at user level

To track billing at the user level using Cost Allocation Tags, see Using Cost Allocation Tags.

RStudio on SageMaker AI management
1504

## Page 534

Amazon SageMaker AI
Developer Guide

Diagnose issues and get support

The following sections describe how to diagnose issues with RStudio on Amazon SageMaker
AI. To get support for RStudio on Amazon SageMaker AI, contact Amazon SageMaker AI support.
For help with purchasing an RStudio license or modifying the number of license seats, contact
sales@rstudio.com.

Upgrade your version

If you receive a warning that there is a version mismatch between your RSession and
RStudioServerPro apps, then you must upgrade the version of your RStudioServerPro app. For
more information, see RStudio Versioning.

View Metrics and Logs

You can monitor your workﬂow performance while using RStudio on Amazon SageMaker AI. View
data logs and information about metrics with the RStudio administrative dashboard or Amazon
CloudWatch.

View your RStudio logs from the RStudio administrative dashboard

You can view metrics and logs directly from the RStudio administrative dashboard.

1.
Log in to your Amazon SageMaker AI domain.

2.
Navigate to the RStudio administrative dashboard following the steps in Use the RStudio
administrative dashboard.

3.
Select the Logs tab.

View your RStudio logs from Amazon CloudWatch Logs

Amazon CloudWatch monitors your AWS resources and the applications that you run on AWS
in real time. You can use Amazon CloudWatch to collect and track metrics, which are variables
that you can measure for your resources and applications. To ensure that your RStudio apps have
permissions for Amazon CloudWatch, you must include the permissions described in Amazon
SageMaker AI domain overview. You don’t need to do any setup to gather Amazon CloudWatch
Logs.

The following steps show how to view Amazon CloudWatch Logs for your RSession.

These logs can be found in the /aws/sagemaker/studio log stream from the AWS CloudWatch
console.

RStudio on SageMaker AI management
1505

## Page 535

Amazon SageMaker AI
Developer Guide

1.
Open the CloudWatch console at https://console.aws.amazon.com/cloudwatch/.

2.
Select Logs from the left side. From the dropdown menu, select Log groups.

3.
On the Log groups screen, search for aws/sagemaker/studio. Select the Log group.

4.
On the aws/sagemaker/studio Log group screen, navigate to the Log streams tab.

5.
To ﬁnd the logs for your domain, search Log streams using the following format:

<DomainId>/domain-shared/rstudioserverpro/default

RStudio on Amazon SageMaker AI user guide

With RStudio support in Amazon SageMaker AI, you can put your production workﬂows in place
and take advantage of SageMaker AI features. The following topics show how to launch an RStudio
session and complete key workﬂows. For information about managing RStudio on SageMaker AI,
see RStudio on Amazon SageMaker AI management.

For information about the onboarding steps to create an Amazon SageMaker AI domain with
RStudio enabled, see Amazon SageMaker AI domain overview.

For information about the AWS Regions that RStudio on SageMaker AI is supported in,
see Supported Regions and Quotas.

Topics

• Collaborate in RStudio

• Base R image

• RSession application colocation

• Launch RSessions from the RStudio Launcher

• Suspend your RSessions

• Delete your RSessions

• RStudio Connect

• Amazon SageMaker AI feature integration with RStudio on Amazon SageMaker AI

Collaborate in RStudio

To share your RStudio project, you can connect RStudio to your Git repo. For information on setting
this up, see  Version Control with Git and SVN.

RStudio on Amazon SageMaker AI user guide
1506

## Page 536

Amazon SageMaker AI
Developer Guide

Note: Project sharing and realtime collaboration are not currently supported when using RStudio
on Amazon SageMaker AI.

Base R image

When launching your RStudio instance, the Base R image serves as the basis of your instance. This
image extends the r-session-complete Docker image.

This Base R image includes the following:

• R v4.0 or higher

• awscli, sagemaker, and boto3 Python packages

• Reticulate package for R SDK integration

RSession application colocation

Users can create multiple RSession applications on the same instance. Each instance type supports
up to four colocated RSession applications. This applies to each user independently. For example,
if two users create applications, then SageMaker AI allocates diﬀerent underlying instances to each
user. Each of these instances would support 4 RSession applications.

Customers only pay for the instance type used regardless of how many Rsession applications are
running on the instance. If a user creates an RSession with a diﬀerent associated instance type,
then a new underlying instance is created.

Launch RSessions from the RStudio Launcher

Important

Custom IAM policies that allow Amazon SageMaker Studio or Amazon SageMaker Studio
Classic to create Amazon SageMaker resources must also grant permissions to add tags to
those resources. The permission to add tags to resources is required because Studio and
Studio Classic automatically tag any resources they create. If an IAM policy allows Studio
and Studio Classic to create resources but does not allow tagging, "AccessDenied" errors can
occur when trying to create resources. For more information, see Provide permissions for
tagging SageMaker AI resources.

RStudio on Amazon SageMaker AI user guide
1507

## Page 537

Amazon SageMaker AI
Developer Guide

AWS managed policies for Amazon SageMaker AI that give permissions to create
SageMaker resources already include permissions to add tags while creating those
resources.

The following sections show how to use the RStudio Launcher to launch RSessions. They also
include information about how to open the RStudio Launcher when using RStudio on Amazon

SageMaker AI.

Open RStudio Launcher

Open the RStudio launcher using the following set of procedures that matches your environment.

Open RStudio Launcher from the Amazon SageMaker AI Console

1.
Open the Amazon SageMaker AI console at https://console.aws.amazon.com/sagemaker/.

2.
From the left navigation, select RStudio.

3.
Under Get Started, select the domain and user proﬁle to launch.

4.
Choose Launch RStudio.

Open RStudio Launcher from Amazon SageMaker Studio

1.
Navigate to Studio following the steps in Launch Amazon SageMaker Studio.

2.
Under Applications, select RStudio.

3.
From the RStudio landing page, choose Launch application.

Open RStudio Launcher from the AWS CLI

The procedure to open the RStudio Launcher using the AWS CLI diﬀers depending on the method
used to manage your users.

IAM Identity Center

1.
Use the AWS access portal to open your Amazon SageMaker AI domain.

2.
Modify the URL path to “/rstudio/default” as follows.

#Studio URL
https://<domain-id>.studio.<region>.sagemaker.aws/jupyter/default/lab

RStudio on Amazon SageMaker AI user guide
1508

## Page 538

Amazon SageMaker AI
Developer Guide

#modified URL
https://<domain-id>.studio.<region>.sagemaker.aws/rstudio/default

IAM

To open the RStudio Launcher from the AWS CLI in IAM mode, complete the following procedure.

1.
Create a presigned URL using the following command.

aws sagemaker create-presigned-domain-url --region <REGION> \
--domain-id <DOMAIN-ID> \
--user-profile-name <USER-PROFILE-NAME>

2.
Append &redirect=RStudioServerPro to the generated URL.

3.
Navigate to the updated URL.

Launch RSessions

After you’ve launched the RStudio Launcher, you can create a new RSession.

1.
Select New Session.

2.
Enter a Session Name.

3.
Select an instance type that your RSession runs on. This defaults to ml.t3.medium.

4.
Select an Image that your RSession uses as the kernel.

5.
Select Start Session.

6.
After your session has been created, you can start it by selecting the name.

Note

If you receive a warning that there is a version mismatch between your RSession and
RStudioServerPro apps, then you must upgrade the version of your RStudioServerPro
app. For more information, see RStudio Versioning.

RStudio on Amazon SageMaker AI user guide
1509

## Page 539

Amazon SageMaker AI
Developer Guide

Suspend your RSessions

The following procedure demonstrates how to suspend an RSession from the RStudio Launcher
when using RStudio on Amazon SageMaker AI. For information about accessing the RStudio
Launcher, see Launch RSessions from the RStudio Launcher.

1.
From the RStudio Launcher, identify the RSession that you want to suspend.

2.
Select Suspend for the session.

Delete your RSessions

The following procedure demonstrates how to delete an RSession from the RStudio Launcher when
using RStudio on Amazon SageMaker AI. For information about accessing the RStudio Launcher,
see Launch RSessions from the RStudio Launcher.

1.
From the RStudio Launcher, identify the RSession that you want to delete.

2.
Select Quit for the session. This opens a new Quit Session window.

3.
From the Quit Session window, select Force Quit, to end all child processes in the session.

4.
Select Quit Session to conﬁrm deletion of the session.

RStudio Connect

RStudio Connect enables data scientists to publish insights, dashboard and web applications from
RStudio on Amazon SageMaker AI. For more information, see Host RStudio Connect and Package
Manager for ML development in RStudio on Amazon SageMaker AI.

For more information on RStudio Connect, see the RStudio Connect User Guide.

Amazon SageMaker AI feature integration with RStudio on Amazon SageMaker AI

One of the beneﬁts of using RStudio on Amazon SageMaker AI is the integration of Amazon
SageMaker AI features. This includes integration with Amazon SageMaker Studio Classic and
Reticulate. The following gives information about these integrations and examples for using them.

Use Amazon SageMaker Studio Classic and RStudio on Amazon SageMaker AI

Your Amazon SageMaker Studio Classic and RStudio instances share the same Amazon EFS ﬁle
system. This means that ﬁles that you import and create using Studio Classic can be accessed
using RStudio and vice versa. This allows you to work on the same ﬁles using both Studio Classic

RStudio on Amazon SageMaker AI user guide
1510

## Page 540

Amazon SageMaker AI
Developer Guide

and RStudio without having to move your ﬁles between the two. For more information on this
workﬂow, see the Announcing Fully Managed RStudio on Amazon SageMaker AI for Data Scientists
blog.

Use Amazon SageMaker SDK with reticulate

The reticulate package is used as an R interface to Amazon SageMaker Python SDK to make API
calls to Amazon SageMaker. The reticulate package translates between R and Python objects,
and Amazon SageMaker AI provides a serverless data science environment to train and deploy
Machine Learning (ML) models at scale. For general information about the reticulate package, see
R Interface to Python.

For a blog that outlines how to use the reticulate package with Amazon SageMaker AI, see Using R
with Amazon SageMaker AI.

The following examples show how to use reticulate for speciﬁc use cases.

• For a notebook that describes how to use reticulate to do batch transform to make predictions,
see Batch Transform Using R with Amazon SageMaker AI.

• For a notebook that describes how to use reticulate to conduct hyperparameter tuning and
generate predictions, see Hyperparameter Optimization Using R with Amazon SageMaker AI.

Code Editor in Amazon SageMaker Studio

Code Editor, based on Code-OSS, Visual Studio Code - Open Source, helps you write, test, debug,
and run your analytics and machine learning code. Code Editor extends and is fully integrated with
Amazon SageMaker Studio. It also supports integrated development environment (IDE) extensions
available in the Open VSX Registry. The following page gives information about Code Editor and
key details for using it.

Code Editor has the AWS Toolkit for VS Code extension pre-installed, which enables connections
to AWS services such as Amazon CodeWhisperer, a general purpose, machine learning-powered
code generator that provides code recommendations in real time. For more information about
extensions, see Code Editor Connections and Extensions.

Important

As of November 30, 2023, the previous Amazon SageMaker Studio experience is now
named Amazon SageMaker Studio Classic. The following section is speciﬁc to using the

Code Editor
1511

## Page 541

Amazon SageMaker AI
Developer Guide

updated Studio experience. For information about using the Studio Classic application, see
Amazon SageMaker Studio Classic.

To launch Code Editor, create a Code Editor private space. The Code Editor space uses a single
Amazon Elastic Compute Cloud (Amazon EC2) instance for your compute and a single Amazon
Elastic Block Store (Amazon EBS) volume for your storage. Everything in your space such as your
code, Git proﬁle, and environment variables are stored on the same Amazon EBS volume. The
volume has 3000 IOPS and a throughput of 125 MBps. Your administrator has conﬁgured the
default Amazon EBS storage settings for your space.

The default storage size is 5 GB, but your administrator can increase the amount of space you get.
For more information, see Change the default storage size.

The working directory of your users within the storage volume is /home/sagemaker-user. If
you specify your own AWS KMS key to encrypt the volume, everything in the working directory is
encrypted using your customer managed key. If you don't specify an AWS KMS key, the data inside

/home/sagemaker-user is encrypted with an AWS managed key. Regardless of whether you
specify an AWS KMS key, all of the data outside of the working directory is encrypted with an AWS
Managed Key.

You can scale your compute up or down by changing the Amazon EC2 instance type that runs your
Code Editor application. Before you change the associated instance type, you must ﬁrst stop your
Code Editor space. For more information, see Code Editor application instances and images.

Your administrator might provide you with a lifecycle conﬁguration to customize your
environment. You can specify the lifecycle conﬁguration when you create the space. For more
information, see Code Editor lifecycle conﬁgurations.

You can also bring your own ﬁle storage system if you have an Amazon EFS volume.

Code Editor
1512

## Page 542

Amazon SageMaker AI
Developer Guide

![Page 542 Diagram 1](images/page-0542-img-01.png)

Topics

• Using the Code Editor

• Code Editor administrator guide

Using the Code Editor

The topics in this section provide guides for using Code Editor, including how to launch, add
connections to AWS services, shut down resources, and more. After creating a Code Editor space,
you can access your Code Editor session directly through the browser.

Within your Code Editor environment, you can do the following:

• Access all artifacts persisted in your home directory

• Clone your GitHub repositories and commit changes

• Access the SageMaker Python SDK

You can return to Studio to review any assets created in your Code Editor environment such as
experiments, pipelines, or training jobs.

Topics

Using the Code Editor
1513

## Page 543

Amazon SageMaker AI
Developer Guide

• Check the version of Code Editor

• Code Editor application instances and images

• Launch a Code Editor application in Studio

• Launch a Code Editor application using the AWS CLI

• Clone a repository in Code Editor

• Code Editor Connections and Extensions

• Shut down Code Editor resources

Check the version of Code Editor

The following steps show how to check the version of your Code Editor application.

To check the Code Editor application version

1.
Launch and run a Code Editor space and navigate to the Code Editor application UI. For more
information, see Launch a Code Editor application in Studio.

2.
In the upper-left corner of the Code Editor UI, choose the menu button

(

).
Then, choose Help. Then, choose About.

Code Editor application instances and images

Only some instances are compatible with Code Editor applications. You can choose the instance
type that is compatible with your use case from the Instance dropdown menu.

The Fast launch instances start up much faster than the other instances. For more information
about fast launch instance types in Studio, Instance Types Available for Use With Amazon
SageMaker Studio Classic Notebooks.

Note

If you use a GPU instance type when conﬁguring your Code Editor application, you must
also use a GPU-based image. The Code Editor space UI automatically selects a compatible
image when you select your instance type.

Using the Code Editor
1514

## Page 544

Amazon SageMaker AI
Developer Guide

Within a space, your data is stored in an Amazon EBS volume that persists independently from the
life of an instance. You won't lose your data when you change instances. If your Code Editor space

is Running, you must stop your space before changing instance types.

The following table lists the ARNs of the available Code Editor CPU and GPU images for each
Region.

Region
CPU
GPU

us-east-1
arn:aws:sagemaker:us-east-1
:885854791233:image/
sagemaker-distribution-cpu

arn:aws:sagemaker:us-east-1
:885854791233:image/
sagemaker-distribution-gpu

us-east-2
arn:aws:sagemaker:us-
east-2:37914896644:image/
sagemaker-distribution-cpu

arn:aws:sagemaker:us-
east-2:37914896644:image/
sagemaker-distribution-gpu

us-west-1
arn:aws:sagemaker:us-west-1
:053634841547:image/
sagemaker-distribution-cpu

arn:aws:sagemaker:us-west-1
:053634841547:image/
sagemaker-distribution-gpu

us-west-2
arn:aws:sagemaker:us-west-2
:542918446943:image/
sagemaker-distribution-cpu

arn:aws:sagemaker:us-west-2
:542918446943:image/
sagemaker-distribution-gpu

af-south-1
arn:aws:sagemaker:af-south-
1:238384257742:image/
sagemaker-distribution-cpu

arn:aws:sagemaker:af-south-
1:238384257742:image/
sagemaker-distribution-gpu

ap-east-1
arn:aws:sagemaker:ap-east-1
:523751269255:image/
sagemaker-distribution-cpu

arn:aws:sagemaker:ap-east-1
:523751269255:image/
sagemaker-distribution-gpu

ap-south-1
arn:aws:sagemaker:ap-south-
1:245090515133:image/
sagemaker-distribution-cpu

arn:aws:sagemaker:ap-south-
1:245090515133:image/
sagemaker-distribution-gpu

Using the Code Editor
1515

## Page 545

Amazon SageMaker AI
Developer Guide

Region
CPU
GPU

ap-northeast-2
arn:aws:sagemaker:ap-northe
ast-2:064688005998:image/
sagemaker-distribution-cpu

arn:aws:sagemaker:ap-northe
ast-2:064688005998:image/
sagemaker-distribution-gpu

ap-southeast-1
arn:aws:sagemaker:ap-southe
ast-1:022667117163:image/
sagemaker-distribution-cpu

arn:aws:sagemaker:ap-southe
ast-1:022667117163:image/
sagemaker-distribution-gpu

ap-southeast-2
arn:aws:sagemaker:ap-southe
ast-2:648430277019:image/
sagemaker-distribution-cpu

arn:aws:sagemaker:ap-southe
ast-2:648430277019:image/
sagemaker-distribution-gpu

ap-northeast-1
arn:aws:sagemaker:ap-northe
ast-1:010972774902:image/
sagemaker-distribution-cpu

arn:aws:sagemaker:ap-northe
ast-1:010972774902:image/
sagemaker-distribution-gpu

ca-central-1
arn:aws:sagemaker:ca-centra
l-1:481561238223:image/
sagemaker-distribution-cpu

arn:aws:sagemaker:ca-centra
l-1:481561238223:image/
sagemaker-distribution-gpu

eu-central-1
arn:aws:sagemaker:eu-centra
l-1:545423591354:image/
sagemaker-distribution-cpu

arn:aws:sagemaker:eu-centra
l-1:545423591354:image/
sagemaker-distribution-gpu

eu-west-1
arn:aws:sagemaker:eu-west-1
:819792524951:image/
sagemaker-distribution-cpu

arn:aws:sagemaker:eu-west-1
:819792524951:image/
sagemaker-distribution-gpu

eu-west-2
arn:aws:sagemaker:eu-west-2
:021081402939:image/
sagemaker-distribution-cpu

arn:aws:sagemaker:eu-west-2
:021081402939:image/
sagemaker-distribution-gpu

eu-west-3
arn:aws:sagemaker:eu-west-3
:856416204555:image/
sagemaker-distribution-cpu

arn:aws:sagemaker:eu-west-3
:856416204555:image/
sagemaker-distribution-gpu

Using the Code Editor
1516

## Page 546

Amazon SageMaker AI
Developer Guide

Region
CPU
GPU

eu-north-1
arn:aws:sagemaker:eu-north-
1:175620155138:image/
sagemaker-distribution-cpu

arn:aws:sagemaker:eu-north-
1:175620155138:image/
sagemaker-distribution-gpu

eu-south-1
arn:aws:sagemaker:eu-south-
1:810671768855:image/
sagemaker-distribution-cpu

arn:aws:sagemaker:eu-south-
1:810671768855:image/
sagemaker-distribution-gpu

sa-east-1
arn:aws:sagemaker:sa-east-1
:567556641782:image/
sagemaker-distribution-cpu

arn:aws:sagemaker:sa-east-1
:567556641782:image/
sagemaker-distribution-gpu

ap-northeast-3
arn:aws:sagemaker:ap-northe
ast-3:564864627153:image/
sagemaker-distribution-cpu

arn:aws:sagemaker:ap-northe
ast-3:564864627153:image/
sagemaker-distribution-gpu

ap-southeast-3
arn:aws:sagemaker:ap-southe
ast-3:370607712162:image/
sagemaker-distribution-cpu

arn:aws:sagemaker:ap-southe
ast-3:370607712162:image/
sagemaker-distribution-gpu

me-south-1
arn:aws:sagemaker:me-south-
1:523774347010:image/
sagemaker-distribution-cpu

arn:aws:sagemaker:me-south-
1:523774347010:image/
sagemaker-distribution-gpu

me-central-1
arn:aws:sagemaker:me-centra
l-1:358593528301:image/
sagemaker-distribution-cpu

arn:aws:sagemaker:me-centra
l-1:358593528301:image/
sagemaker-distribution-gpu

il-central-1
arn:aws:sagemaker:il-centra
l-1:080319125002:image/
sagemaker-distribution-cpu

arn:aws:sagemaker:il-centra
l-1:080319125002:image/
sagemaker-distribution-gpu

cn-north-1
arn:aws:sagemaker:cn-north-
1:674439102856:image/
sagemaker-distribution-cpu

arn:aws:sagemaker:cn-north-
1:674439102856:image/
sagemaker-distribution-gpu

Using the Code Editor
1517

## Page 547

Amazon SageMaker AI
Developer Guide

Region
CPU
GPU

cn-northwest-1
arn:aws:sagemaker:cn-northw
est-1:651871951035:image/
sagemaker-distribution-cpu

arn:aws:sagemaker:cn-northw
est-1:651871951035:image/
sagemaker-distribution-gpu

us-gov-west-1
arn:aws:sagemaker:us-gov-we
st-1:300992924816:image/
sagemaker-distribution-cpu

arn:aws:sagemaker:us-gov-we
st-1:300992924816:image/
sagemaker-distribution-gpu

us-gov-east-1
arn:aws:sagemaker:us-gov-ea
st-1:300993876623:image/
sagemaker-distribution-cpu

arn:aws:sagemaker:us-gov-ea
st-1:300993876623:image/
sagemaker-distribution-gpu

If you encounter instance limits, contact your administrator. To get more storage and compute for
a user, administrators can request an increase to a user's AWS quotas. For more information about
requesting a quota increase, see Amazon SageMaker AI endpoints and quotas.

Launch a Code Editor application in Studio

To conﬁgure and access your Code Editor integrated development environment through Studio,
you must create a Code Editor space. For more information about spaces in Studio, see Amazon
SageMaker Studio spaces.

Using the Code Editor
1518

## Page 548

Amazon SageMaker AI
Developer Guide

![Page 548 Diagram 1](images/page-0548-img-01.png)

The following procedure shows how to create and run a Code Editor space.

To create and run a Code Editor space

1.
Launch the updated Studio experience. For more information, see Launch Amazon SageMaker
Studio.

2.
Do one of the following:

•
Within the updated Amazon SageMaker Studio UI, select Code Editor from the
Applications menu.

•
Within the updated Amazon SageMaker Studio UI, choose View Code Editor spaces in the
Overview section of the Studio homepage.

3.
In the upper-right corner of the Code Editor landing page, choose Create Code Editor space.

4.
Enter a name for your Code Editor space. The name must be 1–62 characters in length using
letters, numbers, and dashes only.

5.
Choose Create space.

6.
After the space is created, you have some options before you choose to run the space:

•
You can edit the Storage (GB), Lifecycle Conﬁguration, or Attach custom EFS ﬁle system
settings. Options for these settings are available based on administrator speciﬁcation.

Using the Code Editor
1519

## Page 549

Amazon SageMaker AI
Developer Guide

•
From the Instance dropdown menu, you can choose the instance type most compatible
with your use case. From the Image dropdown menu, you can choose a SageMaker
Distribution image or a custom image provided by your administrator.

Note

Switching between sagemaker-distribution images changes the underlying version
of Code Editor being used, which may cause incompatibilities due to browser
caching. You should clear the browser cache when switching between images.

If you use a GPU instance type when conﬁguring your Code Editor application, you must
also use a GPU-based image. Within a space, your data is stored in an Amazon EBS volume
that persists independently from the life of an instance. You won't lose your data when

you change instances.

Important

Custom IAM policies that allow Studio users to create spaces must also grant

permissions to list images (sagemaker: ListImage) to view custom images. To
add the permission, see  Add or remove identity permissions in the AWS Identity
and Access Management User Guide.
AWS managed policies for Amazon SageMaker AI that give permissions to create
SageMaker AI resources already include permissions to list images while creating
those resources.

Note

To update space settings, you must ﬁrst stop your space. If your Code Editor uses an
instance with NVMe instance stores, any data stored on the NVMe store is deleted
when the space is stopped.

7.
After updating your settings, choose Run Space in the space detail page.

8.
After the status of the space is Running, choose Open Code Editor to go to your Code Editor
session.

Using the Code Editor
1520

## Page 550

Amazon SageMaker AI
Developer Guide

![Page 550 Diagram 1](images/page-0550-img-01.png)

Launch a Code Editor application using the AWS CLI

To conﬁgure and access your Code Editor integrated development environment through the AWS
Command Line Interface (AWS CLI), you must create a Code Editor space. Be sure to meet the
Complete prerequisites before going through the following steps. Use the following procedure to
create and run a Code Editor space.

To create and run a Code Editor space

1.
Access a space using AWS Identity and Access Management (IAM) or AWS IAM Identity Center
authentication. For more information about accessing spaces using the AWS CLI, see Accessing
spaces using the AWS Command Line Interface in Amazon SageMaker Studio spaces.

2.
Create an application and specify CodeEditor as the app-type using the following
command.

If you use a GPU instance type when creating your Code Editor application, you must also use a
GPU-based image.

aws sagemaker create-app \
--domain-id domain-id \
--space-name space-name \
--app-type CodeEditor \

Using the Code Editor
1521

## Page 551

Amazon SageMaker AI
Developer Guide

--app-name default \
--resource-spec "SageMakerImageArn=arn:aws:sagemaker:region:account-
id:image/sagemaker-distribution-cpu"

For more information about available Code Editor image ARNs, see Code Editor application

instances and images.

3.
After the Code Editor application is in service, launch the application using a presigned URL.

You can use the describe-app API to check if your application is in service. Use the create-

presigned-domain-url API to create a presigned URL:

aws sagemaker create-presigned-domain-url \
--domain-id domain-id \
--space-name space-name \
--user-profile-name user-profile-name \
--session-expiration-duration-in-seconds 43200 \
--landing-uri app:CodeEditor:

4.
Open the generated URL to start working in your Code Editor application.

Clone a repository in Code Editor

You can navigate through folders and clone a repository in the Explorer window of the Code Editor
application UI.

To clone a repository, go through the following steps:

To clone a repository

1.
Open your Code Editor application in the browser, and choose the Exploration button

(

)
in the left navigation pane.

2.
Choose Clone Repository in the Explorer window. Then, provide a repository URL or pick a
repository source in the prompt.

3.
Choose a folder to clone your repository into. Note that the default Code Editor folder is /

home/sagemaker-user/. Cloning your repository may take some time.

4.
To open the cloned repository, choose either Open in New Window or Open.

5.
To return to the Code Editor application UI homepage, choose Cancel.

Using the Code Editor
1522

## Page 552

Amazon SageMaker AI
Developer Guide

6.
Within the repository, a prompt asks if you trust the authors of the ﬁles in your new repository.
You have two choices:

a.
To trust the folder and enable all features, choose Yes, I trust the authors.

b.
To browse the repository content in restricted mode, choose No, I don't trust the authors.

In restricted mode, tasks are not allowed to run, debugging is disabled, workspace settings
are not applied, and extensions have limited functionality.

To exit restricted mode, trust the authors of all ﬁles in your current folder or its parent
folder, and enable all features, choose Manage in the Restricted Mode banner.

Code Editor Connections and Extensions

Code Editor supports IDE connections to AWS services as well as extensions available in the Open
VSX Registry.

Connections to AWS

Code Editor environments are integrated with the AWS Toolkit for VS Code to add connections to
AWS services. To get started with connections to AWS services, you must have valid AWS Identity
and Access Management (IAM) credentials. For more information, see Authentication and access for
the AWS Toolkit for Visual Studio Code.

Within your Code Editor environment, you can add connections to:

• AWS Explorer  – View, modify, and deploy AWS resources in Amazon S3, CloudWatch, and more.

Accessing certain features within AWS Explorer requires certain AWS permissions. For more
information, see Authentication and access for the AWS Toolkit for Visual Studio Code.

• Amazon CodeWhisperer – Build applications faster with AI-powered code suggestions.

To use Amazon CodeWhisperer with Code Editor, you must add the following permissions to your
SageMaker AI execution role.

JSON

{
"Version":"2012-10-17",
"Statement": [
{

Using the Code Editor
1523

## Page 553

Amazon SageMaker AI
Developer Guide

"Sid": "CodeWhispererPermissions",
"Effect": "Allow",
"Action": ["codewhisperer:GenerateRecommendations"],
"Resource": "*"
}
]
}

For more information, see Creating IAM policies and Adding and removing IAM identity
permissions in the IAM User Guide.

Extensions

Code Editor supports IDE extensions available in the Open VSX Registry.

To get started with extensions in your Code Editor environment, choose the Extensions icon

(

)
in the left navigation pane. Here, you can conﬁgure connections to AWS by installing the AWS
Toolkit. For more information, see Installing the AWS Toolkit for Visual Studio Code.

In the search bar, you can search directly for additional extensions through the Open VSX Registry,
such as the AWS Toolkit, Jupyter, Python, and more.

Shut down Code Editor resources

When you're ﬁnished using a Code Editor space, you can use Studio to stop it. That way, you stop
incurring costs for the space.

Alternatively, you can delete unused Code Editor resources by using the AWS CLI.

Stop your Code Editor space using Studio

To stop your Code Editor space in Studio use the following steps:

To stop your Code Editor space in Studio

1.
Return to the Code Editor landing page by doing one of the following:

a.
In the navigation bar in the upper-left corner, choose Code Editor.

b.
Alternatively, in the left navigation pane, choose Code Editor in the Applications menu.

Using the Code Editor
1524

## Page 554

Amazon SageMaker AI
Developer Guide

2.
Find the name of the Code Editor space you created. If the status of your space is Running,
choose Stop in the Action column. You can also stop your space directly in the space detail
page by choosing Stop space. The space may take some time to stop.

![Page 554 Diagram 1](images/page-0554-img-01.png)

Additional resources such as SageMaker AI endpoints, Amazon EMR (Amazon EMR) clusters and
Amazon Simple Storage Service (Amazon S3) buckets created from Studio are not automatically
deleted when your space instance shuts down. To stop accruing charges from resources, delete any
additional resources. For more information, see Delete unused resources.

Delete Code Editor resources using the AWS CLI

You can delete your Code Editor application and space using the AWS Command Line Interface
(AWS CLI).

• DeleteApp

• DeleteSpace

Code Editor administrator guide

You can use Code Editor with an On-Demand Instance for faster start-up time, and conﬁgurable
storage. You can launch a Code Editor application through Amazon SageMaker Studio or through
the AWS CLI. You can also edit Code Editor default settings within the domain console. For more
information, see Edit domain settings. The following topics outline how administrators can
conﬁgure Code Editor, based on Code-OSS, Visual Studio Code - Open Source by changing storage
options, customizing environments, and managing user access, as well as giving information about
the prerequisites needed to use Code Editor.

Code Editor administrator guide
1525

## Page 555

Amazon SageMaker AI
Developer Guide

Topics

• Complete prerequisites

• Give your users access to private spaces

• Change the default storage size

• Code Editor lifecycle conﬁgurations

• Custom images

Complete prerequisites

To use Code Editor, based on Code-OSS, Visual Studio Code - Open Source, you must complete the
following prerequisites.

1.
You must ﬁrst onboard to Amazon SageMaker AI domain and create a user proﬁle. For more
information, see Amazon SageMaker AI domain overview.

2.
If you are interacting with your Code Editor application using the AWS CLI, you must also
complete the following prerequisites.

a.
Update the AWS CLI by following the steps in Installing the current AWS CLI Version.

b.
From your local machine, run aws configure and provide your AWS credentials. For
information about AWS credentials, see Understanding and getting your AWS credentials.

3.
(Optional) To get more storage and compute for your application, you can request an increase
to your AWS quotas. For more information about requesting a quota increase, see Amazon
SageMaker AI endpoints and quotas.

Give your users access to private spaces

Important

Custom IAM policies that allow Amazon SageMaker Studio or Amazon SageMaker Studio
Classic to create Amazon SageMaker resources must also grant permissions to add tags to
those resources. The permission to add tags to resources is required because Studio and
Studio Classic automatically tag any resources they create. If an IAM policy allows Studio
and Studio Classic to create resources but does not allow tagging, "AccessDenied" errors can
occur when trying to create resources. For more information, see Provide permissions for
tagging SageMaker AI resources.

Code Editor administrator guide
1526

## Page 556

Amazon SageMaker AI
Developer Guide

AWS managed policies for Amazon SageMaker AI that give permissions to create
SageMaker resources already include permissions to add tags while creating those
resources.

This section provides a policy that grants user access to private spaces. You can also use the policy
to restrict private spaces and applications that are associated with them to the owner associated
with the user proﬁle.

You must provide your users with permissions to the following:

• Private spaces

• The user proﬁle required for access to the private spaces

To provide permissions, attach the following policy to the IAM roles of your users.

JSON

{
"Version":"2012-10-17",
"Statement": [
{

"Effect": "Allow",
"Action": [
"sagemaker:CreateApp",
"sagemaker:DeleteApp"
],
"Resource": "arn:aws:sagemaker:us-east-1:111122223333:app/*",
"Condition": {
"Null": {
"sagemaker:OwnerUserProfileArn": "true"
}
}
},
{
"Sid": "SMStudioCreatePresignedDomainUrlForUserProfile",
"Effect": "Allow",
"Action": [
"sagemaker:CreatePresignedDomainUrl"

Code Editor administrator guide
1527

## Page 557

Amazon SageMaker AI
Developer Guide

],
"Resource": "arn:aws:sagemaker:us-east-1:111122223333:user-profile/domain-
id/user-profile-name"
},
{
"Sid": "SMStudioAppPermissionsListAndDescribe",
"Effect": "Allow",
"Action": [
"sagemaker:ListApps",
"sagemaker:ListDomains",
"sagemaker:ListUserProfiles",
"sagemaker:ListSpaces",
"sagemaker:DescribeApp",
"sagemaker:DescribeDomain",
"sagemaker:DescribeUserProfile",
"sagemaker:DescribeSpace"
],

"Resource": "*"
},
{
"Sid": "SMStudioAppPermissionsTagOnCreate",
"Effect": "Allow",
"Action": [
"sagemaker:AddTags"
],
"Resource": "arn:aws:sagemaker:us-east-1:111122223333:*/*",
"Condition": {
"Null": {
"sagemaker:TaggingAction": "false"
}
}
},
{
"Sid": "SMStudioRestrictSharedSpacesWithoutOwners",
"Effect": "Allow",
"Action": [
"sagemaker:CreateSpace",
"sagemaker:UpdateSpace",
"sagemaker:DeleteSpace"
],
"Resource": "arn:aws:sagemaker:us-east-1:111122223333:space/domain-id/*",
"Condition": {
"Null": {
"sagemaker:OwnerUserProfileArn": "true"

Code Editor administrator guide
1528

## Page 558

Amazon SageMaker AI
Developer Guide

}
}
},
{
"Sid": "SMStudioRestrictSpacesToOwnerUserProfile",
"Effect": "Allow",
"Action": [
"sagemaker:CreateSpace",
"sagemaker:UpdateSpace",
"sagemaker:DeleteSpace"
],
"Resource": "arn:aws:sagemaker:us-east-1:111122223333:space/domain-id/*",
"Condition": {
"ArnLike": {
"sagemaker:OwnerUserProfileArn": "arn:aws:sagemaker:us-
east-1:111122223333:user-profile/domain-id/user-profile-name"
},

"StringEquals": {
"sagemaker:SpaceSharingType": [
"Private",
"Shared"
]
}
}
},
{
"Sid": "SMStudioRestrictCreatePrivateSpaceAppsToOwnerUserProfile",
"Effect": "Allow",
"Action": [
"sagemaker:CreateApp",
"sagemaker:DeleteApp"
],
"Resource": "arn:aws:sagemaker:us-east-1:111122223333:app/domain-id/*",
"Condition": {
"ArnLike": {
"sagemaker:OwnerUserProfileArn": "arn:aws:sagemaker:us-
east-1:111122223333:user-profile/domain-id/user-profile-name"
},
"StringEquals": {
"sagemaker:SpaceSharingType": [
"Private"
]
}
}

Code Editor administrator guide
1529

## Page 559

Amazon SageMaker AI
Developer Guide

}
]
}

Change the default storage size

You can change the default storage settings of your users. You can also change the default storage
settings based on your organizational requirements and the needs of your users.

To change the storage size of your users, do the following:

1. Update the Amazon EBS storage settings in the domain.

2. Create a user proﬁle and specify the storage settings within it.

Use the following AWS Command Line Interface (AWS CLI) command to update the domain.

aws --region $REGION sagemaker update-domain \
--domain-id $DOMAIN_ID \
--default-user-settings '{
"SpaceStorageSettings": {
"DefaultEbsStorageSettings":{
"DefaultEbsVolumeSizeInGb":5,
"MaximumEbsVolumeSizeInGb":100
}
}
}'

Use the following AWS CLI command to create the user proﬁle and specify the default storage
settings.

aws --region $REGION sagemaker create-user-profile \
--domain-id $DOMAIN_ID \
--user-profile-name $USER_PROFILE_NAME \
--user-settings '{
"SpaceStorageSettings": {
"DefaultEbsStorageSettings":{
"DefaultEbsVolumeSizeInGb":5,
"MaximumEbsVolumeSizeInGb":100
}
}

Code Editor administrator guide
1530

## Page 560

Amazon SageMaker AI
Developer Guide

}'

Use the following AWS CLI commands to update the default storage settings in the user proﬁle.

aws --region $REGION sagemaker update-user-profile \

--domain-id $DOMAIN_ID \
--user-profile-name $USER_PROFILE_NAME \
--user-settings '{
"SpaceStorageSettings": {
"DefaultEbsStorageSettings":{
"DefaultEbsVolumeSizeInGb":25,
"MaximumEbsVolumeSizeInGb":200
}
}
}'

Code Editor lifecycle conﬁgurations

You can use Code Editor lifecycle conﬁgurations to automate customization for your Studio
environment. This customization includes installing custom packages, conﬁguring extensions,
preloading datasets, and setting up source code repositories

The following instructions use the AWS Command Line Interface (AWS CLI) to create, attach,

debug, and detach lifecycle conﬁgurations for the CodeEditor application type:

• Create and attach lifecycle conﬁgurations in Studio

• Debug lifecycle conﬁgurations in Studio

• Detach lifecycle conﬁgurations in Studio

Create and attach lifecycle conﬁgurations in Studio

The following section provides AWS CLI commands to create a lifecycle conﬁguration, attach a
lifecycle conﬁguration when creating a new user proﬁle, and attach a lifecycle conﬁguration when
updating a user proﬁle. For prerequisites and general steps on creating and attaching lifecycle
conﬁgurations in Studio, see Lifecycle conﬁguration creation.

When creating your Studio lifecycle conﬁguration with the create-studio-lifecycle-config

command, be sure to specify that the studio-lifecycle-config-app-type is CodeEditor.
The following example shows how to create a new Studio lifecycle conﬁguration for your Code
Editor application.

Code Editor administrator guide
1531

## Page 561

Amazon SageMaker AI
Developer Guide

aws sagemaker create-studio-lifecycle-config \
--studio-lifecycle-config-name my-code-editor-lcc \
--studio-lifecycle-config-content $LCC_CONTENT \
--studio-lifecycle-config-app-type CodeEditor

Note the ARN of the newly created lifecycle conﬁguration that is returned. When attaching

a lifecycle conﬁguration, provide this ARN within the LifecycleConfigArns list of

CodeEditorAppSettings.

You can attach a lifecycle conﬁguration when creating a user proﬁle or domain. The following
example shows how to create a new user proﬁle with the lifecycle conﬁguration attached. You
can also create a new domain with a lifecycle conﬁguration attached by using the create-domain
command.

# Create a new UserProfile
aws sagemaker create-user-profile \
--domain-id domain-id \
--user-profile-name user-profile-name \
--user-settings '{
"CodeEditorAppSettings": {
"LifecycleConfigArns":
[lifecycle-configuration-arn-list]
}
}'

You can alternatively attach a lifecycle conﬁguration when updating a user proﬁle or domain. The
following example shows how to update a user proﬁle with the lifecycle conﬁguration attached.
You can also update a new domain with a lifecycle conﬁguration attached by using the update-
domain command.

# Update a UserProfile
aws sagemaker update-user-profile \
--domain-id domain-id \
--user-profile-name user-profile-name \
--user-settings '{
"CodeEditorAppSettings": {
"LifecycleConfigArns":
[lifecycle-configuration-arn-list]
}
}'

Code Editor administrator guide
1532

## Page 562

Amazon SageMaker AI
Developer Guide

Debug lifecycle conﬁgurations in Studio

To debug lifecycle conﬁguration scripts for Code Editor, you must use Studio. For instructions
about debugging lifecycle conﬁgurations in Studio, see Debug lifecycle conﬁgurations. To ﬁnd the

logs for a speciﬁc application, search the log streams using the following format:

domain-id/space-name/CodeEditor/default/LifecycleConfigOnStart

Detach lifecycle conﬁgurations in Studio

To detach lifecycle conﬁgurations for Code Editor, you can use either the console or the AWS
CLI. For steps on detaching lifecycle conﬁgurations from the Studio console, see Detach lifecycle
conﬁgurations.

To detach a lifecycle conﬁguration using the AWS CLI, remove the desired lifecycle conﬁguration

from the list of lifecycle conﬁgurations attached to the resource. Then pass the list as part of the
respective command:

• update-user-proﬁle

• update-domain

For example, the following command removes all lifecycle conﬁgurations for the Code Editor
application attached to the domain.

aws sagemaker update-domain --domain-id domain-id \
--default-user-settings '{
"CodeEditorAppSettings": {
"LifecycleConfigArns":
[]
}
}'

Create a lifecycle conﬁguration to clone repositories into a Code Editor application

This section shows how to clone a repository and create a Code Editor application with the lifecycle
conﬁguration attached.

1.
From your local machine, create a ﬁle named my-script.sh with the following content:

#!/bin/bash

Code Editor administrator guide
1533

## Page 563

Amazon SageMaker AI
Developer Guide

set -eux

2.
Clone the repository of your choice in your lifecycle conﬁguration script.

export REPOSITORY_URL="https://github.com/aws-samples/sagemaker-studio-lifecycle-
config-examples.git"
git -C /home/sagemaker-user clone $REPOSITORY_URL

3.
After ﬁnalizing your script, create and attach your lifecycle conﬁguration. For more
information, see Create and attach lifecycle conﬁgurations in Studio.

4.
Create your Code Editor application with the lifecycle conﬁguration attached.

aws sagemaker create-app \
--domain-id domain-id \
--space-name space-name \

--app-type CodeEditor \
--app-name default \
--resource-spec "SageMakerImageArn=arn:aws:sagemaker:region:image-account-
id:image/sagemaker-distribution-
cpu,LifecycleConfigArn=arn:aws:sagemaker:region:user-account-id:studio-lifecycle-
config/my-code-editor-lcc,InstanceType=ml.t3.large"

For more information about available Code Editor image ARNs, see Code Editor application
instances and images.

Create a lifecycle conﬁguration to install Code Editor extensions

This section shows how to create a lifecycle conﬁguration to install extensions from the Open VSX
Registry in your Code Editor environment.

1.
From your local machine, create a ﬁle named my-script.sh with the following content:

#!/bin/bash
set -eux

2.
Within the script, install the Open VSX Registry extension of your choice:

sagemaker-code-editor --install-extension AmazonEMR.emr-tools --extensions-dir /
opt/amazon/sagemaker/sagemaker-code-editor-server-data/extensions

Code Editor administrator guide
1534

## Page 564

Amazon SageMaker AI
Developer Guide

You can retrieve the extension name from the URL of the extension in the Open VSX

Registry. The extension name to use in the sagemaker-code-editor command should

contain all text that follows https://open-vsx.org/extension/ in the URL. Replace all

instances of a slash (/) with a period (.). For example, AmazonEMR/emr-tools should be

AmazonEMR.emr-tools.

![Page 564 Diagram 1](images/page-0564-img-01.png)

3.
After ﬁnalizing your script, create and attach your lifecycle conﬁguration. For more
information, see Create and attach lifecycle conﬁgurations in Studio.

4.
Create your Code Editor application with the lifecycle conﬁguration attached:

aws sagemaker create-app \
--domain-id domain-id \
--space-name space-name \
--app-type CodeEditor \
--app-name default \
--resource-spec "SageMakerImageArn=arn:aws:sagemaker:region:image-account-
id:image/sagemaker-distribution-
cpu,LifecycleConfigArn=arn:aws:sagemaker:region:user-account-id:studio-lifecycle-
config/my-code-editor-lcc,InstanceType=ml.t3.large"

For more information about available Code Editor image ARNs, see Code Editor application
instances and images. For more information about connections and extensions, see Code
Editor Connections and Extensions.

Code Editor administrator guide
1535

## Page 565

Amazon SageMaker AI
Developer Guide

Custom images

If you need functionality that is diﬀerent than what's provided by SageMaker distribution, you can
bring your own image with your custom extensions and packages. You can also use it to personalize
the Code Editor UI for your own branding or compliance needs.

The following page will provide Code Editor-speciﬁc information and templates to create your
own custom SageMaker AI images. This is meant to supplement the Amazon SageMaker Studio
information and instructions on creating your own SageMaker AI image and bringing your own
image to Studio. To learn about custom Amazon SageMaker AI images and how to bring your own
image to Studio, see Bring your own image (BYOI).

Topics

• Health check and URL for applications

• Dockerﬁle examples

Health check and URL for applications

• Base URL – The base URL for the BYOI application must be CodeEditor/default. You can

only have one application and it must always be named default.

• Health check endpoint – You must host your Code Editor server at 0.0.0.0 port 8888 for
SageMaker AI to detect it.

• Authentication – You must pass --without-connection-token when opening sagemaker-

code-editor to allow SageMaker AI to authenticate your users.

Note

If you are using Amazon SageMaker Distribution as the base image, these requirements are

already taken care of as part of the included entrypoint-code-editor script.

Dockerﬁle examples

The following examples are Dockerfiles that meets the above information and Custom image
speciﬁcations.

Code Editor administrator guide
1536

## Page 566

Amazon SageMaker AI
Developer Guide

Note

If you are bringing your own image to SageMaker Uniﬁed Studio, you will need to follow
the Dockerﬁle speciﬁcations in the Amazon SageMaker Uniﬁed Studio User Guide.

Dockerfile examples for SageMaker Uniﬁed Studio can be found in Dockerﬁle example in
the Amazon SageMaker Uniﬁed Studio User Guide.

Example micromamba Dockerﬁle

The following is an example Dockerﬁle to create an image from scratch using a micromamba
base environment:

FROM mambaorg/micromamba:latest

ARG NB_USER="sagemaker-user"
ARG NB_UID=1000
ARG NB_GID=100

USER root

RUN micromamba install -y --name base -c conda-forge sagemaker-code-editor

USER $NB_UID

CMD eval "$(micromamba shell hook --shell=bash)"; \
micromamba activate base; \
sagemaker-code-editor --host 0.0.0.0 --port 8888 \
--without-connection-token \
--base-path "/CodeEditor/default"

Example SageMaker AI Distribution Dockerﬁle

The following is an example Dockerﬁle to create an image based on Amazon SageMaker AI
Distribution:

FROM public.ecr.aws/sagemaker/sagemaker-distribution:latest-cpu
ARG NB_USER="sagemaker-user"
ARG NB_UID=1000
ARG NB_GID=100
ENV MAMBA_USER=$NB_USER

Code Editor administrator guide
1537

## Page 567

Amazon SageMaker AI
Developer Guide

USER root

# install scrapy in the base environment
RUN micromamba install -y --name base -c conda-forge scrapy

# download VSCodeVim
RUN \
wget https://github.com/VSCodeVim/Vim/releases/download/v1.27.2/vim-1.27.2.vsix \
-P /tmp/exts/ --no-check-certificate

# Install the extension
RUN \
extensionloc=/opt/amazon/sagemaker/sagemaker-code-editor-server-data/extensions \
&& sagemaker-code-editor \
--install-extension "/tmp/exts/vim-1.27.2.vsix" \
--extensions-dir "${extensionloc}"

USER $MAMBA_USER
ENTRYPOINT ["entrypoint-code-editor"]

Amazon SageMaker HyperPod

SageMaker HyperPod helps you provision resilient clusters for running machine learning (ML)
workloads and developing state-of-the-art models such as large language models (LLMs),
diﬀusion models, and foundation models (FMs). It accelerates development of FMs by removing
undiﬀerentiated heavy-lifting involved in building and maintaining large-scale compute clusters
powered by thousands of accelerators such as AWS Trainium and NVIDIA A100 and H100 Graphical
Processing Units (GPUs). When accelerators fail, the resiliency features of SageMaker HyperPod
monitor the cluster instances automatically detect and replace the faulty hardware on the ﬂy so
that you can focus on running ML workloads.

To get started, check the section called “Prerequisites”, set up the section called “IAM for
HyperPod”, and choose one of the following orchestrator options supported by SageMaker
HyperPod.

Slurm support in SageMaker HyperPod

SageMaker HyperPod provides support for running machine learning workloads on resilient
clusters by integrating with Slurm, an open-source workload manager. Slurm support in SageMaker
HyperPod enables seamless cluster orchestration through Slurm cluster conﬁguration, allowing

HyperPod
1538

## Page 568

Amazon SageMaker AI
Developer Guide

you to set up head, login, and worker nodes on the SageMaker HyperPod clusters This integration
also facilitates Slurm-based job scheduling for running ML workloads on the cluster, as well as
direct access to cluster nodes for job scheduling. With HyperPod's lifecycle conﬁguration support,
you can customize the computing environment of the clusters to meet your speciﬁc requirements.
Additionally, by leveraging the Amazon SageMaker AI distributed training libraries, you can
optimize the clusters' performance on AWS computing and network resources. To learn more, see
the section called “Slurm orchestration”.

Amazon EKS support in SageMaker HyperPod

SageMaker HyperPod also integrates with Amazon EKS to enable large-scale training of foundation
models on long-running and resilient compute clusters. This allows cluster admin users to
provision HyperPod clusters and attach them to an EKS control plane, enabling dynamic capacity
management, direct access to cluster instances, and resiliency capabilities. For data scientists,
Amazon EKS support in HyperPod allows running containerized workloads for training foundation
models, inference on the EKS cluster, and leveraging the job auto-resume capability for Kubeﬂow
PyTorch training. The architecture involves a 1-to-1 mapping between an EKS cluster (control
plane) and a HyperPod cluster (worker nodes) within a VPC, providing a tightly integrated
solution for running large-scale ML workloads. To learn more, see the section called “Amazon EKS
orchestration”.

UltraServers with HyperPod

HyperPod with UltraServers delivers AI computing power by integrating NVIDIA superchips into
a cohesive, high-performance infrastructure. Each NVL72 UltraServer combines 18 instances
with 72 NVIDIA Blackwell GPUs interconnected via NVLink, enabling faster inference and faster
training performance compared to previous generation instances. This architecture is particularly
valuable for organizations working with trillion-parameter foundation models, as the uniﬁed GPU
memory allows entire models to remain within a single NVLink domain, eliminating cross-node
networking bottlenecks. HyperPod enhances this hardware advantage with intelligent topology-
aware scheduling that optimizes workload placement, automatic instance replacement to minimize
disruptions, and ﬂexible deployment options that support both dedicated and shared resource
conﬁgurations. For teams pushing the boundaries of model size and performance, this integration
provides the computational foundation needed to train and deploy the most advanced AI models
with unprecedented eﬃciency.

SageMaker HyperPod automatically optimizes instance placement across your UltraServers. By
default, HyperPod prioritizes all instances in one UltraServer before using a diﬀerent one. For
example, if you want 14 instances and have 2 UltraServers in your plan, SageMaker AI uses all of

HyperPod
1539

## Page 569

Amazon SageMaker AI
Developer Guide

the instances in the ﬁrst UltraServer. If you want 20 instances, SageMaker AI uses all 18 instances in
the ﬁrst UltraServer and then uses 2 more from the second.

AWS Regions supported by SageMaker HyperPod

SageMaker HyperPod is available in the following AWS Regions.

• us-east-1

• us-east-2

• us-west-1

• us-west-2

• eu-central-1

• eu-north-1

• eu-west-1

• eu-west-2

• eu-south-2

• ap-south-1

• ap-southeast-1

• ap-southeast-2

• ap-southeast-3

• ap-southeast-4

• ap-northeast-1

• sa-east-1

Topics

• Amazon SageMaker HyperPod quickstart

• Prerequisites for using SageMaker HyperPod

• AWS Identity and Access Management for SageMaker HyperPod

• Customer managed AWS KMS key encryption for SageMaker HyperPod

• SageMaker HyperPod recipes

• Orchestrating SageMaker HyperPod clusters with Slurm

• Orchestrating SageMaker HyperPod clusters with Amazon EKS

AWS Regions supported by SageMaker HyperPod
1540

## Page 570

Amazon SageMaker AI
Developer Guide

• Using topology-aware scheduling in Amazon SageMaker HyperPod

• Deploying models on Amazon SageMaker HyperPod

• HyperPod in Studio

• SageMaker HyperPod references

• Amazon SageMaker HyperPod release notes

• Amazon SageMaker HyperPod AMI

Amazon SageMaker HyperPod quickstart

This quickstart guides you through creating your ﬁrst HyperPod cluster with Slurm and Amazon
EKS (EKS) orchestrations. Choose the orchestration that best ﬁts your infrastructure needs to get
started with SageMaker HyperPod.

In this topic:

• Create a Slurm-orchestrated SageMaker HyperPod cluster

• Create an EKS-orchestrated SageMaker HyperPod cluster

• Submit workloads

Create a Slurm-orchestrated SageMaker HyperPod cluster

Follow these steps to create your ﬁrst SageMaker HyperPod cluster with Slurm orchestration.

1.
Open the Amazon SageMaker AI console at https://console.aws.amazon.com/sagemaker/.

2.
Choose HyperPod Clusters in the left navigation pane and then Cluster Management.

3.
On the SageMaker HyperPod Clusters page, choose Create HyperPod cluster.

4.
On the Create HyperPod cluster drop-down, choose Orchestrated by Slurm.

5.
On the cluster creation page, choose Quick setup. With this option, you get started
immediately with default settings. SageMaker AI will create new resources such as VPC,
subnets, security groups, Amazon S3 bucket, IAM role, and FSx for Lustre in the process of
creating your cluster.

6.
On General settings, specify a name for the new cluster. You can’t change the name after the
cluster is created.

7.
On Instance groups, choose Add group. Each instance group can be conﬁgured diﬀerently,
and you can create a heterogeneous cluster that consists of multiple instance groups with

Quickstart
1541

## Page 571

Amazon SageMaker AI
Developer Guide

various instance types. To deploy a cluster, you must add at least one instance group. You can
add one instance group at a time. To create multiple instance groups, repeat the process for
each instance group.

Follow these steps to add an instance group.

a.
For Instance group type, choose a type for your instance group. For this quickstart,

choose Controller (head) for my-controller-group, Login for my-login-group, and

Compute (worker) for worker-group-1.

b.
For Name, specify a name for the instance group. For this quickstart, create three instance

groups named my-controller-group, my-login-group, and worker-group-1.

c.
For Instance capacity, choose either on-demand capacity or a training plan to reserve
your compute resources.

d.
For Instance type, choose the instance for the instance group. For this quickstart, select

ml.c5.xlarge for my-controller-group, ml.m5.4xlarge for my-login-group,

and ml.trn1.32xlarge for worker-group-1.

Ensure that you choose the instance type with suﬃcient quotas in your account, or request
additional quotas by following the instructions at the section called “SageMaker HyperPod
quotas”.

e.
For Instance quantity, specify an integer not exceeding the instance quota for cluster
usage. For this quickstart, enter 1 for all three groups.

f.
For Target Availability Zone, choose the Availability Zone where your instances will be
provisioned. The Availability Zone should correspond to the location of your accelerated
compute capacity.

g.
For Additional storage volume per instance (GB) - optional, specify an integer between
1 and 16384 to set the size of an additional Elastic Block Store (EBS) volume in gigabytes
(GB). The EBS volume is attached to each instance of the instance group. The default

mount path for the additional EBS volume is /opt/sagemaker. After the cluster is
successfully created, you can SSH into the cluster instances (nodes) and verify if the EBS

volume is mounted correctly by running the df -h command. Attaching an additional
EBS volume provides stable, oﬀ-instance, and independently persisting storage, as
described in the Amazon EBS volumes section in the Amazon Elastic Block Store User
Guide.

h.
Choose Add instance group.

Quickstart
1542

## Page 572

Amazon SageMaker AI
Developer Guide

8.
On Quick conﬁguration defaults, review the default settings. This section lists all the default
settings for your cluster creation, including all the new AWS resources that will be created
during the cluster creation process.

9.
Choose Submit.

For more information, see ???.

Create an EKS-orchestrated SageMaker HyperPod cluster

Follow these steps to create your ﬁrst SageMaker HyperPod cluster with Amazon EKS
orchestration.

1.
Open the Amazon SageMaker AI console at https://console.aws.amazon.com/sagemaker/.

2.
Choose HyperPod Clusters in the left navigation pane and then Cluster Management.

3.
On the SageMaker HyperPod Clusters page, choose Create HyperPod cluster.

4.
On the Create HyperPod cluster drop-down, choose Orchestrated by Amazon EKS.

5.
On the cluster creation page, choose Quick conﬁguration. With this option, you can get
started immediately with default settings. SageMaker AI will create new resources such as VPC,
subnets, security groups, Amazon S3 bucket, IAM role, and FSx for Lustre in the process of
creating your cluster.

6.
On General settings, specify a name for the new cluster. You can’t change the name after the
cluster is created.

7.
On Instance groups, choose Add group. Each instance group can be conﬁgured diﬀerently,
and you can create a heterogeneous cluster that consists of multiple instance groups with
various instance types. To deploy a cluster, you must add at least one instance group. You can
add one instance group at a time. To create multiple instance groups, repeat the process for
each instance group.

Follow these steps to add an instance group.

a.
For Instance group type, choose Standard or Restricted Instance Group (RIG). Typically,
you will choose Standard, which provides a general purpose computing environment
without additional security restrictions. Restricted Instance Group (RIG) is a specialized
environment for foundational models customization such as Amazon Nova. For more
information about setting up RIG for Amazon Nova model customization, see ???.

b.
For Name, specify a name for the instance group.

Quickstart
1543

## Page 573

Amazon SageMaker AI
Developer Guide

c.
For Instance capacity, choose either on-demand capacity or a training plan to reserve
your compute resources.

d.
For Instance type, choose the instance for the instance group. Ensure that you choose
the instance type with suﬃcient quotas in your account, or request additional quotas by
following at the section called “SageMaker HyperPod quotas”.

e.
For Instance quantity, specify an integer not exceeding the instance quota for cluster
usage. For this quickstart, enter 1 for all three groups.

f.
For Target Availability Zone, choose the Availability Zone where your instances will be
provisioned. The Availability Zone should correspond to the location of your accelerated
compute capacity.

g.
For Additional storage volume per instance (GB) - optional, specify an integer between
1 and 16384 to set the size of an additional Elastic Block Store (EBS) volume in gigabytes
(GB). The EBS volume is attached to each instance of the instance group. The default

mount path for the additional EBS volume is /opt/sagemaker. After the cluster is
successfully created, you can SSH into the cluster instances (nodes) and verify if the EBS

volume is mounted correctly by running the df -h command. Attaching an additional
EBS volume provides stable, oﬀ-instance, and independently persisting storage, as
described in the Amazon EBS volumes section in the Amazon Elastic Block Store User
Guide.

h.
For Instance deep health checks, choose your option. Deep health checks monitor
instance health during creation and after software updates, automatically recovering
faulty instances through reboots or replacements when enabled.

i.
Choose Add instance group.

8.
On Quick conﬁguration defaults, review the default settings. This section lists all the default
settings for your cluster creation, including all the new AWS resources that will be created
during the cluster creation process.

9.
Choose Submit.

For more information, see ???.

Submit workloads

Follow these workshop tutorials to submit sample workloads.

• Amazon SageMaker HyperPod for Slurm

Quickstart
1544

## Page 574

Amazon SageMaker AI
Developer Guide

• Amazon SageMaker HyperPod for Amazon EKS

Prerequisites for using SageMaker HyperPod

The following sections walk you through prerequisites before getting started with SageMaker
HyperPod.

Topics

• SageMaker HyperPod quotas

• Setting up SageMaker HyperPod with a custom Amazon VPC

• Setting up SageMaker HyperPod clusters across multiple AZs

• Setting up AWS Systems Manager and Run As for cluster user access control

• (Optional) Setting up SageMaker HyperPod with Amazon FSx for Lustre

SageMaker HyperPod quotas

You can create SageMaker HyperPod clusters given the quotas for cluster usage in your AWS
account.

Important

To learn more about SageMaker HyperPod pricing, see the section called “SageMaker
HyperPod pricing” and Amazon SageMaker Pricing.

View Amazon SageMaker HyperPod quotas using the AWS Management Console

Look up the default and applied values of a quota, also referred to as a limit, for cluster usage,
which is used for SageMaker HyperPod.

1. Open the Service Quotas console.

2. In the left navigation pane, choose AWS services.

3. From the AWS services list, search for and select Amazon SageMaker AI.

4. In the Service quotas list, you can see the service quota name, applied value (if it's available),

AWS default quota, and whether the quota value is adjustable.

Prerequisites
1545

## Page 575

Amazon SageMaker AI
Developer Guide

5. In the search bar, type cluster usage. This shows quotas for cluster usage, applied quotas, and

the default quotas.

List of common service quotas to create a HyperPod cluster and its pre-requisites

You might want to check if you have requested service quota limit increases for the following
quotas to create a new HyperPod cluster along with pre-requisites in the SageMaker AI console.
Navigate to the Service Quota console and search for the following terms.

No Quota Name
Search Term
Description

1
Maximum
number
instances
allowed per
SageMaker
HyperPod
cluster

Under SageMaker AI search
for “Maximum number
instances allowed per
SageMaker HyperPod
cluster”

Your account-level quota value must be
more than the number of instance you wish
to add to your cluster

2
Maximum size
of EBS volume
in GB for a
SageMaker
HyperPod
cluster
instance

Under SageMaker AI search
for “Maximum size of
EBS volume in GB for a
HyperPod cluster instance”

Your account-level quota value must be
more than the EBS volume you wish to add
to your cluster

3
Total number
of instances
allowed across
SageMaker
HyperPod
clusters

Under SageMaker AI search
for  “Total number of
instances allowed across
SageMaker HyperPod
clusters”

Your account-level quota value must be
more than the total instances you wish to
add across all your clusters in your account
in aggregate

4
Instance
Quotas

Under SageMaker AI search
for  "ml.<instance_type> for
cluster usage" eg: ml.p5.48x
large for cluster usage

Your account-level quota value for the
particular instance type (eg: ml.p5.48x
large) must be greater than the number of

Prerequisites
1546

## Page 576

Amazon SageMaker AI
Developer Guide

No Quota Name
Search Term
Description

instances to add across all your clusters in
your account in aggregate.

5
VPCs per
Region

Under Amazon Virtual
Private Cloud (Amazon
VPC) search for “VPCs per
Region"

Your account-level quota value must be
enough to create a new VPC in the account
when setting up your HyperPod cluster. Do
check if you have already exhausted this
quota limit by checking the VPC console.
This quota increase is only needed if
you will create a new VPC via the Quick
or Custom cluster setup option in the
SageMaker HyperPod console.

6
Internet
gateways per
Region

Under Amazon Virtual
Private Cloud (Amazon
VPC) search for “Internet
gateways per Region"

Your account-level quota value must be
enough to create one additional Internet
gateway in the account when setting up
your SageMaker HyperPod cluster. This
quota increase is only needed if you will
create a new VPC via the Quick or Custom
cluster setup option in the SageMaker
HyperPod console.

7
Network
interfaces per
Region

Under Amazon Virtual
Private Cloud (Amazon
VPC) search for “Network
interfaces per Region"

Your account-level quota value must have
enough Network Interfaces in the account
when setting up your HyperPod cluster.

8
EC2-VPC
Elastic IPs

Under Amazon Elastic
Compute Cloud (Amazon
EC2) search for “EC2-VPC
Elastic IPs"

Your account-level quota value must
be enough to create a new VPC in the
account when setting up your HyperPod
cluster. Do check whether you have already
exhausted this quota limit by checking the
VPC console. This quota increase is only
needed if you will create a new VPC via the
Quick or Custom cluster setup option in the
SageMaker HyperPod console.

Prerequisites
1547

## Page 577

Amazon SageMaker AI
Developer Guide

Request a Amazon SageMaker HyperPod quota increase using the AWS Management Console

Increase your quotas at the account or resource level.

1. To increase the quota of instances for cluster usage, select the quota that you want to increase.

2. If the quota is adjustable, you can request a quota increase at either the account level or

resource level based on the value listed in the Adjustability column.

3. For Increase quota value, enter the new value. The new value must be greater than the current

value.

4. Choose Request.

5. To view any pending or recently resolved requests in the console, navigate to the Request

history tab from the service's details page, or choose Dashboard from the navigation pane. For
pending requests, choose the status of the request to open the request receipt. The initial status
of a request is Pending. After the status changes to Quota requested, you see the case number

with AWS Support. Choose the case number to open the ticket for your request.

To learn more about requesting a quota increase in general, see Requesting a Quota Increase in the
AWS Service Quotas User Guide.

Setting up SageMaker HyperPod with a custom Amazon VPC

To set up a SageMaker HyperPod cluster with a custom Amazon VPC, review the following
prerequisites.

Note

VPC conﬁguration is mandatory for Amazon EKS orchestration. For Slurm orchestration,
VPC setup is optional.

• Validate Elastic Network Interface (ENI) capacity in your AWS account before creating a
SageMaker HyperPod cluster with a custom VPC. The ENI limit is controlled by Amazon EC2 and
varies by AWS Region. SageMaker HyperPod cannot automatically request quota increases.

To verify your current ENI quota:

1.
Open the Service Quotas console.

2.
In the Manage quotas section, use the  AWS Services drop-down list to search for VPC.

Prerequisites
1548

## Page 578

Amazon SageMaker AI
Developer Guide

3.
Choose to view the quotas of Amazon Virtual Private Cloud (Amazon VPC).

4.
Look for the service quota Network interfaces per Region or the Quota code L-DF5E4CA3.

If your current ENI limit is insuﬃcient for your SageMaker HyperPod cluster needs, request a
quota increase. Ensuring adequate ENI capacity beforehand helps prevent cluster deployment
failures.

• When using a custom VPC to connect a SageMaker HyperPod cluster with AWS resources, provide
the VPC name, ID, AWS Region, subnet IDs, and security group IDs during cluster creation.

Note

When your Amazon VPC and subnets support IPv6 in the VPCConfig of the

cluster or at the Instance group level using the OverrideVPCConfig attribute of

ClusterInstanceGroupSpecification, network communications diﬀer based on
the cluster orchestration platform:

• Slurm-orchestrated clusters automatically conﬁgure nodes with dual IPv6 and
IPv4 addresses, allowing immediate IPv6 network communications. No additional

conﬁguration is required beyond the VPCConfig IPv6 settings.

• In EKS-orchestrated clusters, nodes receive dual-stack addressing, but pods can only
use IPv6 when the Amazon EKS cluster is explicitly IPv6-enabled. You must create a
new IPv6 Amazon EKS cluster - existing IPv4 Amazon EKS clusters cannot be converted
to IPv6. For information about deploying an IPv6 Amazon EKS cluster, see Amazon EKS
IPv6 Cluster Deployment.
Additional resources for IPv6 conﬁguration:

• For information about adding IPv6 support to your VPC, see to IPv6 Support for VPC.

• For information about creating a new IPv6-compatible VPC, see Amazon VPC Creation
Guide.

• To conﬁgure SageMaker HyperPod with a custom Amazon VPC, see Custom Amazon
VPC setup for SageMaker HyperPod.

• Make sure that all resources are deployed in the same AWS Region as the SageMaker HyperPod
cluster. Conﬁgure security group rules to allow inter-resource communication within the VPC. For

example, when creating a VPC in us-west-2, provision subnets across one or more Availability

Zones (such as us-west-2a or us-west-2b), and create a security group allowing intra-group
traﬃc.

Prerequisites
1549

## Page 579

Amazon SageMaker AI
Developer Guide

Note

SageMaker HyperPod supports multi-Availability Zone deployment. For more
information, see the section called “Setting up SageMaker HyperPod clusters across
multiple AZs”.

• Establish Amazon Simple Storage Service (Amazon S3) connectivity for VPC-deployed SageMaker
HyperPod instance groups by creating a VPC endpoint. Without internet access, instance groups
cannot store or retrieve lifecycle scripts, training data, or model artifacts. We recommend that
you create a custom IAM policy restricting Amazon S3 bucket access to the private VPC. For more
information, see Endpoints for Amazon S3 in the AWS PrivateLink Guide.

• For HyperPod clusters using Elastic Fabric Adapter (EFA)-enabled instances, conﬁgure the
security group to allow all inbound and outbound traﬃc to and from the security group itself.

Speciﬁcally, avoid using 0.0.0.0/0 for outbound rules, as this may cause EFA health check
failures. For more information about EFA security group preparation guidelines, see Step 1:
Prepare an EFA-enabled security group in the Amazon EC2 User Guide.

• Consider your subnet's Classless Inter-Domain Routing (CIDR) block size carefully before creating
HyperPod clusters.

• The subnet CIDR block size cannot be changed after creation. This is especially important
when you use large accelerated instances like P5. Without suﬃcient block size, you must
recreate your clusters when scaling up.

• When choosing the appropriate subnet CIDR block size, consider these factors: your instance
types, expected number of instances, and the number of IP addresses consumed by each
instance.

• For Slurm-orchestrated clusters, each P5 instance can create 32 IP addresses (one per network
card). For EKS-orchestrated clusters, each P5 instance can create 81 IP addresses (50 from the
primary card plus one from each of the remaining 31 cards). For detailed speciﬁcations, see
Network speciﬁcations  from Amazon EC2 Instance Types Developer Guide.

• For examples of CloudFormation templates that specify the subnet CIDR block size, see the
HyperPod Slurm template and HyperPod Amazon EKS template in the awsome-distributed-
training repository.

Prerequisites
1550

## Page 580

Amazon SageMaker AI
Developer Guide

Setting up SageMaker HyperPod clusters across multiple AZs

You can conﬁgure your SageMaker HyperPod clusters across multiple Availability Zones (AZs) to
improve reliability and availability.

Note

Elastic Fabric Adapter (EFA) traﬃc cannot cross AZs or VPCs. This does not apply to
normal IP traﬃc from the ENA device of an EFA interface. For more information, see EFA
limitations.

• Default behavior

HyperPod deploys all cluster instances in a single Availability Zone. The VPC conﬁguration
determines the deployment AZ:

• For Slurm-orchestrated clusters, VPC conﬁguration is optional. When no VPC conﬁguration is
provided, HyperPod defaults to one subnet from the platform VPC.

• For EKS-orchestrated clusters, VPC conﬁguration is required.

• For both Slurm and EKS orchestrators, when VpcConfig is provided, HyperPod selects a

subnet from the provided VpcConfig's subnet list. All instance groups inherit the subnet's AZ.

Note

Once you create a cluster, you cannot modify its VpcConfig settings.

To learn more about conﬁguring VPCs for HyperPod clusters, see the preceding section, Setting
up SageMaker HyperPod with a custom Amazon VPC.

• Multi-AZ conﬁguration

You can set up your HyperPod cluster across multiple AZs when creating a cluster or when
adding a new instance group to an existing cluster. To conﬁgure multi-AZ deployments, you
can override the default VPC settings of the cluster by specifying diﬀerent subnets and security
groups, potentially across diﬀerent Availability Zones, for individual instance groups within your
cluster.

Prerequisites
1551

## Page 581

Amazon SageMaker AI
Developer Guide

SageMaker HyperPod API users can use the OverrideVpcConfig property within the

ClusterInstanceGroupSpeciﬁcation when working with the CreateCluster or UpdateCluster

APIs.

The OverrideVpcConfig ﬁeld:

• Cannot be modiﬁed after the instance group is created.

• Is optional. If not speciﬁed, the cluster level VpcConfig is used as default.

• For Slurm-orchestrated clusters, can only be speciﬁed when cluster level VpcConfig is

provided. If no VpcConfig is speciﬁed at cluster level, OverrideVpcConfig cannot be used
for any instance group.

• Contains two required ﬁelds:

• Subnets - accepts between 1 and 16 subnet IDs

• SecurityGroupIds - accepts between 1 and 5 security group IDs

For more information about creating or updating a SageMaker HyperPod cluster using the
SageMaker HyperPod console UI or the AWS CLI:

• Slurm orchestration: See Operating Slurm-orchestrated HyperPod clusters.

• EKS orchestration. See Operating EKS-orchestrated HyperPod clusters.

Note

When running workloads across multiple AZs, be aware that network communication
between AZs introduces additional latency. Consider this impact when designing latency-
sensitive applications.

Setting up AWS Systems Manager and Run As for cluster user access control

the section called “SageMaker HyperPod DLAMI” comes with AWS Systems Manager (SSM) out
of the box to help you manage access to your SageMaker HyperPod cluster instance groups. This
section describes how to create operating system (OS) users in your SageMaker HyperPod clusters
and associate them with IAM users and roles. This is useful to authenticate SSM sessions using the
credentials of the OS user account.

Prerequisites
1552

## Page 582

Amazon SageMaker AI
Developer Guide

Note

Granting users access to HyperPod cluster nodes allows them to install and operate user-
managed software on the nodes. Ensure that you maintain the principle of least-privilege
permissions for users.

Enabling Run As in your AWS account

As an AWS account admin or a cloud administrator, you can manage access to SageMaker
HyperPod clusters at an IAM role or user level by using the Run As feature in SSM. With this
feature, you can start each SSM session using the OS user associated to the IAM role or user.

To enable Run As in your AWS account, follow the steps in Turn on Run As support for Linux
and macOS managed nodes. If you already created OS users in your cluster, make sure that you
associate them with IAM roles or users by tagging them as guided in Option 2 of step 5 under To
turn on Run As support for Linux and macOS managed nodes.

(Optional) Setting up SageMaker HyperPod with Amazon FSx for Lustre

To start using SageMaker HyperPod and mapping data paths between the cluster and your FSx
for Lustre ﬁle system, select one of the AWS Regions supported by SageMaker HyperPod. After
choosing the AWS Region you prefer, you also should determine which Availability Zone (AZ) to
use.

If you use SageMaker HyperPod compute nodes in AZs diﬀerent from the AZs where your FSx
for Lustre ﬁle system is set up within the same AWS Region, there might be communication
and network overhead. We recommend that you to use the same physical AZ as the one for the
SageMaker HyperPod service account to avoid any cross-AZ traﬃc between SageMaker HyperPod
clusters and your FSx for Lustre ﬁle system. Also, make sure that you have conﬁgured it with
your VPC. If you want to use Amazon FSx as the main ﬁle system for storage, you must conﬁgure
SageMaker HyperPod clusters with your VPC.

AWS Identity and Access Management for SageMaker HyperPod

AWS Identity and Access Management (IAM) is an AWS service that helps an administrator securely
control access to AWS resources. IAM administrators control who can be authenticated (signed in)
and authorized (have permissions) to use Amazon EKS resources. IAM is an AWS service that you
can use with no additional charge.

IAM for HyperPod
1553

## Page 583

Amazon SageMaker AI
Developer Guide

Important

Custom IAM policies that allow Amazon SageMaker Studio or Amazon SageMaker Studio
Classic to create Amazon SageMaker resources must also grant permissions to add tags to
those resources. The permission to add tags to resources is required because Studio and
Studio Classic automatically tag any resources they create. If an IAM policy allows Studio
and Studio Classic to create resources but does not allow tagging, "AccessDenied" errors can
occur when trying to create resources. For more information, see Provide permissions for
tagging SageMaker AI resources.
AWS managed policies for Amazon SageMaker AI that give permissions to create
SageMaker resources already include permissions to add tags while creating those
resources.

Let's assume that there are two main layers of SageMaker HyperPod users: cluster admin users and
data scientist users.

• Cluster admin users – Are responsible for creating and managing SageMaker HyperPod clusters.
This includes conﬁguring the HyperPod clusters and managing user access to them.

• Create and conﬁgure SageMaker HyperPod clusters with Slurm or Amazon EKS.

• Create and conﬁgure IAM roles for data scientist users and HyperPod cluster resources.

• For SageMaker HyperPod orchestration with Amazon EKS, create and conﬁgure EKS access
entries, role-based access control (RBAC), and Pod Identity to fulﬁll data science use cases.

• Data scientist users – Focus on ML model training. They use the open-source orchestrator or the
SageMaker HyperPod CLI to submit and manage training jobs.

• Assume and use the IAM Role provided by cluster admin users.

• Interact with the open-source orchestrator CLIs supported by SageMaker HyperPod (Slurm or
Kubernetes) or the SageMaker HyperPod CLI to check clusters capacity, connect to cluster, and
submit workloads.

Set up IAM roles for cluster admins by attaching the right permissions or policies to operate
SageMaker HyperPod clusters. Cluster admins also should create IAM roles to provide to SageMaker
HyperPod resources to assume to run and communicate with necessary AWS resources, such as
Amazon S3, Amazon CloudWatch, and AWS Systems Manager (SSM). Finally, the AWS account

IAM for HyperPod
1554

## Page 584

Amazon SageMaker AI
Developer Guide

admin or the cluster admins should grant scientists permissions to access the SageMaker HyperPod
clusters and run ML workloads.

Depending on which orchestrator you choose, permissions needed for the cluster admin and
scientists may vary. You can also control the scope of permissions for various actions in the roles
using the condition keys per service. Use the following Service Authorization References for adding
detailed scope for the services related to SageMaker HyperPod.

• Amazon Elastic Compute Cloud

• Amazon Elastic Container Registry (for SageMaker HyperPod cluster orchestration with Amazon
EKS)

• Amazon Elastic Kubernetes Service (for SageMaker HyperPod cluster orchestration with Amazon
EKS)

• Amazon FSx

• AWS IAM Identity Center (successor to AWS Single Sign-On)

• AWS Identity and Access Management (IAM)

• Amazon Simple Storage Service

• Amazon SageMaker AI

• AWS Systems Manager

Topics

• IAM permissions for cluster creation

• IAM users for cluster admin

• IAM users for scientists

• IAM role for SageMaker HyperPod

IAM permissions for cluster creation

Creating HyperPod clusters requires the IAM permissions outlined in the following policy example.

If your AWS account has AdministratorAccess permissions, these permissions are granted by
default.

IAM for HyperPod
1555

## Page 585

Amazon SageMaker AI
Developer Guide

JSON

{
"Version":"2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Action": [
"sagemaker:CreateCluster",
"sagemaker:DeleteCluster",
"sagemaker:UpdateCluster"
],
"Resource": "arn:aws:sagemaker:*:*:cluster/*"
},
{
"Effect": "Allow",

"Action": [
"sagemaker:AddTags"
],
"Resource": "arn:aws:sagemaker:*:*:cluster/*"
},
{
"Effect": "Allow",
"Action": [
"sagemaker:ListTags",
"sagemaker:ListClusters",
"sagemaker:ListClusterNodes",
"sagemaker:ListComputeQuotas",
"sagemaker:ListTrainingPlans",
"sagemaker:DescribeCluster",
"sagemaker:DescribeClusterNode"
],
"Resource": "*"
},
{
"Effect": "Allow",
"Action": [
"cloudformation:CreateStack",
"cloudformation:UpdateStack",
"cloudformation:DeleteStack",
"cloudformation:ContinueUpdateRollback",
"cloudformation:SetStackPolicy",
"cloudformation:ValidateTemplate",

IAM for HyperPod
1556

## Page 586

Amazon SageMaker AI
Developer Guide

"cloudformation:DescribeStacks",
"cloudformation:DescribeStackEvents",
"cloudformation:Get*",
"cloudformation:List*"
],
"Resource": "*"
},
{
"Effect": "Allow",
"Action": "iam:PassRole",
"Resource": "arn:aws:iam::*:role/sagemaker-*",
"Condition": {
"StringEquals": {
"iam:PassedToService": [
"sagemaker.amazonaws.com",
"eks.amazonaws.com",
"lambda.amazonaws.com"

]
}
}
},
{
"Effect": "Allow",
"Action": [
"iam:PassRole",
"iam:GetRole"
],
"Resource": "arn:aws:iam::*:role/*",
"Condition": {
"StringEquals": {
"iam:PassedToService": [
"sagemaker.amazonaws.com",
"eks.amazonaws.com",
"lambda.amazonaws.com",
"cloudformation.amazonaws.com"
]
}
}
},
{
"Sid": "AmazonVPCFullAccess",
"Effect": "Allow",
"Action": [
"ec2:AcceptVpcPeeringConnection",

IAM for HyperPod
1557

## Page 587

Amazon SageMaker AI
Developer Guide

"ec2:AcceptVpcEndpointConnections",
"ec2:AllocateAddress",
"ec2:AssignIpv6Addresses",
"ec2:AssignPrivateIpAddresses",
"ec2:AssociateAddress",
"ec2:AssociateDhcpOptions",
"ec2:AssociateRouteTable",
"ec2:AssociateSecurityGroupVpc",
"ec2:AssociateSubnetCidrBlock",
"ec2:AssociateVpcCidrBlock",
"ec2:AttachClassicLinkVpc",
"ec2:AttachInternetGateway",
"ec2:AttachNetworkInterface",
"ec2:AttachVpnGateway",
"ec2:AuthorizeSecurityGroupEgress",
"ec2:AuthorizeSecurityGroupIngress",
"ec2:CreateCarrierGateway",

"ec2:CreateCustomerGateway",
"ec2:CreateDefaultSubnet",
"ec2:CreateDefaultVpc",
"ec2:CreateDhcpOptions",
"ec2:CreateEgressOnlyInternetGateway",
"ec2:CreateFlowLogs",
"ec2:CreateInternetGateway",
"ec2:CreateLocalGatewayRouteTableVpcAssociation",
"ec2:CreateNatGateway",
"ec2:CreateNetworkAcl",
"ec2:CreateNetworkAclEntry",
"ec2:CreateNetworkInterface",
"ec2:CreateNetworkInterfacePermission",
"ec2:CreateRoute",
"ec2:CreateRouteTable",
"ec2:CreateSecurityGroup",
"ec2:CreateSubnet",
"ec2:CreateTags",
"ec2:CreateVpc",
"ec2:CreateVpcEndpoint",
"ec2:CreateVpcEndpointConnectionNotification",
"ec2:CreateVpcEndpointServiceConfiguration",
"ec2:CreateVpcPeeringConnection",
"ec2:CreateVpnConnection",
"ec2:CreateVpnConnectionRoute",
"ec2:CreateVpnGateway",
"ec2:DeleteCarrierGateway",

IAM for HyperPod
1558

## Page 588

Amazon SageMaker AI
Developer Guide

"ec2:DeleteCustomerGateway",
"ec2:DeleteDhcpOptions",
"ec2:DeleteEgressOnlyInternetGateway",
"ec2:DeleteFlowLogs",
"ec2:DeleteInternetGateway",
"ec2:DeleteLocalGatewayRouteTableVpcAssociation",
"ec2:DeleteNatGateway",
"ec2:DeleteNetworkAcl",
"ec2:DeleteNetworkAclEntry",
"ec2:DeleteNetworkInterface",
"ec2:DeleteNetworkInterfacePermission",
"ec2:DeleteRoute",
"ec2:DeleteRouteTable",
"ec2:DeleteSecurityGroup",
"ec2:DeleteSubnet",
"ec2:DeleteTags",
"ec2:DeleteVpc",

"ec2:DeleteVpcEndpoints",
"ec2:DeleteVpcEndpointConnectionNotifications",
"ec2:DeleteVpcEndpointServiceConfigurations",
"ec2:DeleteVpcPeeringConnection",
"ec2:DeleteVpnConnection",
"ec2:DeleteVpnConnectionRoute",
"ec2:DeleteVpnGateway",
"ec2:DescribeAccountAttributes",
"ec2:DescribeAddresses",
"ec2:DescribeAvailabilityZones",
"ec2:DescribeCarrierGateways",
"ec2:DescribeClassicLinkInstances",
"ec2:DescribeCustomerGateways",
"ec2:DescribeDhcpOptions",
"ec2:DescribeEgressOnlyInternetGateways",
"ec2:DescribeFlowLogs",
"ec2:DescribeInstances",
"ec2:DescribeInternetGateways",
"ec2:DescribeIpv6Pools",
"ec2:DescribeLocalGatewayRouteTables",
"ec2:DescribeLocalGatewayRouteTableVpcAssociations",
"ec2:DescribeKeyPairs",
"ec2:DescribeMovingAddresses",
"ec2:DescribeNatGateways",
"ec2:DescribeNetworkAcls",
"ec2:DescribeNetworkInterfaceAttribute",
"ec2:DescribeNetworkInterfacePermissions",

IAM for HyperPod
1559

## Page 589

Amazon SageMaker AI
Developer Guide

"ec2:DescribeNetworkInterfaces",
"ec2:DescribePrefixLists",
"ec2:DescribeRouteTables",
"ec2:DescribeSecurityGroupReferences",
"ec2:DescribeSecurityGroupRules",
"ec2:DescribeSecurityGroups",
"ec2:DescribeSecurityGroupVpcAssociations",
"ec2:DescribeStaleSecurityGroups",
"ec2:DescribeSubnets",
"ec2:DescribeTags",
"ec2:DescribeVpcAttribute",
"ec2:DescribeVpcClassicLink",
"ec2:DescribeVpcClassicLinkDnsSupport",
"ec2:DescribeVpcEndpointConnectionNotifications",
"ec2:DescribeVpcEndpointConnections",
"ec2:DescribeVpcEndpoints",
"ec2:DescribeVpcEndpointServiceConfigurations",

"ec2:DescribeVpcEndpointServicePermissions",
"ec2:DescribeVpcEndpointServices",
"ec2:DescribeVpcPeeringConnections",
"ec2:DescribeVpcs",
"ec2:DescribeVpnConnections",
"ec2:DescribeVpnGateways",
"ec2:DetachClassicLinkVpc",
"ec2:DetachInternetGateway",
"ec2:DetachNetworkInterface",
"ec2:DetachVpnGateway",
"ec2:DisableVgwRoutePropagation",
"ec2:DisableVpcClassicLink",
"ec2:DisableVpcClassicLinkDnsSupport",
"ec2:DisassociateAddress",
"ec2:DisassociateRouteTable",
"ec2:DisassociateSecurityGroupVpc",
"ec2:DisassociateSubnetCidrBlock",
"ec2:DisassociateVpcCidrBlock",
"ec2:EnableVgwRoutePropagation",
"ec2:EnableVpcClassicLink",
"ec2:EnableVpcClassicLinkDnsSupport",
"ec2:GetSecurityGroupsForVpc",
"ec2:ModifyNetworkInterfaceAttribute",
"ec2:ModifySecurityGroupRules",
"ec2:ModifySubnetAttribute",
"ec2:ModifyVpcAttribute",
"ec2:ModifyVpcEndpoint",

IAM for HyperPod
1560

## Page 590

Amazon SageMaker AI
Developer Guide

"ec2:ModifyVpcEndpointConnectionNotification",
"ec2:ModifyVpcEndpointServiceConfiguration",
"ec2:ModifyVpcEndpointServicePermissions",
"ec2:ModifyVpcPeeringConnectionOptions",
"ec2:ModifyVpcTenancy",
"ec2:MoveAddressToVpc",
"ec2:RejectVpcEndpointConnections",
"ec2:RejectVpcPeeringConnection",
"ec2:ReleaseAddress",
"ec2:ReplaceNetworkAclAssociation",
"ec2:ReplaceNetworkAclEntry",
"ec2:ReplaceRoute",
"ec2:ReplaceRouteTableAssociation",
"ec2:ResetNetworkInterfaceAttribute",
"ec2:RestoreAddressToClassic",
"ec2:RevokeSecurityGroupEgress",
"ec2:RevokeSecurityGroupIngress",

"ec2:UnassignIpv6Addresses",
"ec2:UnassignPrivateIpAddresses",
"ec2:UpdateSecurityGroupRuleDescriptionsEgress",
"ec2:UpdateSecurityGroupRuleDescriptionsIngress"
],
"Resource": "*"
},
{
"Sid": "CloudWatchPermissions",
"Effect": "Allow",
"Action": [
"cloudwatch:*",
"logs:*",
"sns:CreateTopic",
"sns:ListSubscriptions",
"sns:ListSubscriptionsByTopic",
"sns:ListTopics",
"sns:Subscribe",
"iam:GetPolicy",
"iam:GetPolicyVersion",
"iam:GetRole",
"oam:ListSinks",
"rum:*",
"synthetics:*",
"xray:*"
],
"Resource": "*"

IAM for HyperPod
1561

## Page 591

Amazon SageMaker AI
Developer Guide

},
{
"Effect": "Allow",
"Action": [
"s3:CreateBucket",
"s3:DeleteBucket",
"s3:PutBucketPolicy",
"s3:PutBucketTagging",
"s3:PutBucketPublicAccessBlock",
"s3:PutBucketLogging",
"s3:DeleteBucketPolicy",
"s3:PutObject",
"s3:DeleteObject",
"s3:PutEncryptionConfiguration",
"s3:AbortMultipartUpload",
"s3:Get*",
"s3:List*"

],
"Resource": [
"arn:aws:s3:::*",
"arn:aws:s3:::*/*"
]
},
{
"Effect": "Allow",
"Action": [
"eks:CreateCluster",
"eks:DeleteCluster",
"eks:CreateNodegroup",
"eks:DeleteNodegroup",
"eks:UpdateNodegroupConfig",
"eks:UpdateNodegroupVersion",
"eks:UpdateClusterConfig",
"eks:UpdateClusterVersion",
"eks:CreateFargateProfile",
"eks:DeleteFargateProfile",
"eks:CreateAddon",
"eks:DeleteAddon",
"eks:UpdateAddon",
"eks:CreateAccessEntry",
"eks:DeleteAccessEntry",
"eks:UpdateAccessEntry",
"eks:AssociateAccessPolicy",
"eks:AssociateIdentityProviderConfig",

IAM for HyperPod
1562

## Page 592

Amazon SageMaker AI
Developer Guide

"eks:DisassociateIdentityProviderConfig",
"eks:TagResource",
"eks:UntagResource",
"eks:AccessKubernetesApi",
"eks:Describe*",
"eks:List*"
],
"Resource": "*"
},
{
"Effect": "Allow",
"Action": [
"ssm:GetParameter",
"ssm:PutParameter",
"ssm:DeleteParameter",
"ssm:DescribeParameters"
],

"Resource": "*"
},
{
"Effect": "Allow",
"Action": [
"kms:Decrypt",
"kms:GenerateDataKey"
],
"Resource": "*",
"Condition": {
"StringLike": {
"kms:ViaService": [
"sagemaker.*.amazonaws.com",
"ec2.*.amazonaws.com",
"s3.*.amazonaws.com",
"eks.*.amazonaws.com"
]
}
}
},
{
"Effect": "Allow",
"Action": [
"lambda:CreateFunction",
"lambda:DeleteFunction",
"lambda:GetFunction",
"lambda:UpdateFunctionCode",

IAM for HyperPod
1563

## Page 593

Amazon SageMaker AI
Developer Guide

"lambda:UpdateFunctionConfiguration",
"lambda:AddPermission",
"lambda:RemovePermission",
"lambda:PublishLayerVersion",
"lambda:DeleteLayerVersion",
"lambda:InvokeFunction",
"lambda:Get*",
"lambda:List*",
"lambda:TagResource"
],
"Resource": [
"arn:aws:lambda:*:*:function:*",
"arn:aws:lambda:*:*:layer:*"
]
},
{
"Effect": "Allow",

"Action": [
"iam:DeleteRole",
"iam:DeleteRolePolicy"
],
"Resource": [
"arn:aws:iam::*:role/*sagemaker*",
"arn:aws:iam::*:role/*eks*",
"arn:aws:iam::*:role/*hyperpod*",
"arn:aws:iam::*:policy/*sagemaker*",
"arn:aws:iam::*:policy/*hyperpod*",
"arn:aws:iam::*:role/*LifeCycleScriptStack*",
"arn:aws:iam::*:role/*LifeCycleScript*"
]
},
{
"Effect": "Allow",
"Action": [
"iam:CreateRole",
"iam:TagRole",
"iam:PutRolePolicy",
"iam:Get*",
"iam:List*",
"iam:AttachRolePolicy",
"iam:DetachRolePolicy"
],
"Resource": [
"arn:aws:iam::*:role/*",

IAM for HyperPod
1564

## Page 594

Amazon SageMaker AI
Developer Guide

"arn:aws:iam::*:policy/*"
]
},
{
"Sid": "FullAccessToFSx",
"Effect": "Allow",
"Action": [
"fsx:AssociateFileGateway",
"fsx:AssociateFileSystemAliases",
"fsx:CancelDataRepositoryTask",
"fsx:CopyBackup",
"fsx:CopySnapshotAndUpdateVolume",
"fsx:CreateAndAttachS3AccessPoint",
"fsx:CreateBackup",
"fsx:CreateDataRepositoryAssociation",
"fsx:CreateDataRepositoryTask",
"fsx:CreateFileCache",

"fsx:CreateFileSystem",
"fsx:CreateFileSystemFromBackup",
"fsx:CreateSnapshot",
"fsx:CreateStorageVirtualMachine",
"fsx:CreateVolume",
"fsx:CreateVolumeFromBackup",
"fsx:DetachAndDeleteS3AccessPoint",
"fsx:DeleteBackup",
"fsx:DeleteDataRepositoryAssociation",
"fsx:DeleteFileCache",
"fsx:DeleteFileSystem",
"fsx:DeleteSnapshot",
"fsx:DeleteStorageVirtualMachine",
"fsx:DeleteVolume",
"fsx:DescribeAssociatedFileGateways",
"fsx:DescribeBackups",
"fsx:DescribeDataRepositoryAssociations",
"fsx:DescribeDataRepositoryTasks",
"fsx:DescribeFileCaches",
"fsx:DescribeFileSystemAliases",
"fsx:DescribeFileSystems",
"fsx:DescribeS3AccessPointAttachments",
"fsx:DescribeSharedVpcConfiguration",
"fsx:DescribeSnapshots",
"fsx:DescribeStorageVirtualMachines",
"fsx:DescribeVolumes",
"fsx:DisassociateFileGateway",

IAM for HyperPod
1565

## Page 595

Amazon SageMaker AI
Developer Guide

"fsx:DisassociateFileSystemAliases",
"fsx:ListTagsForResource",
"fsx:ManageBackupPrincipalAssociations",
"fsx:ReleaseFileSystemNfsV3Locks",
"fsx:RestoreVolumeFromSnapshot",
"fsx:TagResource",
"fsx:UntagResource",
"fsx:UpdateDataRepositoryAssociation",
"fsx:UpdateFileCache",
"fsx:UpdateFileSystem",
"fsx:UpdateSharedVpcConfiguration",
"fsx:UpdateSnapshot",
"fsx:UpdateStorageVirtualMachine",
"fsx:UpdateVolume"
],
"Resource": "*"
}

]
}

IAM users for cluster admin

Cluster administrators (admins) operate and conﬁgure SageMaker HyperPod clusters, performing
the tasks in the section called “Managing Slurm clusters”. The following policy example includes
the minimum set of permissions for cluster administrators to run the SageMaker HyperPod core
APIs and manage SageMaker HyperPod clusters within your AWS account.

Note

IAM users with cluster admin roles can use condition keys to provide granular access
control when managing SageMaker HyperPod cluster resources speciﬁcally for the

CreateCluster and UpdateCluster actions. To ﬁnd the condition keys supported for

these actions, search for CreateCluster or UpdateCluster in the Actions deﬁned by
SageMaker AI.

Slurm

{

IAM for HyperPod
1566

## Page 596

Amazon SageMaker AI
Developer Guide

"Version":"2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Action": [
"sagemaker:CreateCluster",
"sagemaker:ListClusters"
],
"Resource": "*"
},
{
"Effect": "Allow",
"Action": [
"sagemaker:DeleteCluster",
"sagemaker:DescribeCluster",
"sagemaker:DescribeClusterNode",
"sagemaker:ListClusterNodes",

"sagemaker:UpdateCluster",
"sagemaker:UpdateClusterSoftware",
"sagemaker:BatchDeleteClusterNodes"
],
"Resource": "arn:aws:sagemaker:us-east-1:111122223333:cluster/*"
}
]
}

Amazon EKS

{
"Version":"2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Action": "iam:PassRole",
"Resource": "arn:aws:iam::111122223333:role/execution-role-name"
},
{
"Effect": "Allow",
"Action": [
"sagemaker:CreateCluster",
"sagemaker:DeleteCluster",
"sagemaker:DescribeCluster",

IAM for HyperPod
1567

## Page 597

Amazon SageMaker AI
Developer Guide

"sagemaker:DescribeClusterNode",
"sagemaker:ListClusterNodes",
"sagemaker:ListClusters",
"sagemaker:UpdateCluster",
"sagemaker:UpdateClusterSoftware",
"sagemaker:BatchAddClusterNodes",
"sagemaker:BatchDeleteClusterNodes",
"sagemaker:ListComputeQuotas",
"sagemaker:ListClusterSchedulerConfigs",
"sagemaker:DeleteClusterSchedulerConfig",
"sagemaker:DeleteComputeQuota",
"eks:DescribeCluster",
"eks:CreateAccessEntry",
"eks:DescribeAccessEntry",
"eks:DeleteAccessEntry",
"eks:AssociateAccessPolicy",
"iam:CreateServiceLinkedRole"

],
"Resource": "*"
}
]
}

To grant permissions to access the SageMaker AI console, use the sample policy provided at
Permissions required to use the Amazon SageMaker AI console.

To grant permissions to access the Amazon EC2 Systems Manager console, use the sample policy
provided at Using the AWS Systems Manager console in the  AWS Systems Manager User Guide.

You might also consider attaching the AmazonSageMakerFullAccess policy to the role; however,

note that the AmazonSageMakerFullAccess policy grants permissions to the entire SageMaker
API calls, features, and resources.

For guidance on IAM users in general, see IAM users in the AWS Identity and Access Management
User Guide.

IAM users for scientists

Scientists log into and run ML workloads on SageMaker HyperPod cluster nodes provisioned
by cluster admins. For scientists in your AWS account, you should grant the permission

IAM for HyperPod
1568

## Page 598

Amazon SageMaker AI
Developer Guide

"ssm:StartSession" to run the SSM start-session command. The following is a policy
example for IAM users.

Slurm

Add the following policy to grant SSM session permissions to connect to an SSM target for all
resources. This allows you to access HyperPod clusters.

JSON

{
"Version":"2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Action": [
"ssm:StartSession",
"ssm:TerminateSession"
],
"Resource": "*"
}
]
}

Amazon EKS

Grant the following IAM role permissions for data scientists to run hyperpod list-clusters

and hyperpod connect-cluster commands among the HyperPod CLI commands. To learn
more about the HyperPod CLI, see the section called “Running jobs on clusters”. It also includes
SSM session permissions to connect to an SSM target for all resources. This allows you to access
HyperPod clusters.

JSON

{
"Version":"2012-10-17",
"Statement": [
{
"Sid": "DescribeHyerpodClusterPermissions",
"Effect": "Allow",

IAM for HyperPod
1569

## Page 599

Amazon SageMaker AI
Developer Guide

"Action": [
"sagemaker:DescribeCluster"
],
"Resource": "arn:aws:sagemaker:us-
east-2:111122223333:cluster/hyperpod-cluster-name"
},
{
"Sid": "UseEksClusterPermissions",
"Effect": "Allow",
"Action": [
"eks:DescribeCluster"
],
"Resource": "arn:aws:sagemaker:us-east-2:111122223333:cluster/eks-
cluster-name"
},
{
"Sid": "ListClustersPermission",

"Effect": "Allow",
"Action": [
"sagemaker:ListClusters"
],
"Resource": "*"
},
{
"Effect": "Allow",
"Action": [
"ssm:StartSession",
"ssm:TerminateSession"
],
"Resource": "*"
}
]
}

To grant data scientists IAM users or roles access to Kubernetes APIs in the cluster, see also
Grant IAM users and roles access to Kubernetes APIs in the Amazon EKS User Guide.

IAM role for SageMaker HyperPod

For SageMaker HyperPod clusters to run and communicate with necessary AWS resources, you
need create an IAM role for HyperPod cluster to assume.

IAM for HyperPod
1570

## Page 600

Amazon SageMaker AI
Developer Guide

Start with attaching the managed role the section called
“AmazonSageMakerHyperPodServiceRolePolicy”. Given this AWS managed policy, SageMaker
HyperPod cluster instance groups assume the role to communicate with Amazon CloudWatch,
Amazon S3, and AWS Systems Manager Agent (SSM Agent). This managed policy is the minimum
requirement for SageMaker HyperPod resources to run properly, so you must provide an IAM role
with this policy to all instance groups.

Tip

Depending on your preference on designing the level of permissions for multiple instance
groups, you can also set up multiple IAM roles and attach them to diﬀerent instance
groups. When you set up your cluster user access to speciﬁc SageMaker HyperPod cluster
nodes, the nodes assume the role with the selective permissions you manually attached.
When you set up the access for scientists to speciﬁc cluster nodes through AWS Systems
Manager (see also the section called “Setting up AWS Systems Manager and Run As
for cluster user access control”), the cluster nodes assume the role with the selective
permissions you manually attach.

After you are done with creating IAM roles, make notes of their names and ARNs. You use the roles
when creating a SageMaker HyperPod cluster, granting the correct permissions required for each
instance group to communicate with necessary AWS resources.

Slurm

For HyperPod orchestrated with Slurm, you must attach the following managed policy to the
SageMaker HyperPod IAM role.

• AmazonSageMakerClusterInstanceRolePolicy

(Optional) Additional permissions for using SageMaker HyperPod with Amazon Virtual
Private Cloud

If you want to use your own Amazon Virtual Private Cloud (VPC) instead of the default
SageMaker AI VPC, you should add the following additional permissions to the IAM role for
SageMaker HyperPod.

{

IAM for HyperPod
1571

## Page 601

Amazon SageMaker AI
Developer Guide

"Effect": "Allow",
"Action": [
"ec2:CreateNetworkInterface",
"ec2:CreateNetworkInterfacePermission",
"ec2:DeleteNetworkInterface",
"ec2:DeleteNetworkInterfacePermission",
"ec2:DescribeNetworkInterfaces",
"ec2:DescribeVpcs",
"ec2:DescribeDhcpOptions",
"ec2:DescribeSubnets",
"ec2:DescribeSecurityGroups",
"ec2:DetachNetworkInterface"
],
"Resource": "*"
}
{
"Effect": "Allow",

"Action": "ec2:CreateTags",
"Resource": [
"arn:aws:ec2:*:*:network-interface/*"
]
}

The following list breaks down which permissions are needed to enable SageMaker HyperPod
cluster functionalities when you conﬁgure the cluster with your own Amazon VPC.

• The following ec2 permissions are required to enable conﬁguring a SageMaker HyperPod
cluster with your VPC.

{
"Effect": "Allow",
"Action": [
"ec2:CreateNetworkInterface",
"ec2:CreateNetworkInterfacePermission",
"ec2:DeleteNetworkInterface",
"ec2:DeleteNetworkInterfacePermission",
"ec2:DescribeNetworkInterfaces",
"ec2:DescribeVpcs",
"ec2:DescribeDhcpOptions",
"ec2:DescribeSubnets",
"ec2:DescribeSecurityGroups"
],
"Resource": "*"

IAM for HyperPod
1572

## Page 602

Amazon SageMaker AI
Developer Guide

}

• The following ec2 permission is required to enable the SageMaker HyperPod auto-resume
functionality.

{
"Effect": "Allow",
"Action": [
"ec2:DetachNetworkInterface"
],
"Resource": "*"
}

• The following ec2 permission allows SageMaker HyperPod to create tags on the network
interfaces within your account.

{
"Effect": "Allow",
"Action": "ec2:CreateTags",
"Resource": [
"arn:aws:ec2:*:*:network-interface/*"
]
}

Amazon EKS

For HyperPod orchestrated with Amazon EKS, you must attach the following managed policies
to the SageMaker HyperPod IAM role.

• AmazonSageMakerClusterInstanceRolePolicy

In addition to the managed policies, attach the following permission policy to the role.

JSON

{
"Version":"2012-10-17",
"Statement": [
{

IAM for HyperPod
1573

## Page 603

Amazon SageMaker AI
Developer Guide

"Effect": "Allow",
"Action": [
"ec2:AssignPrivateIpAddresses",
"ec2:AttachNetworkInterface",
"ec2:CreateNetworkInterface",
"ec2:CreateNetworkInterfacePermission",
"ec2:DeleteNetworkInterface",
"ec2:DeleteNetworkInterfacePermission",
"ec2:DescribeInstances",
"ec2:DescribeInstanceTypes",
"ec2:DescribeNetworkInterfaces",
"ec2:DescribeTags",
"ec2:DescribeVpcs",
"ec2:DescribeDhcpOptions",
"ec2:DescribeSubnets",
"ec2:DescribeSecurityGroups",
"ec2:DetachNetworkInterface",

"ec2:ModifyNetworkInterfaceAttribute",
"ec2:UnassignPrivateIpAddresses",
"ecr:BatchCheckLayerAvailability",
"ecr:BatchGetImage",
"ecr:GetAuthorizationToken",
"ecr:GetDownloadUrlForLayer",
"eks-auth:AssumeRoleForPodIdentity"
],
"Resource": "*"
},
{
"Effect": "Allow",
"Action": [
"ec2:CreateTags"
],
"Resource": [
"arn:aws:ec2:*:*:network-interface/*"
]
}
]
}

IAM for HyperPod
1574

## Page 604

Amazon SageMaker AI
Developer Guide

Note

The "eks-auth:AssumeRoleForPodIdentity" permission is optional. It's required if
you plan to use EKS Pod identity.

SageMaker HyperPod service-linked role

For Amazon EKS support in SageMaker HyperPod, HyperPod creates a service-linked role with
the section called “AmazonSageMakerHyperPodServiceRolePolicy” to monitor and support
resiliency on your EKS cluster such as replacing nodes and restarting jobs.

Additional IAM policies for Amazon EKS cluster with restricted instance group (RIG)

Workloads running in restricted instance groups rely on the execution role to load data from
Amazon S3. You must add the additional Amazon S3 permissions to the execution role so that
customization jobs running in restricted instance groups can properly fetch input data.

JSON

{
"Version":"2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Action": [
"s3:ListBucket"
],
"Resource": [
"arn:aws:s3:::amzn-s3-demo-bucket"
]
},
{
"Effect": "Allow",
"Action": [
"s3:GetObject",
"s3:PutObject",
"s3:DeleteObject"
],
"Resource": [
"arn:aws:s3:::amzn-s3-demo-bucket/*"

IAM for HyperPod
1575

## Page 605

Amazon SageMaker AI
Developer Guide

]
}
]
}

Customer managed AWS KMS key encryption for SageMaker HyperPod

By default, the root Amazon EBS volume attached to your SageMaker HyperPod cluster is
encrypted using an AWS KMS key owned by AWS. You now have the option to encrypt both the
root Amazon EBS volume and the secondary volume with your own customer managed KMS keys.
The following topic describes how customer managed keys (CMKs) work with volumes in HyperPod
clusters.

Note

The following exclusions apply when using customer managed keys for SageMaker
HyperPod clusters:

• Customer managed key encryption is only supported for clusters using the continuous
node provisioning mode. Restricted instance groups don't support customer managed
keys.

• HyperPod clusters don’t currently support passing AWS KMS encryption context in
customer managed key encryption requests. Therefore, ensure your KMS key policy is not
scoped down using encryption context conditions, as this prevents the cluster from using
the key.

• KMS key transition isn't currently supported, so you can't change the KMS key speciﬁed
in your conﬁguration. To use a diﬀerent key, create a new instance group with the desired
key and delete your old instance group.

• Specifying customer managed keys for HyperPod clusters through the console isn't
currently supported.

Permissions

Before you can use your customer managed key with HyperPod, you must complete the following
prerequisites:

Customer managed key encryption
1576

## Page 606

Amazon SageMaker AI
Developer Guide

• Ensure that the AWS IAM execution role that you're using for SageMaker AI has the following

permissions for AWS KMS added. The  kms:CreateGrant permission allows HyperPod to take
the following actions using permissions to your KMS key:

• Scaling out your instance count (UpdateCluster operations)

• Adding cluster nodes (BatchAddClusterNodes operations)

• Patching software (UpdateClusterSoftware operations)

For more information on updating your IAM role's permissions, see Adding and removing IAM
identity permissions in the IAM User Guide.

JSON

{
"Version":"2012-10-17",

"Statement": [
{
"Effect": "Allow",
"Action": [
"kms:CreateGrant",
"kms:DescribeKey"
],
"Resource": "*"
}
]
}

• Add the following permissions to your KMS key policy. For more information, see Change a key
policy in the AWS KMS Developer Guide.

JSON

{
"Version":"2012-10-17",
"Id": "hyperpod-key-policy",
"Statement": [
{
"Sid": "Enable IAM User Permissions",
"Effect": "Allow",
"Principal": {
"AWS": "arn:aws:iam::111122223333:root"
},
"Action": "kms:*",
Customer managed key encryption
1577

## Page 607

Amazon SageMaker AI
Developer Guide

"Resource": "*"
},
{
"Effect": "Allow",
"Principal": {
"AWS": "arn:aws:iam::111122223333:role/<iam-role>"
},
"Action": "kms:CreateGrant",
"Resource": "arn:aws:kms:us-east-1:111122223333:key/key-id",
"Condition": {
"StringEquals": {
"kms:ViaService": "sagemaker.us-east-1.amazonaws.com"
},
"Bool": {
"kms:GrantIsForAWSResource": "true"
}
}

},
{
"Effect": "Allow",
"Principal": {
"AWS": "arn:aws:iam::111122223333:role/<iam-role>"
},
"Action": "kms:DescribeKey",
"Resource": "arn:aws:kms:us-east-1:111122223333:key/key-id",
"Condition": {
"StringEquals": {
"kms:ViaService": "sagemaker.us-east-1.amazonaws.com"
}
}
}
]
}

How to use your KMS key

You can specify your customer managed keys when creating or updating a cluster using the

CreateCluster and UpdateCluster API operations. The InstanceStorageConfigs structure allows

up to two EbsVolumeConfig conﬁgurations, in which you can conﬁgure the root Amazon EBS
volume and, optionally, a secondary volume. You can use the same KMS key or a diﬀerent KMS key
for each volume, depending on your needs.

Customer managed key encryption
1578

## Page 608

Amazon SageMaker AI
Developer Guide

You can choose to specify a customer managed key for neither, both, or either volume. However,
you can't specify two root volumes or two secondary volumes.

When conﬁguring the root volume, the following requirements apply:

• RootVolume must be set to True. The default value is False, which conﬁgures the secondary
volume instead.

• The VolumeKmsKeyId ﬁeld is required and you must specify your customer managed key. This is
because the root volume must always be encrypted with either an AWS owned key or a customer
managed key (if you don't specify your own, then an AWS owned key is used).

• You can't specify the VolumeSizeInGB ﬁeld for root volumes since HyperPod determines the
size of the root volume for you.

When conﬁguring the secondary volume, the following requirements apply:

• RootVolume must be False (the default value of this ﬁeld is False).

• The VolumeKmsKeyId ﬁeld is optional. You can use the same customer managed key you
speciﬁed for the root volume, or you can use a diﬀerent key.

• The VolumeSizeInGB ﬁeld is required, since you must specify your desired size for the
secondary volume.

Important

When using customer managed keys, we strongly recommend that you use diﬀerent KMS
keys for each instance group in your cluster. Using the same customer managed key across
multiple instance groups might lead to unintentional continued permissions even if you try
to revoke a grant. For example, if you revoke an AWS KMS grant for one instance group's
volumes, that instance group might still allow scaling and patching operations due to
grants existing on other instance groups using the same key. To prevent this issue, ensure
that you assign unique KMS keys to each instance group in your cluster. If you need to
restrict permissions on instance groups, you can try one of the following options:

• Disable the KMS key.

• Apply deny policies to the KMS key policy.

• Revoke all instance group grants for the key (rather than revoking one grant).

• Delete the instance group.

Customer managed key encryption
1579

## Page 609

Amazon SageMaker AI
Developer Guide

• Delete the cluster.

The following examples show how to specify customer managed keys for both root and secondary

volumes using the CreateCluster and UpdateCluster APIs. These examples show only the required
ﬁelds for customer managed key integration. To conﬁgure a customer managed key for only one of

the volumes, then only specify one EbsVolumeConfig.

For more information about conﬁguring cluster creation and update requests, see Creating a
SageMaker HyperPod cluster and Updating SageMaker HyperPod cluster conﬁguration.

CreateCluster

The following example shows a  create-cluster AWS CLI request with customer managed key
encryption.

aws sagemaker create-cluster \
--cluster-name <your-hyperpod-cluster> \
--instance-groups '[{
"ExecutionRole": "arn:aws:iam::111122223333:role/<your-SageMaker-Execution-
Role>",
"InstanceCount": 2,
"InstanceGroupName": "<your-ig-name>",
"InstanceStorageConfigs": [
{
"EbsVolumeConfig": {
"RootVolume": True,
"VolumeKmsKeyId": "arn:aws:kms:us-east-1:111122223333:key/root-
volume-key-id"
}
},
{
"EbsVolumeConfig": {
"VolumeSizeInGB": 100,
"VolumeKmsKeyId": "arn:aws:kms:us-
east-1:111122223333:key/secondary-volume-key-id"
}
}
],
"InstanceType": "<desired-instance-type>"
}]' \
--vpc-config '{

Customer managed key encryption
1580

## Page 610

Amazon SageMaker AI
Developer Guide

"SecurityGroupIds": ["<sg-id>"],
"Subnets": ["<subnet-id>"]
}'

UpdateCluster

The following example shows an  update-cluster AWS CLI request with customer managed key
encryption.

aws sagemaker update-cluster \
--cluster-name <your-hyperpod-cluster> \
--instance-groups '[{
"InstanceGroupName": "<your-ig-name>",
"InstanceStorageConfigs": [
{
"EbsVolumeConfig": {
"RootVolume": true,
"VolumeKmsKeyId": "arn:aws:kms:us-east-1:111122223333:key/root-
volume-key-id"
}
},
{
"EbsVolumeConfig": {
"VolumeSizeInGB": 100,
"VolumeKmsKeyId": "arn:aws:kms:us-
east-1:111122223333:key/secondary-volume-key-id"
}
}
]
}]'

SageMaker HyperPod recipes

Amazon SageMaker HyperPod recipes are pre-conﬁgured training stacks provided by AWS to help
you quickly start training and ﬁne-tuning publicly available foundation models (FMs) from various
model families such as Llama, Mistral, Mixtral, or DeepSeek. Recipes automate the end-to-end
training loop, including loading datasets, applying distributed training techniques, and managing
checkpoints for faster recovery from faults.

SageMaker HyperPod recipes are particularly beneﬁcial for users who may not have deep machine
learning expertise, as they abstract away much of the complexity involved in training large models.

HyperPod recipes
1581

## Page 611

Amazon SageMaker AI
Developer Guide

You can run recipes within SageMaker HyperPod or as SageMaker training jobs.

The following tables are maintained in the SageMaker HyperPod GitHub repository and provide
the most up-to-date information on the models supported for pre-training and ﬁne-tuning, their
respective recipes and launch scripts, supported instance types, and more.

• For the most current list of supported models, recipes, and launch scripts for pre-training, see
the pre-training table.

• For the most current list of supported models, recipes, and launch scripts for ﬁne-tuning, see the
ﬁne-tuning table.

For SageMaker HyperPod users, the automation of end-to-end training workﬂows comes from the
integration of the training adapter with SageMaker HyperPod recipes. The training adapter is built
on the NVIDIA NeMo framework and the Neuronx Distributed Training package. If you're familiar
with using NeMo, the process of using the training adapter is the same. The training adapter runs
the recipe on your cluster.

HyperPod recipes
1582

## Page 612

Amazon SageMaker AI
Developer Guide

![Page 612 Diagram 1](images/page-0612-img-01.png)

You can also train your own model by deﬁning your own custom recipe.

To get started with a tutorial, see Tutorials.

Topics

• Tutorials

• Default conﬁgurations

• Cluster-speciﬁc conﬁgurations

• Considerations

HyperPod recipes
1583

## Page 613

Amazon SageMaker AI
Developer Guide

• Advanced settings

• Appendix

Tutorials

The following quick-start tutorials help you get started with using the recipes for training:

• SageMaker HyperPod with Slurm Orchestration

• Pre-training

• HyperPod Slurm cluster pre-training tutorial (GPU)

• Trainium Slurm cluster pre-training tutorial

• Fine-tuning

• HyperPod Slurm cluster PEFT-Lora tutorial (GPU)

• HyperPod Slurm cluster DPO tutorial (GPU)

• SageMaker HyperPod with K8s Orchestration

• Pre-training

• Kubernetes cluster pre-training tutorial (GPU)

• Trainium SageMaker training jobs pre-training tutorial

• SageMaker training jobs

• Pre-training

• SageMaker training jobs pre-training tutorial (GPU)

• Trainium SageMaker training jobs pre-training tutorial

HyperPod Slurm cluster pre-training tutorial (GPU)

The following tutorial sets up Slurm environment and starts a training job on a Llama 8 billion
parameter model.

Prerequisites

Before you start setting up your environment to run the recipe, make sure you have:

• Set up a HyperPod GPU Slurm cluster.

• Your HyperPod Slurm cluster must have Nvidia Enroot and Pyxis enabled (these are
enabled by default).

HyperPod recipes
1584

## Page 614

Amazon SageMaker AI
Developer Guide

• A shared storage location. It can be an Amazon FSx ﬁle system or an NFS system that's
accessible from the cluster nodes.

• Data in one of the following formats:

• JSON

• JSONGZ (Compressed JSON)

• ARROW

• (Optional) You must get a HuggingFace token if you're using the model weights from
HuggingFace for pre-training or ﬁne-tuning. For more information about getting the
token, see User access tokens.

HyperPod GPU Slurm environment setup

To initiate a training job on a HyperPod GPU Slurm cluster, do the following:

1. SSH into the head node of your Slurm cluster.

2. After you log in, set up the virtual environment. Make sure you're using Python 3.9 or greater.

#set up a virtual environment
python3 -m venv ${PWD}/venv
source venv/bin/activate

3. Clone the SageMaker HyperPod recipes and SageMaker HyperPod adapter repositories to a

shared storage location.

git clone https://github.com/aws/sagemaker-hyperpod-training-adapter-for-nemo.git
git clone --recursive https://github.com/aws/sagemaker-hyperpod-recipes.git
cd sagemaker-hyperpod-recipes
pip3 install -r requirements.txt

4. Create a squash ﬁle using Enroot. To ﬁnd the most recent release of the SMP container, see

Release notes for the SageMaker model parallelism library. To gain a deeper understanding of
how to use the Enroot ﬁle, see Build AWS-optimized Nemo-Launcher image.

REGION="<region>"
IMAGE="658645717510.dkr.ecr.${REGION}.amazonaws.com/smdistributed-
modelparallel:2.4.1-gpu-py311-cu121"
aws ecr get-login-password --region ${REGION} | docker login --username AWS --
password-stdin 658645717510.dkr.ecr.${REGION}.amazonaws.com

HyperPod recipes
1585

## Page 615

Amazon SageMaker AI
Developer Guide

enroot import -o $PWD/smdistributed-modelparallel.sqsh dockerd://${IMAGE}
mv $PWD/smdistributed-modelparallel.sqsh "/fsx/<any-path-in-the-shared-filesystem>"

5. To use the Enroot squash ﬁle to start training, use the following example to modify the

recipes_collection/config.yaml ﬁle.

container: /fsx/path/to/your/smdistributed-modelparallel.sqsh

Launch the training job

After you install the dependencies, start a training job from the sagemaker-hyperpod-

recipes/launcher_scripts directory. You get the dependencies by cloning the SageMaker
HyperPod recipes repository:

First, pick your training recipe from Github, the model name is speciﬁed as part of the recipe. We

use the launcher_scripts/llama/run_hf_llama3_8b_seq16k_gpu_p5x16_pretrain.sh

script to launch a Llama 8b with sequence length 8192 pre-training recipe, llama/

hf_llama3_8b_seq16k_gpu_p5x16_pretrain, in the following example.

• IMAGE: The container from the environment setup section.

• (Optional) You can provide the HuggingFace token if you need pre-trained weights from
HuggingFace by setting the following key-value pair:

recipes.model.hf_access_token=<your_hf_token>

#!/bin/bash
IMAGE="${YOUR_IMAGE}"
SAGEMAKER_TRAINING_LAUNCHER_DIR="${SAGEMAKER_TRAINING_LAUNCHER_DIR:-${PWD}}"

TRAIN_DIR="${YOUR_TRAIN_DIR}" # Location of training dataset
VAL_DIR="${YOUR_VAL_DIR}" # Location of validation dataset

# experiment ouput directory
EXP_DIR="${YOUR_EXP_DIR}"

HYDRA_FULL_ERROR=1 python3 "${SAGEMAKER_TRAINING_LAUNCHER_DIR}/main.py" \
recipes=training/llama/hf_llama3_8b_seq16k_gpu_p5x16_pretrain \
base_results_dir="${SAGEMAKER_TRAINING_LAUNCHER_DIR}/results" \
recipes.run.name="hf_llama3_8b" \

HyperPod recipes
1586

## Page 616

Amazon SageMaker AI
Developer Guide

recipes.exp_manager.exp_dir="$EXP_DIR" \
recipes.model.data.train_dir="$TRAIN_DIR" \
recipes.model.data.val_dir="$VAL_DIR" \
container="${IMAGE}" \
+cluster.container_mounts.0="/fsx:/fsx"

After you've conﬁgured all the required parameters in the launcher script, you can run the script
using the following command.

bash launcher_scripts/llama/run_hf_llama3_8b_seq16k_gpu_p5x16_pretrain.sh

For more information about the Slurm cluster conﬁguration, see Running a training job on
HyperPod Slurm.

Trainium Slurm cluster pre-training tutorial

The following tutorial sets up a Trainium environment on a Slurm cluster and starts a training job
on a Llama 8 billion parameter model.

Prerequisites

Before you start setting up your environment, make sure you have:

• Set up a SageMaker HyperPod Trainium Slurm cluster.

• A shared storage location. It can be an Amazon FSx ﬁle system or NFS system that's
accessible from the cluster nodes.

• Data in one of the following formats:

• JSON

• JSONGZ (Compressed JSON)

• ARROW

• (Optional) You must get a HuggingFace token if you're using the model weights from
HuggingFace for pre-training or ﬁne-tuning. For more information about getting the
token, see User access tokens.

Set up the Trainium environment on the Slurm Cluster

To initiate a training job on a Slurm cluster, do the following:

HyperPod recipes
1587

## Page 617

Amazon SageMaker AI
Developer Guide

• SSH into the head node of your Slurm cluster.

• After you log in, set up the Neuron environment. For information about setting up Neuron, see
Neuron setup steps. We recommend relying on the Deep learning AMI's that come pre-installed
with Neuron's drivers, such as Ubuntu 20 with DLAMI Pytorch.

• Clone the SageMaker HyperPod recipes repository to a shared storage location in the cluster. The
shared storage location can be an Amazon FSx ﬁle system or NFS system that's accessible from
the cluster nodes.

git clone --recursive https://github.com/aws/sagemaker-hyperpod-recipes.git
cd sagemaker-hyperpod-recipes
pip3 install -r requirements.txt

• Go through the following tutorial: HuggingFace Llama3-8B Pretraining

• Prepare a model conﬁguration. The model conﬁgurations available in the Neuron repo. For the
model conﬁguration used the in this tutorial, see llama3 8b model conﬁg

Launch the training job in Trainium

To launch a training job in Trainium, specify a cluster conﬁguration and a Neuron recipe.
For example, to launch a llama3 8b pre-training job in Trainium, set the launch script,

launcher_scripts/llama/run_hf_llama3_8b_seq8k_trn1x4_pretrain.sh, to the
following:

• MODEL_CONFIG: The model conﬁg from the environment setup section

• (Optional) You can provide the HuggingFace token if you need pre-trained weights from
HuggingFace by setting the following key-value pair:

recipes.model.hf_access_token=<your_hf_token>

#!/bin/bash

#Users should set up their cluster type in /recipes_collection/config.yaml

SAGEMAKER_TRAINING_LAUNCHER_DIR=${SAGEMAKER_TRAINING_LAUNCHER_DIR:-"$(pwd)"}

COMPILE=0
TRAIN_DIR="${TRAIN_DIR}" # Location of training dataset

HyperPod recipes
1588

## Page 618

Amazon SageMaker AI
Developer Guide

MODEL_CONFIG="${MODEL_CONFIG}" # Location of config.json for the model

HYDRA_FULL_ERROR=1 python3 "${SAGEMAKER_TRAINING_LAUNCHER_DIR}/main.py" \
base_results_dir="${SAGEMAKER_TRAINING_LAUNCHER_DIR}/results" \
instance_type="trn1.32xlarge" \
recipes.run.compile="$COMPILE" \
recipes.run.name="hf-llama3-8b" \
recipes.trainer.num_nodes=4 \
recipes=training/llama/hf_llama3_8b_seq8k_trn1x4_pretrain \
recipes.data.train_dir="$TRAIN_DIR" \
recipes.model.model_config="$MODEL_CONFIG"

To launch the training job, run the following command:

bash launcher_scripts/llama/run_hf_llama3_8b_seq8k_trn1x4_pretrain.sh

For more information about the Slurm cluster conﬁguration, see Running a training job on
HyperPod Slurm.

HyperPod Slurm cluster DPO tutorial (GPU)

The following tutorial sets up a Slurm environment and starts a direct preference optimization
(DPO) job on a Llama 8 billion parameter model.

Prerequisites

Before you start setting up your environment, make sure you have:

• Set up HyperPod GPU Slurm cluster

• Your HyperPod Slurm cluster must have Nvidia Enroot and Pyxis enabled (these are
enabled by default).

• A shared storage location. It can be an Amazon FSx ﬁle system or NFS system that's
accessible from the cluster nodes.

• A tokenized binary preference dataset in one of the following formats:

• JSON

• JSONGZ (Compressed JSON)

• ARROW

HyperPod recipes
1589

## Page 619

Amazon SageMaker AI
Developer Guide

• (Optional) If you need the pre-trained weights from HuggingFace or if you're training
a Llama 3.2 model, you must get the HuggingFace token before you start training. For
more information about getting the token, see User access tokens.

Set up the HyperPod GPU Slurm environment

To initiate a training job on a Slurm cluster, do the following:

• SSH into the head node of your Slurm cluster.

• After you log in, set up the virtual environment. Make sure you're using Python 3.9 or greater.

#set up a virtual environment
python3 -m venv ${PWD}/venv
source venv/bin/activate

• Clone the SageMaker HyperPod recipes and SageMaker HyperPod adapter repositories to a
shared storage location. The shared storage location can be an Amazon FSx ﬁle system or NFS
system that's accessible from the cluster nodes.

git clone https://github.com/aws/sagemaker-hyperpod-training-adapter-for-nemo.git
git clone --recursive https://github.com/aws/sagemaker-hyperpod-recipes.git
cd sagemaker-hyperpod-recipes
pip3 install -r requirements.txt

• Create a squash ﬁle using Enroot. To ﬁnd the most recent release of the SMP container, see
Release notes for the SageMaker model parallelism library. For more information about using
the Enroot ﬁle, see Build AWS-optimized Nemo-Launcher image.

REGION="<region>"
IMAGE="658645717510.dkr.ecr.${REGION}.amazonaws.com/smdistributed-
modelparallel:2.4.1-gpu-py311-cu121"
aws ecr get-login-password --region ${REGION} | docker login --username AWS --
password-stdin 658645717510.dkr.ecr.${REGION}.amazonaws.com
enroot import -o $PWD/smdistributed-modelparallel.sqsh dockerd://${IMAGE}
mv $PWD/smdistributed-modelparallel.sqsh "/fsx/<any-path-in-the-shared-filesystem>"

• To use the Enroot squash ﬁle to start training, use the following example to modify the

recipes_collection/config.yaml ﬁle.

HyperPod recipes
1590

## Page 620

Amazon SageMaker AI
Developer Guide

container: /fsx/path/to/your/smdistributed-modelparallel.sqsh

Launch the training job

To launch a DPO job for the Llama 8 billion parameter model with a sequence length of

8192 on a single Slurm compute node, set the launch script, launcher_scripts/llama/

run_hf_llama3_8b_seq8k_gpu_dpo.sh, to the following:

• IMAGE: The container from the environment setup section.

• HF_MODEL_NAME_OR_PATH: Deﬁne the name or the path of the pre-trained weights in the
hf_model_name_or_path parameter of the recipe.

• (Optional) You can provide the HuggingFace token if you need pre-trained weights from
HuggingFace by setting the following key-value pair:

recipes.model.hf_access_token=${HF_ACCESS_TOKEN}

Note

The reference model used for DPO in this setup is automatically derived from the base
model being trained (no separate reference model is explicitly deﬁned). DPO speciﬁc
hyperparameters are preconﬁgured with the following default values:

• beta: 0.1 (controls the strength of KL divergence regularization)

• label_smoothing: 0.0 (no smoothing applied to preference labels)

recipes.dpo.beta=${BETA}
recipes.dpo.label_smoothing=${LABEL_SMOOTHING}

#!/bin/bash
IMAGE="${YOUR_IMAGE}"
SAGEMAKER_TRAINING_LAUNCHER_DIR="${SAGEMAKER_TRAINING_LAUNCHER_DIR:-${PWD}}"

TRAIN_DIR="${YOUR_TRAIN_DIR}" # Location of training dataset
VAL_DIR="${YOUR_VAL_DIR}" # Location of validation dataset

HyperPod recipes
1591

## Page 621

Amazon SageMaker AI
Developer Guide

# experiment output directory
EXP_DIR="${YOUR_EXP_DIR}"
HF_ACCESS_TOKEN="${YOUR_HF_TOKEN}"
HF_MODEL_NAME_OR_PATH="${HF_MODEL_NAME_OR_PATH}"
BETA="${BETA}"
LABEL_SMOOTHING="${LABEL_SMOOTHING}"

# Add hf_model_name_or_path and turn off synthetic_data
HYDRA_FULL_ERROR=1 python3 ${SAGEMAKER_TRAINING_LAUNCHER_DIR}/main.py \
recipes=fine-tuning/llama/hf_llama3_8b_seq8k_gpu_dpo \
base_results_dir=${SAGEMAKER_TRAINING_LAUNCHER_DIR}/results \
recipes.run.name="hf_llama3_dpo" \
recipes.exp_manager.exp_dir="$EXP_DIR" \
recipes.model.data.train_dir="$TRAIN_DIR" \
recipes.model.data.val_dir="$VAL_DIR" \
recipes.model.hf_model_name_or_path="$HF_MODEL_NAME_OR_PATH" \
container="${IMAGE}" \

+cluster.container_mounts.0="/fsx:/fsx" \
recipes.model.hf_access_token="${HF_ACCESS_TOKEN}" \
recipes.dpo.enabled=true \
recipes.dpo.beta="${BETA}" \
recipes.dpo.label_smoothing="${LABEL_SMOOTHING}$" \

After you've conﬁgured all the required parameters in the preceding script, you can initiate the
training job by running it.

bash launcher_scripts/llama/run_hf_llama3_8b_seq8k_gpu_dpo.sh

For more information about the Slurm cluster conﬁguration, see Running a training job on
HyperPod Slurm.

HyperPod Slurm cluster PEFT-Lora tutorial (GPU)

The following tutorial sets up Slurm environment and starts a parameter-eﬃcient ﬁne-tuning
(PEFT) job on a Llama 8 billion parameter model.

Prerequisites

Before you start setting up your environment, make sure you have:

• Set up HyperPod GPU Slurm cluster

HyperPod recipes
1592

## Page 622

Amazon SageMaker AI
Developer Guide

• Your HyperPod Slurm cluster must have Nvidia Enroot and Pyxis enabled (these are
enabled by default).

• A shared storage location. It can be an Amazon FSx ﬁle system or NFS system that's
accessible from the cluster nodes.

• Data in one of the following formats:

• JSON

• JSONGZ (Compressed JSON)

• ARROW

• (Optional) If you need the pre-trained weights from HuggingFace or if you're training
a Llama 3.2 model, you must get the HuggingFace token before you start training. For
more information about getting the token, see User access tokens.

Set up the HyperPod GPU Slurm environment

To initiate a training job on a Slurm cluster, do the following:

• SSH into the head node of your Slurm cluster.

• After you log in, set up the virtual environment. Make sure you're using Python 3.9 or greater.

#set up a virtual environment
python3 -m venv ${PWD}/venv
source venv/bin/activate

• Clone the SageMaker HyperPod recipes and SageMaker HyperPod adapter repositories to a
shared storage location. The shared storage location can be an Amazon FSx ﬁle system or NFS
system that's accessible from the cluster nodes.

git clone https://github.com/aws/sagemaker-hyperpod-training-adapter-for-nemo.git
git clone --recursive https://github.com/aws/sagemaker-hyperpod-recipes.git
cd sagemaker-hyperpod-recipes
pip3 install -r requirements.txt

• Create a squash ﬁle using Enroot. To ﬁnd the most recent release of the SMP container, see
Release notes for the SageMaker model parallelism library. For more information about using
the Enroot ﬁle, see Build AWS-optimized Nemo-Launcher image.

REGION="<region>"

HyperPod recipes
1593

## Page 623

Amazon SageMaker AI
Developer Guide

IMAGE="658645717510.dkr.ecr.${REGION}.amazonaws.com/smdistributed-
modelparallel:2.4.1-gpu-py311-cu121"
aws ecr get-login-password --region ${REGION} | docker login --username AWS --
password-stdin 658645717510.dkr.ecr.${REGION}.amazonaws.com
enroot import -o $PWD/smdistributed-modelparallel.sqsh dockerd://${IMAGE}
mv $PWD/smdistributed-modelparallel.sqsh "/fsx/<any-path-in-the-shared-filesystem>"

• To use the Enroot squash ﬁle to start training, use the following example to modify the

recipes_collection/config.yaml ﬁle.

container: /fsx/path/to/your/smdistributed-modelparallel.sqsh

Launch the training job

To launch a PEFT job for the Llama 8 billion parameter model with a sequence length of

8192 on a single Slurm compute node, set the launch script, launcher_scripts/llama/

run_hf_llama3_8b_seq8k_gpu_lora.sh, to the following:

• IMAGE: The container from the environment setup section.

• HF_MODEL_NAME_OR_PATH: Deﬁne the name or the path of the pre-trained weights in the
hf_model_name_or_path parameter of the recipe.

• (Optional) You can provide the HuggingFace token if you need pre-trained weights from
HuggingFace by setting the following key-value pair:

recipes.model.hf_access_token=${HF_ACCESS_TOKEN}

#!/bin/bash
IMAGE="${YOUR_IMAGE}"
SAGEMAKER_TRAINING_LAUNCHER_DIR="${SAGEMAKER_TRAINING_LAUNCHER_DIR:-${PWD}}"

TRAIN_DIR="${YOUR_TRAIN_DIR}" # Location of training dataset
VAL_DIR="${YOUR_VAL_DIR}" # Location of validation dataset

# experiment output directory
EXP_DIR="${YOUR_EXP_DIR}"
HF_ACCESS_TOKEN="${YOUR_HF_TOKEN}"
HF_MODEL_NAME_OR_PATH="${YOUR_HF_MODEL_NAME_OR_PATH}"

# Add hf_model_name_or_path and turn off synthetic_data

HyperPod recipes
1594

## Page 624

Amazon SageMaker AI
Developer Guide

HYDRA_FULL_ERROR=1 python3 ${SAGEMAKER_TRAINING_LAUNCHER_DIR}/main.py \
recipes=fine-tuning/llama/hf_llama3_8b_seq8k_gpu_lora \
base_results_dir=${SAGEMAKER_TRAINING_LAUNCHER_DIR}/results \
recipes.run.name="hf_llama3_lora" \
recipes.exp_manager.exp_dir="$EXP_DIR" \
recipes.model.data.train_dir="$TRAIN_DIR" \
recipes.model.data.val_dir="$VAL_DIR" \
recipes.model.hf_model_name_or_path="$HF_MODEL_NAME_OR_PATH" \
container="${IMAGE}" \
+cluster.container_mounts.0="/fsx:/fsx" \
recipes.model.hf_access_token="${HF_ACCESS_TOKEN}"

After you've conﬁgured all the required parameters in the preceding script, you can initiate the
training job by running it.

bash launcher_scripts/llama/run_hf_llama3_8b_seq8k_gpu_lora.sh

For more information about the Slurm cluster conﬁguration, see Running a training job on
HyperPod Slurm.

Kubernetes cluster pre-training tutorial (GPU)

There are two ways to launch a training job in a GPU Kubernetes cluster:

• (Recommended) HyperPod command-line tool

• The NeMo style launcher

Prerequisites

Before you start setting up your environment, make sure you have:

• A HyperPod GPU Kubernetes cluster is setup properly.

• A shared storage location. It can be an Amazon FSx ﬁle system or NFS system that's
accessible from the cluster nodes.

• Data in one of the following formats:

• JSON

• JSONGZ (Compressed JSON)

• ARROW

HyperPod recipes
1595

## Page 625

Amazon SageMaker AI
Developer Guide

• (Optional) You must get a HuggingFace token if you're using the model weights from
HuggingFace for pre-training or ﬁne-tuning. For more information about getting the
token, see User access tokens.

GPU Kubernetes environment setup

To set up a GPU Kubernetes environment, do the following:

• Set up the virtual environment. Make sure you're using Python 3.9 or greater.

python3 -m venv ${PWD}/venv
source venv/bin/activate

• Install dependencies using one of the following methods:

• (Recommended): HyperPod command-line tool method:

# install HyperPod command line tools
git clone https://github.com/aws/sagemaker-hyperpod-cli
cd sagemaker-hyperpod-cli
pip3 install .

• SageMaker HyperPod recipes method:

# install SageMaker HyperPod Recipes.
git clone --recursive git@github.com:aws/sagemaker-hyperpod-recipes.git
cd sagemaker-hyperpod-recipes
pip3 install -r requirements.txt

• Set up kubectl and eksctl

• Install Helm

• Connect to your Kubernetes cluster

aws eks update-kubeconfig --region "CLUSTER_REGION" --name "CLUSTER_NAME"
hyperpod connect-cluster --cluster-name "CLUSTER_NAME" [--region "CLUSTER_REGION"]
[--namespace <namespace>]

HyperPod recipes
1596

## Page 626

Amazon SageMaker AI
Developer Guide

Launch the training job with the SageMaker HyperPod CLI

We recommend using the SageMaker HyperPod command-line interface (CLI) tool to submit
your training job with your conﬁgurations. The following example submits a training job for the

hf_llama3_8b_seq16k_gpu_p5x16_pretrain model.

• your_training_container: A Deep Learning container. To ﬁnd the most recent release of the
SMP container, see Release notes for the SageMaker model parallelism library.

• (Optional) You can provide the HuggingFace token if you need pre-trained weights from
HuggingFace by setting the following key-value pair:

"recipes.model.hf_access_token": "<your_hf_token>"

hyperpod start-job --recipe training/llama/hf_llama3_8b_seq16k_gpu_p5x16_pretrain \
--persistent-volume-claims fsx-claim:data \
--override-parameters \
'{
"recipes.run.name": "hf-llama3-8b",
"recipes.exp_manager.exp_dir": "/data/<your_exp_dir>",
"container": "658645717510.dkr.ecr.<region>.amazonaws.com/smdistributed-
modelparallel:2.4.1-gpu-py311-cu121",
"recipes.model.data.train_dir": "<your_train_data_dir>",
"recipes.model.data.val_dir": "<your_val_data_dir>",
"cluster": "k8s",
"cluster_type": "k8s"
}'

After you've submitted a training job, you can use the following command to verify if you
submitted it successfully.

kubectl get pods
NAME                             READY   STATUS             RESTARTS        AGE
hf-llama3-<your-alias>-worker-0   0/1     running         0               36s

If the STATUS is PENDING or ContainerCreating, run the following command to get more
details.

kubectl describe pod name_of_pod

HyperPod recipes
1597

## Page 627

Amazon SageMaker AI
Developer Guide

After the job STATUS changes to Running, you can examine the log by using the following
command.

kubectl logs name_of_pod

The STATUS becomes Completed when you run kubectl get pods.

Launch the training job with the recipes launcher

Alternatively, you can use the SageMaker HyperPod recipes to submit your training job. Using the

recipes involves updating k8s.yaml, config.yaml, and running the launch script.

• In k8s.yaml, update persistent_volume_claims. It mounts the Amazon FSx claim to the /

data directory of each computing pod

persistent_volume_claims:
- claimName: fsx-claim
mountPath: data

• In config.yaml, update repo_url_or_path under git.

git:
repo_url_or_path: <training_adapter_repo>
branch: null
commit: null
entry_script: null
token: null

• Update launcher_scripts/llama/

run_hf_llama3_8b_seq16k_gpu_p5x16_pretrain.sh

• your_contrainer: A Deep Learning container. To ﬁnd the most recent release of the SMP
container, see Release notes for the SageMaker model parallelism library.

• (Optional) You can provide the HuggingFace token if you need pre-trained weights from
HuggingFace by setting the following key-value pair:

recipes.model.hf_access_token=<your_hf_token>

#!/bin/bash
#Users should setup their cluster type in /recipes_collection/config.yaml

HyperPod recipes
1598

## Page 628

Amazon SageMaker AI
Developer Guide

REGION="<region>"
IMAGE="658645717510.dkr.ecr.${REGION}.amazonaws.com/smdistributed-
modelparallel:2.4.1-gpu-py311-cu121"
SAGEMAKER_TRAINING_LAUNCHER_DIR=${SAGEMAKER_TRAINING_LAUNCHER_DIR:-"$(pwd)"}
EXP_DIR="<your_exp_dir>" # Location to save experiment info including logging,
checkpoints, ect
TRAIN_DIR="<your_training_data_dir>" # Location of training dataset
VAL_DIR="<your_val_data_dir>" # Location of talidation dataset

HYDRA_FULL_ERROR=1 python3 "${SAGEMAKER_TRAINING_LAUNCHER_DIR}/main.py" \
recipes=training/llama/hf_llama3_8b_seq8k_gpu_p5x16_pretrain \
base_results_dir="${SAGEMAKER_TRAINING_LAUNCHER_DIR}/results" \
recipes.run.name="hf-llama3" \
recipes.exp_manager.exp_dir="$EXP_DIR" \
cluster=k8s \
cluster_type=k8s \
container="${IMAGE}" \

recipes.model.data.train_dir=$TRAIN_DIR \
recipes.model.data.val_dir=$VAL_DIR

• Launch the training job

bash launcher_scripts/llama/run_hf_llama3_8b_seq16k_gpu_p5x16_pretrain.sh

After you've submitted the training job, you can use the following command to verify if you
submitted it successfully.

kubectl get pods

NAME READY   STATUS             RESTARTS        AGE
hf-llama3-<your-alias>-worker-0   0/1     running         0               36s

If the STATUS is PENDING or ContainerCreating, run the following command to get more
details.

kubectl describe pod <name-of-pod>

After the job STATUS changes to Running, you can examine the log by using the following
command.

HyperPod recipes
1599

## Page 629

Amazon SageMaker AI
Developer Guide

kubectl logs name_of_pod

The STATUS will turn to Completed when you run kubectl get pods.

For more information about the k8s cluster conﬁguration, see Running a training job on HyperPod
k8s.

Trainium Kubernetes cluster pre-training tutorial

You can use one of the following methods to start a training job in a Trainium Kubernetes cluster.

• (Recommended) HyperPod command-line tool

• The NeMo style launcher

Prerequisites

Before you start setting up your environment, make sure you have:

• Set up a HyperPod Trainium Kubernetes cluster

• A shared storage location that can be an Amazon FSx ﬁle system or NFS system that's
accessible from the cluster nodes.

• Data in one of the following formats:

• JSON

• JSONGZ (Compressed JSON)

• ARROW

• (Optional) You must get a HuggingFace token if you're using the model weights from
HuggingFace for pre-training or ﬁne-tuning. For more information about getting the
token, see User access tokens.

Set up your Trainium Kubernetes environment

To set up the Trainium Kubernetes environment, do the following:

1. Complete the steps in the following tutorial: HuggingFace Llama3-8B Pretraining starting from

Download the dataset.

HyperPod recipes
1600

## Page 630

Amazon SageMaker AI
Developer Guide

2. Prepare a model conﬁguration. They're available in the Neuron repo. For this tutorial, you can

use the llama3 8b model conﬁg.

3. Virtual environment setup. Make sure you're using Python 3.9 or greater.

python3 -m venv ${PWD}/venv
source venv/bin/activate

4. Install the dependencies

• (Recommended) Use the following HyperPod command-line tool

# install HyperPod command line tools
git clone https://github.com/aws/sagemaker-hyperpod-cli
cd sagemaker-hyperpod-cli
pip3 install .

• If you're using SageMaker HyperPod recipes, specify the following

# install SageMaker HyperPod Recipes.
git clone --recursive git@github.com:aws/sagemaker-hyperpod-recipes.git
cd sagemaker-hyperpod-recipes
pip3 install -r requirements.txt

5. Set up kubectl and eksctl

6. Install Helm

7. Connect to your Kubernetes cluster

aws eks update-kubeconfig --region "${CLUSTER_REGION}" --name "${CLUSTER_NAME}"
hyperpod connect-cluster --cluster-name "${CLUSTER_NAME}" [--region
"${CLUSTER_REGION}"] [--namespace <namespace>]

8. Container: The Neuron container

Launch the training job with the SageMaker HyperPod CLI

We recommend using the SageMaker HyperPod command-line interface (CLI) tool to submit
your training job with your conﬁgurations. The following example submits a training job for the

hf_llama3_8b_seq8k_trn1x4_pretrain Trainium model.

• your_neuron_container: The Neuron container.

HyperPod recipes
1601

## Page 631

Amazon SageMaker AI
Developer Guide

• your_model_config: The model conﬁguration from the environment setup section

• (Optional) You can provide the HuggingFace token if you need pre-trained weights from
HuggingFace by setting the following key-value pair:

"recipes.model.hf_access_token": "<your_hf_token>"

hyperpod start-job --recipe training/llama/hf_llama3_8b_seq8k_trn1x4_pretrain \
--persistent-volume-claims fsx-claim:data \
--override-parameters \
'{
"cluster": "k8s",
"cluster_type": "k8s",
"container": "<your_neuron_contrainer>",
"recipes.run.name": "hf-llama3",
"recipes.run.compile": 0,
"recipes.model.model_config": "<your_model_config>",
"instance_type": "trn1.32xlarge",
"recipes.data.train_dir": "<your_train_data_dir>"
}'

After you've submitted a training job, you can use the following command to verify if you
submitted it successfully.

kubectl get pods
NAME                              READY   STATUS             RESTARTS        AGE
hf-llama3-<your-alias>-worker-0   0/1     running         0               36s

If the STATUS is PENDING or ContainerCreating, run the following command to get more
details.

kubectl describe pod name_of_pod

After the job STATUS changes to Running, you can examine the log by using the following
command.

kubectl logs name_of_pod

The STATUS will turn to Completed when you run kubectl get pods.

HyperPod recipes
1602

## Page 632

Amazon SageMaker AI
Developer Guide

Launch the training job with the recipes launcher

Alternatively, use SageMaker HyperPod recipes to submit your training job. To submit the training

job using a recipe, update k8s.yaml and config.yaml. Run the bash script for the model to

launch it.

• In k8s.yaml, update persistent_volume_claims to mount the Amazon FSx claim to the /data
directory in the compute nodes

persistent_volume_claims:
- claimName: fsx-claim
mountPath: data

• Update launcher_scripts/llama/run_hf_llama3_8b_seq8k_trn1x4_pretrain.sh

• your_neuron_contrainer: The container from the environment setup section

• your_model_config: The model conﬁg from the environment setup section

(Optional) You can provide the HuggingFace token if you need pre-trained weights from
HuggingFace by setting the following key-value pair:

recipes.model.hf_access_token=<your_hf_token>

#!/bin/bash
#Users should set up their cluster type in /recipes_collection/config.yaml
IMAGE="<your_neuron_contrainer>"
MODEL_CONFIG="<your_model_config>"
SAGEMAKER_TRAINING_LAUNCHER_DIR=${SAGEMAKER_TRAINING_LAUNCHER_DIR:-"$(pwd)"}
TRAIN_DIR="<your_training_data_dir>" # Location of training dataset
VAL_DIR="<your_val_data_dir>" # Location of talidation dataset

HYDRA_FULL_ERROR=1 python3 "${SAGEMAKER_TRAINING_LAUNCHER_DIR}/main.py" \
recipes=training/llama/hf_llama3_8b_seq8k_trn1x4_pretrain \
base_results_dir="${SAGEMAKER_TRAINING_LAUNCHER_DIR}/results" \
recipes.run.name="hf-llama3-8b" \
instance_type=trn1.32xlarge \
recipes.model.model_config="$MODEL_CONFIG" \
cluster=k8s \
cluster_type=k8s \
container="${IMAGE}" \
recipes.data.train_dir=$TRAIN_DIR \

HyperPod recipes
1603

## Page 633

Amazon SageMaker AI
Developer Guide

recipes.data.val_dir=$VAL_DIR

• Launch the job

bash launcher_scripts/llama/run_hf_llama3_8b_seq8k_trn1x4_pretrain.sh

After you've submitted a training job, you can use the following command to verify if you
submitted it successfully.

kubectl get pods
NAME                             READY   STATUS             RESTARTS        AGE
hf-llama3-<your-alias>-worker-0   0/1     running         0               36s

If the STATUS is at PENDING or ContainerCreating, run the following command to get more
details.

kubectl describe pod name_of_pod

After the job STATUS changes to Running, you can examine the log by using the following
command.

kubectl logs name_of_pod

The STATUS will turn to Completed when you run kubectl get pods.

For more information about the k8s cluster conﬁguration, see Trainium Kubernetes cluster pre-
training tutorial.

SageMaker training jobs pre-training tutorial (GPU)

This tutorial guides you through the process of setting up and running a pre-training job using
SageMaker training jobs with GPU instances.

• Set up your environment

• Launch a training job using SageMaker HyperPod recipes

Before you begin, make sure you have following prerequisites.

HyperPod recipes
1604

## Page 634

Amazon SageMaker AI
Developer Guide

Prerequisites

Before you start setting up your environment, make sure you have:

• Amazon FSx ﬁle system or an Amazon S3 bucket where you can load the data and output
the training artifacts.

• Requested a Service Quota for 1x ml.p4d.24xlarge and 1x ml.p5.48xlarge on Amazon
SageMaker AI. To request a service quota increase, do the following:

1.
On the AWS Service Quotas console, navigate to AWS services,

2.
Choose Amazon SageMaker AI.

3.
Choose one ml.p4d.24xlarge and one ml.p5.48xlarge instance.

• Create an AWS Identity and Access Management(IAM) role with the following managed
policies to give SageMaker AI permissions to run the examples.

• AmazonSageMakerFullAccess

• AmazonEC2FullAccess

• Data in one of the following formats:

• JSON

• JSONGZ (Compressed JSON)

• ARROW

• (Optional) You must get a HuggingFace token if you're using the model weights from
HuggingFace for pre-training or ﬁne-tuning. For more information about getting the
token, see User access tokens.

GPU SageMaker training jobs environment setup

Before you run a SageMaker training job, conﬁgure your AWS credentials and preferred region

by running the aws configure command. As an alternative to the conﬁgure command, you

can provide your credentials through environment variables such as AWS_ACCESS_KEY_ID,

AWS_SECRET_ACCESS_KEY, and AWS_SESSION_TOKEN. For more information, see SageMaker AI
Python SDK.

We strongly recommend using a SageMaker AI Jupyter notebook in SageMaker AI JupyterLab to
launch a SageMaker training job. For more information, see SageMaker JupyterLab.

HyperPod recipes
1605

## Page 635

Amazon SageMaker AI
Developer Guide

• (Optional) Set up the virtual environment and dependencies. If you are using a Jupyter notebook
in Amazon SageMaker Studio, you can skip this step. Make sure you're using Python 3.9 or
greater.

# set up a virtual environment
python3 -m venv ${PWD}/venv
source venv/bin/activate
# install dependencies after git clone.

git clone --recursive git@github.com:aws/sagemaker-hyperpod-recipes.git
cd sagemaker-hyperpod-recipes
pip3 install -r requirements.txt
# Set the aws region.

aws configure set <your_region>

• Install SageMaker AI Python SDK

pip3 install --upgrade sagemaker

• Container: The GPU container is set automatically by the SageMaker AI Python SDK. You can
also provide your own container.

Note

If you're running a Llama 3.2 multi-modal training job, the transformers version must

be 4.45.2 or greater.

Append transformers==4.45.2 to requirements.txt in source_dir only when you're
using the SageMaker AI Python SDK. For example, append it if you're using it in a notebook in
SageMaker AI JupyterLab.

If you are using HyperPod recipes to launch using cluster type sm_jobs, this will be done
automatically.

HyperPod recipes
1606

## Page 636

Amazon SageMaker AI
Developer Guide

Launch the training job using a Jupyter Notebook

You can use the following Python code to run a SageMaker training job with your recipe. It
leverages the PyTorch estimator from the SageMaker AI Python SDK to submit the recipe. The
following example launches the llama3-8b recipe on the SageMaker AI Training platform.

import os

import sagemaker,boto3
from sagemaker.debugger import TensorBoardOutputConfig

from sagemaker.pytorch import PyTorch

sagemaker_session = sagemaker.Session()
role = sagemaker.get_execution_role()

bucket = sagemaker_session.default_bucket()
output = os.path.join(f"s3://{bucket}", "output")
output_path = "<s3-URI>"

overrides = {
"run": {
"results_dir": "/opt/ml/model",
},
"exp_manager": {
"exp_dir": "",
"explicit_log_dir": "/opt/ml/output/tensorboard",
"checkpoint_dir": "/opt/ml/checkpoints",
},
"model": {
"data": {
"train_dir": "/opt/ml/input/data/train",
"val_dir": "/opt/ml/input/data/val",
},
},
}

tensorboard_output_config = TensorBoardOutputConfig(
s3_output_path=os.path.join(output, 'tensorboard'),
container_local_output_path=overrides["exp_manager"]["explicit_log_dir"]
)

estimator = PyTorch(
output_path=output_path,

HyperPod recipes
1607

## Page 637

Amazon SageMaker AI
Developer Guide

base_job_name=f"llama-recipe",
role=role,
instance_type="ml.p5.48xlarge",
training_recipe="training/llama/hf_llama3_8b_seq8k_gpu_p5x16_pretrain",
recipe_overrides=recipe_overrides,
sagemaker_session=sagemaker_session,
tensorboard_output_config=tensorboard_output_config,
)

estimator.fit(inputs={"train": "s3 or fsx input", "val": "s3 or fsx input"}, wait=True)

The preceding code creates a PyTorch estimator object with the training recipe and then ﬁts the

model using the fit() method. Use the training_recipe parameter to specify the recipe you want
to use for training.

Note

If you're running a Llama 3.2 multi-modal training job, the transformers version must be
4.45.2 or greater.

Append transformers==4.45.2 to requirements.txt in source_dir only when you're using
SageMaker AI Python SDK directly. For example, you must append the version to the text ﬁle when
you're using a Jupyter notebook.

When you deploy the endpoint for a SageMaker training job, you must specify the image URI that
you're using. If don't provide the image URI, the estimator uses the training image as the image
for the deployment. The training images that SageMaker HyperPod provides don't contain the
dependencies required for inference and deployment. The following is an example of how an
inference image can be used for deployment:

from sagemaker import image_uris
container=image_uris.retrieve(framework='pytorch',region='us-
west-2',version='2.0',py_version='py310',image_scope='inference',
instance_type='ml.p4d.24xlarge')
predictor =
estimator.deploy(initial_instance_count=1,instance_type='ml.p4d.24xlarge',image_uri=container)

HyperPod recipes
1608

## Page 638

Amazon SageMaker AI
Developer Guide

Note

Running the preceding code on Sagemaker notebook instance might need more than
the default 5GB of storage that SageMaker AI JupyterLab provides. If you run into space
not available issues, create a new notebook instance where you use a diﬀerent notebook
instance and increase the storage of the notebook.

Launch the training job with the recipes launcher

Update the ./recipes_collection/cluster/sm_jobs.yaml ﬁle to look like the following:

sm_jobs_config:
output_path: <s3_output_path>
tensorboard_config:
output_path: <s3_output_path>
container_logs_path: /opt/ml/output/tensorboard  # Path to logs on the container
wait: True  # Whether to wait for training job to finish
inputs:  # Inputs to call fit with. Set either s3 or file_system, not both.
s3:  # Dictionary of channel names and s3 URIs. For GPUs, use channels for train
and validation.
train: <s3_train_data_path>
val: null
additional_estimator_kwargs:  # All other additional args to pass to estimator. Must
be int, float or string.
max_run: 180000
enable_remote_debug: True
recipe_overrides:
exp_manager:
explicit_log_dir: /opt/ml/output/tensorboard
data:
train_dir: /opt/ml/input/data/train
model:
model_config: /opt/ml/input/data/train/config.json
compiler_cache_url: "<compiler_cache_url>"

Update ./recipes_collection/config.yaml to specify sm_jobs in the cluster and

cluster_type.

defaults:
- _self_

HyperPod recipes
1609

## Page 639

Amazon SageMaker AI
Developer Guide

- cluster: sm_jobs  # set to `slurm`, `k8s` or `sm_jobs`, depending on the desired
cluster
- recipes: training/llama/hf_llama3_8b_seq8k_trn1x4_pretrain
cluster_type: sm_jobs  # bcm, bcp, k8s or sm_jobs. If bcm, k8s or sm_jobs, it must
match - cluster above.

Launch the job with the following command

python3 main.py --config-path recipes_collection --config-name config

For more information about conﬁguring SageMaker training jobs, see Run a training job on
SageMaker training jobs.

Trainium SageMaker training jobs pre-training tutorial

This tutorial guides you through the process of setting up and running a pre-training job using
SageMaker training jobs with AWS Trainium instances.

• Set up your environment

• Launch a training job

Before you begin, make sure you have the following prerequisites.

Prerequisites

Before you start setting up your environment, make sure you have:

• Amazon FSx ﬁle system or S3 bucket where you can load the data and output the
training artifacts.

• Request a Service Quota for the ml.trn1.32xlarge instance on Amazon SageMaker AI.
To request a service quota increase, do the following:

To request a service quota increase for ml.trn1.32xlarge instance

1.
Navigate to the AWS Service Quotas console.

2.
Choose AWS services.

3.
Select JupyterLab.

4.
Specify one instance for ml.trn1.32xlarge.

HyperPod recipes
1610

## Page 640

Amazon SageMaker AI
Developer Guide

• Create an AWS Identity and Access Management (IAM) role with the

AmazonSageMakerFullAccess and AmazonEC2FullAccess managed policies. These
policies provide Amazon SageMaker AI with permissions to run the examples.

• Data in one of the following formats:

• JSON

• JSONGZ (Compressed JSON)

• ARROW

• (Optional) If you need the pre-trained weights from HuggingFace or if you're training
a Llama 3.2 model, you must get the HuggingFace token before you start training. For
more information about getting the token, see User access tokens.

Set up your environment for Trainium SageMaker training jobs

Before you run a SageMaker training job, use the aws configure command to conﬁgure your
AWS credentials and preferred region . As an alternative, you can also provide your credentials

through environment variables such as the AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY,

and AWS_SESSION_TOKEN. For more information, see SageMaker AI Python SDK.

We strongly recommend using a SageMaker AI Jupyter notebook in SageMaker AI JupyterLab to
launch a SageMaker training job. For more information, see SageMaker JupyterLab.

• (Optional) If you are using Jupyter notebook in Amazon SageMaker Studio, you can skip running
the following command. Make sure to use a version >= python 3.9

# set up a virtual environment
python3 -m venv ${PWD}/venv
source venv/bin/activate
# install dependencies after git clone.

git clone --recursive git@github.com:aws/sagemaker-hyperpod-recipes.git
cd sagemaker-hyperpod-recipes
pip3 install -r requirements.txt

• Install SageMaker AI Python SDK

pip3 install --upgrade sagemaker

HyperPod recipes
1611

## Page 641

Amazon SageMaker AI
Developer Guide

• • If you are running a llama 3.2 multi-modal training job, the transformers version must be

4.45.2 or greater.

• Append transformers==4.45.2 to requirements.txt in source_dir only when you're
using the SageMaker AI Python SDK.

• If you are using HyperPod recipes to launch using sm_jobs as the cluster type, you don't
have to specify the transformers version.

• Container: The Neuron container is set automatically by SageMaker AI Python SDK.

Launch the training job with a Jupyter Notebook

You can use the following Python code to run a SageMaker training job using your recipe. It
leverages the PyTorch estimator from the SageMaker AI Python SDK to submit the recipe. The
following example launches the llama3-8b recipe as a SageMaker AI Training Job.

• compiler_cache_url: Cache to be used to save the compiled artifacts, such as an Amazon S3
artifact.

import os
import sagemaker,boto3
from sagemaker.debugger import TensorBoardOutputConfig

from sagemaker.pytorch import PyTorch

sagemaker_session = sagemaker.Session()
role = sagemaker.get_execution_role()

recipe_overrides = {
"run": {
"results_dir": "/opt/ml/model",
},
"exp_manager": {
"explicit_log_dir": "/opt/ml/output/tensorboard",
},
"data": {
"train_dir": "/opt/ml/input/data/train",
},
"model": {
"model_config": "/opt/ml/input/data/train/config.json",
},

HyperPod recipes
1612

## Page 642

Amazon SageMaker AI
Developer Guide

"compiler_cache_url": "<compiler_cache_url>"
}

tensorboard_output_config = TensorBoardOutputConfig(
s3_output_path=os.path.join(output, 'tensorboard'),
container_local_output_path=overrides["exp_manager"]["explicit_log_dir"]
)

estimator = PyTorch(
output_path=output_path,
base_job_name=f"llama-trn",
role=role,
instance_type="ml.trn1.32xlarge",
sagemaker_session=sagemaker_session,
training_recipe="training/llama/hf_llama3_70b_seq8k_trn1x16_pretrain",
recipe_overrides=recipe_overrides,
)

estimator.fit(inputs={"train": "your-inputs"}, wait=True)

The preceding code creates a PyTorch estimator object with the training recipe and then ﬁts the

model using the fit() method. Use the training_recipe parameter to specify the recipe you
want to use for training.

Launch the training job with the recipes launcher

• Update ./recipes_collection/cluster/sm_jobs.yaml

• compiler_cache_url: The URL used to save the artifacts. It can be an Amazon S3 URL.

sm_jobs_config:
output_path: <s3_output_path>
wait: True
tensorboard_config:
output_path: <s3_output_path>
container_logs_path: /opt/ml/output/tensorboard  # Path to logs on the container
wait: True  # Whether to wait for training job to finish
inputs:  # Inputs to call fit with. Set either s3 or file_system, not both.
s3:  # Dictionary of channel names and s3 URIs. For GPUs, use channels for train
and validation.
train: <s3_train_data_path>
val: null
additional_estimator_kwargs:  # All other additional args to pass to estimator.
Must be int, float or string.

HyperPod recipes
1613

## Page 643

Amazon SageMaker AI
Developer Guide

max_run: 180000
image_uri: <your_image_uri>
enable_remote_debug: True
py_version: py39
recipe_overrides:
model:
exp_manager:
exp_dir: <exp_dir>
data:
train_dir: /opt/ml/input/data/train
val_dir: /opt/ml/input/data/val

• Update ./recipes_collection/config.yaml

defaults:
- _self_
- cluster: sm_jobs
- recipes: training/llama/hf_llama3_8b_seq8k_trn1x4_pretrain
cluster_type: sm_jobs # bcm, bcp, k8s or sm_jobs. If bcm, k8s or sm_jobs, it must
match - cluster above.

instance_type: ml.trn1.32xlarge
base_results_dir: ~/sm_job/hf_llama3_8B # Location to store the results, checkpoints
and logs.

• Launch the job with main.py

python3 main.py --config-path recipes_collection --config-name config

For more information about conﬁguring SageMaker training jobs, see SageMaker training jobs pre-
training tutorial (GPU).

Default conﬁgurations

This section outlines the essential components and settings required to initiate and customize your
Large Language Model (LLM) training processes using SageMaker HyperPod. This section covers the
key repositories, conﬁguration ﬁles, and recipe structures that form the foundation of your training
jobs. Understanding these default conﬁgurations is crucial for eﬀectively setting up and managing
your LLM training workﬂows, whether you're using pre-deﬁned recipes or customizing them to suit
your speciﬁc needs.

HyperPod recipes
1614

## Page 644

Amazon SageMaker AI
Developer Guide

Topics

• GitHub repositories

• General conﬁguration

GitHub repositories

To launch a training job, you utilize ﬁles from two distinct GitHub repositories:

• SageMaker HyperPod recipes

• SageMaker HyperPod training adapter for NeMo

These repositories contain essential components for initiating, managing, and customizing Large
Language Model (LLM) training processes. You use the scripts from the repositories to set up and
run the training jobs for your LLMs.

HyperPod recipe repository

Use the SageMaker HyperPod recipes repository to get a recipe.

1. main.py: This ﬁle serves as the primary entry point for initiating the process of submitting a

training job to either a cluster or a SageMaker training job.

2. launcher_scripts: This directory contains a collection of commonly used scripts designed to

facilitate the training process for various Large Language Models (LLMs).

3. recipes_collection: This folder houses a compilation of pre-deﬁned LLM recipes provided

by the developers. Users can leverage these recipes in conjunction with their custom data to
train LLM models tailored to their speciﬁc requirements.

You use the SageMaker HyperPod recipes to launch training or ﬁne-tuning jobs. Regardless of the
cluster you're using, the process of submitting the job is the same. For example, you can use the
same script to submit a job to a Slurm or Kubernetes cluster. The launcher dispatches a training job
based on three conﬁguration ﬁles:

1. General Conﬁguration (config.yaml): Includes common settings such as the default

parameters or environment variables used in the training job.

2. Cluster Conﬁguration (cluster): For training jobs using clusters only. If you're submitting a

training job to a Kubernetes cluster, you might need to specify information such as volume,

HyperPod recipes
1615

## Page 645

Amazon SageMaker AI
Developer Guide

label, or restart policy. For Slurm clusters, you might need to specify the Slurm job name. All the
parameters are related to the speciﬁc cluster that you're using.

3. Recipe (recipes): Recipes contain the settings for your training job, such as the model types,

sharding degree, or dataset paths. For example, you can specify Llama as your training model
and train it using model or data parallelism techniques like Fully Sharded Distributed Parallel
(FSDP) across eight machines. You can also specify diﬀerent checkpoint frequencies or paths for
your training job.

After you've speciﬁed a recipe, you run the launcher script to specify an end-to-end training job

on a cluster based on the conﬁgurations through the main.py entry point. For each recipe that
you use, there are accompanying shell scripts located in the launch_scripts folder. These examples
guide you through submitting and initiating training jobs. The following ﬁgure illustrates how a
SageMaker HyperPod recipe launcher submits a training job to a cluster based on the preceding.
Currently, the SageMaker HyperPod recipe launcher is built on top of the Nvidia NeMo Framework
Launcher. For more information, see NeMo Launcher Guide.

![Page 645 Diagram 1](images/page-0645-img-01.png)

HyperPod recipe adapter repository

The SageMaker HyperPod training adapter is a training framework. You can use it to manage
the entire lifecycle of your training jobs. Use the adapter to distribute the pre-training or ﬁne-
tuning of your models across multiple machines. The adaptor uses diﬀerent parallelism techniques
to distribute the training. It also handles the implementation and management of saving the
checkpoints. For more details, see Advanced settings.

Use the SageMaker HyperPod recipe adapter repository to use the recipe adapter.

HyperPod recipes
1616

## Page 646

Amazon SageMaker AI
Developer Guide

1. src: This directory contains the implementation of Large-scale Language Model (LLM) training,

encompassing various features such as model parallelism, mixed-precision training, and
checkpointing management.

2. examples: This folder provides a collection of examples demonstrating how to create an entry

point for training an LLM model, serving as a practical guide for users.

General conﬁguration

The conﬁg.yaml ﬁle speciﬁes the training recipe and the cluster. It also includes runtime
conﬁgurations such as environment variables for the training job.

defaults:
- _self_
- cluster: slurm
- recipes: training/llama/hf_llama3_8b_seq8192_gpu
instance_type: p5.48xlarge
git:
repo_url_or_path: null
branch: null
commit: null
entry_script: null
token: null
env_vars:
NCCL_DEBUG: WARN

You can modify the following parameters in config.yaml:

1. defaults: Specify your default settings, such as the default cluster or default recipes.

2. instance_type: Modify the Amazon EC2 instance type to match the instance type that you're

using.

3. git: Specify the location of the SageMaker HyperPod recipe adapter repository for the training

job.

4. env_vars: You can specify the environment variables to be passed into your runtime training

job. For example, you can adjust the logging level of NCCL by specifying the NCCL_DEBUG
environment variable.

The recipe is the core conﬁguration that deﬁnes your training job architecture. This ﬁle includes
many important pieces of information for your training job, such as the following:

HyperPod recipes
1617

## Page 647

Amazon SageMaker AI
Developer Guide

• Whether to use model parallelism

• The source of your datasets

• Mixed precision training

• Checkpointing-related conﬁgurations

You can use the recipes as-is. You can also use the following information to modify them.

run

The following is the basic run information for running your training job.

run:
name: llama-8b
results_dir: ${base_results_dir}/${.name}
time_limit: "6-00:00:00"
model_type: hf

1. name: Specify the name for your training job in the conﬁguration ﬁle.

2. results_dir: You can specify the directory where the results of your training job are stored.

3. time_limit: You can set a maximum training time for your training job to prevent it from

occupying hardware resources for too long.

4. model_type: You can specify the type of model you are using. For example, you can specify hf

if your model is from HuggingFace.

exp_manager

The exp_manager conﬁgures the experiment. With the exp_manager, you can specify ﬁelds such as
the output directory or checkpoint settings. The following is an example of how you can conﬁgure
the exp_manager.

exp_manager:
exp_dir: null
name: experiment
create_tensorboard_logger: True

HyperPod recipes
1618

## Page 648

Amazon SageMaker AI
Developer Guide

1. exp_dir: The experiment directory includes the standard output and standard error ﬁles for

your training job. By default, it uses your current directory.

2. name: The experiment name used to identify your experiment under the exp_dir.

3. create_tensorboard_logger: Specify True or False to enable or disable the TensorBoard

logger.

Checkpointing

Here are three types of checkpointing we support:

• Auto checkpointing

• Manual checkpointing

• Full checkpointing

Auto checkpointing

If you're saving or loading checkpoints that are automatically managed by the SageMaker

HyperPod recipe adapter, you can enable auto_checkpoint. To enable auto_checkpoint, set

enabled to True. You can use auto checkpointing for both training and ﬁne-tuning. You can use
auto checkpoinitng for both shared ﬁle systems and Amazon S3.

exp_manager
checkpoint_dir: ${recipes.exp_manager.exp_dir}/checkpoints/
auto_checkpoint:
enabled: True

Auto checkpoint is saving the local_state_dict asynchronously with an automatically computed
optimal saving interval.

Note

Under this checkpointing mode, the auto saved checkpointing doesn't support re-sharding
between training runs. To resume from the latest auto saved checkpoint, you must preserve
the same shard degrees. You don't need to specify extra information to auto resume.

HyperPod recipes
1619

## Page 649

Amazon SageMaker AI
Developer Guide

Manual checkpointing

You can modify checkpoint_callback_params to asynchronously save an intermediate
checkpoint in shared_state_dict. For example, you can specify the following conﬁguration to
enable sharded checkpointing every 10 steps and keep the latest 3 checkpoints.

Sharded checkpointing allows you to change the shard degrees between training runs and load the

checkpoint by setting resume_from_checkpoint.

Note

• If is a PEFT ﬁne tuning, sharded checkpointing doesn't support Amazon S3.

• Auto and manual checkpointing are mutually exclusive.

• Only FSDP shard degrees and replication degrees changes are allowed.

exp_manager:
checkpoint_callback_params:
# Set save_top_k = 0 to disable sharded checkpointing
save_top_k: 3
every_n_train_steps: 10
monitor: "step"
mode: "max"
save_last: False
resume_from_checkpoint: ${recipes.exp_manager.exp_dir}/checkpoints/

To learn more about checkpointing, see Checkpointing using SMP.

Full checkpointing

The exported full_state_dict checkpoint can be used for inference or ﬁne tuning. You can load a full
checkpoint through hf_model_name_or_path. Under this mode, only the model weights are saved.

To export the full_state_dict model, you can set the following parameters.

Note

Currently, full checkpointing isn't supported for Amazon S3 checkpointing. You can't set

the S3 path for exp_manager.checkpoint_dir if you're enabling full checkpointing.

HyperPod recipes
1620

## Page 650

Amazon SageMaker AI
Developer Guide

However, you can set exp_manager.export_full_model.final_export_dir to a

speciﬁc directory on your local ﬁlesystem while setting exp_manager.checkpoint_dir

to an Amazon S3 path.

exp_manager:
export_full_model:
# Set every_n_train_steps = 0 to disable full checkpointing
every_n_train_steps: 0
save_last: True
final_export_dir : null

model

Deﬁne various aspects of your model architecture and training process. This includes settings for
model parallelism, precision, and data handling. Below are the key components you can conﬁgure
within the model section:

model parallelism

After you've speciﬁed the recipe, you deﬁne the model that you're training. You can also deﬁne
the model parallelism. For example, you can deﬁne tensor_model_parallel_degree. You can enable
other features like training with FP8 precision. For example, you can train a model with tensor
parallelism and context parallelism:

model:
model_type: llama_v3
# Base configs
train_batch_size: 4
val_batch_size: 1
seed: 12345
grad_clip: 1.0

# Model parallelism
tensor_model_parallel_degree: 4
expert_model_parallel_degree: 1
context_parallel_degree: 2

To gain a better understanding of diﬀerent types of model parallelism techniques, you can refer to
the following approaches:

HyperPod recipes
1621

## Page 651

Amazon SageMaker AI
Developer Guide

1. the section called “Tensor parallelism”

2. the section called “Expert parallelism”

3. the section called “Context parallelism”

4. the section called “Hybrid sharded data parallelism”

FP8

To enable FP8 (8-bit ﬂoating-point precision), you can specify the FP8-related conﬁguration in the
following example:

model:
# FP8 config
fp8: True
fp8_amax_history_len: 1024
fp8_amax_compute_algo: max

It's important to note that the FP8 data format is currently supported only on the P5 instance type.
If you are using an older instance type, such as P4, disable the FP8 feature for your model training
process. For more information about FP8, see Mixed precision training.

data

You can specify your custom datasets for your training job by adding the data paths under data.
The data module in our system supports the following data formats:

1. JSON

2. JSONGZ (Compressed JSON)

3. ARROW

However, you are responsible for preparing your own pre-tokenized dataset. If you're an advanced
user with speciﬁc requirements, there is also an option to implement and integrate a customized
data module. For more information on HuggingFace datasets, see Datasets.

model:
data:
train_dir: /path/to/your/train/data
val_dir: /path/to/your/val/data

HyperPod recipes
1622

## Page 652

Amazon SageMaker AI
Developer Guide

dataset_type: hf
use_synthetic_data: False

You can specify how you're training the model. By default, the recipe uses pre-training instead of
ﬁne-tuning. The following example conﬁgures the recipe to run a ﬁne-tuning job with LoRA (Low-
Rank Adaptation).

model:
# Fine tuning config
do_finetune: True
# The path to resume from, needs to be HF compatible
hf_model_name_or_path: null
hf_access_token: null
# PEFT config
peft:
peft_type: lora
rank: 32
alpha: 16
dropout: 0.1

For information about the recipes, see SageMaker HyperPod recipes.

Cluster-speciﬁc conﬁgurations

SageMaker HyperPod oﬀers ﬂexibility in running training jobs across diﬀerent cluster
environments. Each environment has its own conﬁguration requirements and setup process. This
section outlines the steps and conﬁgurations needed for running training jobs in SageMaker
HyperPod Slurm, SageMaker HyperPod k8s, and SageMaker training jobs. Understanding these
conﬁgurations is crucial for eﬀectively leveraging the power of distributed training in your chosen
environment.

You can use a recipe in the following cluster environments:

• SageMaker HyperPod Slurm Orchestration

• SageMaker HyperPod Amazon Elastic Kubernetes Service Orchestration

• SageMaker training jobs

To launch a training job in a cluster, set and install the corresponding cluster conﬁguration and
environment.

HyperPod recipes
1623

## Page 653

Amazon SageMaker AI
Developer Guide

Topics

• Running a training job on HyperPod Slurm

• Running a training job on HyperPod k8s

• Running a SageMaker training job

Running a training job on HyperPod Slurm

SageMaker HyperPod Recipes supports submitting a training job to a GPU/Trainium slurm cluster.
Before you submit the training job, update the cluster conﬁguration. Use one of the following
methods to update the cluster conﬁguration:

• Modify slurm.yaml

• Override it through the command line

After you've updated the cluster conﬁguration, install the environment.

Conﬁgure the cluster

To submit a training job to a Slurm cluster, specify the Slurm-speciﬁc conﬁguration. Modify

slurm.yaml to conﬁgure the Slurm cluster. The following is an example of a Slurm cluster
conﬁguration. You can modify this ﬁle for your own training needs:

job_name_prefix: 'sagemaker-'
slurm_create_submission_file_only: False
stderr_to_stdout: True
srun_args:
# - "--no-container-mount-home"
slurm_docker_cfg:
docker_args:
# - "--runtime=nvidia"
post_launch_commands:
container_mounts:
- "/fsx:/fsx"

1. job_name_prefix: Specify a job name preﬁx to easily identify your submissions to the Slurm

cluster.

2. slurm_create_submission_file_only: Set this conﬁguration to True for a dry run to help

you debug.

HyperPod recipes
1624

## Page 654

Amazon SageMaker AI
Developer Guide

3. stderr_to_stdout: Specify whether you're redirecting your standard error (stderr) to standard

output (stdout).

4. srun_args: Customize additional srun conﬁgurations, such as excluding speciﬁc compute

nodes. For more information, see the srun documentation.

5. slurm_docker_cfg: The SageMaker HyperPod recipe launcher launches a Docker container to

run your training job. You can specify additional Docker arguments within this parameter.

6. container_mounts: Specify the volumes you're mounting into the container for the recipe

launcher, for your training jobs to access the ﬁles in those volumes.

Running a training job on HyperPod k8s

SageMaker HyperPod Recipes supports submitting a training job to a GPU/Trainium Kubernetes
cluster. Before you submit the training job do one of the following:

• Modify the k8s.yaml cluster conﬁguration ﬁle

• Override the cluster conﬁguration through the command line

After you've done either of the preceding steps, install the corresponding environment.

Conﬁgure the cluster using k8s.yaml

To submit a training job to a Kubernetes cluster, you specify Kubernetes-speciﬁc conﬁgurations.
The conﬁgurations include the cluster namespace or the location of the persistent volume.

pullPolicy: Always
restartPolicy: Never
namespace: default
persistent_volume_claims:
- null

1. pullPolicy: You can specify the pull policy when you submit a training job. If you specify

"Always," the Kubernetes cluster always pulls your image from the repository. For more
information, see Image pull policy.

2. restartPolicy: Specify whether to restart your training job if it fails.

3. namespace: You can specify the Kubernetes namespace where you're submitting the training

job.

HyperPod recipes
1625

## Page 655

Amazon SageMaker AI
Developer Guide

4. persistent_volume_claims: You can specify a shared volume for your training job for all

training processes to access the ﬁles in the volume.

Running a SageMaker training job

SageMaker HyperPod Recipes supports submitting a SageMaker training job. Before you submit the

training job, you must update the cluster conﬁguration, sm_job.yaml, and install corresponding
environment.

Use your recipe as a SageMaker training job

You can use your recipe as a SageMaker training job if you aren't hosting a cluster. You must modify

the SageMaker training job conﬁguration ﬁle, sm_job.yaml, to run your recipe.

sm_jobs_config:
output_path: null
tensorboard_config:
output_path: null
container_logs_path: null
wait: True
inputs:
s3:
train: null
val: null
file_system:
directory_path: null
additional_estimator_kwargs:
max_run: 1800

1. output_path: You can specify where you're saving your model to an Amazon S3 URL.

2. tensorboard_config: You can specify a TensorBoard related conﬁguration such as the output

path or TensorBoard logs path.

3. wait: You can specify whether you're waiting for the job to be completed when you submit your

training job.

4. inputs: You can specify the paths for your training and validation data. The data source can be

from a shared ﬁlesystem such as Amazon FSx or an Amazon S3 URL.

5. additional_estimator_kwargs: Additional estimator arguments for submitting a training

job to the SageMaker training job platform. For more information, see Algorithm Estimator.

HyperPod recipes
1626

## Page 656

Amazon SageMaker AI
Developer Guide

Considerations

When you're using a Amazon SageMaker HyperPod recipes, there are some factors that can impact
the process of model training.

• The transformers version must be 4.45.2 or greater for Llama 3.2. If you're using a Slurm or
K8s workﬂow, the version is automatically updated.

• Mixtral does not support 8-bit ﬂoating point precision (FP8)

• Amazon EC2 p4 instance does not support FP8

Advanced settings

The SageMaker HyperPod recipe adapter is built on top of the Nvidia Nemo and Pytorch-lightning

frameworks. If you've already used these frameworks, integrating your custom models or features
into the SageMaker HyperPod recipe adapter is a similar process. In addition to modifying the
recipe adapter, you can change your own pre-training or ﬁne-tuning script. For guidance on writing
your custom training script, see examples.

Use the SageMaker HyperPod adapter to create your own model

Within the recipe adapter, you can customize the following ﬁles in the following locations:

1. collections/data: Contains a module responsible for loading datasets. Currently, it only

supports datasets from HuggingFace. If you have more advanced requirements, the code
structure allows you to add custom data modules within the same folder.

2. collections/model: Includes the deﬁnitions of various language models. Currently, it

supports common large language models like Llama, Mixtral, and Mistral. You have the ﬂexibility
to introduce your own model deﬁnitions within this folder.

3. collections/parts: This folder contains strategies for training models in a distributed

manner. One example is the Fully Sharded Data Parallel (FSDP) strategy, which allows for
sharding a large language model across multiple accelerators. Additionally, the strategies
support various forms of model parallelism. You also have the option to introduce your own
customized training strategies for model training.

4. utils: Contains various utilities aimed at facilitating the management of a training job. It

serves as a repository where for your own tools. You can use your own tools for tasks such as
troubleshooting or benchmarking. You can also add your own personalized PyTorch Lightning

HyperPod recipes
1627

## Page 657

Amazon SageMaker AI
Developer Guide

callbacks within this folder. You can use PyTorch Lightning callbacks to seamlessly integrate
speciﬁc functionalities or operations into the training lifecycle.

5. conf: Contains the conﬁguration schema deﬁnitions used for validating speciﬁc parameters in

a training job. If you introduce new parameters or conﬁgurations, you can add your customized
schema to this folder. You can use the customized schema to deﬁne the validation rules. You
can validate data types, ranges, or any other parameter constraint. You can also deﬁne you own
custom schema to validate the parameters.

Appendix

Use the following information to get information about monitoring and analyzing training results.

Monitor training results

Monitoring and analyzing training results is essential for developers to assess convergence and
troubleshoot issues. SageMaker HyperPod recipes oﬀer Tensorboard integration to analyze training
behavior. To address the challenges of proﬁling large distributed training jobs, these recipes also
incorporate VizTracer. VizTracer is a low-overhead tool for tracing and visualizing Python code
execution. For more information about VizTracer, see VizTracer.

The following sections guide you through the process of implementing these features in your
SageMaker HyperPod recipes.

Tensorboard

Tensorboard is a powerful tool for visualizing and analyzing the training process. To enable
Tensorboard, modify your recipe by setting the following parameter:

exp_manager:
exp_dir: null
name: experiment
create_tensorboard_logger: True

After you enable the Tensorboard logger, the training logs are generated and stored within the
experiment directory. The experiment directed is deﬁned in exp_manager.exp_dir. To access and
analyze these logs locally, use the following procedure:

HyperPod recipes
1628

## Page 658

Amazon SageMaker AI
Developer Guide

To access and analyze logs

1.
Download the Tensorboard experiment folder from your training environment to your local
machine.

2.
Open a terminal or command prompt on your local machine.

3.
Navigate to the directory containing the downloaded experiment folder.

4.
Launch Tensorboard with the following the command.

tensorboard --port=<port> --bind_all --logdir experiment.

5.
Open your web browser and visit http://localhost:8008.

You can now see the status and visualizations of your training jobs within the Tensorboard
interface. Seeing the status and visualizations helps you monitor and analyze the training process.
Monitoring and analyzing the training process helps you gain insights into the behavior and
performance of your models. For more information about how you monitor and analyze the
training with Tensorboard, see the NVIDIA NeMo Framework User Guide.

VizTracer

To enable VizTracer, you can modify your recipe by setting the model.viztracer.enabled parameter
to true. For example, you can update your llama recipe to enable VizTracer by adding the following
conﬁguration:

model:
viztracer:
enabled: true

After the training has completed, your VizTracer proﬁle is in the experiment folder exp_dir/
result.json. To analyze your proﬁle, you can download it and open it using the vizviewer tool:

vizviewer --port <port> result.json

This command launches the vizviewer on port 9001. You can view your VizTracer by specifying
http://localhost:<port> in your browser. After you open VizTracer, you begin analyzing the training.
For more information about using VizTracer, see VizTracer documentation.

HyperPod recipes
1629

## Page 659

Amazon SageMaker AI
Developer Guide

SageMaker JumpStart versus SageMaker HyperPod

While SageMaker JumpStart provides ﬁne-tuning capabilities, the SageMaker HyperPod recipes
provide the following:

• Additional ﬁne-grained control over the training loop

• Recipe customization for your own models and data

• Support for model parallelism

Use the SageMaker HyperPod recipes when you need access to the model's hyperparameters,
multi-node training, and customization options for the training loop.

For more information about ﬁne-tuning your models in SageMaker JumpStart, see Fine-tune

publicly available foundation models with the JumpStartEstimator class

Orchestrating SageMaker HyperPod clusters with Slurm

Slurm support in SageMaker HyperPod helps you provision resilient clusters for running machine
learning (ML) workloads and developing state-of-the-art models such as large language models
(LLMs), diﬀusion models, and foundation models (FMs). It accelerates development of FMs by
removing undiﬀerentiated heavy-lifting involved in building and maintaining large-scale compute
clusters powered by thousands of accelerators such as AWS Trainium and NVIDIA A100 and H100
Graphical Processing Units (GPUs). When accelerators fail, the resiliency features of SageMaker
HyperPod monitors the cluster instances automatically detect and replace the faulty hardware on
the ﬂy so that you can focus on running ML workloads. Additionally, with lifecycle conﬁguration
support in SageMaker HyperPod, you can customize your computing environment to best suit
your needs and conﬁgure it with the Amazon SageMaker AI distributed training libraries to achieve
optimal performance on AWS.

Operating clusters

You can create, conﬁgure, and maintain SageMaker HyperPod clusters graphically through the
console user interface (UI) and programmatically through the AWS command line interface (CLI)
or AWS SDK for Python (Boto3). With Amazon VPC, you can secure the cluster network and also
take advantage of conﬁguring your cluster with resources in your VPC, such as Amazon FSx for
Lustre, which oﬀers the fastest throughput. You can also give diﬀerent IAM roles to cluster instance
groups, and limit actions that your cluster resources and users can operate. To learn more, see the
section called “Managing Slurm clusters”.

Slurm orchestration
1630

## Page 660

Amazon SageMaker AI
Developer Guide

Conﬁguring your ML environment

SageMaker HyperPod runs the section called “SageMaker HyperPod DLAMI”, which sets up an ML
environment on the HyperPod clusters. You can conﬁgure additional customizations to the DLAMI
by providing lifecycle scripts to support your use case. To learn more about how to set up lifecycle
scripts, see the section called “Getting started” and the section called “Lifecycle scripts”.

Scheduling jobs

After you successfully create a HyperPod cluster, cluster users can log into the cluster nodes (such
as head or controller node, log-in node, and worker node) and schedule jobs for running machine
learning workloads. To learn more, see the section called “Jobs on HyperPod clusters”.

Resiliency against hardware failures

SageMaker HyperPod runs health checks on cluster nodes and provides a workload auto-resume
functionality. With the cluster resiliency features of HyperPod, you can resume your workload from
the last checkpoint you saved, after faulty nodes are replaced with healthy ones in clusters with
more than 16 nodes. To learn more, see the section called “Cluster resiliency”.

Logging and managing clusters

You can ﬁnd SageMaker HyperPod resource utilization metrics and lifecycle logs in Amazon

CloudWatch, and manage SageMaker HyperPod resources by tagging them. Each CreateCluster

API run creates a distinct log stream, named in <cluster-name>-<timestamp> format. In
the log stream, you can check the host names, the name of failed lifecycle scripts, and outputs

from the failed scripts such as stdout and stderr. For more information, see the section called
“Cluster management”.

Compatible with SageMaker AI tools

Using SageMaker HyperPod, you can conﬁgure clusters with AWS optimized collective
communications libraries oﬀered by SageMaker AI, such as the SageMaker AI distributed data

parallelism (SMDDP) library. The SMDDP library implements the AllGather operation optimized
to the AWS compute and network infrastructure for the most performant SageMaker AI machine
learning instances powered by NVIDIA A100 GPUs. To learn more, see the section called “Running
distributed training workloads”.

Instance placement with UltraServers

Slurm orchestration
1631

## Page 661

Amazon SageMaker AI
Developer Guide

SageMaker AI automatically allocates jobs to instances within your UltraServer based on a best
eﬀort strategy of using all of the instances in one UltraServer before using another one. For
example, if you request 14 instances and have 2 UltraServers in your training plan, SageMaker
AI uses all of the instances in the ﬁrst UltraServer. If you requested 20 instances and have 2
UltraServers in your training plan, SageMaker AI will will use all 17 instances in the ﬁrst UltraServer
and then use 3 from the second UltraServer.

Topics

• Getting started with SageMaker HyperPod

• SageMaker HyperPod Slurm cluster operations

• Customizing SageMaker HyperPod clusters using lifecycle scripts

• SageMaker HyperPod multi-head node support

• Jobs on SageMaker HyperPod clusters

• SageMaker HyperPod cluster resources monitoring

• SageMaker HyperPod cluster resiliency

• SageMaker HyperPod cluster management

• SageMaker HyperPod FAQs

Getting started with SageMaker HyperPod

Get started with creating your ﬁrst SageMaker HyperPod cluster and learn the cluster operation
functionalities of SageMaker HyperPod. You can create a SageMaker HyperPod cluster through
the SageMaker AI console UI or the AWS CLI commands. This tutorial shows how to create a
new SageMaker HyperPod cluster with Slurm, which is a popular workload scheduler software.
After you go through this tutorial, you will know how to log into the cluster nodes using the

AWS Systems Manager commands (aws ssm). After you complete this tutorial, see also the
section called “Managing Slurm clusters” to learn more about the SageMaker HyperPod basic
oparations, and the section called “Jobs on HyperPod clusters” to learn how to schedule jobs on the
provisioned cluster.

Tip

To ﬁnd practical examples and solutions, see also the SageMaker HyperPod workshop.

In this topic:

Slurm orchestration
1632

## Page 662

Amazon SageMaker AI
Developer Guide

• Getting started with SageMaker HyperPod using the SageMaker AI console

• Creating SageMaker HyperPod clusters using CloudFormation templates

• Getting started with SageMaker HyperPod using the AWS CLI

Getting started with SageMaker HyperPod using the SageMaker AI console

The following tutorial demonstrates how to create a new SageMaker HyperPod cluster and set it up
with Slurm through the SageMaker AI console UI. Following the tutorial, you'll create a HyperPod

cluster with three Slurm nodes, my-controller-group, my-login-group, and worker-

group-1.

Topics

• Create cluster

• Deploy resources

• Delete the cluster and clean resources

Create cluster

To navigate to the SageMaker HyperPod Clusters page and choose Slurm orchestration, follow
these steps.

1.
Open the Amazon SageMaker AI console at https://console.aws.amazon.com/sagemaker/.

2.
Choose HyperPod Clusters in the left navigation pane and then Cluster Management.

3.
On the SageMaker HyperPod Clusters page, choose Create HyperPod cluster.

4.
On the Create HyperPod cluster drop-down, choose Orchestrated by Slurm.

5.
On the Slurm cluster creation page, you will see two options. Choose the option that best ﬁts
your needs.

a.
Quick setup - To get started immediately with default settings, choose Quick setup. With
this option, SageMaker AI will create new resources such as VPC, subnets, security groups,
Amazon S3 bucket, IAM role, and FSx for Lustre in the process of creating your cluster.

b.
Custom setup - To integrate with existing AWS resources or have speciﬁc networking,
security, or storage requirements, choose Custom setup. With this option, you can choose
to use the existing resources or create new ones, and you can customize the conﬁguration
that best ﬁts your needs.

Slurm orchestration
1633

## Page 663

Amazon SageMaker AI
Developer Guide

Quick setup

On the Quick setup section, follow these steps to create your HyperPod cluster with Slurm
orchestration.

General settings

Specify a name for the new cluster. You can’t change the name after the cluster is created.

Instance groups

To add an instance group, choose Add group. Each instance group can be conﬁgured diﬀerently,
and you can create a heterogeneous cluster that consists of multiple instance groups with various
instance types. To deploy a cluster, you must add at least one instance group for Controller and
Compute group types.

Important

You can add one instance group at a time. To create multiple instance groups, repeat the
process for each instance group.

Follow these steps to add an instance group.

1.
For Instance group type, choose a type for your instance group. For this tutorial, choose

Controller (head) for my-controller-group, Login for my-login-group, and Compute

(worker) for worker-group-1.

2.
For Name, specify a name for the instance group. For this tutorial, create three instance groups

named my-controller-group, my-login-group, and worker-group-1.

3.
For Instance capacity, choose either on-demand capacity or a training plan to reserve your
compute resources.

4.
For Instance type, choose the instance for the instance group. For this tutorial, select

ml.c5.xlarge for my-controller-group, ml.m5.4xlarge for my-login-group, and

ml.trn1.32xlarge for worker-group-1.

Slurm orchestration
1634

## Page 664

Amazon SageMaker AI
Developer Guide

Important

Ensure that you choose an instance type with suﬃcient quotas and enough unassigned
IP addresses for your account. To view or request additional quotas, see the section
called “SageMaker HyperPod quotas”.

5.
For Instance quantity, specify an integer not exceeding the instance quota for cluster usage.
For this tutorial, enter 1 for all three groups.

6.
For Target Availability Zone, choose the Availability Zone where your instances will be
provisioned. The Availability Zone should correspond to the location of your accelerated
compute capacity.

7.
For Additional storage volume per instance (GB) - optional, specify an integer between 1 and
16384 to set the size of an additional Elastic Block Store (EBS) volume in gigabytes (GB). The
EBS volume is attached to each instance of the instance group. The default mount path for

the additional EBS volume is /opt/sagemaker. After the cluster is successfully created, you
can SSH into the cluster instances (nodes) and verify if the EBS volume is mounted correctly

by running the df -h command. Attaching an additional EBS volume provides stable, oﬀ-
instance, and independently persisting storage, as described in the Amazon EBS volumes
section in the Amazon Elastic Block Store User Guide.

8.
Choose Add instance group.

Quick setup defaults

This section lists all the default settings for your cluster creation, including all the new AWS
resources that will be created during the cluster creation process. Review the default settings.

Custom setup

On the Custom setup section, follow these steps to create your HyperPod cluster with Slurm
orchestration.

General settings

Specify a name for the new cluster. You can’t change the name after the cluster is created.

For Instance recovery, choose Automatic - recommended or None.

Slurm orchestration
1635

## Page 665

Amazon SageMaker AI
Developer Guide

Networking

Conﬁgure your network settings for the cluster creation. These settings can't be changed after the
cluster is created.

1.
For VPC, choose your own VPC if you already have one that gives SageMaker AI access to your
VPC. To create a new VPC, follow the instructions at Create a VPC in the Amazon Virtual Private
Cloud User Guide. You can leave it as None to use the default SageMaker AI VPC.

2.
For VPC IPv4 CIDR block, enter the starting IP of your VPC.

3.
For Availability Zones, choose the Availability Zones (AZ) where HyperPod will create subnets
for your cluster. Choose AZs that match the location of your accelerated compute capacity.

4.
For Security groups, create a security group or choose up to ﬁve security groups conﬁgured
with rules to allow inter-resource communication within the VPC.

Instance groups

To add an instance group, choose Add group. Each instance group can be conﬁgured diﬀerently,
and you can create a heterogeneous cluster that consists of multiple instance groups with various
instance types. To deploy a cluster, you must add at least one instance group.

Important

You can add one instance group at a time. To create multiple instance groups, repeat the
process for each instance group.

Follow these steps to add an instance group.

1.
For Instance group type, choose a type for your instance group. For this tutorial, choose

Controller (head) for my-controller-group, Login for my-login-group, and Compute

(worker) for worker-group-1.

2.
For Name, specify a name for the instance group. For this tutorial, create three instance groups

named my-controller-group, my-login-group, and worker-group-1.

3.
For Instance capacity, choose either on-demand capacity or a training plan to reserve your
compute resources.

Slurm orchestration
1636

## Page 666

Amazon SageMaker AI
Developer Guide

4.
For Instance type, choose the instance for the instance group. For this tutorial, select

ml.c5.xlarge for my-controller-group, ml.m5.4xlarge for my-login-group, and

ml.trn1.32xlarge for worker-group-1.

Important

Ensure that you choose an instance type with suﬃcient quotas and enough unassigned
IP addresses for your account. To view or request additional quotas, see the section
called “SageMaker HyperPod quotas”.

5.
For Instance quantity, specify an integer not exceeding the instance quota for cluster usage.
For this tutorial, enter 1 for all three groups.

6.
For Target Availability Zone, choose the Availability Zone where your instances will be
provisioned. The Availability Zone should correspond to the location of your accelerated
compute capacity.

7.
For Additional storage volume per instance (GB) - optional, specify an integer between 1 and
16384 to set the size of an additional Elastic Block Store (EBS) volume in gigabytes (GB). The
EBS volume is attached to each instance of the instance group. The default mount path for

the additional EBS volume is /opt/sagemaker. After the cluster is successfully created, you
can SSH into the cluster instances (nodes) and verify if the EBS volume is mounted correctly

by running the df -h command. Attaching an additional EBS volume provides stable, oﬀ-
instance, and independently persisting storage, as described in the Amazon EBS volumes
section in the Amazon Elastic Block Store User Guide.

8.
Choose Add instance group.

Lifecycle scripts

You can choose to use the default lifecycle scripts or the custom lifecycle scripts, which will be
stored in your Amazon S3 bucket. You can view the default lifecycle scripts in the Awesome
Distributed Training GitHub repository. To learn more about the lifecycle scripts, see the section
called “Lifecycle scripts”.

1.
For Lifecycle scripts, choose to use default or custom lifecycle scripts.

2.
For S3 bucket for lifecycle scripts, choose to create a new bucket or use an existing bucket to
store the lifecycle scripts.

Slurm orchestration
1637

## Page 667

Amazon SageMaker AI
Developer Guide

Permissions

Choose or create an IAM role that allows HyperPod to run and access necessary AWS resources on
your behalf.

Storage

Conﬁgure the FSx for Lustre ﬁle system to be provisioned on the HyperPod cluster.

1.
For File system, choose an existing FSx for Lustre ﬁle system, to create a new FSx for Lustre
ﬁle system, or don't provision an FSx for Lustre ﬁle system.

2.
For Throughput per unit of storage, choose the throughput that will be available per TiB of
provisioned storage.

3.
For Storage capacity, enter a capacity value in TB.

4.
For Data compression type, choose LZ4 to enable data compression.

5.
For Lustre version, view the value that's recommended for the new ﬁle systems.

Tags - optional

For Tags - optional, add key and value pairs to the new cluster and manage the cluster as an AWS
resource. To learn more, see Tagging your AWS resources.

Deploy resources

After you complete the cluster conﬁgurations using either Quick setup or Custom setup, choose
the following option to start resource provisioning and cluster creation.

• Submit - SageMaker AI will start provisioning the default conﬁguration resources and creating
the cluster.

• Download CloudFormation template parameters - You will download the conﬁguration
parameter JSON ﬁle and run AWS CLI command to deploy the CloudFormation stack to provision
the conﬁguration resources and creating the cluster. You can edit the downloaded parameter
JSON ﬁle if needed. If you choose this option, see more instructions in the section called “
CloudFormation”.

Delete the cluster and clean resources

After you have successfully tested creating a SageMaker HyperPod cluster, it continues running

in the InService state until you delete the cluster. We recommend that you delete any clusters

Slurm orchestration
1638

## Page 668

Amazon SageMaker AI
Developer Guide

created using on-demand SageMaker AI instances when not in use to avoid incurring continued
service charges based on on-demand pricing. In this tutorial, you have created a cluster that
consists of two instance groups. One of them uses a C5 instance, so make sure you delete the
cluster by following the instructions at the section called “Delete a SageMaker HyperPod cluster”.

However, if you have created a cluster with reserved compute capacity, the status of the clusters
does not aﬀect service billing.

To clean up the lifecycle scripts from the S3 bucket used for this tutorial, go to the S3 bucket you
used during cluster creation and remove the ﬁles entirely.

If you have tested running any workloads on the cluster, make sure if you have uploaded any data
or if your job saved any artifacts to diﬀerent S3 buckets or ﬁle system services such as Amazon FSx
for Lustre and Amazon Elastic File System. To prevent any incurring charges, delete all artifacts and
data from the storage or ﬁle system.

Creating SageMaker HyperPod clusters using CloudFormation templates

You can create SageMaker HyperPod clusters using the CloudFormation templates for HyperPod.
You must install AWS CLI to proceed.

In this topic:

• Conﬁgure resources in the console and deploy using CloudFormation

• Conﬁgure resources and deploy using CloudFormation

Conﬁgure resources in the console and deploy using CloudFormation

You can conﬁgure resources using the AWS Management Console and deploy using the
CloudFormation templates.

Follow these steps.

1.
Instead of choosing Submit, choose Download CloudFormation template parameters at the
end of the tutorial in ???. The tutorial contains important conﬁguration information you will
need to create your cluster successfully.

Slurm orchestration
1639

## Page 669

Amazon SageMaker AI
Developer Guide

Important

If you choose Submit, you will not be able to deploy a cluster with the same name
until you delete the cluster.

After you choose Download CloudFormation template parameters, the Using the
conﬁguration ﬁle to create the cluster using the AWS CLI window will appear on the right
side of the page.

2.
On the Using the conﬁguration ﬁle to create the cluster using the AWS CLI window, choose
Download conﬁguration parameters ﬁle. The ﬁle will be downloaded to your machine. You
can edit the conﬁguration JSON ﬁle based on your needs or leave it as-is, if no change is
required.

3.
In the terminal, navigate to the location of the parameter ﬁle file://params.json.

4.
Run the create-stack AWS CLI command to deploy the CloudFormation stack that will
provision the conﬁgured resources and create the HyperPod cluster.

aws cloudformation create-stack
--stack-name my-stack
--template-url https://aws-sagemaker-hyperpod-cluster-setup.amazonaws.com/
templates-slurm/main-stack-slurm-based-template.yaml
--parameters file://params.json
--capabilities CAPABILITY_IAM CAPABILITY_NAMED_IAM

5.
To view the status of the resources provisioning, navigate to the CloudFormation console.

After the cluster creation completes, view the new cluster under Clusters in the main pane of
the SageMaker HyperPod console. You can check the status of it displayed under the Status
column.

6.
After the status of the cluster turns to InService, you can start logging into the cluster
nodes. To access the cluster nodes and start running ML workloads, see the section called “Jobs
on HyperPod clusters”.

Conﬁgure resources and deploy using CloudFormation

You can conﬁgure resources and deploy using the CloudFormation templates for SageMaker
HyperPod.

Slurm orchestration
1640

## Page 670

Amazon SageMaker AI
Developer Guide

Follow these steps.

1.
Download a CloudFormation template for SageMaker HyperPod from the sagemaker-
hyperpod-cluster-setup GitHub repository.

2.
Run the create-stack AWS CLI command to deploy the CloudFormation stack that will
provision the conﬁgured resources and create the HyperPod cluster.

aws cloudformation create-stack
--stack-name my-stack
--template-url URL_of_the_file_that_contains_the_template_body
--parameters file://params.json
--capabilities CAPABILITY_IAM CAPABILITY_NAMED_IAM

3.
To view the status of the resources provisioning, navigate to the CloudFormation console.

After the cluster creation completes, view the new cluster under Clusters in the main pane of
the SageMaker HyperPod console. You can check the status of it displayed under the Status
column.

4.
After the status of the cluster turns to InService, you can start logging into the cluster
nodes. To access the cluster nodes and start running ML workloads, see the section called “Jobs
on HyperPod clusters”.

Getting started with SageMaker HyperPod using the AWS CLI

Create your ﬁrst SageMaker HyperPod cluster using the AWS CLI commands for HyperPod.

Create your ﬁrst SageMaker HyperPod cluster with Slurm

The following tutorial demonstrates how to create a new SageMaker HyperPod cluster and set it up
with Slurm through the AWS CLI commands for SageMaker HyperPod. Following the tutorial, you'll

create a HyperPod cluster with three Slurm nodes: my-controller-group, my-login-group,

and worker-group-1.

With the API-driven conﬁguration approach, you deﬁne Slurm node types and partition

assignments directly in the CreateCluster API request using SlurmConfig. This eliminates the

need for a separate provisioning_parameters.json ﬁle and provides built-in validation, drift
detection, and per-instance-group FSx conﬁguration.

Slurm orchestration
1641

## Page 671

Amazon SageMaker AI
Developer Guide

1.
First, prepare and upload lifecycle scripts to an Amazon S3 bucket. During cluster creation,
HyperPod runs them in each instance group. Upload lifecycle scripts to Amazon S3 using the
following command.

aws s3 sync \
~/local-dir-to-lifecycle-scripts/* \
s3://sagemaker-<unique-s3-bucket-name>/<lifecycle-script-directory>/src

Note

The S3 bucket path should start with a preﬁx sagemaker-, because the IAM role for

SageMaker HyperPod with AmazonSageMakerClusterInstanceRolePolicy only
allows access to Amazon S3 buckets that starts with the speciﬁc preﬁx.

If you are starting from scratch, use sample lifecycle scripts provided in the Awsome
Distributed Training GitHub repository. The following sub-steps show how to download and
upload the sample lifecycle scripts to an Amazon S3 bucket.

a.
Download a copy of the lifecycle script samples to a directory on your local computer.

git clone https://github.com/aws-samples/awsome-distributed-training/

b.
Go into the directory 1.architectures/5.sagemaker_hyperpods/

LifecycleScripts/base-config, where you can ﬁnd a set of lifecycle scripts.

cd awsome-distributed-training/1.architectures/5.sagemaker_hyperpods/
LifecycleScripts/base-config

To learn more about the lifecycle script samples, see the section called “Lifecycle scripts”.

c.
Upload the scripts to s3://sagemaker-<unique-s3-bucket-name>/<lifecycle-

script-directory>/src. You can do so by using the Amazon S3 console, or by running
the following AWS CLI Amazon S3 command.

aws s3 sync \
~/local-dir-to-lifecycle-scripts/* \
s3://sagemaker-<unique-s3-bucket-name>/<lifecycle-script-directory>/src

Slurm orchestration
1642

## Page 672

Amazon SageMaker AI
Developer Guide

Note

With API-driven conﬁguration, you do not need to create or upload a

provisioning_parameters.json ﬁle. The Slurm conﬁguration is deﬁned directly in
the CreateCluster API request in the next step.

2.
Prepare a CreateCluster request ﬁle in JSON format and save as create_cluster.json.

With API-driven conﬁguration, you specify the Slurm node type and partition assignment for

each instance group using the SlurmConfig ﬁeld. You also conﬁgure the cluster-level Slurm

settings using Orchestrator.Slurm.

For ExecutionRole, provide the ARN of the IAM role you created with the managed

AmazonSageMakerClusterInstanceRolePolicy in the section called “Prerequisites”.

{
"ClusterName": "my-hyperpod-cluster",
"InstanceGroups": [
{
"InstanceGroupName": "my-controller-group",
"InstanceType": "ml.c5.xlarge",
"InstanceCount": 1,
"SlurmConfig": {
"NodeType": "Controller"
},
"LifeCycleConfig": {
"SourceS3Uri": "s3://sagemaker-<unique-s3-bucket-name>/<lifecycle-
script-directory>/src",
"OnCreate": "on_create.sh"
},
"ExecutionRole": "arn:aws:iam::<account-id>:role/
HyperPodExecutionRole",
"InstanceStorageConfigs": [
{
"EbsVolumeConfig": {
"VolumeSizeInGB": 500
}
}
]
},

Slurm orchestration
1643

## Page 673

Amazon SageMaker AI
Developer Guide

{
"InstanceGroupName": "my-login-group",
"InstanceType": "ml.m5.4xlarge",
"InstanceCount": 1,
"SlurmConfig": {
"NodeType": "Login"
},
"LifeCycleConfig": {
"SourceS3Uri": "s3://sagemaker-<unique-s3-bucket-name>/<lifecycle-
script-directory>/src",
"OnCreate": "on_create.sh"
},
"ExecutionRole": "arn:aws:iam::<account-id>:role/HyperPodExecutionRole"
},
{
"InstanceGroupName": "worker-group-1",
"InstanceType": "ml.trn1.32xlarge",

"InstanceCount": 1,
"SlurmConfig": {
"NodeType": "Compute",
"PartitionNames": ["partition-1"]
},
"LifeCycleConfig": {
"SourceS3Uri": "s3://sagemaker-<unique-s3-bucket-name>/<lifecycle-
script-directory>/src",
"OnCreate": "on_create.sh"
},
"ExecutionRole": "arn:aws:iam::<account-id>:role/HyperPodExecutionRole"
}
],
"Orchestrator": {
"Slurm": {
"SlurmConfigStrategy": "Managed"
}
}
}

SlurmConﬁg ﬁelds:

Slurm orchestration
1644

## Page 674

Amazon SageMaker AI
Developer Guide

Field
Description

NodeType
The Slurm role for the instance group. Valid values: Controller ,

Login, Compute

PartitionNames
The Slurm partition(s) to assign compute nodes to. Only valid for

Compute node type.

Orchestrator.Slurm ﬁelds:

Field
Description

SlurmConf

Controls how HyperPod manages slurm.conf . Valid values:

igStrategy

Managed (default), Overwrite , Merge

SlurmConﬁgStrategy options:

• Managed (recommended): HyperPod fully manages slurm.conf and detects unauthorized
changes (drift detection). Updates fail if drift is detected.

• Overwrite: HyperPod overwrites slurm.conf on updates, ignoring any manual changes.

• Merge: HyperPod preserves manual changes and merges them with API conﬁguration.

Adding FSx for Lustre (optional):

To mount an FSx for Lustre ﬁlesystem to your compute nodes, add FsxLustreConfig

to the InstanceStorageConfigs for the instance group. This requires a Custom VPC
conﬁguration.

{
"InstanceGroupName": "worker-group-1",
"InstanceType": "ml.trn1.32xlarge",
"InstanceCount": 1,
"SlurmConfig": {
"NodeType": "Compute",
"PartitionNames": ["partition-1"]

Slurm orchestration
1645

## Page 675

Amazon SageMaker AI
Developer Guide

},
"InstanceStorageConfigs": [
{
"FsxLustreConfig": {
"DnsName": "fs-0abc123def456789.fsx.us-west-2.amazonaws.com",
"MountPath": "/fsx",
"MountName": "abcdefgh"
}
}
],
"LifeCycleConfig": {
"SourceS3Uri": "s3://sagemaker-<unique-s3-bucket-name>/<lifecycle-script-
directory>/src",
"OnCreate": "on_create.sh"
},
"ExecutionRole": "arn:aws:iam::<account-id>:role/HyperPodExecutionRole"
}

Adding FSx for OpenZFS (optional):

You can also mount FSx for OpenZFS ﬁlesystems:

"InstanceStorageConfigs": [
{
"FsxOpenZfsConfig": {
"DnsName": "fs-0xyz789abc123456.fsx.us-west-2.amazonaws.com",
"MountPath": "/shared"
}
}
]

Note

Each instance group can have at most one FSx for Lustre and one FSx for OpenZFS
conﬁguration. Diﬀerent instance groups can mount diﬀerent ﬁlesystems.

Adding VPC conﬁguration (required for FSx):

If using FSx, you must specify a Custom VPC conﬁguration:

Slurm orchestration
1646

## Page 676

Amazon SageMaker AI
Developer Guide

{
"ClusterName": "my-hyperpod-cluster",
"InstanceGroups": [
{
"InstanceGroupName": "my-controller-group",
"InstanceType": "ml.c5.xlarge",
"InstanceCount": 1,
"SlurmConfig": {
"NodeType": "Controller"
},
"LifeCycleConfig": {
"SourceS3Uri": "s3://sagemaker-<unique-s3-bucket-name>/<lifecycle-
script-directory>/src",
"OnCreate": "on_create.sh"
},
"ExecutionRole": "arn:aws:iam::<account-id>:role/HyperPodExecutionRole"

},
],
"Orchestrator": {
"Slurm": {
"SlurmConfigStrategy": "Managed"
}
},
"VpcConfig": {
"SecurityGroupIds": ["sg-0abc123def456789a"],
"Subnets": ["subnet-0abc123def456789a"]
}
}

3.
Run the following command to create the cluster.

aws sagemaker create-cluster --cli-input-json file://complete/path/to/
create_cluster.json

This should return the ARN of the created cluster.

{
"ClusterArn": "arn:aws:sagemaker:us-west-2:111122223333:cluster/my-hyperpod-
cluster"
}

Slurm orchestration
1647

## Page 677

Amazon SageMaker AI
Developer Guide

If you receive an error due to resource limits, ensure that you change the instance type to one
with suﬃcient quotas in your account, or request additional quotas by following the section
called “SageMaker HyperPod quotas”.

Common validation errors:

Error
Resolution

"Cluster must have exactly one
InstanceGroup with Controller
node type"

Ensure exactly one instance group has SlurmConf

ig.NodeType
: "Controller"

"Partitions can only be assigned to
Compute node types"

Remove PartitionNames  from Controller  or

Login instance groups

"FSx conﬁgurations are only
supported for Custom VPC"

Add VpcConfig  to your request when using FSx

4.
Run describe-cluster to check the status of the cluster.

aws sagemaker describe-cluster --cluster-name my-hyperpod-cluster

Example response:

{
"ClusterArn": "arn:aws:sagemaker:us-west-2:111122223333:cluster/my-hyperpod-
cluster",
"ClusterName": "my-hyperpod-cluster",
"ClusterStatus": "Creating",
"InstanceGroups": [
{
"InstanceGroupName": "my-controller-group",
"InstanceType": "ml.c5.xlarge",
"InstanceCount": 1,
"CurrentCount": 0,
"TargetCount": 1,
"SlurmConfig": {
"NodeType": "Controller"
},
"LifeCycleConfig": {

Slurm orchestration
1648

## Page 678

Amazon SageMaker AI
Developer Guide

"SourceS3Uri": "s3://sagemaker-<bucket>/src",
"OnCreate": "on_create.sh"
},
"ExecutionRole": "arn:aws:iam::111122223333:role/HyperPodExecutionRole"
},
{
"InstanceGroupName": "my-login-group",
"InstanceType": "ml.m5.4xlarge",
"InstanceCount": 1,
"CurrentCount": 0,
"TargetCount": 1,
"SlurmConfig": {
"NodeType": "Login"
},
"LifeCycleConfig": {
"SourceS3Uri": "s3://sagemaker-<bucket>/src",
"OnCreate": "on_create.sh"

},
"ExecutionRole": "arn:aws:iam::111122223333:role/HyperPodExecutionRole"
},
{
"InstanceGroupName": "worker-group-1",
"InstanceType": "ml.trn1.32xlarge",
"InstanceCount": 1,
"CurrentCount": 0,
"TargetCount": 1,
"SlurmConfig": {
"NodeType": "Compute",
"PartitionNames": ["partition-1"]
},
"LifeCycleConfig": {
"SourceS3Uri": "s3://sagemaker-<bucket>/src",
"OnCreate": "on_create.sh"
},
"ExecutionRole": "arn:aws:iam::111122223333:role/HyperPodExecutionRole"
}
],
"Orchestrator": {
"Slurm": {
"SlurmConfigStrategy": "Managed"
}
},
"CreationTime": "2024-01-15T10:30:00Z"

Slurm orchestration
1649

## Page 679

Amazon SageMaker AI
Developer Guide

}

After the status of the cluster turns to InService, proceed to the next step. Cluster creation
typically takes 10-15 minutes.

5.
Run list-cluster-nodes to check the details of the cluster nodes.

aws sagemaker list-cluster-nodes --cluster-name my-hyperpod-cluster

Example response:

{
"ClusterNodeSummaries": [
{
"InstanceGroupName": "my-controller-group",

"InstanceId": "i-0abc123def456789a",
"InstanceType": "ml.c5.xlarge",
"InstanceStatus": {
"Status": "Running",
"Message": ""
},
"LaunchTime": "2024-01-15T10:35:00Z"
},
{
"InstanceGroupName": "my-login-group",
"InstanceId": "i-0abc123def456789b",
"InstanceType": "ml.m5.4xlarge",
"InstanceStatus": {
"Status": "Running",
"Message": ""
},
"LaunchTime": "2024-01-15T10:35:00Z"
},
{
"InstanceGroupName": "worker-group-1",
"InstanceId": "i-0abc123def456789c",
"InstanceType": "ml.trn1.32xlarge",
"InstanceStatus": {
"Status": "Running",
"Message": ""
},
"LaunchTime": "2024-01-15T10:36:00Z"
}

Slurm orchestration
1650

## Page 680

Amazon SageMaker AI
Developer Guide

]
}

The InstanceId is what your cluster users need for logging (aws ssm) into them. For more
information about logging into the cluster nodes and running ML workloads, see the section
called “Jobs on HyperPod clusters”.

6.
Connect to your cluster using AWS Systems Manager Session Manager.

aws ssm start-session \
--target sagemaker-cluster:my-hyperpod-cluster_my-login-
group-i-0abc123def456789b \
--region us-west-2

Once connected, verify Slurm is conﬁgured correctly:

# Check Slurm nodes
sinfo

# Check Slurm partitions
sinfo -p partition-1

# Submit a test job
srun -p partition-1 --nodes=1 hostname

Delete the cluster and clean resources

After you have successfully tested creating a SageMaker HyperPod cluster, it continues running

in the InService state until you delete the cluster. We recommend that you delete any clusters
created using on-demand SageMaker AI capacity when not in use to avoid incurring continued
service charges based on on-demand pricing. In this tutorial, you have created a cluster that
consists of three instance groups. Make sure you delete the cluster by running the following
command.

aws sagemaker delete-cluster --cluster-name my-hyperpod-cluster

To clean up the lifecycle scripts from the Amazon S3 bucket used for this tutorial, go to the
Amazon S3 bucket you used during cluster creation and remove the ﬁles entirely.

Slurm orchestration
1651

## Page 681

Amazon SageMaker AI
Developer Guide

aws s3 rm s3://sagemaker-<unique-s3-bucket-name>/<lifecycle-script-directory>/src --
recursive

If you have tested running any model training workloads on the cluster, also check if you have
uploaded any data or if your job has saved any artifacts to diﬀerent Amazon S3 buckets or ﬁle
system services such as Amazon FSx for Lustre and Amazon Elastic File System. To prevent
incurring charges, delete all artifacts and data from the storage or ﬁle system.

Related topics

• the section called “SageMaker HyperPod Slurm conﬁguration”

• the section called “Lifecycle scripts”

• the section called “FSx conﬁguration via InstanceStorageConﬁgs”

• the section called “Managing Slurm clusters”

SageMaker HyperPod Slurm cluster operations

This section provides guidance on managing SageMaker HyperPod through the SageMaker AI
console UI or the AWS Command Line Interface (CLI). You'll learn how to perform various tasks
related to SageMaker HyperPod, whether you prefer a visual interface or working with commands.

Topics

• Managing SageMaker HyperPod Slurm clusters using the SageMaker console

• Managing SageMaker HyperPod Slurm clusters using the AWS CLI

Managing SageMaker HyperPod Slurm clusters using the SageMaker console

The following topics provide guidance on how to manage SageMaker HyperPod through the
console UI.

Topics

• Create a SageMaker HyperPod cluster

• Browse your SageMaker HyperPod clusters

• View details of each SageMaker HyperPod cluster

• Edit a SageMaker HyperPod cluster

Slurm orchestration
1652

## Page 682

Amazon SageMaker AI
Developer Guide

• Delete a SageMaker HyperPod cluster

Create a SageMaker HyperPod cluster

See the instructions in ??? to create a new SageMaker HyperPod cluster through the SageMaker
HyperPod console UI.

Browse your SageMaker HyperPod clusters

Under Clusters in the main pane of the SageMaker HyperPod console on the SageMaker HyperPod
console main page, all created clusters should appear listed under the Clusters section, which
provides a summary view of clusters, their ARNs, status, and creation time.

View details of each SageMaker HyperPod cluster

Under Clusters on the console main page, the cluster Names are activated as links. Choose the
cluster name link to see details of each cluster.

Edit a SageMaker HyperPod cluster

1. Under Clusters in the main pane of the SageMaker HyperPod console, choose the cluster you

want to update.

2. Select your cluster, and choose Edit.

3. In the Edit <your-cluster> page, you can edit the conﬁgurations of existing instance groups,

add more instance groups, delete instance groups, and change tags for the cluster. After making
changes, choose Submit.

a. In the Conﬁgure instance groups section, you can add more instance groups by choosing

Create instance group.

b. In the Conﬁgure instance groups section, you can choose Edit to change its conﬁguration or

Delete to remove the instance group permanently.

Important

When deleting an instance group, consider the following points:

• Your SageMaker HyperPod cluster must always maintain at least one instance
group.

• Ensure all critical data is backed up before removal

• The removal process cannot be undone.

Slurm orchestration
1653

## Page 683

Amazon SageMaker AI
Developer Guide

Note

Deleting an instance group will terminate all compute resources associated with that
group.

c. In the Tags section, you can update tags for the cluster.

Delete a SageMaker HyperPod cluster

1. Under Clusters in the main pane of the SageMaker HyperPod console, choose the cluster you

want to delete.

2. Select your cluster, and choose Delete.

3. In the pop-up window for cluster deletion, review the cluster information carefully to conﬁrm

that you chose the right cluster to delete.

4. After you reviewed the cluster information, choose Yes, delete cluster.

5. In the text ﬁeld to conﬁrm this deletion, type delete.

6. Choose Delete on the lower right corner of the pop-up window to ﬁnish sending the cluster

deletion request.

Managing SageMaker HyperPod Slurm clusters using the AWS CLI

The following topics provide guidance on writing SageMaker HyperPod API request ﬁles in JSON
format and run them using the AWS CLI commands.

Topics

• Create a new cluster

• Describe a cluster

• List details of cluster nodes

• Describe details of a cluster node

• List clusters

• Update cluster conﬁguration

• Update the SageMaker HyperPod platform software of a cluster

• Scale down a cluster

Slurm orchestration
1654

## Page 684

Amazon SageMaker AI
Developer Guide

• Delete a cluster

Create a new cluster

1.
Prepare lifecycle conﬁguration scripts and upload them to an S3 bucket, such as s3://

sagemaker-amzn-s3-demo-bucket/lifecycle-script-directory/src/. The

following step 2 assumes that there’s an entry point script named on_create.sh in the
speciﬁed S3 bucket.

Important

Make sure that you set the S3 path to start with s3://sagemaker-. The
the section called “IAM role for SageMaker HyperPod” has the managed

AmazonSageMakerClusterInstanceRolePolicy attached, which allows access to

S3 buckets with the speciﬁc preﬁx sagemaker-.

2.
Prepare a CreateCluster API request ﬁle in JSON format. You should conﬁgure instance groups

to match with the Slurm cluster you design in the provisioning_parameters.json ﬁle
that'll be used during cluster creating as part of running a set of lifecycle scripts. To learn
more, see the section called “Lifecycle scripts”. The following template has two instance
groups to meet the minimum requirement for a Slurm cluster: one controller (head) node

and one compute (worker) node. For ExecutionRole, provide the ARN of the IAM role you

created with the managed AmazonSageMakerClusterInstanceRolePolicy from the
section the section called “IAM role for SageMaker HyperPod”.

// create_cluster.json
{
"ClusterName": "your-hyperpod-cluster",
"InstanceGroups": [
{
"InstanceGroupName": "controller-group",
"InstanceType": "ml.m5.xlarge",
"InstanceCount": 1,
"LifeCycleConfig": {
"SourceS3Uri": "s3://amzn-s3-demo-bucket-sagemaker/lifecycle-
script-directory/src/",
"OnCreate": "on_create.sh"
},
"ExecutionRole": "arn:aws:iam::111122223333:role/iam-role-for-cluster",

Slurm orchestration
1655

## Page 685

Amazon SageMaker AI
Developer Guide

// Optional: Configure an additional storage per instance group.
"InstanceStorageConfigs": [
{
// Attach an additional EBS volume to each instance within the
instance group.
// The default mount path for the additional EBS volume is /opt/
sagemaker.
"EbsVolumeConfig":{
// Specify an integer between 1 and 16384 in gigabytes (GB).
"VolumeSizeInGB": integer,
}
}
]
},
{
"InstanceGroupName": "worker-group-1",
"InstanceType": "ml.p4d.xlarge",

"InstanceCount": 1,
"LifeCycleConfig": {
"SourceS3Uri": "s3://amzn-s3-demo-bucket-sagemaker/lifecycle-
script-directory/src/",
"OnCreate": "on_create.sh"
},
"ExecutionRole": "arn:aws:iam::111122223333:role/iam-role-for-cluster"
}
],
// Optional
"Tags": [
{
"Key": "string",
"Value": "string"
}
],
// Optional
"VpcConfig": {
"SecurityGroupIds": [ "string" ],
"Subnets": [ "string" ]
}
}

Depending on how you design the cluster structure through your lifecycle scripts, you can

conﬁgure up to 20 instance groups under the InstanceGroups parameter.

Slurm orchestration
1656

## Page 686

Amazon SageMaker AI
Developer Guide

For the Tags request parameter, you can add custom tags for managing the SageMaker
HyperPod cluster as an AWS resource. You can add tags to your cluster in the same way you
add them in other AWS services that support tagging. To learn more about tagging AWS
resources in general, see Tagging AWS Resources User Guide.

For the VpcConfig request parameter, specify the information of a VPC you want to use.
For more information, see the section called “Setting up SageMaker HyperPod with a custom
Amazon VPC”.

3.
Run the create-cluster command as follows.

aws sagemaker create-cluster \
--cli-input-json file://complete/path/to/create_cluster.json

This should return the ARN of the new cluster.

Describe a cluster

Run describe-cluster to check the status of the cluster. You can specify either the name or the ARN
of the cluster.

aws sagemaker describe-cluster --cluster-name your-hyperpod-cluster

After the status of the cluster turns to InService, proceed to the next step. Using this API, you
can also retrieve failure messages from running other HyperPod API operations.

List details of cluster nodes

Run list-cluster-nodes to check the key information of the cluster nodes.

aws sagemaker list-cluster-nodes --cluster-name your-hyperpod-cluster

This returns a response, and the InstanceId is what you need to use for logging (using aws ssm)
into them.

Describe details of a cluster node

Run describe-cluster-node to retrieve details of a cluster node. You can get the cluster node ID
from list-cluster-nodes output. You can specify either the name or the ARN of the cluster.

Slurm orchestration
1657

## Page 687

Amazon SageMaker AI
Developer Guide

aws sagemaker describe-cluster-node \
--cluster-name your-hyperpod-cluster \
--node-id i-111222333444555aa

List clusters

Run list-clusters to list all clusters in your account.

aws sagemaker list-clusters

You can also add additional ﬂags to ﬁlter the list of clusters down. To learn more about what this
command runs at low level and additional ﬂags for ﬁltering, see the ListClusters API reference.

Update cluster conﬁguration

Run update-cluster to update the conﬁguration of a cluster.

Note

You can use the UpdateCluster API to scale down or remove entire instance groups from
your SageMaker HyperPod cluster. For additional instructions on how to scale down or
delete instance groups, see the section called “Scale down a cluster”.

1. Create an UpdateCluster request ﬁle in JSON format. Make sure that you specify the right

cluster name and instance group name to update. You can change the instance type, the number
of instances, the lifecycle conﬁguration entrypoint script, and the path to the script.

a. For ClusterName, specify the name of the cluster you want to update.

b. For InstanceGroupName

i. To update an existing instance group, specify the name of the instance group you want to

update.

ii. To add a new instance group, specify a new name not existing in your cluster.

c. For InstanceType

i. To update an existing instance group, you must match the instance type you initially

speciﬁed to the group.

ii. To add a new instance group, specify an instance type you want to conﬁgure the group

with.

Slurm orchestration
1658

## Page 688

Amazon SageMaker AI
Developer Guide

d. For InstanceCount

i. To update an existing instance group, specify an integer that corresponds to your desired

number of instances. You can provide a higher or lower value (down to 0) to scale the
instance group up or down.

ii. To add a new instance group, specify an integer greater or equal to 1.

e. For LifeCycleConfig, you can change both SourceS3Uri and OnCreate values as you

want to update the instance group.

f. For ExecutionRole

i. For updating an existing instance group, keep using the same IAM role you attached during

cluster creation.

ii. For adding a new instance group, specify an IAM role you want to attach.

g. For ThreadsPerCore

i. For updating an existing instance group, keep using the same value you speciﬁed during

cluster creation.

ii. For adding a new instance group, you can choose any value from the allowed options per

instance type. For more information, search the instance type and see the Valid threads
per core column in the reference table at CPU cores and threads per CPU core per instance
type in the Amazon EC2 User Guide.

The following code snippet is a JSON request ﬁle template you can use. For more information
about the request syntax and parameters of this API, see the UpdateCluster API reference.

// update_cluster.json
{
// Required
"ClusterName": "name-of-cluster-to-update",
// Required
"InstanceGroups": [
{
"InstanceGroupName": "name-of-instance-group-to-update",
"InstanceType": "ml.m5.xlarge",
"InstanceCount": 1,
"LifeCycleConfig": {
"SourceS3Uri": "s3://amzn-s3-demo-bucket-sagemaker/lifecycle-script-
directory/src/",
"OnCreate": "on_create.sh"
},

Slurm orchestration
1659

## Page 689

Amazon SageMaker AI
Developer Guide

"ExecutionRole": "arn:aws:iam::111122223333:role/iam-role-for-cluster",
// Optional: Configure an additional storage per instance group.
"InstanceStorageConfigs": [
{
// Attach an additional EBS volume to each instance within the
instance group.
// The default mount path for the additional EBS volume is /opt/
sagemaker.
"EbsVolumeConfig":{
// Specify an integer between 1 and 16384 in gigabytes (GB).
"VolumeSizeInGB": integer,
}
}
]
},
// add more blocks of instance groups as needed
{ ... }

]
}

2. Run the following update-cluster command to submit the request.

aws sagemaker update-cluster \
--cli-input-json file://complete/path/to/update_cluster.json

Update the SageMaker HyperPod platform software of a cluster

Run update-cluster-software to update existing clusters with software and security patches

provided by the SageMaker HyperPod service. For --cluster-name, specify either the name or
the ARN of the cluster to update.

Important

Note that you must back up your work before running this API. The patching process
replaces the root volume with the updated AMI, which means that your previous data
stored in the instance root volume will be lost. Make sure that you back up your data from
the instance root volume to Amazon S3 or Amazon FSx for Lustre. For more information,
see the section called “Use the backup script provided by SageMaker HyperPod”.

Slurm orchestration
1660

## Page 690

Amazon SageMaker AI
Developer Guide

aws sagemaker update-cluster-software --cluster-name your-hyperpod-cluster

This command calls the UpdateClusterSoftware API. After the API call, SageMaker HyperPod
checks if there's a newer DLAMI available for the cluster instances. If a DLAMI update is required,
SageMaker HyperPod will update the cluster instances to use the latest the section called
“SageMaker HyperPod DLAMI” and run your lifecycle scripts in the Amazon S3 bucket that
you speciﬁed during cluster creation or update. If the cluster is already using the latest DLAMI,
SageMaker HyperPod will not make any changes to the cluster or run the lifecycle scripts again.
The SageMaker HyperPod service team regularly rolls out new the section called “SageMaker
HyperPod DLAMI”s for enhancing security and improving user experiences. We recommend
that you always keep updating to the latest SageMaker HyperPod DLAMI. For future SageMaker
HyperPod DLAMI updates for security patching, follow up with the section called “HyperPod
release notes”.

Tip

If the security patch fails, you can retrieve failure messages by running the

DescribeCluster API as instructed at the section called “Describe a cluster”.

Note

You can only run this API programatically. The patching functionality is not implemented in
the SageMaker HyperPod console UI.

Use the backup script provided by SageMaker HyperPod

SageMaker HyperPod provides a script to back up and restore your data at

1.architectures/5.sagemaker-hyperpod/patching-backup.sh in the Awsome Distributed
Training GitHub repository. The script provides the following two functions.

To back up data to an S3 bucket before patching

sudo bash patching-backup.sh --create <s3-buckup-bucket-path>

Slurm orchestration
1661

## Page 691

Amazon SageMaker AI
Developer Guide

After you run the command, the script checks squeue if there are queued jobs, stops Slurm if

there's no job in the queue, backs up mariadb, and copies local items on disc deﬁned under

LOCAL_ITEMS. You can add more ﬁles and directories to LOCAL_ITEMS.

# Define files and directories to back up.
LOCAL_ITEMS=(
"/var/spool/slurmd"
"/var/spool/slurmctld"
"/etc/systemd/system/slurmctld.service"
"/home/ubuntu/backup_slurm_acct_db.sql"
# ... Add more items as needed
)

Also, you can add custom code to the provided script to back up any applications for your use case.

To restore data from an S3 bucket after patching

sudo bash patching-backup.sh --restore <s3-buckup-bucket-path>

Scale down a cluster

You can scale down the number of instances or delete instance groups in your SageMaker
HyperPod cluster to optimize resource allocation or reduce costs.

You scale down by either using the UpdateCluster API operation to randomly terminate
instances from your instance group down to a speciﬁed number, or by terminating speciﬁc

instances using the BatchDeleteClusterNodes API operation. You can also completely remove

entire instance groups using the UpdateCluster API. For more information about how to scale
down using these methods, see Scaling down a SageMaker HyperPod cluster.

Note

You cannot remove instances that are conﬁgured as Slurm controller nodes. Attempting
to delete a Slurm controller node results in a validation error with the error code

NODE_ID_IN_USE.

Delete a cluster

Run delete-cluster to delete a cluster. You can specify either the name or the ARN of the cluster.

Slurm orchestration
1662

## Page 692

Amazon SageMaker AI
Developer Guide

aws sagemaker delete-cluster --cluster-name your-hyperpod-cluster

Customizing SageMaker HyperPod clusters using lifecycle scripts

SageMaker HyperPod oﬀers always up-and-running compute clusters, which are highly
customizable as you can write lifecycle scripts to tell SageMaker HyperPod how to set up the
cluster resources. The following topics are best practices for preparing lifecycle scripts to set up
SageMaker HyperPod clusters with open source workload manager tools.

The following topics discuss in-depth best practices for preparing lifecycle scripts to set up Slurm
conﬁgurations on SageMaker HyperPod.

High-level overview

The following procedure is the main ﬂow of provisioning a HyperPod cluster and setting it up with
Slurm. The steps are put in order of a bottom-up approach.

1. Plan how you want to create Slurm nodes on a HyperPod cluster. For example, if you want to

conﬁgure two Slurm nodes, you'll need to set up two instance groups in a HyperPod cluster.

2. Prepare Slurm conﬁguration. Choose one of the following approaches:

• Option A: API-driven conﬁguration (recommended) – Deﬁne Slurm node types and

partitions directly in the CreateCluster API payload using SlurmConfig within each
instance group. With this approach:

• No provisioning_parameters.json ﬁle is needed

• Slurm topology is deﬁned in the API payload alongside instance group deﬁnitions

• FSx ﬁlesystems are conﬁgured per-instance-group via InstanceStorageConfigs

• Conﬁguration strategy is controlled via Orchestrator.Slurm.SlurmConfigStrategy

Example SlurmConfig in an instance group:

{
"InstanceGroupName": "gpu-compute",
"InstanceType": "ml.p4d.24xlarge",
"InstanceCount": 8,
"SlurmConfig": {
"NodeType": "Compute",
"PartitionNames": ["gpu-training"]
}

Slurm orchestration
1663

## Page 693

Amazon SageMaker AI
Developer Guide

}

• Option B: Legacy conﬁguration – Prepare a provisioning_parameters.json ﬁle,
which is a the section called “Conﬁguration form for provisioning_parameters.json”.

provisioning_parameters.json should contain Slurm node conﬁguration information to

be provisioned on the HyperPod cluster. This should reﬂect the design of Slurm nodes from
Step 1.

3. Prepare a set of lifecycle scripts to set up Slurm on HyperPod to install software packages and

set up an environment in the cluster for your use case. You should structure the lifecycle scripts

to collectively run in order in a central Python script (lifecycle_script.py), and write an

entrypoint shell script (on_create.sh) to run the Python script. The entrypoint shell script is
what you need to provide to a HyperPod cluster creation request later in Step 5.

Also, note that you should write the scripts to expect resource_config.json that will be

generated by HyperPod during cluster creation. resource_config.json contains HyperPod
cluster resource information such as IP addresses, instance types, and ARNs, and is what you
need to use for conﬁguring Slurm.

4. Collect all the ﬁles from the previous steps into a folder. The folder structure depends on the

conﬁguration approach you selected in Step 2.

If you selected Option A (API-driven conﬁguration):

Your folder only needs lifecycle scripts for custom setup tasks. Slurm conﬁguration and FSx
mounting are handled automatically by HyperPod based on the API payload.

### lifecycle_files // your local folder

### on_create.sh
### lifecycle_script.py
### ... // more setup scripts to be fed into lifecycle_script.py

Note

The provisioning_parameters.json ﬁle is not required when using API-driven
conﬁguration.

If you selected Option B (legacy conﬁguration):

Slurm orchestration
1664

## Page 694

Amazon SageMaker AI
Developer Guide

Your folder must include provisioning_parameters.json and the full set of lifecycle
scripts.

### lifecycle_files // your local folder

### provisioning_parameters.json
### on_create.sh
### lifecycle_script.py
### ... // more setup scrips to be fed into lifecycle_script.py

5. Upload all the ﬁles to an S3 bucket. Copy and keep the S3 bucket path. Note that

you should create an S3 bucket path starting with sagemaker- because you need
to choose an the section called “IAM role for SageMaker HyperPod” attached with

AmazonSageMakerClusterInstanceRolePolicy, which only allows S3 bucket paths

starting with the preﬁx sagemaker-. The following command is an example command to
upload all the ﬁles to an S3 bucket.

aws s3 cp --recursive ./lifecycle_files s3://sagemaker-hyperpod-lifecycle/src

6. Prepare a HyperPod cluster creation request.

• Option 1: If you use the AWS CLI, write a cluster creation request in JSON format

(create_cluster.json) following the instructions at the section called “Create a new
cluster”.

• Option 2: If you use the SageMaker AI console UI, ﬁll the Create a cluster request form in the
HyperPod console UI following the instructions at the section called “Create a SageMaker
HyperPod cluster”.

At this stage, make sure that you create instance groups in the same structure that you planned
in Step 1 and 2. Also, make sure that you specify the S3 bucket from Step 5 in the request forms.

7. Submit the cluster creation request. HyperPod provisions a cluster based on the request, and

then creates a resource_config.json ﬁle in the HyperPod cluster instances, and sets up
Slurm on the cluster running the lifecycle scripts.

The following topics walk you through and dive deep into details on how to organize conﬁguration
ﬁles and lifecycle scripts to work properly during HyperPod cluster creation.

Topics

Slurm orchestration
1665

## Page 695

Amazon SageMaker AI
Developer Guide

• Base lifecycle scripts provided by HyperPod

• What particular conﬁgurations HyperPod manages in Slurm conﬁguration ﬁles

• Slurm log rotations

• Mounting Amazon FSx for Lustre and Amazon FSx for OpenZFS to a HyperPod cluster

• Validating the JSON conﬁguration ﬁles before creating a Slurm cluster on HyperPod

• Validating runtime before running production workloads on a HyperPod Slurm cluster

• Developing lifecycle scripts interactively on a HyperPod cluster node

Base lifecycle scripts provided by HyperPod

This section walks you through every component of the basic ﬂow of setting up Slurm on
HyperPod in a top-down approach. It starts from preparing a HyperPod cluster creation request

to run the CreateCluster API, and dives deep into the hierarchical structure down to lifecycle
scripts. Use the sample lifecycle scripts provided in the Awsome Distributed Training GitHub
repository. Clone the repository by running the following command.

git clone https://github.com/aws-samples/awsome-distributed-training/

The base lifecycle scripts for setting up a Slurm cluster on SageMaker HyperPod are available at

1.architectures/5.sagemaker_hyperpods/LifecycleScripts/base-config.

cd awsome-distributed-training/1.architectures/5.sagemaker_hyperpods/LifecycleScripts/
base-config

The following ﬂowchart shows a detailed overview of how you should design the base lifecycle
scripts. The descriptions below the diagram and the procedural guide explain how they work during

the HyperPod CreateCluster API call.

Slurm orchestration
1666

## Page 696

Amazon SageMaker AI
Developer Guide

![Page 696 Diagram 1](images/page-0696-img-01.png)

Figure: A detailed ﬂow chart of HyperPod cluster creation and the structure of lifecycle
scripts. (1) The dashed arrows are directed to where the boxes are "called into" and shows
the ﬂow of conﬁguration ﬁles and lifecycle scripts preparation. It starts from preparing

provisioning_parameters.json and lifecycle scripts. These are then coded in

lifecycle_script.py for a collective execution in order. And the execution of the

lifecycle_script.py script is done by the on_create.sh shell script, which to be run in the
HyperPod instance terminal. (2) The solid arrows show the main HyperPod cluster creation ﬂow and

how the boxes are "called into" or "submitted to". on_create.sh is required for cluster creation

request, either in create_cluster.json or the Create a cluster request form in the console

UI. After you submit the request, HyperPod runs the CreateCluster API based on the given
conﬁguration information from the request and the lifecycle scripts. (3) The dotted arrow indicates

that the HyperPod platform creates resource_config.json in the cluster instances during cluster

Slurm orchestration
1667

## Page 697

Amazon SageMaker AI
Developer Guide

resource provisioning. resource_config.json contains HyperPod cluster resource information
such as the cluster ARN, instance types, and IP addresses. It is important to note that you should

prepare the lifecycle scripts to expect the resource_config.json ﬁle during cluster creation. For
more information, see the procedural guide below.

The following procedural guide explains what happens during HyperPod cluster creation and how
the base lifecycle scripts are designed.

1. create_cluster.json – To submit a HyperPod cluster creation request, you prepare a

CreateCluster request ﬁle in JSON format. In this best practices example, we assume that the

request ﬁle is named create_cluster.json. Write create_cluster.json to provision a
HyperPod cluster with instance groups. The best practice is to add the same number of instance
groups as the number of Slurm nodes you plan to conﬁgure on the HyperPod cluster. Make sure
that you give distinctive names to the instance groups that you'll assign to Slurm nodes you plan
to set up.

Also, you are required to specify an S3 bucket path to store your entire set of conﬁguration ﬁles

and lifecycle scripts to the ﬁeld name InstanceGroups.LifeCycleConfig.SourceS3Uri

in the CreateCluster request form, and specify the ﬁle name of an entrypoint shell script

(assume that it's named on_create.sh) to InstanceGroups.LifeCycleConfig.OnCreate.

Note

If you are using the Create a cluster submission form in the HyperPod console UI, the

console manages ﬁlling and submitting the CreateCluster request on your behalf,

and runs the CreateCluster API in the backend. In this case, you don't need to

create create_cluster.json; instead, make sure that you specify the correct cluster
conﬁguration information to the Create a cluster submission form.

2. on_create.sh – For each instance group, you need to provide an entrypoint shell script,

on_create.sh, to run commands, run scripts to install software packages, and set up
the HyperPod cluster environment with Slurm. The two things you need to prepare are a

provisioning_parameters.json required by HyperPod for setting up Slurm and a set of
lifecycle scripts for installing software packages. This script should be written to ﬁnd and run the

following ﬁles as shown in the sample script at on_create.sh.

Slurm orchestration
1668

## Page 698

Amazon SageMaker AI
Developer Guide

Note

Make sure that you upload the entire set of lifecycle scripts to the S3

location you specify in create_cluster.json. You should also place your

provisioning_parameters.json in the same location.

a. provisioning_parameters.json – This is a the section called “Conﬁguration form for

provisioning_parameters.json”. The on_create.sh script ﬁnds this JSON ﬁle and deﬁnes
environment variable for identifying the path to it. Through this JSON ﬁle, you can conﬁgure
Slurm nodes and storage options such as Amazon FSx for Lustre for Slurm to communicate

with. In provisioning_parameters.json, make sure that you assign the HyperPod cluster

instance groups using the names you speciﬁed in create_cluster.json to the Slurm
nodes appropriately based on how you plan to set them up.

The following diagram shows an example of how the two JSON conﬁguration ﬁles

create_cluster.json and provisioning_parameters.json should be written to
assign HyperPod instance groups to Slurm nodes. In this example, we assume a case of setting
up three Slurm nodes: controller (management) node, log-in node (which is optional), and
compute (worker) node.

Tip

To help you validate these two JSON ﬁles, the HyperPod service team provides a

validation script, validate-config.py. To learn more, see the section called
“Validating conﬁguration ﬁles”.

Slurm orchestration
1669

## Page 699

Amazon SageMaker AI
Developer Guide

![Page 699 Diagram 1](images/page-0699-img-01.png)

Figure: Direct comparison between create_cluster.json for HyperPod cluster creation

and provisiong_params.json for Slurm conﬁguration. The number of instance groups in

create_cluster.json should match with the number of nodes you want to conﬁgure as
Slurm nodes. In case of the example in the ﬁgure, three Slurm nodes will be conﬁgured on a
HyperPod cluster of three instance groups. You should assign the HyperPod cluster instance
groups to Slurm nodes by specifying the instance group names accordingly.

b. resource_config.json – During cluster creation, the lifecycle_script.py script

is written to expect a resource_config.json ﬁle from HyperPod. This ﬁle contains
information about the cluster, such as instance types and IP addresses.

When you run the CreateCluster API, HyperPod creates a resource conﬁguration ﬁle at /

opt/ml/config/resource_config.json based on the create_cluster.json ﬁle. The

ﬁle path is saved to the environment variable named SAGEMAKER_RESOURCE_CONFIG_PATH.

Slurm orchestration
1670

## Page 700

Amazon SageMaker AI
Developer Guide

Important

The resource_config.json ﬁle is auto-generated by the HyperPod platform,
and you DO NOT need to create it. The following code is to show an example of

resource_config.json that would be created from the cluster creation based

on create_cluster.json in the previous step, and to help you understand what

happens in the backend and how an auto-generated resource_config.json would

look.

{

"ClusterConfig": {
"ClusterArn": "arn:aws:sagemaker:us-west-2:111122223333:cluster/
abcde01234yz",
"ClusterName": "your-hyperpod-cluster"
},
"InstanceGroups": [
{
"Name": "controller-machine",
"InstanceType": "ml.c5.xlarge",
"Instances": [
{
"InstanceName": "controller-machine-1",
"AgentIpAddress": "111.222.333.444",
"CustomerIpAddress": "111.222.333.444",
"InstanceId": "i-12345abcedfg67890"
}
]
},
{
"Name": "login-group",
"InstanceType": "ml.m5.xlarge",
"Instances": [
{
"InstanceName": "login-group-1",
"AgentIpAddress": "111.222.333.444",
"CustomerIpAddress": "111.222.333.444",
"InstanceId": "i-12345abcedfg67890"
}

Slurm orchestration
1671

## Page 701

Amazon SageMaker AI
Developer Guide

]
},
{
"Name": "compute-nodes",
"InstanceType": "ml.trn1.32xlarge",
"Instances": [
{
"InstanceName": "compute-nodes-1",
"AgentIpAddress": "111.222.333.444",
"CustomerIpAddress": "111.222.333.444",
"InstanceId": "i-12345abcedfg67890"
},
{
"InstanceName": "compute-nodes-2",
"AgentIpAddress": "111.222.333.444",
"CustomerIpAddress": "111.222.333.444",
"InstanceId": "i-12345abcedfg67890"

},
{
"InstanceName": "compute-nodes-3",
"AgentIpAddress": "111.222.333.444",
"CustomerIpAddress": "111.222.333.444",
"InstanceId": "i-12345abcedfg67890"
},
{
"InstanceName": "compute-nodes-4",
"AgentIpAddress": "111.222.333.444",
"CustomerIpAddress": "111.222.333.444",
"InstanceId": "i-12345abcedfg67890"
}
]
}
]
}

c. lifecycle_script.py – This is the main Python script that collectively runs lifecycle

scripts setting up Slurm on the HyperPod cluster while being provisioned. This script reads in

provisioning_parameters.json and resource_config.json from the paths that are

speciﬁed or identiﬁed in on_create.sh, passes the relevant information to each lifecycle
script, and then runs the lifecycle scripts in order.

Lifecycle scripts are a set of scripts that you have a complete ﬂexibility to customize
to install software packages and set up necessary or custom conﬁgurations during

Slurm orchestration
1672

## Page 702

Amazon SageMaker AI
Developer Guide

cluster creation, such as setting up Slurm, creating users, installing Conda or Docker.

The sample lifecycle_script.py script is prepared to run other base lifecycle

scripts in the repository, such as launching Slurm deamons (start_slurm.sh),

mounting Amazon FSx for Lustre (mount_fsx.sh), and setting up MariaDB accounting

(setup_mariadb_accounting.sh) and RDS accounting (setup_rds_accounting.sh).
You can also add more scripts, package them under the same directory, and add code lines

to lifecycle_script.py to let HyperPod run the scripts. For more information about the
base lifecycle scripts, see also 3.1 Lifecycle scripts in the Awsome Distributed Training GitHub
repository.

Note

HyperPod runs the section called “SageMaker HyperPod DLAMI” on each instance of
a cluster, and the AMI has pre-installed software packages complying compatibilities
between them and HyperPod functionalities. Note that if you reinstall any of the pre-
installed packages, you are responsible for installing compatible packages and note
that some HyperPod functionalities might not work as expected.

In addition to the default setups, more scripts for installing the following software are

available under the utils folder. The lifecycle_script.py ﬁle is already prepared to
include code lines for running the installation scripts, so see the following items to search
those lines and uncomment to activate them.

i. The following code lines are for installing Docker, Enroot, and Pyxis. These packages are

required to run Docker containers on a Slurm cluster.

To enable this installation step, set the enable_docker_enroot_pyxis parameter to

True in the config.py ﬁle.

# Install Docker/Enroot/Pyxis
if Config.enable_docker_enroot_pyxis:
ExecuteBashScript("./utils/install_docker.sh").run()
ExecuteBashScript("./utils/install_enroot_pyxis.sh").run(node_type)

ii. You can integrate your HyperPod cluster with Amazon Managed Service for Prometheus

and Amazon Managed Grafana to export metrics about the HyperPod cluster and cluster
nodes to Amazon Managed Grafana dashboards. To export metrics and use the Slurm
dashboard, the NVIDIA DCGM Exporter dashboard, and the EFA Metrics dashboard on

Slurm orchestration
1673

## Page 703

Amazon SageMaker AI
Developer Guide

Amazon Managed Grafana, you need to install the Slurm exporter for Prometheus, the
NVIDIA DCGM exporter, and the EFA node exporter. For more information about installing
the exporter packages and using Grafana dashboards on an Amazon Managed Grafana
workspace, see the section called “Cluster resources monitoring”.

To enable this installation step, set the enable_observability parameter to True in

the config.py ﬁle.

# Install metric exporting software and Prometheus for observability

if Config.enable_observability:
if node_type == SlurmNodeType.COMPUTE_NODE:
ExecuteBashScript("./utils/install_docker.sh").run()
ExecuteBashScript("./utils/install_dcgm_exporter.sh").run()
ExecuteBashScript("./utils/install_efa_node_exporter.sh").run()

if node_type == SlurmNodeType.HEAD_NODE:
wait_for_scontrol()
ExecuteBashScript("./utils/install_docker.sh").run()
ExecuteBashScript("./utils/install_slurm_exporter.sh").run()
ExecuteBashScript("./utils/install_prometheus.sh").run()

3. Make sure that you upload all conﬁguration ﬁles and setup scripts from Step 2 to the S3

bucket you provide in the CreateCluster request in Step 1. For example, assume that your

create_cluster.json has the following.

"LifeCycleConfig": {

"SourceS3URI": "s3://sagemaker-hyperpod-lifecycle/src",
"OnCreate": "on_create.sh"
}

Then, your "s3://sagemaker-hyperpod-lifecycle/src" should contain on_create.sh,

lifecycle_script.py, provisioning_parameters.json, and all other setup scripts.
Assume that you have prepared the ﬁles in a local folder as follows.

### lifecycle_files // your local folder
### provisioning_parameters.json
### on_create.sh
### lifecycle_script.py

Slurm orchestration
1674

## Page 704

Amazon SageMaker AI
Developer Guide

### ... // more setup scrips to be fed into lifecycle_script.py

To upload the ﬁles, use the S3 command as follows.

aws s3 cp --recursive ./lifecycle_scripts s3://sagemaker-hyperpod-lifecycle/src

What particular conﬁgurations HyperPod manages in Slurm conﬁguration ﬁles

When you create a Slurm cluster on HyperPod, the HyperPod agent sets up the slurm.conf and

gres.conf ﬁles at /opt/slurm/etc/ to manage the Slurm cluster based on your HyperPod
cluster creation request and lifecycle scripts. The following list shows which speciﬁc parameters the
HyperPod agent handles and overwrites.

Important

We strongly recommend that you do not change these parameters managed by HyperPod.

• In slurm.conf, HyperPod sets up the following basic parameters: ClusterName,

SlurmctldHost, PartitionName, and NodeName.

Also, to enable the the section called “Automatic node recovery and auto-resume” functionality,

HyperPod requires the TaskPlugin and SchedulerParameters parameters set as follows.
The HyperPod agent sets up these two parameters with the required values by default.

TaskPlugin=task/none
SchedulerParameters=permit_job_expansion

• In gres.conf, HyperPod manages NodeName for GPU nodes.

Slurm log rotations

SageMaker HyperPod provides automatic log rotation for Slurm daemon logs to help manage disk
space usage and maintain system performance. Log rotation is crucial for preventing logs from
consuming excessive disk space and ensuring optimal system operation by automatically archiving
and removing old log ﬁles while maintaining recent logging information. Slurm log rotations are
enabled by default when you create a cluster.

Slurm orchestration
1675

## Page 705

Amazon SageMaker AI
Developer Guide

How log rotation works

When enabled, the log rotation conﬁguration:

• Monitors all Slurm log ﬁles with the extension .log located in the /var/log/slurm/ folder on
the controller, login and compute nodes.

• Rotates logs when they reach 50 MB in size.

• Maintains up to two rotated log ﬁles before deleting them.

• Sends SIGUSR2 signal to Slurm daemons (slurmctld, slurmd, and slurmdbd) after rotation.

List of log ﬁles rotated

Slurm logs are located in the /var/log/slurm/ directory. Log rotation is enabled for all ﬁles that

match /var/log/slurm/*.log. When rotation occurs, rotated ﬁles have numerical suﬃxes (such

as slurmd.log.1). The following list is not exhaustive but shows some of the critical log ﬁles that
rotate automatically:

• /var/log/slurm/slurmctld.log

• /var/log/slurm/slurmd.log

• /var/log/slurm/slurmdb.log

• /var/log/slurm/slurmrestd.log

Enable or disable log rotation

You can control the log rotation feature using the enable_slurm_log_rotation parameter in

the config.py script of your cluster's lifecycle scripts, as shown in the following example:

class Config:
# Set false if you want to disable log rotation of Slurm daemon logs
enable_slurm_log_rotation = True  # Default value

To disable log rotation, set the parameter to False, as shown in the following example:

enable_slurm_log_rotation = False

Slurm orchestration
1676

## Page 706

Amazon SageMaker AI
Developer Guide

Note

Lifecycle scripts run on all Slurm nodes (controller, login, and compute nodes) during
cluster creation. They also run on new nodes when added to the cluster. Updating the log
rotation conﬁgurations must be done manually after cluster creation. The log rotation

conﬁguration is stored in /etc/logrotate.d/sagemaker-hyperpod-slurm. We
recommend keeping log rotation enabled to prevent log ﬁles from consuming excessive

disk space. To disable log rotation, delete the sagemaker-hyperpod-slurm ﬁle or

comment out its contents by adding # at the start of each line in the sagemaker-

hyperpod-slurm ﬁle.

Default log rotation settings

The following settings are conﬁgured automatically for each log ﬁle rotated:

Setting
Value
Description

rotate
2
Number of rotated log ﬁles to keep

size
50 MB
Maximum size before rotation

enabled
Copies and truncates the original log ﬁle

copytrunc

ate

compress
disabled
Rotated logs are not compressed

missingok
enabled
No error if log ﬁle is missing

notifempty
enabled
Doesn't rotate empty ﬁles

noolddir
enabled
Rotated ﬁles stay in same directory

Mounting Amazon FSx for Lustre and Amazon FSx for OpenZFS to a HyperPod cluster

To mount an Amazon FSx for Lustre shared ﬁle system to your HyperPod cluster, set up the
following.

1. Use your Amazon VPC.

Slurm orchestration
1677

## Page 707

Amazon SageMaker AI
Developer Guide

a. For HyperPod cluster instances to communicate within your VPC, make sure that you attach

the the section called “Setting up SageMaker HyperPod with a custom Amazon VPC” to the
IAM role for SageMaker HyperPod.

b. In create_cluster.json, include the following VPC information.

"VpcConfig": {
"SecurityGroupIds": [ "string" ],
"Subnets": [ "string" ]
}

For more tips about setting up Amazon VPC, see the section called “Prerequisites”.

2. To ﬁnish conﬁguring Slurm with Amazon FSx for Lustre, you can use one of the following

approaches. You can ﬁnd the Amazon FSx information either from the Amazon FSx for Lustre

console in your account or by running the following AWS CLI command, aws fsx describe-

file-systems.

Option A: API-Driven Conﬁguration (Recommended)

Specify the Amazon FSx conﬁguration directly in the CreateCluster API payload using

InstanceStorageConfigs within each instance group. This approach supports both FSx for
Lustre and FSx for OpenZFS, and allows per-instance-group FSx conﬁguration.

"InstanceStorageConfigs": [
{
"FsxLustreConfig": {
"DnsName": "fs-12345678a90b01cde.fsx.us-west-2.amazonaws.com",
"MountPath": "/fsx",
"MountName": "1abcdefg"
}
}
]

For FSx for OpenZFS, use FsxOpenZfsConfig instead:

"InstanceStorageConfigs": [
{
"FsxOpenZfsConfig": {
"DnsName": "fs-12345678a90b01cde.fsx.us-west-2.amazonaws.com",
"MountPath": "/fsx-openzfs"

Slurm orchestration
1678

## Page 708

Amazon SageMaker AI
Developer Guide

}
}
]

For more details, see Getting started with SageMaker HyperPod using the AWS CLI.

Option B: Legacy Conﬁguration

Specify the Amazon FSx DNS name and Amazon FSx mount name in

provisioning_parameters.json as shown in the ﬁgure in the the section called “Base
lifecycle scripts” section.

"fsx_dns_name": "fs-12345678a90b01cde.fsx.us-west-2.amazonaws.com",
"fsx_mountname": "1abcdefg"

Validating the JSON conﬁguration ﬁles before creating a Slurm cluster on HyperPod

To validate the JSON conﬁguration ﬁles before submitting a cluster creation request, use the

conﬁguration validation script validate-config.py. This script parses and compares your
HyperPod cluster conﬁguration JSON ﬁle and Slurm conﬁguration JSON ﬁle, and identiﬁes if
there's any resource misconﬁguration between the two ﬁles and also across Amazon EC2, Amazon

VPC, and Amazon FSx resources. For example, to validate the create_cluster.json and

provisioning_parameters.json ﬁles from the the section called “Base lifecycle scripts”
section, run the validation script as follows.

python3 validate-config.py --cluster-config create_cluster.json --provisioning-
parameters provisioning_parameters.json

The following is an example output of a successful validation.

##  Validated instance group name worker-group-1 is correct ...

##  Validated subnet subnet-012345abcdef67890 ...
##  Validated security group sg-012345abcdef67890 ingress rules ...
##  Validated security group sg-012345abcdef67890 egress rules ...
##  Validated FSx Lustre DNS name fs-012345abcdef67890.fsx.us-east-1.amazonaws.com
##  Validated FSx Lustre mount name abcdefgh
# Cluster Validation succeeded

Slurm orchestration
1679

## Page 709

Amazon SageMaker AI
Developer Guide

Validating runtime before running production workloads on a HyperPod Slurm cluster

To check the runtime before running any production workloads on a Slurm cluster on HyperPod,

use the runtime validation script hyperpod-precheck.py. This script checks if the Slurm cluster
has all packages installed for running Docker, if the cluster has a properly mounted FSx for Lustre
ﬁle system and a user directory sharing the ﬁle system, and if the Slurm deamon is running on all
compute nodes.

To run the script on multiple nodes at once, use srun as shown in the following example command
of running the script on a Slurm cluster of 8 nodes.

# The following command runs on 8 nodes
srun -N 8 python3 hyperpod-precheck.py

Note

To learn more about the validation script such as what runtime validation functions the
script provides and guidelines to resolve issues that don't pass the validations, see Runtime
validation before running workloads in the Awsome Distributed Training GitHub repository.

Developing lifecycle scripts interactively on a HyperPod cluster node

This section explains how you can interactively develop lifecycle scripts without repeatedly creating
and deleting a HyperPod cluster.

1. Create a HyperPod cluster with the base lifecycle scripts.

2. Log in to a cluster node.

3. Develop a script (configure_xyz.sh) by editing and running it repeatedly on the node.

a. HyperPod runs the lifecycle scripts as the root user, so we recommend that you run the

configure_xyz.sh as the root user while developing to make sure that the script is tested
under the same condition while run by HyperPod.

4. Integrate the script into lifecycle_script.py by adding a code line similar to the following.

ExecuteBashScript("./utils/configure_xyz.sh").run()

5. Upload the updated lifecycle scripts to the S3 bucket that you initially used for uploading the

base lifecycle scripts.

Slurm orchestration
1680

## Page 710

Amazon SageMaker AI
Developer Guide

6. Test the integrated version of lifecycle_script.py by creating a new HyperPod cluster. You

can also use manual instance replacement to test the updated lifecycle scripts by creating new
instances. For detailed instructions, see Manually replace a node. Note that only worker nodes
are replaceable.

SageMaker HyperPod multi-head node support

You can create multiple controller (head) nodes in a single SageMaker HyperPod Slurm cluster, with
one serving as the primary controller node and the others serving as backup controller nodes. The
primary controller node is responsible for controlling the compute (worker) nodes and handling
Slurm operations. The backup controller nodes constantly monitor the primary controller node. If
the primary controller node fails or becomes unresponsive, one of the backup controller nodes will
automatically take over as the new primary controller node.

Conﬁguring multiple controller nodes in SageMaker HyperPod Slurm clusters provides several
key beneﬁts. It eliminates the risk of single controller node failure by providing controller head
nodes, enables automatic failover to backup controller nodes with faster recovery, and allows you
to manage your own accounting databases and Slurm conﬁguration independently.

Key concepts

The following provides details about the concepts related to SageMaker HyperPod multiple
controller (head) nodes support for Slurm clusters.

Controller node

A controller node is an Amazon EC2 instance within a cluster that runs critical Slurm services for
managing and coordinating the cluster's operations. Speciﬁcally, it hosts the Slurm controller
daemon (slurmctld) and the Slurm database daemon (slurmdbd). A controller node is also known
as a head node.

Primary controller node

A primary controller node is the active and currently controlling controller node in a Slurm cluster.
It is identiﬁed by Slurm as the primary controller node responsible for managing the cluster.
The primary controller node receives and executes commands from users to control and allocate
resources on the compute nodes for running jobs.

Backup controller node

Slurm orchestration
1681

## Page 711

Amazon SageMaker AI
Developer Guide

A backup controller node is an inactive and standby controller node in a Slurm cluster. It is
identiﬁed by Slurm as a backup controller node that is not currently managing the cluster. The
backup controller node runs the Slurm controller daemon (slurmctld) in standby mode. Any
controller commands executed on the backup controller nodes will be propagated to the primary
controller node for execution. Its primary purpose is to continuously monitor the primary controller
node and take over its responsibilities if the primary controller node fails or becomes unresponsive.

Compute node

A compute node is an Amazon EC2 instance within a cluster that hosts the Slurm worker daemon
(slurmd). The compute node's primary function is to execute jobs assigned by the Slurm controller
daemon (slurmctld) running on the primary controller node. When a job is scheduled, the compute
node receives instructions from the Slurm controller daemon (slurmctld) to carry out the necessary
tasks and computations for that job within the node itself. A compute is also known as a worker

node.

How it works

The following diagram illustrates how diﬀerent AWS services work together to support the
multiple controller (head) nodes architecture for SageMaker HyperPod Slurm clusters.

![Page 711 Diagram 1](images/page-0711-img-01.png)

Slurm orchestration
1682

## Page 712

Amazon SageMaker AI
Developer Guide

The AWS services that work together to support the SageMaker HyperPod multiple controller
(head) nodes architecture include the following.

AWS services that work together to support the SageMaker HyperPod multiple controller nodes
architecture

Service
Description

IAM (AWS
Identity
and Access
Managemen
t)

Deﬁnes two IAM roles to control the
access permissions: one role for the
compute node instance group and the
other for the controller node instance
group.

Amazon
RDS for
MariaDB

Stores accounting data for Slurm,
which holds job records and metering
data.

AWS
Secrets
Manager

Stores and manages credentials that
can be accessed by Amazon FSx for
Lustre.

Amazon
FSx for
Lustre

Stores Slurm conﬁgurations and
runtime state.

Amazon
VPC

Provides an isolated network
environment where the HyperPod

cluster and its resources are deployed.

Amazon
SNS

Sends notiﬁcations to administrators
when there are status changes (Slurm

controller is ON or OFF) related to the
primary controller (head) node.

The HyperPod cluster itself consists of controller nodes (primary and backup) and compute nodes.
The controller nodes run the Slurm controller (SlurmCtld) and database (SlurmDBd) components,
which manage and monitor the workload across the compute nodes.

Slurm orchestration
1683

## Page 713

Amazon SageMaker AI
Developer Guide

The controller nodes access Slurm conﬁgurations and runtime state stored in the Amazon FSx for
Lustre ﬁle system. The Slurm accounting data is stored in the Amazon RDS for MariaDB database.
AWS Secrets Manager provides secure access to the database credentials for the controller nodes.

If there is a status change (Slurm controller is ON or OFF) in the Slurm controller nodes, Amazon
SNS sends notiﬁcations to the admin for further action.

This multiple controller nodes architecture eliminates the single point of failure of a single
controller (head) node, enables fast and automatic failover recovery, and gives you control over the
Slurm accounting database and conﬁgurations.

Setting up multiple controller nodes for a SageMaker HyperPod Slurm cluster

This topic explains how to conﬁgure multiple controller (head) nodes in a SageMaker HyperPod
Slurm cluster using lifecycle scripts. Before you start, review the prerequisites listed in ??? and

familiarize yourself with the lifecycle scripts in ???. The instructions in this topic use AWS CLI
commands in Amazon Linux environment. Note that the environment variables used in these
commands are available in the current session unless explicitly preserved.

To set up multiple controller (head) nodes for a SageMaker HyperPod Slurm cluster, follow
these steps.

• Provisioning resources using CloudFormation stacks

• Creating and attaching an IAM policy

• Preparing and uploading lifecycle scripts

• Creating a SageMaker HyperPod cluster

• Considering important notes

• Reviewing environment variables reference

Provisioning resources using CloudFormation stacks

To set up multiple controller nodes in a HyperPod Slurm cluster, provision AWS resources through
two CloudFormation stacks: ??? and ???.

Provision basic resources

Follow these steps to provision basic resources for your Amazon SageMaker HyperPod Slurm
cluster.

Slurm orchestration
1684

## Page 714

Amazon SageMaker AI
Developer Guide

1.
Download the sagemaker-hyperpod.yaml template ﬁle to your machine. This YAML ﬁle is an
CloudFormation template that deﬁnes the following resources to create for your Slurm cluster.

• An execution IAM role for the compute node instance group

• An Amazon S3 bucket to store the lifecycle scripts

• Public and private subnets (private subnets have internet access through NAT gateways)

• Internet Gateway/NAT gateways

• Two Amazon EC2 security groups

• An Amazon FSx volume to store conﬁguration ﬁles

2.
Run the following CLI command to create a CloudFormation stack named sagemaker-

hyperpod. Deﬁne the Availability Zone (AZ) IDs for your cluster in PrimarySubnetAZ and

BackupSubnetAZ. For example, use1-az4 is an AZ ID for an Availability Zone in the us-

east-1 Region. For more information, see Availability Zone IDs and ???.

aws cloudformation deploy \
--template-file /path_to_template/sagemaker-hyperpod.yaml \
--stack-name sagemaker-hyperpod \
--parameter-overrides PrimarySubnetAZ=use1-az4 BackupSubnetAZ=use1-az1 \
--capabilities CAPABILITY_IAM

For more information, see deploy from the AWS Command Line Interface Reference. The stack
creation can take a few minutes to complete. When it's complete, you will see the following in
your command line interface.

Waiting for changeset to be created..
Waiting for stack create/update to complete
Successfully created/updated stack - sagemaker-hyperpod

3.
(Optional) Verify the stack in the CloudFormation console.

• From the left navigation, choose Stack.

• On the Stack page, ﬁnd and choose sagemaker-hyperpod.

• Choose the tabs like Resources and Outputs to review the resources and outputs.

4.
Create environment variables from the stack (sagemaker-hyperpod) outputs. You will use
values of these variables to ???.

source .env
Slurm orchestration
1685

## Page 715

Amazon SageMaker AI
Developer Guide

PRIMARY_SUBNET=$(aws --region $REGION cloudformation describe-stacks
--stack-name $SAGEMAKER_STACK_NAME --query 'Stacks[0].Outputs[?
OutputKey==`PrimaryPrivateSubnet`].OutputValue' --output text)
BACKUP_SUBNET=$(aws --region $REGION cloudformation describe-stacks
--stack-name $SAGEMAKER_STACK_NAME --query 'Stacks[0].Outputs[?
OutputKey==`BackupPrivateSubnet`].OutputValue' --output text)
EMAIL=$(bash -c 'read -p "INPUT YOUR SNSSubEmailAddress HERE: " && echo $REPLY')
DB_USER_NAME=$(bash -c 'read -p "INPUT YOUR DB_USER_NAME HERE: " && echo $REPLY')
SECURITY_GROUP=$(aws --region $REGION cloudformation describe-stacks
--stack-name $SAGEMAKER_STACK_NAME --query 'Stacks[0].Outputs[?
OutputKey==`SecurityGroup`].OutputValue' --output text)
ROOT_BUCKET_NAME=$(aws --region $REGION cloudformation describe-stacks
--stack-name $SAGEMAKER_STACK_NAME --query 'Stacks[0].Outputs[?
OutputKey==`AmazonS3BucketName`].OutputValue' --output text)
SLURM_FSX_DNS_NAME=$(aws --region $REGION cloudformation describe-
stacks --stack-name $SAGEMAKER_STACK_NAME --query 'Stacks[0].Outputs[?
OutputKey==`FSxLustreFilesystemDNSname`].OutputValue' --output text)

SLURM_FSX_MOUNT_NAME=$(aws --region $REGION cloudformation describe-
stacks --stack-name $SAGEMAKER_STACK_NAME --query 'Stacks[0].Outputs[?
OutputKey==`FSxLustreFilesystemMountname`].OutputValue' --output text)
COMPUTE_NODE_ROLE=$(aws --region $REGION cloudformation describe-
stacks --stack-name $SAGEMAKER_STACK_NAME --query 'Stacks[0].Outputs[?
OutputKey==`AmazonSagemakerClusterExecutionRoleArn`].OutputValue' --output text)

When you see prompts asking for your email address and database user name, enter values
like the following.

INPUT YOUR SNSSubEmailAddress HERE: Email_address_to_receive_SNS_notifications
INPUT YOUR DB_USER_NAME HERE: Database_user_name_you_define

To verify variable values, use the print $variable command.

print $REGION
us-east-1

Provision additional resources to support multiple controller nodes

Follow these steps to provision additional resources for your Amazon SageMaker HyperPod Slurm
cluster with multiple controller nodes.

Slurm orchestration
1686

## Page 716

Amazon SageMaker AI
Developer Guide

1.
Download the sagemaker-hyperpod-slurm-multi-headnode.yaml template ﬁle to your
machine. This second YAML ﬁle is an CloudFormation template that deﬁnes the additional
resources to create for multiple controller nodes support in your Slurm cluster.

• An execution IAM role for the controller node instance group

• An Amazon RDS for MariaDB instance

• An Amazon SNS topic and subscription

• AWS Secrets Manager credentials for Amazon RDS for MariaDB

2.
Run the following CLI command to create a CloudFormation stack named sagemaker-

hyperpod-mh. This second stack uses the CloudFormation template to create additional AWS
resources to support the multiple controller nodes architecture.

aws cloudformation deploy \
--template-file /path_to_template/slurm-multi-headnode.yaml \
--stack-name sagemaker-hyperpod-mh \
--parameter-overrides \
SlurmDBSecurityGroupId=$SECURITY_GROUP \
SlurmDBSubnetGroupId1=$PRIMARY_SUBNET \
SlurmDBSubnetGroupId2=$BACKUP_SUBNET \
SNSSubEmailAddress=$EMAIL \
SlurmDBUsername=$DB_USER_NAME \
--capabilities CAPABILITY_NAMED_IAM

For more information, see deploy from the AWS Command Line Interface Reference. The stack
creation can take a few minutes to complete. When it's complete, you will see the following in
your command line interface.

Waiting for changeset to be created..
Waiting for stack create/update to complete
Successfully created/updated stack - sagemaker-hyperpod-mh

3.
(Optional) Verify the stack in the AWS Cloud Formation console.

• From the left navigation, choose Stack.

• On the Stack page, ﬁnd and choose sagemaker-hyperpod-mh.

• Choose the tabs like Resources and Outputs to review the resources and outputs.

Slurm orchestration
1687

## Page 717

Amazon SageMaker AI
Developer Guide

4.
Create environment variables from the stack (sagemaker-hyperpod-mh)
outputs. You will use values of these variables to update the conﬁguration ﬁle

(provisioning_parameters.json) in ???.

source .env
SLURM_DB_ENDPOINT_ADDRESS=$(aws --region us-east-1 cloudformation describe-
stacks --stack-name $MULTI_HEAD_SLURM_STACK --query 'Stacks[0].Outputs[?
OutputKey==`SlurmDBEndpointAddress`].OutputValue' --output text)
SLURM_DB_SECRET_ARN=$(aws --region us-east-1 cloudformation describe-
stacks --stack-name $MULTI_HEAD_SLURM_STACK --query 'Stacks[0].Outputs[?
OutputKey==`SlurmDBSecretArn`].OutputValue' --output text)
SLURM_EXECUTION_ROLE_ARN=$(aws --region us-east-1 cloudformation describe-
stacks --stack-name $MULTI_HEAD_SLURM_STACK --query 'Stacks[0].Outputs[?
OutputKey==`SlurmExecutionRoleArn`].OutputValue' --output text)
SLURM_SNS_FAILOVER_TOPIC_ARN=$(aws --region us-east-1 cloudformation describe-
stacks --stack-name $MULTI_HEAD_SLURM_STACK --query 'Stacks[0].Outputs[?
OutputKey==`SlurmFailOverSNSTopicArn`].OutputValue' --output text)

Creating and attaching an IAM policy

This section explains how to create an IAM policy and attach it to the execution role you created in
???.

1.
Download the IAM policy example to your machine from the GitHub repository.

2.
Create an IAM policy with the downloaded example, using the create-policy CLI command.

aws --region us-east-1 iam create-policy \
--policy-name AmazonSagemakerExecutionPolicy \
--policy-document file://1.AmazonSageMakerClustersExecutionRolePolicy.json

Example output of the command.

{
"Policy": {
"PolicyName": "AmazonSagemakerExecutionPolicy",
"PolicyId": "ANPAXISIWY5UYZM7WJR4W",
"Arn": "arn:aws:iam::111122223333:policy/AmazonSagemakerExecutionPolicy",
"Path": "/",
"DefaultVersionId": "v1",
"AttachmentCount": 0,

Slurm orchestration
1688

## Page 718

Amazon SageMaker AI
Developer Guide

"PermissionsBoundaryUsageCount": 0,
"IsAttachable": true,
"CreateDate": "2025-01-22T20:01:21+00:00",
"UpdateDate": "2025-01-22T20:01:21+00:00"
}
}

3.
Attach the policy AmazonSagemakerExecutionPolicy to the Slurm execution role you
created in ???, using the attach-role-policy CLI command.

aws --region us-east-1 iam attach-role-policy \
--role-name AmazonSagemakerExecutionRole \
--policy-arn arn:aws:iam::111122223333:policy/AmazonSagemakerExecutionPolicy

This command doesn't produce any output.

(Optional) If you use environment variables, here are the example commands.

• To get the role name and policy name

POLICY=$(aws --region $REGION iam list-policies --query 'Policies[?
PolicyName==AmazonSagemakerExecutionPolicy].Arn' --output text)
ROLENAME=$(aws --region $REGION iam list-roles --query "Roles[?
Arn=='${SLURM_EXECUTION_ROLE_ARN}'].RoleName" —output text)

• To attach the policy

aws  --region us-east-1 iam attach-role-policy \
--role-name $ROLENAME --policy-arn $POLICY

For more information, see the section called “IAM role for SageMaker HyperPod”.

Preparing and uploading lifecycle scripts

After creating all the required resources, you'll need to set up lifecycle scripts for your SageMaker
HyperPod cluster. These lifecycle scripts provide a base conﬁguration you can use to create a basic
HyperPod Slurm cluster.

Prepare the lifecycle scripts

Follow these steps to get the lifecycle scripts.

Slurm orchestration
1689

## Page 719

Amazon SageMaker AI
Developer Guide

1.
Download the lifecycle scripts from the GitHub repository to your machine.

2.
Upload the lifecycle scripts to the Amazon S3 bucket you created in ???, using the cp CLI
command.

aws s3 cp --recursive LifeCycleScripts/base-config s3://${ROOT_BUCKET_NAME}/
LifeCycleScripts/base-config

Create conﬁguration ﬁle

Follow these steps to create the conﬁguration ﬁle and upload it to the same Amazon S3 bucket
where you store the lifecycle scripts.

1.
Create a conﬁguration ﬁle named provisioning_parameters.json with the following

conﬁguration. Note that slurm_sns_arn is optional. If not provided, HyperPod will not set up
the Amazon SNS notiﬁcations.

cat <<EOF > /tmp/provisioning_parameters.json
{
"version": "1.0.0",
"workload_manager": "slurm",
"controller_group": "$CONTOLLER_IG_NAME",
"login_group": "my-login-group",
"worker_groups": [
{
"instance_group_name": "$COMPUTE_IG_NAME",
"partition_name": "dev"
}
],
"fsx_dns_name": "$SLURM_FSX_DNS_NAME",
"fsx_mountname": "$SLURM_FSX_MOUNT_NAME",
"slurm_configurations": {
"slurm_database_secret_arn": "$SLURM_DB_SECRET_ARN",
"slurm_database_endpoint": "$SLURM_DB_ENDPOINT_ADDRESS",
"slurm_shared_directory": "/fsx",
"slurm_database_user": "$DB_USER_NAME",
"slurm_sns_arn": "$SLURM_SNS_FAILOVER_TOPIC_ARN"
}
}
EOF

Slurm orchestration
1690

## Page 720

Amazon SageMaker AI
Developer Guide

2.
Upload the provisioning_parameters.json ﬁle to the same Amazon S3 bucket where
you store the lifecycle scripts.

aws s3 cp /tmp/provisioning_parameters.json s3://${ROOT_BUCKET_NAME}/
LifeCycleScripts/base-config/provisioning_parameters.json

Note

If you are using API-driven conﬁguration, the provisioning_parameters.json ﬁle
is not required. With API-driven conﬁguration, you deﬁne Slurm node types, partitions,
and FSx mounting directly in the CreateCluster API payload. For details, see Getting
started with SageMaker HyperPod using the AWS CLI.

Verify ﬁles in Amazon S3 bucket

After you upload all the lifecycle scripts and the provisioning_parameters.json ﬁle, your
Amazon S3 bucket should look like the following.

![Page 720 Diagram 1](images/page-0720-img-01.png)

For more information, see Start with base lifecycle scripts provided by HyperPod.

Slurm orchestration
1691

## Page 721

Amazon SageMaker AI
Developer Guide

Creating a SageMaker HyperPod cluster

After setting up all the required resources and uploading the scripts to the Amazon S3 bucket, you
can create a cluster.

1.
To create a cluster, run the create-cluster AWS CLI command. The creation process can
take up to 15 minutes to complete.

aws --region $REGION sagemaker create-cluster \
--cluster-name $HP_CLUSTER_NAME \
--vpc-config '{
"SecurityGroupIds":["'$SECURITY_GROUP'"],
"Subnets":["'$PRIMARY_SUBNET'", "'$BACKUP_SUBNET'"]
}' \
--instance-groups '[{
"InstanceGroupName": "'$CONTOLLER_IG_NAME'",

"InstanceType": "ml.t3.medium",
"InstanceCount": 2,
"LifeCycleConfig": {
"SourceS3Uri": "s3://'$BUCKET_NAME'",
"OnCreate": "on_create.sh"
},
"ExecutionRole": "'$SLURM_EXECUTION_ROLE_ARN'",
"ThreadsPerCore": 1
},
{
"InstanceGroupName": "'$COMPUTE_IG_NAME'",
"InstanceType": "ml.c5.xlarge",
"InstanceCount": 2,
"LifeCycleConfig": {
"SourceS3Uri": "s3://'$BUCKET_NAME'",
"OnCreate": "on_create.sh"
},
"ExecutionRole": "'$COMPUTE_NODE_ROLE'",
"ThreadsPerCore": 1
}]'

After successful execution, the command returns the cluster ARN like the following.

{
"ClusterArn": "arn:aws:sagemaker:us-east-1:111122223333:cluster/cluster_id"
}

Slurm orchestration
1692

## Page 722

Amazon SageMaker AI
Developer Guide

2.
(Optional) To check the status of your cluster, you can use the SageMaker AI console (https://
console.aws.amazon.com/sagemaker/). From the left navigation, choose HyperPod Clusters,
then choose Cluster Management. Choose a cluster name to open the cluster details page. If
your cluster is created successfully, you will see the cluster status is InService.

![Page 722 Diagram 1](images/page-0722-img-01.png)

Considering important notes

This section provides several important notes which you might ﬁnd helpful.

1. To migrate to a multi-controller Slurm cluster, complete these steps.

a. Follow the instructions in the section called “Provisioning resources” to provision all the

required resources.

b. Follow the instructions in the section called “Preparing and uploading lifecycle scripts” to

upload the updated lifecycle scripts. When updating the provisioning_parameters.json

ﬁle, move your existing controller group to the worker_groups section, and add a new

controller group name in the controller_group section.

c. Run the update-cluster API call to create a new controller group and keep the original

compute instance groups and controller group.

2. To scale down the number of controller nodes, use the update-cluster CLI command. For each

controller instance group, the minimum number of controller nodes you can scale down to is 1.
This means that you cannot scale down the number of controller nodes to 0.

Slurm orchestration
1693

## Page 723

Amazon SageMaker AI
Developer Guide

Important

For clusters created before Jan 24, 2025, you must ﬁrst update your cluster software
using the UpdateClusterSoftware API before running the update-cluster CLI command.

The following is an example CLI command to scale down the number of controller nodes.

aws sagemaker update-cluster \
--cluster-name my_cluster \
--instance-groups '[{
"InstanceGroupName": "controller_ig_name",
"InstanceType": "ml.t3.medium",
"InstanceCount": 3,
"LifeCycleConfig": {
"SourceS3Uri": "s3://amzn-s3-demo-bucket1",
"OnCreate": "on_create.sh"
},
"ExecutionRole": "slurm_execution_role_arn",
"ThreadsPerCore": 1
},
{
"InstanceGroupName": "compute-ig_name",
"InstanceType": "ml.c5.xlarge",
"InstanceCount": 2,
"LifeCycleConfig": {
"SourceS3Uri": "s3://amzn-s3-demo-bucket1",
"OnCreate": "on_create.sh"
},
"ExecutionRole": "compute_node_role_arn",
"ThreadsPerCore": 1
}]'

3. To batch delete the controller nodes, use the batch-delete-cluster-nodes CLI command. For

each controller instance group, you must keep at least one controller node. If you want to batch
delete all the controller nodes, the API operation won't work.

Slurm orchestration
1694

## Page 724

Amazon SageMaker AI
Developer Guide

Important

For clusters created before Jan 24, 2025, you must ﬁrst update your cluster software
using the UpdateClusterSoftware API before running the batch-delete-cluster-nodes CLI
command.

The following is an example CLI command to batch delete the controller nodes.

aws sagemaker batch-delete-cluster-nodes --cluster-name my_cluster --node-
ids instance_ids_to_delete

4. To troubleshoot your cluster creation issues, check the failure message from the cluster details

page in your SageMaker AI console. You can also use CloudWatch logs to troubleshoot cluster

creation issues. From the CloudWatch console, choose Log groups. Then, search clusters to
see the list of log groups related to your cluster creation.

![Page 724 Diagram 1](images/page-0724-img-01.png)

Reviewing environment variables reference

The following environment variables are deﬁned and used in the tutorial of the section called
“Setting up multiple controller nodes”. These environment variables are only available in the

current session unless explicitly preserved. They are deﬁned using the $variable_name syntax.
Variables with key/value pairs represent AWS-created resources, while variables without keys are
user-deﬁned.

Slurm orchestration
1695

## Page 725

Amazon SageMaker AI
Developer Guide

Environment variables reference

Variable Description

• Example key: BackupPrivateSubnet

$BACKUP_S

UBNET

• Example value: subnet-04a8ab51748510a51

• Description: The backup private subnet ID used for
HyperPod Slurm cluster creation.

• Example value: compute-nodes

$COMPUTE_

IG_NAME

• Description: The name of the compute instance group
used for cluster creation.

• Example key: AmazonSagemakerClusterExecu

$COMPUTE_

NODE_ROLE

tionRoleArn

• Example value: arn:aws:iam::111122223333:r

ole/sagemaker-hyperpod-AmazonSagemak

erClusterExecutionR-123OTacPcKk1

• Description: The Amazon Resource Name (ARN) of the
IAM role for the compute instance group.

• Example value: controller-machine

$CONTOLLE

R_IG_NAME

• Description: The name of the controller instance group
for cluster creation.

• Example value: mydbuser

$DB_USER_

NAME

• Description: The database username for Slurm
accounting database access, used in the section called
“Provision additional resources to support multiple
controller nodes”.

$EMAIL
• Example value: 123abc@email.com

• Description: The email address for Amazon SNS
notiﬁcations, used in the section called “Provision
additional resources to support multiple controller
nodes”.

Slurm orchestration
1696

## Page 726

Amazon SageMaker AI
Developer Guide

Variable Description

• Example key: PrimaryPrivateSubnet

$PRIMARY_

SUBNET

• Example value: subnet-01a56ebc42df102a7

• Description: The primary private subnet ID used for
HyperPod Slurm cluster creation.

$POLICY • Example value: arn:aws:iam::111122223333:p

olicy/AmazonSagemakerExecutionPolicy

• Description: The IAM policy ARN you create and attach
to the Slurm execution role for the controller instance
group.

$REGION • Example value: us-east-1

• Description: The AWS Region where you create all the
resources.

• Example key: SecurityGroup

$ROOT_BUC

KET_NAME

• Example value: sagemaker-lifecycle-ab21400

0

• Description: The name of the Amazon S3 bucket where
lifecycle scripts are uploaded.

• Example key: AmazonS3BucketName

$SECURITY

_GROUP

• Example value: sg-006a5d175cb35675a

• Description: The security group ID used for the section
called “Provision additional resources to support
multiple controller nodes”.

• Example key: SlurmDBEndpointAddress

$SLURM_DB

_ENDPOINT

• Example value: sagemaker-hyperpod-mh-slurm

_ADDRESS

dbinstance-sxcmatjv0ei0.clplgxt06ysb

.us-east-1.rds.amazonaws.com

• Description: The Amazon RDS database endpoint used
in cluster creation.

Slurm orchestration
1697

## Page 727

Amazon SageMaker AI
Developer Guide

Variable Description

• Example key: SlurmDBSecretArn

$SLURM_DB

_SECRET_A

• Example value: arn:aws:secretsmanager:us-e

RN

ast-1:111122223333:secret:sagemaker-

hyperpod-mh-db-secret-us-east-1-dmz7

2K

• Description: The database secret ARN used in cluster
creation.

• Example key: SlurmExecutionRoleArn

$SLURM_EX

ECUTION_R

• Example value: arn:aws:iam::111122223333:r

OLE_ARN

ole/sagemaker-hyperpod-mhSlurmExecut

ionRole-us-east-1

• Description: The IAM role ARN for the controller
instance group, used in cluster creation.

• Example key: FSxLustreFilesystemDNSname

$SLURM_FS

X_DNS_NAM

• Example value: fs-0662da327f9326017.fsx.us

E

-east-1.amazonaws.com

• Description: The Domain Name System (DNS) of the
FSx for Lustre ﬁlesystem, used in the section called
“Provision additional resources to support multiple
controller nodes” and the section called “Create
conﬁguration ﬁle”.

• Example key: FSxLustreFilesystemMountname

$SLURM_FS

X_MOUNT_N

• Example value: lbajka4v

AME

• Description: The mount ID of the FSx for Lustre
ﬁlesystem, used in the section called “Provision
additional resources to support multiple controller
nodes” and the section called “Create conﬁguration
ﬁle”.

Slurm orchestration
1698

## Page 728

Amazon SageMaker AI
Developer Guide

Variable Description

• Example key: SlurmFailOverSNSTopicArn

$SLURM_SN

S_FAILOVE

• Example value: arn:aws:sns:us-east-1:11112

R_TOPIC_A

2223333:sagemaker-hyperpod-mhSlurmFa

RN

ilOverTopic-us-east-1

• Description: The Amazon SNS topic ARN, used in the
section called “Create conﬁguration ﬁle”.

Jobs on SageMaker HyperPod clusters

The following topics provide procedures and examples of accessing compute nodes and running
ML workloads on provisioned SageMaker HyperPod clusters. Depending on how you have set up
the environment on your HyperPod cluster, there are many ways to run ML workloads on HyperPod
clusters. Examples of running ML workloads on HyperPod clusters are also provided in the Awsome
Distributed Training GitHub repository. The following topics walk you through how to log in to the
provisioned HyperPod clusters and get you started with running sample ML workloads.

Tip

To ﬁnd practical examples and solutions, see also the SageMaker HyperPod workshop.

Topics

• Accessing your SageMaker HyperPod cluster nodes

• Scheduling a Slurm job on a SageMaker HyperPod cluster

• Running Docker containers on a Slurm compute node on HyperPod

• Running distributed training workloads with Slurm on HyperPod

Accessing your SageMaker HyperPod cluster nodes

You can access your InService cluster through AWS Systems Manager (SSM) by running the AWS

CLI command aws ssm start-session with the SageMaker HyperPod cluster host name in

format of sagemaker-cluster:[cluster-id]_[instance-group-name]-[instance-id].
You can retrieve the cluster ID, the instance ID, and the instance group name from the SageMaker

Slurm orchestration
1699

## Page 729

Amazon SageMaker AI
Developer Guide

HyperPod console or by running describe-cluster and list-cluster-nodes from the AWS

CLI commands for SageMaker HyperPod. For example, if your cluster ID is aa11bbbbb222, the

cluster node name is controller-group, and the cluster node ID is i-111222333444555aa, the

SSM start-session command should be the following.

Note

Granting users access to HyperPod cluster nodes allows them to install and operate user-
managed software on the nodes. Ensure that you maintain the principle of least-privilege
permissions for users.
If you haven't set up AWS Systems Manager, follow the instructions provided at the section
called “Setting up AWS Systems Manager and Run As for cluster user access control”.

$ aws ssm start-session \
--target sagemaker-cluster:aa11bbbbb222_controller-group-i-111222333444555aa \
--region us-west-2
Starting session with SessionId: s0011223344aabbccdd
root@ip-111-22-333-444:/usr/bin#

Note that this initially connects you as the root user. Before running jobs, switch to the ubuntu
user by running the following command.

root@ip-111-22-333-444:/usr/bin# sudo su - ubuntu
ubuntu@ip-111-22-333-444:/usr/bin#

For advanced settings for practical use of HyperPod clusters, see the following topics.

Topics

• Additional tips for accessing your SageMaker HyperPod cluster nodes

• Set up a multi-user environment through the Amazon FSx shared space

• Set up a multi-user environment by integrating HyperPod clusters with Active Directory

Additional tips for accessing your SageMaker HyperPod cluster nodes

Use the easy-ssh.sh script provided by HyperPod for simplifying the connection process

Slurm orchestration
1700

## Page 730

Amazon SageMaker AI
Developer Guide

To make the previous process into a single line command, the HyperPod team provides the easy-

ssh.sh script that retrieves your cluster information, aggregates them into the SSM command,

and connects to the compute node. You don't need to manually look for the required HyperPod

cluster information as this script runs describe-cluster and list-cluster-nodes commands

and parses the information needed for completing the SSM command. The following example

commands show how to run the easy-ssh.sh script. If it runs successfully, you'll be connected
to the cluster as the root user. It also prints a code snippet to set up SSH by adding the HyperPod

cluster as a remote host through an SSM proxy. By setting up SSH, you can connect your local
development environment such as Visual Studio Code with the HyperPod cluster.

$ chmod +x easy-ssh.sh
$ ./easy-ssh.sh -c <node-group> <cluster-name>
Cluster id: <cluster_id>
Instance id: <instance_id>
Node Group: <node-group>
Add the following to your ~/.ssh/config to easily connect:

$ cat <<EOF >> ~/.ssh/config
Host <cluster-name>
User ubuntu
ProxyCommand sh -c "aws ssm start-session  --target sagemaker-
cluster:<cluster_id>_<node-group>-<instance_id> --document-name AWS-StartSSHSession --
parameters 'portNumber=%p'"
EOF

Add your ssh keypair and then you can do:

$ ssh <cluster-name>

aws ssm start-session --target sagemaker-cluster:<cluster_id>_<node-
group>-<instance_id>

Starting session with SessionId: s0011223344aabbccdd
root@ip-111-22-333-444:/usr/bin#

Note that this initially connects you as the root user. Before running jobs, switch to the ubuntu
user by running the following command.

root@ip-111-22-333-444:/usr/bin# sudo su - ubuntu
ubuntu@ip-111-22-333-444:/usr/bin#

Slurm orchestration
1701

## Page 731

Amazon SageMaker AI
Developer Guide

Set up for easy access with SSH by using the HyperPod compute node as a remote host

To further simplify access to the compute node using SSH from a local machine, the easy-

ssh.sh script outputs a code snippet of setting up the HyperPod cluster as a remote host as
shown in the previous section. The code snippet is auto-generated to help you directly add to the

~/.ssh/config ﬁle on your local device. The following procedure shows how to set up for easy

access using SSH through the SSM proxy, so that you or your cluster users can directly run ssh

<cluster-name> to connect to the HyperPod cluster node.

1. On your local device, add the HyperPod compute node with a user name as a remote host to

the ~/.ssh/config ﬁle. The following command shows how to append the auto-generated

code snippet from the easy-ssh.sh script to the ~/.ssh/config ﬁle. Make sure that you

copy it from the auto-generated output of the easy-ssh.sh script that has the correct cluster
information.

$ cat <<EOF >> ~/.ssh/config
Host <cluster-name>
User ubuntu
ProxyCommand sh -c "aws ssm start-session  --target sagemaker-
cluster:<cluster_id>_<node-group>-<instance_id> --document-name AWS-StartSSHSession
--parameters 'portNumber=%p'"
EOF

2. On the HyperPod cluster node, add the public key on your local device to the ~/.ssh/

authorized_keys ﬁle on the HyperPod cluster node.

a. Print the public key ﬁle on your local machine.

$ cat ~/.ssh/id_rsa.pub

This should return your key. Copy the output of this command.

(Optional) If you don't have a public key, create one by running the following command.

$ ssh-keygen -t rsa -q -f "$HOME/.ssh/id_rsa" -N ""

b. Connect to the cluster node and switch to the user to add the key. The following command is

an example of accessing as the ubuntu user. Replace ubuntu to the user name for which you
want to set up the easy access with SSH.

Slurm orchestration
1702

## Page 732

Amazon SageMaker AI
Developer Guide

$ ./easy-ssh.sh -c <node-group> <cluster-name>
$ sudo su - ubuntu
ubuntu@ip-111-22-333-444:/usr/bin#

c. Open the ~/.ssh/authorized_keys ﬁle and add the public key at the end of the ﬁle.

ubuntu@ip-111-22-333-444:/usr/bin# vim ~/.ssh/authorized_keys

After you ﬁnish setting up, you can connect to the HyperPod cluster node as the user by running a
simpliﬁed SSH command as follows.

$ ssh <cluster-name>
ubuntu@ip-111-22-333-444:/usr/bin#

Also, you can use the host for remote development from an IDE on your local device, such as Visual
Studio Code Remote - SSH.

Set up a multi-user environment through the Amazon FSx shared space

You can use the Amazon FSx shared space to manage a multi-user environment in a Slurm cluster
on SageMaker HyperPod. If you have conﬁgured your Slurm cluster with Amazon FSx during the
HyperPod cluster creation, this is a good option for setting up workspace for your cluster users.
Create a new user and setup the home directory for the user on the Amazon FSx shared ﬁle system.

Tip

To allow users to access your cluster through their user name and dedicated directories, you
should also associate them with IAM roles or users by tagging them as guided in Option 2
of step 5 under the procedure To turn on Run As support for Linux and macOS managed
nodes provided at Turn on Run As support for Linux and macOS managed nodes in the
AWS Systems Manager User Guide. See also the section called “Setting up AWS Systems
Manager and Run As for cluster user access control”.

To set up a multi-user environment while creating a Slurm cluster on SageMaker HyperPod

The SageMaker HyperPod service team provides a script add_users.sh as part of the base
lifecycle script samples.

Slurm orchestration
1703

## Page 733

Amazon SageMaker AI
Developer Guide

1. Prepare a text ﬁle named shared_users.txt that you need to create in the following format.

The ﬁrst column is for user names, the second column is for unique user IDs, and the third
column is for the user directories in the Amazon FSx shared space.

username1,uid1,/fsx/username1
username2,uid2,/fsx/username2
...

2. Make sure that you upload the shared_users.txt and add_users.sh ﬁles to the S3 bucket

for HyperPod lifecycle scripts. While the cluster creation, cluster update, or cluster software

update is in progress, the add_users.sh reads in the shared_users.txt and sets up the user
directories properly.

To create new users and add to an existing Slurm cluster running on SageMaker HyperPod

1. On the head node, run the following command to save a script that helps create a user. Make

sure that you run this with sudo permissions.

$ cat > create-user.sh << EOL
#!/bin/bash

set -x

# Prompt user to get the new user name.
read -p "Enter the new user name, i.e. 'sean':
" USER

# create home directory as /fsx/<user>
# Create the new user on the head node
sudo useradd \$USER -m -d /fsx/\$USER --shell /bin/bash;
user_id=\$(id -u \$USER)

# add user to docker group
sudo usermod -aG docker \${USER}

# setup SSH Keypair
sudo -u \$USER ssh-keygen -t rsa -q -f "/fsx/\$USER/.ssh/id_rsa" -N ""
sudo -u \$USER cat /fsx/\$USER/.ssh/id_rsa.pub | sudo -u \$USER tee /fsx/\$USER/.ssh/
authorized_keys

# add user to compute nodes

Slurm orchestration
1704

## Page 734

Amazon SageMaker AI
Developer Guide

read -p "Number of compute nodes in your cluster, i.e. 8:
" NUM_NODES
srun -N \$NUM_NODES sudo useradd -u \$user_id \$USER -d /fsx/\$USER --shell /bin/
bash;

# add them as a sudoer
read -p "Do you want this user to be a sudoer? (y/N):
" SUDO
if [ "\$SUDO" = "y" ]; then
sudo usermod -aG sudo \$USER
sudo srun -N \$NUM_NODES sudo usermod -aG sudo \$USER
echo -e "If you haven't already you'll need to run:\n\nsudo visudo /
etc/sudoers\n\nChange the line:\n\n%sudo   ALL=(ALL:ALL) ALL\n\nTo\n\n%sudo
ALL=(ALL:ALL) NOPASSWD: ALL\n\nOn each node."
fi
EOL

2. Run the script with the following command. You'll be prompted for adding the name of a user

and the number of compute nodes that you want to allow the user to access.

$ bash create-user.sh

3. Test the user by running the following commands.

$ sudo su - <user> && ssh $(srun hostname)

4. Add the user information to the shared_users.txt ﬁle, so the user will be created on any new

compute nodes or new clusters.

Set up a multi-user environment by integrating HyperPod clusters with Active Directory

In practical use cases, HyperPod clusters are typically used by multiple users: machine learning
(ML) researchers, software engineers, data scientists, and cluster administrators. They edit their
own ﬁles and run their own jobs without impacting each other's work. To set up a multi-user
environment, use the Linux user and group mechanism to statically create multiple users on each
instance through lifecycle scripts. However, the drawback to this approach is that you need to
duplicate user and group settings across multiple instances in the cluster to keep a consistent
conﬁguration across all instances when you make updates such as adding, editing, and removing
users.

Slurm orchestration
1705

## Page 735

Amazon SageMaker AI
Developer Guide

To solve this, you can use Lightweight Directory Access Protocol (LDAP) and LDAP over TLS/SSL
(LDAPS) to integrate with a directory service such as AWS Directory Service for Microsoft Active
Directory. To learn more about setting up Active Directory and a multi-user environment in a
HyperPod cluster, see the blog post Integrate HyperPod clusters with Active Directory for seamless
multi-user login.

Scheduling a Slurm job on a SageMaker HyperPod cluster

You can launch training jobs using the standard Slurm sbatch or srun commands. For example,

to launch an 8-node training job, you can run srun -N 8 --exclusive train.sh SageMaker

HyperPod supports training in a range of environments, including conda, venv, docker, and

enroot. You can conﬁgure an ML environment by running lifecycle scripts on your SageMaker
HyperPod clusters. You also have an option to attach a shared ﬁle system such as Amazon FSx,
which can also be used as a virtual environment.

The following example shows how to run a job for training Llama-2 with the Fully Sharded Data
Parallelism (FSDP) technique on a SageMaker HyperPod cluster with an Amazon FSx shared ﬁle
system. You can also ﬁnd more examples from the Awsome Distributed Training GitHub repository.

Tip

All SageMaker HyperPod examples are available in the 3.test_cases folder of the
Awsome Distributed Training GitHub repository.

1.
Clone the Awsome Distributed Training GitHub repository, and copy the training job examples
to your Amazon FSx ﬁle system.

$ TRAINING_DIR=/fsx/users/my-user/fsdp
$ git clone https://github.com/aws-samples/awsome-distributed-training/

2.
Run the create_conda_env.sh script. This creates a conda environment on your Amazon
FSx ﬁle system. Make sure that the ﬁle system is accessible to all nodes in the cluster.

3.
Build the virtual Conda environment by launching a single node slurm job as follows.

$ srun -N 1 /path_to/create_conda_env.sh

4.
After the environment is built, you can launch a training job by pointing to the environment
path on the shared volume. You can launch both single-node and multi-node training jobs

Slurm orchestration
1706

## Page 736

Amazon SageMaker AI
Developer Guide

with the same setup. To launch a job, create a job launcher script (also called an entry point
script) as follows.

#!/usr/bin/env bash
set -ex

ENV_PATH=/fsx/users/my_user/pytorch_env
TORCHRUN=$ENV_PATH/bin/torchrun
TRAINING_SCRIPT=/fsx/users/my_user/pt_train.py

WORLD_SIZE_JOB=$SLURM_NTASKS
RANK_NODE=$SLURM_NODEID
PROC_PER_NODE=8
MASTER_ADDR=(`scontrol show hostnames \$SLURM_JOB_NODELIST | head -n 1`)
MASTER_PORT=$(expr 10000 + $(echo -n $SLURM_JOBID | tail -c 4))

DIST_ARGS="--nproc_per_node=$PROC_PER_NODE \
--nnodes=$WORLD_SIZE_JOB \
--node_rank=$RANK_NODE \
--master_addr=$MASTER_ADDR \
--master_port=$MASTER_PORT \
"
$TORCHRUN $DIST_ARGS $TRAINING_SCRIPT

Tip

If you want to make your training job more resilient against hardware failures by using
the auto-resume capability of SageMaker HyperPod, you need to properly set up the

environment variable MASTER_ADDR in the entrypoint script. To learn more, see the
section called “Automatic node recovery and auto-resume”.

This tutorial assumes that this script is saved as /fsx/users/my_user/train.sh.

5.
With this script in the shared volume at /fsx/users/my_user/train.sh, run the following

srun command to schedule the Slurm job.

$ cd /fsx/users/my_user/
$ srun -N 8 train.sh

Slurm orchestration
1707

## Page 737

Amazon SageMaker AI
Developer Guide

Running Docker containers on a Slurm compute node on HyperPod

To run Docker containers with Slurm on SageMaker HyperPod, you need to use Enroot and Pyxis.
The Enroot package helps convert Docker images into a runtime that Slurm can understand, while

the Pyxis enables scheduling the runtime as a Slurm job through an srun command, srun --

container-image=docker/image:tag.

Tip

The Docker, Enroot, and Pyxis packages should be installed during cluster creation as
part of running the lifecycle scripts as guided in the section called “Base lifecycle scripts”.
Use the base lifecycle scripts provided by the HyperPod service team when creating a
HyperPod cluster. Those base scripts are set up to install the packages by default. In the

config.py script, there's the Config class with the boolean type parameter for installing

the packages set to True (enable_docker_enroot_pyxis=True). This is called by

and parsed in the lifecycle_script.py script, which calls install_docker.sh and

install_enroot_pyxis.sh scripts from the utils folder. The installation scripts are
where the actual installations of the packages take place. Additionally, the installation
scripts identify if they can detect NVMe store paths from the instances they are run on

and set up the root paths for Docker and Enroot to /opt/dlami/nvme. The default root

volume of any fresh instance is mounted to /tmp only with a 100GB EBS volume, which
runs out if the workload you plan to run involves training of LLMs and thus large size
Docker containers. If you use instance families such as P and G with local NVMe storage,

you need to make sure that you use the NVMe storage attached at /opt/dlami/nvme, and
the installation scripts take care of the conﬁguration processes.

To check if the root paths are set up properly

On a compute node of your Slurm cluster on SageMaker HyperPod, run the following commands
to make sure that the lifecycle script worked properly and the root volume of each node is set to

/opt/dlami/nvme/*. The following commands shows examples of checking the Enroot runtime
path and the data root path for 8 compute nodes of a Slurm cluster.

$ srun -N 8 cat /etc/enroot/enroot.conf | grep "ENROOT_RUNTIME_PATH"
ENROOT_RUNTIME_PATH        /opt/dlami/nvme/tmp/enroot/user-$(id -u)
... // The same or similar lines repeat 7 times

Slurm orchestration
1708

## Page 738

Amazon SageMaker AI
Developer Guide

$ srun -N 8 cat /etc/docker/daemon.json
{
"data-root": "/opt/dlami/nvme/docker/data-root"
}
... // The same or similar lines repeat 7 times

After you conﬁrm that the runtime paths are properly set to /opt/dlami/nvme/*, you're ready to
build and run Docker containers with Enroot and Pyxis.

To test Docker with Slurm

1. On your compute node, try the following commands to check if Docker and Enroot are properly

installed.

$ docker --help
$ enroot --help

2. Test if Pyxis and Enroot installed correctly by running one of the NVIDIA CUDA Ubuntu images.

$ srun --container-image=nvidia/cuda:XX.Y.Z-base-ubuntuXX.YY nvidia-smi
pyxis: importing docker image: nvidia/cuda:XX.Y.Z-base-ubuntuXX.YY
pyxis: imported docker image: nvidia/cuda:XX.Y.Z-base-ubuntuXX.YY
DAY MMM DD HH:MM:SS YYYY
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.141.03   Driver Version: 470.141.03   CUDA Version: XX.YY    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla T4            Off  | 00000000:00:1E.0 Off |                    0 |
| N/A   40C    P0    27W /  70W |      0MiB / 15109MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

Slurm orchestration
1709

## Page 739

Amazon SageMaker AI
Developer Guide

You can also test it by creating a script and running an sbatch command as follows.

$ cat <<EOF >> container-test.sh
#!/bin/bash
#SBATCH --container-image=nvidia/cuda:XX.Y.Z-base-ubuntuXX.YY
nvidia-smi
EOF

$ sbatch container-test.sh
pyxis: importing docker image: nvidia/cuda:XX.Y.Z-base-ubuntuXX.YY
pyxis: imported docker image: nvidia/cuda:XX.Y.Z-base-ubuntuXX.YY
DAY MMM DD HH:MM:SS YYYY
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.141.03   Driver Version: 470.141.03   CUDA Version: XX.YY    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla T4            Off  | 00000000:00:1E.0 Off |                    0 |
| N/A   40C    P0    27W /  70W |      0MiB / 15109MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

To run a test Slurm job with Docker

After you have completed setting up Slurm with Docker, you can bring any pre-built Docker images
and run using Slurm on SageMaker HyperPod. The following is a sample use case that walks you
through how to run a training job using Docker and Slurm on SageMaker HyperPod. It shows
an example job of model-parallel training of the Llama 2 model with the SageMaker AI model
parallelism (SMP) library.

Slurm orchestration
1710

## Page 740

Amazon SageMaker AI
Developer Guide

1. If you want to use one of the pre-built ECR images distributed by SageMaker AI or DLC, make

sure that you give your HyperPod cluster the permissions to pull ECR images through the the
section called “IAM role for SageMaker HyperPod”. If you use your own or an open source Docker
image, you can skip this step. Add the following permissions to the the section called “IAM role
for SageMaker HyperPod”. In this tutorial, we use the SMP Docker image pre-packaged with the
SMP library .

JSON

{
"Version":"2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Action": [

"ecr:BatchCheckLayerAvailability",
"ecr:BatchGetImage",
"ecr-public:*",
"ecr:GetDownloadUrlForLayer",
"ecr:GetAuthorizationToken",
"sts:*"
],
"Resource": "*"
}
]
}

2. On the compute node, clone the repository and go to the folder that provides the example

scripts of training with SMP.

$ git clone https://github.com/aws-samples/awsome-distributed-training/
$ cd awsome-distributed-training/3.test_cases/17.SM-modelparallelv2

3. In this tutorial, run the sample script docker_build.sh that pulls the SMP Docker image, build

the Docker container, and runs it as an Enroot runtime. You can modify this as you want.

$ cat docker_build.sh
#!/usr/bin/env bash

region=us-west-2
dlc_account_id=658645717510

Slurm orchestration
1711

## Page 741

Amazon SageMaker AI
Developer Guide

aws ecr get-login-password --region $region | docker login --username AWS --password-
stdin $dlc_account_id.dkr.ecr.$region.amazonaws.com

docker build -t smpv2 .
enroot import -o smpv2.sqsh  dockerd://smpv2:latest

$ bash docker_build.sh

4. Create a batch script to launch a training job using sbatch. In this tutorial, the provided

sample script launch_training_enroot.sh launches a model-parallel training job of the
70-billion-parameter Llama 2 model with a synthetic dataset on 8 compute nodes. A set of

training scripts are provided at 3.test_cases/17.SM-modelparallelv2/scripts, and

launch_training_enroot.sh takes train_external.py as the entrypoint script.

Important

To use the a Docker container on SageMaker HyperPod, you must mount the /var/

log directory from the host machine, which is the HyperPod compute node in this case,

onto the /var/log directory in the container. You can set it up by adding the following
variable for Enroot.

"${HYPERPOD_PATH:="/var/log/aws/clusters":"/var/log/aws/clusters"}"

$ cat launch_training_enroot.sh
#!/bin/bash

# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: MIT-0

#SBATCH --nodes=8 # number of nodes to use, 2 p4d(e) = 16 A100 GPUs
#SBATCH --job-name=smpv2_llama # name of your job
#SBATCH --exclusive # job has exclusive use of the resource, no sharing
#SBATCH --wait-all-nodes=1

set -ex;

###########################
###### User Variables #####

Slurm orchestration
1712

## Page 742

Amazon SageMaker AI
Developer Guide

###########################

#########################
model_type=llama_v2
model_size=70b

# Toggle this to use synthetic data
use_synthetic_data=1

# To run training on your own data  set Training/Test Data path  -> Change this to
the tokenized dataset path in Fsx. Acceptable formats are huggingface (arrow) and
Jsonlines.
# Also change the use_synthetic_data to 0

export TRAINING_DIR=/fsx/path_to_data
export TEST_DIR=/fsx/path_to_data

export CHECKPOINT_DIR=$(pwd)/checkpoints

# Variables for Enroot
: "${IMAGE:=$(pwd)/smpv2.sqsh}"
: "${HYPERPOD_PATH:="/var/log/aws/clusters":"/var/log/aws/clusters"}" # This is
needed for validating its hyperpod cluster
: "${TRAIN_DATA_PATH:=$TRAINING_DIR:$TRAINING_DIR}"
: "${TEST_DATA_PATH:=$TEST_DIR:$TEST_DIR}"
: "${CHECKPOINT_PATH:=$CHECKPOINT_DIR:$CHECKPOINT_DIR}"

###########################
## Environment Variables ##
###########################

#export NCCL_SOCKET_IFNAME=en
export NCCL_ASYNC_ERROR_HANDLING=1

export NCCL_PROTO="simple"
export NCCL_SOCKET_IFNAME="^lo,docker"
export RDMAV_FORK_SAFE=1
export FI_EFA_USE_DEVICE_RDMA=1
export NCCL_DEBUG_SUBSYS=off
export NCCL_DEBUG="INFO"
export SM_NUM_GPUS=8
export GPU_NUM_DEVICES=8
export FI_EFA_SET_CUDA_SYNC_MEMOPS=0

Slurm orchestration
1713

## Page 743

Amazon SageMaker AI
Developer Guide

# async runtime error ...
export CUDA_DEVICE_MAX_CONNECTIONS=1

#########################
## Command and Options ##
#########################

if [ "$model_size" == "7b" ]; then
HIDDEN_WIDTH=4096
NUM_LAYERS=32
NUM_HEADS=32
LLAMA_INTERMEDIATE_SIZE=11008
DEFAULT_SHARD_DEGREE=8
# More Llama model size options
elif [ "$model_size" == "70b" ]; then

HIDDEN_WIDTH=8192
NUM_LAYERS=80
NUM_HEADS=64
LLAMA_INTERMEDIATE_SIZE=28672
# Reduce for better perf on p4de
DEFAULT_SHARD_DEGREE=64
fi

if [ -z "$shard_degree" ]; then
SHARD_DEGREE=$DEFAULT_SHARD_DEGREE
else
SHARD_DEGREE=$shard_degree
fi

if [ -z "$LLAMA_INTERMEDIATE_SIZE" ]; then
LLAMA_ARGS=""
else
LLAMA_ARGS="--llama_intermediate_size $LLAMA_INTERMEDIATE_SIZE "
fi

if [ $use_synthetic_data == 1 ]; then
echo "using synthetic data"
declare -a ARGS=(
--container-image $IMAGE
--container-mounts $HYPERPOD_PATH,$CHECKPOINT_PATH

Slurm orchestration
1714

## Page 744

Amazon SageMaker AI
Developer Guide

)
else
echo "using real data...."
declare -a ARGS=(
--container-image $IMAGE
--container-mounts $HYPERPOD_PATH,$TRAIN_DATA_PATH,$TEST_DATA_PATH,
$CHECKPOINT_PATH
)
fi

declare -a TORCHRUN_ARGS=(
# change this to match the number of gpus per node:
--nproc_per_node=8 \
--nnodes=$SLURM_JOB_NUM_NODES \
--rdzv_id=$SLURM_JOB_ID \
--rdzv_backend=c10d \

--rdzv_endpoint=$(hostname) \
)

srun -l "${ARGS[@]}" torchrun "${TORCHRUN_ARGS[@]}" /path_to/train_external.py \
--train_batch_size 4 \
--max_steps 100 \
--hidden_width $HIDDEN_WIDTH \
--num_layers $NUM_LAYERS \
--num_heads $NUM_HEADS \
${LLAMA_ARGS} \
--shard_degree $SHARD_DEGREE \
--model_type $model_type \
--profile_nsys 1 \
--use_smp_implementation 1 \
--max_context_width 4096 \
--tensor_parallel_degree 1 \
--use_synthetic_data $use_synthetic_data \
--training_dir $TRAINING_DIR \
--test_dir $TEST_DIR \
--dataset_type hf \
--checkpoint_dir $CHECKPOINT_DIR \
--checkpoint_freq 100 \

$ sbatch launch_training_enroot.sh

Slurm orchestration
1715

## Page 745

Amazon SageMaker AI
Developer Guide

To ﬁnd the downloadable code examples, see Run a model-parallel training job using the
SageMaker AI model parallelism library, Docker and Enroot with Slurm in the Awsome Distributed
Training GitHub repository. For more information about distributed training with a Slurm cluster on
SageMaker HyperPod, proceed to the next topic at the section called “Running distributed training
workloads”.

Running distributed training workloads with Slurm on HyperPod

SageMaker HyperPod is specialized for workloads of training large language models (LLMs) and
foundation models (FMs). These workloads often require the use of multiple parallelism techniques
and optimized operations for ML infrastructure and resources. Using SageMaker HyperPod, you can
use the following SageMaker AI distributed training frameworks:

• The SageMaker AI distributed data parallelism (SMDDP) library that oﬀers collective
communication operations optimized for AWS.

• The SageMaker AI model parallelism (SMP) library that implements various model parallelism
techniques.

Topics

• Using SMDDP on a SageMaker HyperPod

• Using SMP on a SageMaker HyperPod cluster

Using SMDDP on a SageMaker HyperPod

The SMDDP library is a collective communication library that improves compute performance
of distributed data parallel training. The SMDDP library works with the following open source
distributed training frameworks:

• PyTorch distributed data parallel (DDP)

• PyTorch fully sharded data parallelism (FSDP)

• DeepSpeed

• Megatron-DeepSpeed

The SMDDP library addresses communications overhead of the key collective communication
operations by oﬀering the following for SageMaker HyperPod.

Slurm orchestration
1716

## Page 746

Amazon SageMaker AI
Developer Guide

• The library oﬀers AllGather optimized for AWS. AllGather is a key operation used in sharded
data parallel training, which is a memory-eﬃcient data parallelism technique oﬀered by popular
libraries. These include the SageMaker AI model parallelism (SMP) library, DeepSpeed Zero
Redundancy Optimizer (ZeRO), and PyTorch Fully Sharded Data Parallelism (FSDP).

• The library performs optimized node-to-node communication by fully utilizing the AWS network
infrastructure and the SageMaker AI ML instance topology.

To run sample data-parallel training jobs

Explore the following distributed training samples implementing data parallelism techniques using
the SMDDP library.

• awsome-distributed-training/3.test_cases/12.SM-dataparallel-FSDP

• awsome-distributed-training/3.test_cases/13.SM-dataparallel-deepspeed

To set up an environment for using the SMDDP library on SageMaker HyperPod

The following are training environment requirements for using the SMDDP library on SageMaker
HyperPod.

• PyTorch v2.0.1 and later

• CUDA v11.8 and later

• libstdc++ runtime version greater than 3

• Python v3.10.x and later

• ml.p4d.24xlarge and ml.p4de.24xlarge, which are supported instance types by the
SMDDP library

• imdsv2 enabled on training host

Depending on how you want to run the distributed training job, there are two options to install the
SMDDP library:

• A direct installation using the SMDDP binary ﬁle.

• Using the SageMaker AI Deep Learning Containers (DLCs) pre-installed with the SMDDP library.

Slurm orchestration
1717

## Page 747

Amazon SageMaker AI
Developer Guide

Docker images pre-installed with the SMDDP library or the URLs to the SMDDP binary ﬁles are
listed at Supported Frameworks in the SMDDP library documentation.

To install the SMDDP library on the SageMaker HyperPod DLAMI

• pip install --no-cache-dir https://smdataparallel.s3.amazonaws.com/

binary/pytorch/<pytorch-version>/cuXYZ/YYYY-MM-DD/

smdistributed_dataparallel-X.Y.Z-cp310-cp310-linux_x86_64.whl

Note

If you work in a Conda environment, ensure that you install PyTorch using conda

install instead of pip.

conda install pytorch==X.Y.Z  torchvision==X.Y.Z torchaudio==X.Y.Z pytorch-
cuda=X.Y.Z -c pytorch -c nvidia

To use the SMDDP library on a Docker container

• The SMDDP library is pre-installed on the SageMaker AI Deep Learning Containers (DLCs). To ﬁnd
the list of SageMaker AI framework DLCs for PyTorch with the SMDDP library, see Supported
Frameworks in the SMDDP library documentation. You can also bring your own Docker container
with required dependencies installed to use the SMDDP library. To learn more about setting up a
custom Docker container to use the SMDDP library, see also the section called “Create your own
docker container with the library”.

Important

To use the SMDDP library in a Docker container, mount the /var/log directory from the

host machine onto /var/log in the container. This can be done by adding the following
option when running your container.

docker run <OTHER_OPTIONS> -v /var/log:/var/log ...

To learn how to run data-parallel training jobs with SMDDP in general, see the section called
“Distributed training with the SMDDP library”.

Slurm orchestration
1718

## Page 748

Amazon SageMaker AI
Developer Guide

Using SMP on a SageMaker HyperPod cluster

The SageMaker AI model parallelism (SMP) library oﬀers various state-of-the-art model parallelism
techniques, including:

• fully sharded data parallelism

• expert parallelism

• mixed precision training with FP16/BF16 and FP8 data types

• tensor parallelism

The SMP library is also compatible with open source frameworks such as PyTorch FSDP, NVIDIA
Megatron, and NVIDIA Transformer Engine.

To run a sample model-parallel training workload

The SageMaker AI service teams provide sample training jobs implementing model parallelism

with the SMP library at awsome-distributed-training/3.test_cases/17.SM-

modelparallelv2.

SageMaker HyperPod cluster resources monitoring

To achieve comprehensive observability into your SageMaker HyperPod cluster resources and
software components, integrate the cluster with Amazon Managed Service for Prometheus
and Amazon Managed Grafana. The integration with Amazon Managed Service for Prometheus
enables the export of metrics related to your HyperPod cluster resources, providing insights
into their performance, utilization, and health. The integration with Amazon Managed Grafana
enables the visualization of these metrics through various Grafana dashboards that oﬀer intuitive
interface for monitoring and analyzing the cluster's behavior. By leveraging these services, you
gain a centralized and uniﬁed view of your HyperPod cluster, facilitating proactive monitoring,
troubleshooting, and optimization of your distributed training workloads.

Tip

To ﬁnd practical examples and solutions, see also the SageMaker HyperPod workshop.

Slurm orchestration
1719

## Page 749

Amazon SageMaker AI
Developer Guide

![Page 749 Diagram 1](images/page-0749-img-01.png)

Figure: This architecture diagram shows an overview of conﬁguring SageMaker HyperPod with
Amazon Managed Service for Prometheus and Amazon Managed Grafana.

Proceed to the following topics to set up for SageMaker HyperPod cluster observability.

Topics

• Prerequisites for SageMaker HyperPod cluster observability

• Installing metrics exporter packages on your HyperPod cluster

• Validating Prometheus setup on the head node of a HyperPod cluster

• Setting up an Amazon Managed Grafana workspace

• Exported metrics reference

• Amazon SageMaker HyperPod Slurm metrics

Prerequisites for SageMaker HyperPod cluster observability

Before proceeding with the steps to the section called “Installing metrics exporter packages”,
ensure that the following prerequisites are met.

Slurm orchestration
1720

## Page 750

Amazon SageMaker AI
Developer Guide

Enable IAM Identity Center

To enable observability for your SageMaker HyperPod cluster, you must ﬁrst enable IAM Identity
Center. This is a prerequisite for deploying an CloudFormation stack that sets up the Amazon
Managed Grafana workspace and Amazon Managed Service for Prometheus. Both of these services

also require the IAM Identity Center for authentication and authorization, ensuring secure user
access and management of the monitoring infrastructure.

For detailed guidance on enabling IAM Identity Center, see the Enabling IAM Identity Center
section in the AWS IAM Identity Center User Guide.

After successfully enabling IAM Identity Center, set up a user account that will serve as the
administrative user throughout the following conﬁguration precedures.

Create and deploy an CloudFormation stack for SageMaker HyperPod observability

Create and deploy a CloudFormation stack for SageMaker HyperPod observability to monitor
HyperPod cluster metrics in real time using Amazon Managed Service for Prometheus and Amazon
Managed Grafana. To deploy the stack, note that you also should enable your IAM Identity Center
beforehand.

Use the sample CloudFormation script cluster-observability.yaml that helps you set up
Amazon VPC subnets, Amazon FSx for Lustre ﬁle systems, Amazon S3 buckets, and IAM roles
required to create a HyperPod cluster observability stack.

Installing metrics exporter packages on your HyperPod cluster

In the base conﬁguration lifecycle scripts that the SageMaker HyperPod team provides also
includes installation of various metric exporter packages. To activate the installation step, the only

thing you need to do is to set the parameter enable_observability=True in the config.py
ﬁle. The lifecycle scripts are designed to bootstrap your cluster with the following open-source
metric exporter packages.

Name
Script deployment target
node

Exporter description

Slurm exporter for
Prometheus

Head (controller) node
Exports Slurm Accounting
metrics.

Slurm orchestration
1721

## Page 751

Amazon SageMaker AI
Developer Guide

Elastic Fabric Adapter (EFA)
node exporter

Compute node
Exports metrics from cluster
nodes and EFA. The package
is a fork of the Prometheus
node exporter.

NVIDIA Data Center GPU
Management (DCGM)
exporter

Compute node
Exports NVIDIA DCGM metrics
about health and performan
ce of NVIDIA GPUs.

With enable_observability=True in the config.py ﬁle, the following installation step is

activated in the lifecycle_script.py script.

# Install metric exporting software and Prometheus for observability
if Config.enable_observability:
if node_type == SlurmNodeType.COMPUTE_NODE:
ExecuteBashScript("./utils/install_docker.sh").run()
ExecuteBashScript("./utils/install_dcgm_exporter.sh").run()
ExecuteBashScript("./utils/install_efa_node_exporter.sh").run()

if node_type == SlurmNodeType.HEAD_NODE:
wait_for_scontrol()
ExecuteBashScript("./utils/install_docker.sh").run()
ExecuteBashScript("./utils/install_slurm_exporter.sh").run()
ExecuteBashScript("./utils/install_prometheus.sh").run()

On the compute nodes, the script installs the NVIDIA Data Center GPU Management (DCGM)
exporter and the Elastic Fabric Adapter (EFA) node exporter. The DCGM exporter is an exporter
for Prometheus that collects metrics from NVIDIA GPUs, enabling monitoring of GPU usage,
performance, and health. The EFA node exporter, on the other hand, gathers metrics related to the
EFA network interface, which is essential for low-latency and high-bandwidth communication in
HPC clusters.

On the head node, the script installs the Slurm exporter for Prometheus and the Prometheus open-
source software. The Slurm exporter provides Prometheus with metrics related to Slurm jobs,
partitions, and node states.

Note that the lifecycle scripts are designed to install all the exporter packages as docker containers,
so the Docker package also should be installed on both the head and compute nodes. The scripts

Slurm orchestration
1722

## Page 752

Amazon SageMaker AI
Developer Guide

for these components are conveniently provided in the utils folder of the Awsome Distributed
Training GitHub repository.

After you have successfully set up your HyperPod cluster installed with the exporter packages,
proceed to the next topic to ﬁnish setting up Amazon Managed Service for Prometheus and
Amazon Managed Grafana.

Validating Prometheus setup on the head node of a HyperPod cluster

After you have successfully set up your HyperPod cluster installed with the exporter packages,
check if Prometheus is properly set up on the head node of your HyperPod cluster.

1. Connect to the head node of your cluster. For instructions on accessing a node, see the section

called “Accessing cluster nodes”.

2. Run the following command to verify the Prometheus conﬁg and service ﬁle created by the

lifecycle script install_prometheus.sh is running on the controller node. The output should

show the Active status as active (running).

$ sudo systemctl status prometheus
• prometheus service - Prometheus Exporter
Loaded: loaded (/etc/systemd/system/prometheus.service; enabled; preset:disabled)
Active: active (running) since DAY YYYY-MM-DD HH:MM:SS UTC; Ss ago
Main PID: 12345 (prometheus)
Tasks: 7 (limit: 9281)
Memory: 35M
CPU: 234ms
CGroup: /system.slice/prometheus.service
-12345 /usr/bin/prometheus--config.file=/etc/prometheus/prometheus.yml

3. Validate the Prometheus conﬁguration ﬁle as follows. The output must be similar to the

following, with three exporter conﬁgured with the right compute node IP addresses.

$ cat /etc/prometheus/prometheus.yml
global:
scrape_interval: 15s
evaluation_interval: 15s
scrape_timeout: 15s

scrape_configs:
- job_name: 'slurm_exporter'
static_configs:

Slurm orchestration
1723

## Page 753

Amazon SageMaker AI
Developer Guide

- targets:
- 'localhost:8080'
- job_name: 'dcgm_exporter'
static_configs:
- targets:
- '<ComputeNodeIP>:9400'
- '<ComputeNodeIP>:9400'
- job_name: 'efa_node_exporter'
static_configs:
- targets:
- '<ComputeNodeIP>:9100'
- '<ComputeNodeIP>:9100'

remote_write:
- url: <AMPReoteWriteURL>
queue_config:
max_samples_per_send: 1000

max_shards: 200
capacity: 2500
sigv4:
region: <Region>

4. To test if Prometheus is exporting Slurm, DCGM, and EFA metrics properly, run the following

curl command for Prometheus on port :9090 on the head node.

$ curl -s http://localhost:9090/metrics | grep -E 'slurm|dcgm|efa'

With the metrics exported to Amazon Managed Service for Prometheus Workspace through the
Prometheus remote write conﬁguration from the controller node, you can proceed to the next
topic to set up Amazon Managed Grafana dashboards to display the metrics.

Setting up an Amazon Managed Grafana workspace

Create a new Amazon Managed Grafana workspace or update an existing Amazon Managed
Grafana workspace with Amazon Managed Service for Prometheus as the data source.

Topics

• Create a Grafana workspace and set Amazon Managed Service for Prometheus as a data source

• Open the Grafana workspace and ﬁnish setting up the data source

• Import open-source Grafana dashboards

Slurm orchestration
1724

## Page 754

Amazon SageMaker AI
Developer Guide

Create a Grafana workspace and set Amazon Managed Service for Prometheus as a data source

To visualize metrics from Amazon Managed Service for Prometheus, create an Amazon Managed
Grafana workspace and set it up to use Amazon Managed Service for Prometheus as a data source.

1. To create a Grafana workspace, follow the instructions at Creating a workspace in the Amazon

Managed Service for Prometheus User Guide.

a. In Step 13, select Amazon Managed Service for Prometheus as the data source.

b. In Step 17, you can add the admin user and also other users in your IAM Identity Center.

For more information, see also the following resources.

• Set up Amazon Managed Grafana for use with Amazon Managed Service for Prometheus in the
Amazon Managed Service for Prometheus User Guide

• Use AWS data source conﬁguration to add Amazon Managed Service for Prometheus as a data
source in the Amazon Managed Grafana User Guide

Open the Grafana workspace and ﬁnish setting up the data source

After you have successfully created or updated an Amazon Managed Grafana workspace, select the
workspace URL to open the workspace. This prompts you to enter a user name and the password
of the user that you have set up in IAM Identity Center. You should log in using the admin user to
ﬁnish setting up the workspace.

1. In the workspace Home page, choose Apps, AWS Data Sources, and Data sources.

2. In the Data sources page, and choose the Data sources tab.

3. For Service, choose Amazon Managed Service for Prometheus.

4. In the Browse and provision data sources section, choose the AWS region where you

provisioned an Amazon Managed Service for Prometheus workspace.

5. From the list of data sources in the selected Region, choose the one for Amazon Managed

Service for Prometheus. Make sure that you check the resource ID and the resource alias of
the Amazon Managed Service for Prometheus workspace that you have set up for HyperPod
observability stack.

Slurm orchestration
1725

## Page 755

Amazon SageMaker AI
Developer Guide

Import open-source Grafana dashboards

After you've successfully set up your Amazon Managed Grafana workspace with Amazon Managed
Service for Prometheus as the data source, you'll start collecting metrics to Prometheus, and
then should start seeing the various dashboards showing charts, information, and more. The
Grafana open source software provides various dashboards, and you can import them into Amazon
Managed Grafana.

To import open-source Grafana dashboards to Amazon Managed Grafana

1. In the Home page of your Amazon Managed Grafana workspace, choose Dashboards.

2. Choose the drop down menu button with the UI text New, and select Import.

3. Paste the URL to the Slurm Dashboard.

https://grafana.com/grafana/dashboards/4323-slurm-dashboard/

4. Select Load.

5. Repeat the previous steps to import the following dashboards.

a. Node Exporter Full Dashboard

https://grafana.com/grafana/dashboards/1860-node-exporter-full/

b. NVIDIA DCGM Exporter Dashboard

https://grafana.com/grafana/dashboards/12239-nvidia-dcgm-exporter-dashboard/

c. EFA Metrics Dashboard

https://grafana.com/grafana/dashboards/20579-efa-metrics-dev/

d. FSx for Lustre Metrics Dashboard

https://grafana.com/grafana/dashboards/20906-fsx-lustre/

Exported metrics reference

The following sections present comprehensive lists of metrics exported from SageMaker
HyperPod to Amazon Managed Service for Prometheus upon the successful conﬁguration of the

Slurm orchestration
1726

## Page 756

Amazon SageMaker AI
Developer Guide

CloudFormation stack for SageMaker HyperPod observability. You can start monitoring these
metrics visualized in the Amazon Managed Grafana dashboards.

Slurm exporter dashboard

Provides visualized information of Slurm clusters on SageMaker HyperPod.

Types of metrics

• Cluster Overview: Displaying the total number of nodes, jobs, and their states.

• Job Metrics: Visualizing job counts and states over time.

• Node Metrics: Showing node states, allocation, and available resources.

• Partition Metrics: Monitoring partition-speciﬁc metrics such as CPU, memory, and GPU
utilization.

• Job Eﬃciency: Calculating job eﬃciency based on resources utilized.

List of metrics

Metric name
Description

slurm_job_count
Total number of jobs in the Slurm cluster

slurm_job_state_count
Count of jobs in each state (e.g., running,
pending, completed)

slurm_node_count
Total number of nodes in the Slurm cluster

slurm_node_state_count
Count of nodes in each state (e.g., idle, alloc,
mix)

slurm_partition_node_count
Count of nodes in each partition

slurm_partition_job_count
Count of jobs in each partition

slurm_partition_alloc_cpus
Total number of allocated CPUs in each
partition

slurm_partition_free_cpus
Total number of available CPUs in each
partition

Slurm orchestration
1727

## Page 757

Amazon SageMaker AI
Developer Guide

Metric name
Description

slurm_partition_alloc_memory
Total allocated memory in each partition

slurm_partition_free_memory
Total available memory in each partition

slurm_partition_alloc_gpus
Total allocated GPUs in each partition

slurm_partition_free_gpus
Total available GPUs in each partition

Node exporter dashboard

Provides visualized information of system metrics collected by the Prometheus node exporter from
the HyperPod cluster nodes.

Types of metrics

• System overview: Displaying CPU load averages and memory usage.

• Memory metrics: Visualizing memory utilization including total memory, free memory, and swap
space.

• Disk usage: Monitoring disk space utilization and availability.

• Network traﬃc: Showing network bytes received and transmitted over time.

• File system metrics: Analyzing ﬁle system usage and availability.

• Disk I/O metrics: Visualizing disk read and write activity.

List of metrics

For a complete list of metrics exported, see the Node exporter  and procfs GitHub repositories. The
following table shows a subset of the metrics that provides insights into system resource utilization
such as CPU load, memory usage, disk space, and network activity.

Metric name
Description

node_load1
1-minute load average

node_load5
5-minute load average

Slurm orchestration
1728

## Page 758

Amazon SageMaker AI
Developer Guide

Metric name
Description

node_load15
15-minute load average

node_memory_MemTotal
Total system memory

node_memory_MemFree
Free system memory

node_memory_MemAvailable
Available memory for allocation to processes

node_memory_Buffers
Memory used by the kernel for buﬀering

node_memory_Cached
Memory used by the kernel for caching ﬁle
system data

node_memory_SwapTotal
Total swap space available

node_memory_SwapFree
Free swap space

node_memory_SwapCached
Memory that once was swapped out, is
swapped back in but still in swap

node_filesystem_avail_bytes
Available disk space in bytes

node_filesystem_size_bytes
Total disk space in bytes

node_filesystem_free_bytes
Free disk space in bytes

node_network_receive_bytes
Network bytes received

node_network_transmit_bytes
Network bytes transmitted

node_disk_read_bytes
Disk bytes read

node_disk_written_bytes
Disk bytes written

NVIDIA DCGM exporter dashboard

Provides visualized information of NVIDIA GPU metrics collected by the NVIDIA DCGM exporter.

Types of metrics

Slurm orchestration
1729

## Page 759

Amazon SageMaker AI
Developer Guide

• GPU Overview: Displaying GPU utilization, temperatures, power usage, and memory usage.

• Temperature Metrics: Visualizing GPU temperatures over time.

• Power Usage: Monitoring GPU power draw and power usage trends.

• Memory Utilization: Analyzing GPU memory usage including used, free, and total memory.

• Fan Speed: Showing GPU fan speeds and variations.

• ECC Errors: Tracking GPU memory ECC errors and pending errors.

List of metrics

The following table shows a list of the metrics that provides insights into the NVIDIA GPU health
and performance, including clock frequencies, temperatures, power usage, memory utilization, fan
speeds, and error metrics.

Metric name
Description

DCGM_FI_DEV_SM_CLOCK
SM clock frequency (in MHz)

DCGM_FI_DEV_MEM_CLOCK
Memory clock frequency (in MHz)

DCGM_FI_DEV_MEMORY_TEMP
Memory temperature (in C)

DCGM_FI_DEV_GPU_TEMP
GPU temperature (in C)

DCGM_FI_DEV_POWER_USAGE
Power draw (in W)

Total energy consumption since boot (in mJ)

DCGM_FI_DEV_TOTAL_ENERGY_CO

NSUMPTION

DCGM_FI_DEV_PCIE_REPLAY_COUNTER
Total number of PCIe retries

DCGM_FI_DEV_MEM_COPY_UTIL
Memory utilization (in %)

DCGM_FI_DEV_ENC_UTIL
Encoder utilization (in %)

DCGM_FI_DEV_DEC_UTIL
Decoder utilization (in %)

DCGM_FI_DEV_XID_ERRORS
Value of the last XID error encountered

DCGM_FI_DEV_FB_FREE
Frame buﬀer memory free (in MiB)

Slurm orchestration
1730

## Page 760

Amazon SageMaker AI
Developer Guide

Metric name
Description

DCGM_FI_DEV_FB_USED
Frame buﬀer memory used (in MiB)

Total number of NVLink bandwidth counters

DCGM_FI_DEV_NVLINK_BANDWIDT

for all lanes

H_TOTAL

DCGM_FI_DEV_VGPU_LICENSE_STATUS
vGPU License status

Number of remapped rows for uncorrectable
errors

DCGM_FI_DEV_UNCORRECTABLE_R

EMAPPED_ROWS

Number of remapped rows for correctable
errors

DCGM_FI_DEV_CORRECTABLE_REM

APPED_ROWS

DCGM_FI_DEV_ROW_REMAP_FAILURE
Whether remapping of rows has failed

EFA metrics dashboard

Provides visualized information of the metrics from Amazon Elastic Fabric Adapter (EFA) equipped
on P instances collected by the EFA node exporter.

Types of metrics

• EFA error metrics: Visualizing errors such as allocation errors, command errors, and memory
map errors.

• EFA network traﬃc: Monitoring received and transmitted bytes, packets, and work requests.

• EFA RDMA performance: Analyzing RDMA read and write operations, including bytes transferred
and error rates.

• EFA port lifespan: Displaying the lifespan of EFA ports over time.

• EFA keep-alive packets: Tracking the number of keep-alive packets received.

List of metrics

The following table shows a list of the metrics that provides insights into various aspects of EFA
operation, including errors, completed commands, network traﬃc, and resource utilization.

Slurm orchestration
1731

## Page 761

Amazon SageMaker AI
Developer Guide

Metric name
Description

node_amazonefa_info
Non-numeric data from /sys/class/inﬁnib
and/, value is always 1.

node_amazonefa_lifespan
Lifespan of the port

node_amazonefa_rdma_read_bytes
Number of bytes read with RDMA

Number of read response bytes with RDMA

node_amazonefa_rdma_read_re

sp_bytes

node_amazonefa_rdma_read_wr_err
Number of read write errors with RDMA

node_amazonefa_rdma_read_wrs
Number of read rs with RDMA

node_amazonefa_rdma_write_bytes
Number of bytes written with RDMA

Number of bytes written and received with
RDMA

node_amazonefa_rdma_write_r

ecv_bytes

node_amazonefa_rdma_write_wr_err
Number of bytes written with error RDMA

node_amazonefa_rdma_write_wrs
Number of bytes written wrs RDMA

node_amazonefa_recv_bytes
Number of bytes received

node_amazonefa_recv_wrs
Number of bytes received wrs

node_amazonefa_rx_bytes
Number of bytes received

node_amazonefa_rx_drops
Number of packets dropped

node_amazonefa_rx_pkts
Number of packets received

node_amazonefa_send_bytes
Number of bytes sent

node_amazonefa_send_wrs
Number of wrs sent

node_amazonefa_tx_bytes
Number of bytes transmitted

Slurm orchestration
1732

## Page 762

Amazon SageMaker AI
Developer Guide

Metric name
Description

node_amazonefa_tx_pkts
Number of packets transmitted

FSx for Lustre metrics dashboard

Provides visualized information of the metrics from Amazon FSx for Lustre ﬁle system collected by
Amazon CloudWatch.

Note

The Grafana FSx for Lustre dashboard utilizes Amazon CloudWatch as its data source,
which diﬀers from the other dashboards that you have conﬁgured to use Amazon Managed
Service for Prometheus. To ensure accurate monitoring and visualization of metrics related

to your FSx for Lustre ﬁle system, conﬁgure the FSx for Lustre dashboard to use Amazon
CloudWatch as the data source, specifying the same AWS Region where your FSx for Lustre
ﬁle system is deployed.

Types of metrics

• DataReadBytes: The number of bytes for ﬁle system read operations.

• DataWriteBytes: The number of bytes for ﬁle system write operations.

• DataReadOperations: The number of read operations.

• DataWriteOperations: The number of write operations.

• MetadataOperations: The number of meta data operations.

• FreeDataStorageCapacity: The amount of available storage capacity.

Amazon SageMaker HyperPod Slurm metrics

Amazon SageMaker HyperPod provides a set of Amazon CloudWatch metrics that you can use to
monitor the health and performance of your HyperPod clusters. These metrics are collected from

the Slurm workload manager running on your HyperPod clusters and are available in the /aws/

sagemaker/Clusters CloudWatch namespace.

Slurm orchestration
1733

## Page 763

Amazon SageMaker AI
Developer Guide

Cluster level metrics

The following cluster-level metrics are available for HyperPod. These metrics use the ClusterId
dimension to identify the speciﬁc HyperPod cluster.

CloudWatch metric name
Notes
Amazon EKS Container
Insights metric name

cluster_node_count
Total number of nodes in the
cluster

cluster_node_count

cluster_idle_node_count
Number of idle nodes in the
cluster

N/A

cluster_failed_node_count
Number of failed nodes in the
cluster

cluster_failed_node_count

cluster_cpu_count
Total CPU cores in the cluster
node_cpu_limit

cluster_idle_cpu_count
Number of idle CPU cores in
the cluster

N/A

cluster_gpu_count
Total GPUs in the cluster
node_gpu_limit

cluster_idle_gpu_count
Number of idle GPUs in the
cluster

N/A

cluster_running_task_count
Number of running Slurm
jobs in the cluster

N/A

cluster_pending_task_count
Number of pending Slurm
jobs in the cluster

N/A

cluster_preempted_task_coun
t

Number of preempted Slurm
jobs in the cluster

N/A

cluster_avg_task_wait_time
Average wait time for Slurm
jobs in the cluster

N/A

Slurm orchestration
1734

## Page 764

Amazon SageMaker AI
Developer Guide

CloudWatch metric name
Notes
Amazon EKS Container
Insights metric name

cluster_max_task_wait_time
Maximum wait time for Slurm
jobs in the cluster

N/A

Instance level metrics

The following instance-level metrics are available for HyperPod. These metrics also use the

ClusterId dimension to identify the speciﬁc HyperPod cluster.

CloudWatch metric name
Notes
Amazon EKS Container
Insights metric name

node_gpu_utilization
Average GPU utilization
across all instances

node_gpu_utilization

node_gpu_memory_ut
ilization

Average GPU memory utilizati
on across all instances

node_gpu_memory_ut
ilization

node_cpu_utilization
Average CPU utilization
across all instances

node_cpu_utilization

node_memory_utilization
Average memory utilization
across all instances

node_memory_utilization

SageMaker HyperPod cluster resiliency

SageMaker HyperPod through Slurm orchestration provides the following cluster resiliency
features.

Topics

• Health monitoring agent

• Automatic node recovery and auto-resume

• Manually replace or reboot a node using Slurm

Slurm orchestration
1735

## Page 765

Amazon SageMaker AI
Developer Guide

Health monitoring agent

This section describes the set of health checks that SageMaker HyperPod uses to regularly monitor
cluster instance health for issues with devices such as accelerators (GPU and Trainium cores) and
networking (EFA). SageMaker HyperPod health-monitoring agent (HMA) continuously monitors the
health status of each GPU-based or Trainium-based instance. When it detects any instance or GPU
failures, the agent marks the instance as unhealthy.

SageMaker HyperPod HMA performs the same health checks for both EKS and Slurm orchestrators.
For more information about HMA, see Health Monitoring System.

Automatic node recovery and auto-resume

Note

As of September 11, 2025, HyperPod with Slurm orchestration now supports health
monitoring agents. Run UpdateClusterSoftware and update to the latest version of the AMI
in order to use this functionality.

This section talks about Amazon SageMaker HyperPod's two complementary resilience features:
automatic node recovery that replaces faulty infrastructure without manual intervention, and auto-
resume functionality that restarts training jobs from the last checkpoint after hardware failures.

How automatic node recovery works

During cluster creation or update, cluster admin users can select the node (instance) recovery

option between Automatic (Recommended) and None at the cluster level. If set to Automatic,
SageMaker HyperPod reboots or replaces faulty nodes automatically.

Important

We recommend setting the Automatic option. By default, the clusters are set up with
Automatic node recovery.

Automatic node recovery runs when issues are found from health-monitoring agent, basic health

checks, and deep health checks. If set to None, the health monitoring agent will label the instances
when a fault is detected, but it will not automatically initiate any repair or recovery actions on the
aﬀected nodes. We do not recommend this option.

Slurm orchestration
1736

## Page 766

Amazon SageMaker AI
Developer Guide

Running a training job with the Amazon SageMaker HyperPod auto-resume functionality

This section describes how to run a training job with the SageMaker HyperPod auto-resume
functionality, which provides a zero-touch resiliency infrastructure to automatically recover a
training job from the last saved checkpoint in the event of a hardware failure.

With the auto-resume functionality, if a job fails due to a hardware failure or any transient issues
in-between training, SageMaker HyperPod auto-resume starts the node replacement workﬂow
and restarts the job after the faulty nodes are replaced. The following hardware checks are run
whenever a job fails while using auto-resume:

Category
Utility name
Instance type
compatibility

Description

Accelerator
NVIDIA SMI
GPU
nvidia-smi utility is
a well-known CLI to
manage and monitor
GPUs. The built-in
health checker parses
the output from

nvidia-smi  to
determine the health
of the instance.

Accelerator
Neuron sysfs
Trainium
For Trainium-
powered instances
, the health of the
Neuron devices
is determined by
reading counters
from Neuron sysfs
propagated directly
by the Neuron driver.

Network
EFA
GPU and Trainium
To aid in the
diagnostic of Elastic
Fabric Adaptor (EFA)
devices, the EFA

Slurm orchestration
1737

## Page 767

Amazon SageMaker AI
Developer Guide

Category
Utility name
Instance type
compatibility

Description

health checker runs
a series of connectiv
ity tests using all
available EFA cards
within the instance.

Note

When Generic Resources (GRES) are attached to a Slurm node, Slurm typically doesn't
permit changes in the node allocation, such as replacing nodes, and thus doesn’t allow to
resume a failed job. Unless explicitly forbidden, the HyperPod auto-resume functionality
automatically re-queues any faulty job associated with the GRES-enabled nodes. This
process involves stopping the job, placing it back into the job queue, and then restarting
the job from the beginning.

Using the SageMaker HyperPod auto-resume functionality with Slurm

When you use SageMaker HyperPod auto-resume with Slurm, you should run the job inside an

exclusive allocation acquired either by using salloc or sbatch. In any case, you need to modify

the entrypoint script to make sure that all setup steps run in a single srun command when
resuming the job. Through the entrypoint script, it is important to set up the environment on
the replaced node to be consistent with the environment that the job step was running before
it was stopped. The following procedure shows how to prepare an entrypoint script to keep the

environment consistent and run it as a single srun command.

Tip

If you use sbatch, you can keep the batch script simple by creating a separate script for

setting up the environment and using a single srun command.

1. Create a script using the following code example and save it as train_auto_resume.sh. This

script deploys training environment setups assuming that there is no manual conﬁguration

Slurm orchestration
1738

## Page 768

Amazon SageMaker AI
Developer Guide

previously made to the replaced node. This ensures that the environment is node-agnostic, so
that when a node is replaced, the same environment is provisioned on the node before resuming
the job.

Note

The following code example shows how to discover the Slurm node list associated with

the job. Do not use the $SLURM_JOB_NODELIST environment variable provided by
Slurm, because its value might be outdated after SageMaker HyperPod auto-resumes

the job. The following code example shows how to deﬁne a new NODE_LIST variable to

replace SLURM_JOB_NODELIST, and then set up the MASTER_NODE and MASTER_ADDR

variables oﬀ of the NODE_LIST variable.

#!/bin/bash

# Filename: train_auto_resume.sh
# Sample containerized script to launch a training job with a single srun which can
be auto-resumed.

# Place your training environment setup here.
# Example: Install conda, docker, activate virtual env, etc.

# Get the list of nodes for a given job
NODE_LIST=$(scontrol show jobid=$SLURM_JOBID | \ # Show details of the SLURM job
awk -F= '/NodeList=/{print $2}' | \  # Extract NodeList field
grep -v Exc)                         # Exclude nodes marked as excluded

# Determine the master node from the node list
MASTER_NODE=$(scontrol show hostname $NODE_LIST | \ # Convert node list to hostnames
head -n 1)                            # Select the first hostname as
master node

# Get the master node address
MASTER_ADDR=$(scontrol show node=$MASTER_NODE | \ # Show node information
awk -F= '/NodeAddr=/{print $2}' | \ # Extract NodeAddr
awk '{print $1}')                   # Print the first part of NodeAddr

# Torchrun command to launch the training job
torchrun_cmd="torchrun --nnodes=$SLURM_NNODES \

Slurm orchestration
1739

## Page 769

Amazon SageMaker AI
Developer Guide

--nproc_per_node=1 \
--node_rank=$SLURM_NODE \
--master-addr=$MASTER_ADDR \
--master_port=1234 \
<your_training_script.py>"

# Execute the torchrun command in the 'pytorch' Conda environment,
# streaming output live
/opt/conda/bin/conda run --live-stream -n pytorch $torchrun_cmd

Tip

You can use the preceding script to add more commands for installing any additional
dependencies for your job. However, we recommend that you keep the dependency
installation scripts to the set of lifecycle scripts that are used during cluster creation.
If you use a virtual environment hosted on a shared directory, you can also utilize this
script to activate the virtual environment.

2. Launch the job with SageMaker HyperPod auto-resume enabled by adding the ﬂag --auto-

resume=1 to indicate that the srun command should be automatically retried in case of
hardware failure.

Note

If you have set up a resource allocation using sbatch or salloc, you can run multiple

srun commands within the allocation. In the event of a failure, the SageMaker

HyperPod auto-resume functionality only operates in the current job step of the srun

command with the ﬂag --auto-resume=1. In other words, activating auto-resume in

an srun command doesn't apply to other srun commands launched within a resource
allocation session.

The following are srun command examples with auto-resume enabled.

Using sbatch

Because most of the logic for setting up the environment is already in

train_auto_resume.sh, the batch script should be simple and similar to the following code

example. Assume that the following batch script is saved as batch.sh.

Slurm orchestration
1740

## Page 770

Amazon SageMaker AI
Developer Guide

#!/bin/bash
#SBATCH --nodes 2
#SBATCH --exclusive
srun --auto-resume=1 train_auto_resume.sh

Run the preceding batch script using the following command.

sbatch batch.sh

Using salloc

Start by acquiring an exclusive allocation, and run the srun command with the --auto-resume
ﬂag and the entrypoint script.

salloc -N 2 --exclusive
srun --auto-resume=1 train_auto_resume.sh

How automatic node recovery and auto-resume work together

When both automatic node recovery and auto-resume systems are active, they follow a
coordinated approach to handling failures. If the HMA detects a hardware fault, the node is marked
for drain regardless of job-level status. With node automatic recovery enabled, the nodes are
automatically replaced once all the jobs running in the nodes exit. In this scenario, for jobs with
auto-resume enabled, if there is a non-zero exit status in the step, the auto resume kicks in (the
jobs resume once nodes are replaced). Jobs without auto-resume enabled will simply exit, requiring
manual resubmission by administrators or users.

Note

If you use auto-resume, the nodes are always replaced (no reboots) when hardware failures
are detected.

Manually replace or reboot a node using Slurm

This section talks about when you should manually reboot or replace a node, with instructions on
how to do both.

Slurm orchestration
1741

## Page 771

Amazon SageMaker AI
Developer Guide

When to manually reboot or replace a node

The HyperPod auto-resume functionality monitors if the state of your Slurm nodes turns to fail

or down. You can check the state of Slurm nodes by running sinfo.

If a node remains stuck or unresponsive and the auto-resume process does not recover it, you can
manually initiate recovery. The choice between rebooting and replacing a node depends on the
nature of the issue. Consider rebooting when facing temporary or software-related problems, such
as system hangs, memory leaks, GPU driver issues, kernel updates, or hung processes. However, if
you encounter persistent or hardware-related problems like failing GPUs, memory or networking
faults, repeated health check failures, or nodes that remain unresponsive after multiple reboot
attempts, node replacement is the more appropriate solution.

Ways to manually reboot or replace nodes

SageMaker HyperPod oﬀers two methods for manual node recovery. The preferred approach
is using the SageMaker HyperPod Reboot and Replace APIs, which provides a faster and more
transparent recovery process that works across all orchestrators. Alternatively, you can use

traditional Slurm commands like scontrol update, though this legacy method requires direct
access to the Slurm's controller node. Both methods activate the same SageMaker HyperPod
recovery processes.

Manually reboot a node using reboot API

You can use the BatchRebootClusterNodes to manually reboot a faulty node in your SageMaker
HyperPod cluster.

Here is an example of running the reboot operation on two Instances of a cluster using the AWS
Command Line Interface:

aws sagemaker batch-reboot-cluster-nodes \
--cluster-name arn:aws:sagemaker:ap-northeast-1:123456789:cluster/test-
cluster \
--node-ids i-0123456789abcdef0 i-0fedcba9876543210

Manually replace a node using replace API

You can use the BatchReplaceClusterNodes to manually replace a faulty node in your SageMaker
HyperPod cluster.

Slurm orchestration
1742

## Page 772

Amazon SageMaker AI
Developer Guide

Here is an example of running the replace operation on two Instances of a cluster using the AWS
Command Line Interface:

aws sagemaker batch-replace-cluster-nodes \
--cluster-name arn:aws:sagemaker:ap-northeast-1:123456789:cluster/test-
cluster \
--node-ids i-0123456789abcdef0 i-0fedcba9876543210

Manually reboot a node using Slurm

You can also use the scontrol Slurm commands to trigger node recovery. These commands interact
directly with the Slurm control plane and invoke the same underlying SageMaker HyperPod
recovery mechanisms.

In the following command , replace <ip-ipv4> with the Slurm node name (host name) of the faulty
instance you want to reboot.

scontrol update node=<ip-ipv4> state=fail reason="Action:Reboot"

This marks the node as FAIL with the speciﬁed reason. SageMaker HyperPod detects this and
reboots the instance. Avoid changing the node state or restarting the Slurm controller during the
operation.

Manually replace a node using Slurm

You can use the scontrol update command as follows to replace a node.

In the following command, replace <ip-ipv4> with the Slurm node name (host name) of the
faulty instance you want to replace.

scontrol update node=<ip-ipv4> state=fail reason="Action:Replace"

After running this command, the node will go into the fail state, waits for the currently running
jobs to ﬁnish, is replaced with a healthy instance, and is recovered with the same host name. This
process takes time depending on the available instances in your Availability Zone and the time it
takes to run your lifecycle scripts. During the update and replacement processes, avoid changing
the state of the node manually again or restarting the Slurm controller; doing so can lead to a

replacement failure. If the node does not get recovered nor turn to the idle state after a long
time, contact AWS Support.

Slurm orchestration
1743

## Page 773

Amazon SageMaker AI
Developer Guide

Manually force change a node

If the faulty node is continuously stuck in the fail state, the last resort you might try is to

manually force change the node state to down. This requires administrator privileges (sudo
permissions).

Warning

Proceed carefully before you run the following command as it forces kill all jobs, and you
might lose all unsaved work.

scontrol update node=<ip-ipv4> state=down reason="Action:Replace"

SageMaker HyperPod cluster management

The following topics discuss logging and managing SageMaker HyperPod clusters.

Logging SageMaker HyperPod events

All events and logs from SageMaker HyperPod are saved to Amazon CloudWatch under the log

group name /aws/sagemaker/Clusters/[ClusterName]/[ClusterID]. Every call to the

CreateCluster API creates a new log group. The following list contains all of the available log
streams collected in each log group.

Log Group Name
Log Stream Name

/aws/sagemaker/Clusters/[Cl

LifecycleConfig/[instance-group-

usterName]/[ClusterID]

name]/[instance-id]

Logging SageMaker HyperPod at instance level

You can access the LifecycleScript logs published to CloudWatch during cluster instance
conﬁguration. Every instance within the created cluster generates a separate log stream,

distinguishable by the LifecycleConfig/[instance-group-name]/[instance-id] format.

All logs that are written to /var/log/provision/provisioning.log are
uploaded to the preceding CloudWatch stream. Sample LifecycleScripts at

Slurm orchestration
1744

## Page 774

Amazon SageMaker AI
Developer Guide

1.architectures/5.sagemaker_hyperpods/LifecycleScripts/base-config redirect

their stdout and stderr to this location. If you are using your custom scripts, write your logs to

the /var/log/provision/provisioning.log location for them to be available in CloudWatch.

Lifecycle script log markers

CloudWatch logs for lifecycle scripts include speciﬁc markers to help you track execution progress

and identify issues:

Marker
Description

START
Indicates the beginning of

lifecycle script logs for the

instance

[SageMaker] Lifecycle scripts

Indicates the S3 location and

were provided, with S3 uri: [s3://

entrypoint script that will be

bucket-name/]
and entrypoint

used

script: [script-name.sh]

[SageMaker] Downloading lifecycle

Indicates scripts are being

scripts

downloaded from the specified S3

location

[SageMaker] Lifecycle scripts have

Indicates scripts have been

been downloaded

successfully downloaded from S3

[SageMaker] The lifecycle scripts

Indicates successful completion

succeeded

of all lifecycle scripts

[SageMaker] The lifecycle scripts

Indicates failed execution of

failed

lifecycle scripts

These markers help you quickly identify where in the lifecycle script execution process an issue
occurred. When troubleshooting failures, review the log entries to identify where the process
stopped or failed.

Lifecycle script failure messages

Slurm orchestration
1745

## Page 775

Amazon SageMaker AI
Developer Guide

If the lifecycle script exists but fails during execution, you will receive an error message that
includes the CloudWatch log group name and log stream name. In the event that there are lifecycle
script failures across multiple instances, the error message will indicate only one failed instance,
but the log group should contain streams for all instances.

You can view the error message by running the DescribeCluster API or by viewing the cluster details
page in the SageMaker console. In the console, a View lifecycle script logs button is provided that
navigates directly to the CloudWatch log stream. The error message has the following format:

Instance [instance-id] failed to provision with the following error: "Lifecycle scripts
did not run successfully. To view lifecycle script logs,
visit log group ‘/aws/sagemaker/Clusters/[cluster-name]/[cluster-id]' and log stream
‘LifecycleConfig/[instance-group-name]/[instance-id]’.
If you cannot find corresponding lifecycle script logs in CloudWatch, please make sure
you follow one of the options here:
https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-faq-
slurm.html#hyperpod-faqs-q1.” Note that multiple instances may be impacted.

Tagging resources

AWS Tagging system helps manage, identify, organize, search for, and ﬁlter resources. SageMaker
HyperPod supports tagging, so you can manage the clusters as an AWS resource. During cluster
creation or editing an existing cluster, you can add or edit tags for the cluster. To learn more about
tagging in general, see Tagging your AWS resources.

Using the SageMaker HyperPod console UI

When you are creating a new cluster and editing a cluster, you can add, remove, or edit tags.

Using the SageMaker HyperPod APIs

When you write a CreateCluster or UpdateCluster API request ﬁle in JSON format, edit the Tags
section.

Using the AWS CLI tagging commands for SageMaker AI

To tag a cluster

Use aws sagemaker add-tags as follows.

aws sagemaker add-tags --resource-arn cluster_ARN --tags Key=string,Value=string

Slurm orchestration
1746

## Page 776

Amazon SageMaker AI
Developer Guide

To untag a cluster

Use aws sagemaker delete-tags as follows.

aws sagemaker delete-tags --resource-arn cluster_ARN --tag-keys "tag_key"

To list tags for a resource

Use aws sagemaker list-tags as follows.

aws sagemaker list-tags --resource-arn cluster_ARN

SageMaker HyperPod FAQs

Use the following frequently asked questions to troubleshoot problems with using SageMaker
HyperPod.

Amazon SageMaker HyperPod FAQs

• Why can't I ﬁnd log groups of my SageMaker HyperPod cluster in Amazon CloudWatch?

• What particular conﬁgurations does HyperPod manage in Slurm conﬁguration ﬁles such as
slurm.conf and gres.conf?

• How do I run Docker on Slurm nodes on HyperPod?

• Why does my parallel training job fail when I use NVIDIA Collective Communications Library
(NCCL) with Slurm on SageMaker HyperPod platform?

• How do I use local NVMe store of P instances for launching Docker or Enroot containers with
Slurm?

• How to set up EFA security groups?

• How do I monitor my HyperPod cluster nodes? Are there any CloudWatch metrics exported from
HyperPod?

• Can I add an additional storage to the HyperPod cluster nodes? The cluster instances have
limited local instance store.

• Why are my compute nodes showing as "DOWN" or "DRAINED" after a reboot?

• Why do my nodes keep getting drained due to Out of Memory (OOM) issues?

• How can I ensure resources are properly cleaned up after jobs complete?

Slurm orchestration
1747

## Page 777

Amazon SageMaker AI
Developer Guide

Why can't I ﬁnd log groups of my SageMaker HyperPod cluster in Amazon CloudWatch?

By default, agent logs and instance start-up logs are sent to the HyperPod platform account’s
CloudWatch. In case of user lifecycle scripts, lifecycle conﬁguration logs are sent to your account’s
CloudWatch.

If you use the sample lifecycle scripts provided by the HyperPod service team, you can expect to

ﬁnd the lifecycle conﬁguration logs written to /var/log/provision/provisioning.log, and
you wouldn’t encounter this problem.

However, if you use custom paths for collecting logs from lifecycle provisioning and can’t ﬁnd the
log groups appearing in your account's CloudWatch, it might be due to mismatches in the log ﬁle
paths speciﬁed in your lifecycle scripts and what the CloudWatch agent running on the HyperPod
cluster instances looks for. In this case, it means that you need to properly set up your lifecycle
scripts to send logs to the CloudWatch agent, and also set up the CloudWatch agent conﬁguration
accordingly. To resolve the problem, choose one of the following options.

• Option 1: Update your lifecycle scripts to write logs to /var/log/provision/

provisioning.log.

• Option 2: Update the CloudWatch agent to look for your custom paths for logging lifecycle
provisioning.

1. Each HyperPod cluster instance contains a CloudWatch agent conﬁguration

ﬁle in JSON format at /opt/aws/amazon-cloudwatch-agent/

sagemaker_cwagent_config.json. In the conﬁguration ﬁle, ﬁnd the ﬁeld name

logs.logs_collected.files.collect_list.file_path. With the default setup

by HyperPod, the key-value pair should be "file_path": "/var/log/provision/

provisioning.log" as documented at the section called “Logging SageMaker HyperPod at
instance level”. The following code snippet shows how the JSON ﬁle looks with the HyperPod
default conﬁguration.

"logs": {
"logs_collected": {
"files": {
"collect_list": [
{
"file_path": "/var/log/provision/provisioning.log",
"log_group_name": "/aws/sagemaker/Clusters/[ClusterName]/
[ClusterID]",

Slurm orchestration
1748

## Page 778

Amazon SageMaker AI
Developer Guide

"log_stream_name": "LifecycleConfig/[InstanceGroupName]/
{instance_id}",
"retention_in_days": -1
}
]
}
},
"force_flush_interval": 3
}

2. Replace the value for the "file_path" ﬁeld name with the custom path you use in your

lifecycle scripts. For example, if you have set up your lifecycle scripts to write to /var/log/

custom-provision/custom-provisioning.log, update the value to match with it as
follows.

"file_path": "/var/log/custom-provision/custom-provisioning.log"

3. Restart the CloudWatch agent with the conﬁguration ﬁle to ﬁnish applying the custom path.

For example, the following CloudWatch command shows how to restart the CloudWatch
agent with the CloudWatch agent conﬁguration ﬁle from step 1. For more information, see
also Troubleshooting the CloudWatch agent.

sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl \
-a fetch-config -m ec2 -s -c \
file:/opt/aws/amazon-cloudwatch-agent/sagemaker_cwagent_config.json

What particular conﬁgurations does HyperPod manage in Slurm conﬁguration ﬁles such as

slurm.conf and gres.conf?

When you create a Slurm cluster on HyperPod, the HyperPod agent sets up the slurm.conf and

gres.conf ﬁles at /opt/slurm/etc/ to manage the Slurm cluster based on your HyperPod
cluster creation request and lifecycle scripts. The following list shows what speciﬁc parameters the
HyperPod agent handles and overwrites.

Important

We strongly recommend that you DON’T change these parameters managed by HyperPod.

Slurm orchestration
1749

## Page 779

Amazon SageMaker AI
Developer Guide

• In slurm.conf, HyperPod sets up the following basic parameters: ClusterName,

SlurmctldHost, PartitionName, and NodeName.

Also, to enable the the section called “Automatic node recovery and auto-resume” functionality,

HyperPod requires the TaskPlugin and SchedulerParameters parameters set as follows.
The HyperPod agent sets up these two parameters with the required values by default.

TaskPlugin=task/none
SchedulerParameters=permit_job_expansion

• In gres.conf, HyperPod manages NodeName for GPU nodes.

How do I run Docker on Slurm nodes on HyperPod?

To help you run Docker on your Slurm nodes running on HyperPod, the HyperPod service team

provides setup scripts that you can include as part of the lifecycle conﬁguration for cluster
creation. To learn more, see the section called “Base lifecycle scripts” and the section called
“Running Docker containers”.

Why does my parallel training job fail when I use NVIDIA Collective Communications Library
(NCCL) with Slurm on SageMaker HyperPod platform?

By default, the Linux OS sets the #RemoveIPC=yes ﬂag. Slurm and mpirun jobs that use NCCL
generate inter-process communication (IPC) resources under non-root user sessions. These user
sessions might log out during the job process.

When you run jobs with Slurm or mpirun, if systemd detects that the user isn't logged in, it cleans
up the IPC resources. Slurm and mpirun jobs can run without the user being logged in, but that
requires that you disable cleanup at the systemd level and set it up at the Slurm level instead. For
more information, see  Systemd in the NCCL documentation.

To disable cleanup at the systemd level, complete the following steps.

1.
Set the ﬂag #RemoveIPC=no in the ﬁle /etc/systemd/logind.conf if you're running
training jobs that use Slurm and NCCL.

2.
By default, Slurm doesn't clean up shared resources. We recommend that you set up a Slurm
epilog script to clean up shared resources. This cleanup is useful when you have a lot of shared
resources and want to clean them up after training jobs. The following is an example script.

#!/bin/bash

Slurm orchestration
1750

## Page 780

Amazon SageMaker AI
Developer Guide

: <<'SUMMARY'
Script: epilog.sh

Use this script with caution, as it can potentially delete unnecessary resources
and cause issues if you don't use it correctly.

Note: You must save this script in a shared in a shared location that is accessible
to all nodes in the cluster, such as /fsx volume.
Workers must be able to access the script to run the script after jobs.

SUMMARY

# Define the log directory and create it if it doesn't exist
LOG_DIR="/<PLACEHOLDER>/epilogue" #NOTE: Update PLACEHOLDER to be a shared value
path, such as /fsx/epilogue.
mkdir -p "$LOG_DIR"

# Name the log file using the Slurm job name and job ID
log_file="$LOG_DIR/epilogue-${SLURM_JOB_NAME}_${SLURM_JOB_ID}.log"

logging() {
echo "[$(date)] $1" | tee -a "$log_file"
}

# Slurm epilogue script to clean up IPC resources
logging "Starting IPC cleanup for Job $SLURM_JOB_ID"

# Clean up shared memory segments by username
for seg in $(ipcs -m | awk -v owner="$SLURM_JOB_USER" '$3 == owner {print $2}'); do
if ipcrm -m "$seg"; then
logging "Removed shared memory segment $seg"
else
logging "Failed to remove shared memory segment $seg"
fi
done

# Clean up semaphores by username
for sem in $(ipcs -s | awk -v user="$SLURM_JOB_USER" '$3 == user {print $2}'); do
if ipcrm -s "$sem"; then
logging "Removed semaphore $sem"
else
logging "Failed to remove semaphore $sem"
fi
done

Slurm orchestration
1751

## Page 781

Amazon SageMaker AI
Developer Guide

# Clean up NCCL IPC
NCCL_IPC_PATH="/dev/shm/nccl-*"
for file in $NCCL_IPC_PATH; do
if [ -e "$file" ]; then
if rm "$file"; then
logging "Removed NCCL IPC file $file"
else
logging "Failed to remove NCCL IPC file $file"
fi
fi
done
logging "IPC cleanup completed for Job $SLURM_JOB_ID"
exit 0

For more information about the Epilog parameter, see Slurm documentation.

3.
In the slurm.conf ﬁle from the controller node, add in a line to point to the epilog script you
created.

Epilog="/path/to/epilog.sh"  #For example: /fsx/epilogue/epilog.sh

4.
Run the following commands to change permissions of the script and to make it executable.

chown slurm:slurm /path/to/epilog.sh
chmod +x  /path/to/epilog.sh

5.
To apply all of your changes, run scontrol reconfigure.

How do I use local NVMe store of P instances for launching Docker or Enroot containers with
Slurm?

Because the default root volume of your head node usually is limited by 100GB EBS volume,
you need to set up Docker and Enroot to use local NVMe instance store. To learn how to set up
NVMe store and use it for launching Docker containers, see the section called “Running Docker
containers”.

How to set up EFA security groups?

If you want to create a HyperPod cluster with EFA-enabled instances, make sure that you set up a
security group to allow all inbound and outbound traﬃc to and from the security group itself. To
learn more, see Step 1: Prepare an EFA-enabled security group in the Amazon EC2 User Guide.

Slurm orchestration
1752

## Page 782

Amazon SageMaker AI
Developer Guide

How do I monitor my HyperPod cluster nodes? Are there any CloudWatch metrics exported
from HyperPod?

To gain observability into the resource utilization of your HyperPod cluster, we recommend that
you integrate the HyperPod cluster with Amazon Managed Grafana and Amazon Managed Service
for Prometheus. With various open-source Grafana dashboards and exporter packages, you can
export and visualize metrics related to the HyperPod cluster resources. To learn more about
setting up SageMaker HyperPod with Amazon Managed Grafana and Amazon Managed Service for
Prometheus, see the section called “Cluster resources monitoring”. Note that SageMaker HyperPod
currently doesn't support the exportation of system metrics to Amazon CloudWatch.

Can I add an additional storage to the HyperPod cluster nodes? The cluster instances have
limited local instance store.

If the default instance storage is insuﬃcient for your workload, you can conﬁgure additional
storage per instance. Starting from the release on June 20, 2024, you can add an additional
Amazon Elastic Block Store (EBS) volume to each instance in your SageMaker HyperPod cluster.
Note that this capability cannot be applied to existing instance groups of SageMaker HyperPod
clusters created before June 20, 2024. You can utilize this capability by patching existing
SageMaker HyperPod clusters created before June 20, 2024 and adding new instance groups to
them. This capability is fully eﬀective for any SageMaker HyperPod clusters created after June 20,
2024.

Why are my compute nodes showing as "DOWN" or "DRAINED" after a reboot?

This typically occurs when nodes are rebooted using sudo reboot instead of Slurm's

control interface. To properly reboot nodes, use the Slurm command scontrol reboot

nextstate=resume <list_of_nodes>. This ensures Slurm maintains proper control of the
node state and resumes normal operation after reboot.

For GPU instances (like NVIDIA P5), this can also happen if the node can't complete its boot

process within Slurm's default time limit (60 seconds). To resolve this, increase the TimeToResume

parameter in slurm.conf to 300 seconds. This gives GPU instances suﬃcient time to boot and
initialize drivers.

Why do my nodes keep getting drained due to Out of Memory (OOM) issues?

OOM issues occur when jobs exceed the node's memory capacity. To prevent this, implement

cgroups to enforce memory limits per job. This prevents a single job from aﬀecting the entire
node and improves isolation and stability.

Slurm orchestration
1753

## Page 783

Amazon SageMaker AI
Developer Guide

Example setup in slurm.conf:

TaskPlugin=task/cgroup

Example setup in cgroup.conf:

CgroupAutomount=yes
ConstrainCores=yes
CgroupPlugin=autodetect
ConstrainDevices=yes
ConstrainRAMSpace=yes
ConstrainSwapSpace=yes
SignalChildrenProcesses=yes
MaxRAMPercent=99
MaxSwapPercent=80
MinRAMSpace=100

For more information, see Control Group in Slurm, Cgroup and PAM-based login control for Slurm
compute nodes, and Conﬁgure Cgroups for Slurm.

How can I ensure resources are properly cleaned up after jobs complete?

Implement epilogue scripts to automatically clean up resources after jobs complete. Resources
might not be cleared correctly when jobs crash unexpectedly, contain bugs that prevent normal
cleanup, or when shared memory buﬀers (include those shared between processes and GPU
drivers) retain allocated.

Epilogue scripts can perform tasks such as clearing GPU memory, removing temporary ﬁles,
and unmounting ﬁle systems. These scripts have limitations when resources are not exclusively
allocated to a single job. For detailed instructions and sample scripts, see the second bullet point
of the question the section called “Why does my parallel training job fail when I use NVIDIA
Collective Communications Library (NCCL) with Slurm on SageMaker HyperPod platform?”. For
more information, see Enable Slurm epilog script.

Orchestrating SageMaker HyperPod clusters with Amazon EKS

SageMaker HyperPod is a SageMaker AI-managed service that enables large-scale training of
foundation models on long-running and resilient compute clusters, integrating with Amazon
EKS for orchestrating the HyperPod compute resources. You can run uninterrupted training jobs

Amazon EKS orchestration
1754

## Page 784

Amazon SageMaker AI
Developer Guide

spanning weeks or months at scale using Amazon EKS clusters with HyperPod resiliency features
that check for various hardware failures and automatically recover faulty nodes.

Key features for cluster admin users include the following.

• Provisioning resilient HyperPod clusters and attaching them to an EKS control plane

• Enabling dynamic capacity management, such as adding more nodes, updating software, and
deleting clusters

• Enabling access to the cluster instances directly through kubectl or SSM/SSH

• Oﬀering resiliency capabilities, including basic health checks, deep health checks, a health-
monitoring agent, and support for PyTorch job auto-resume

• Integrating with observability tools such as Amazon CloudWatch Container Insights, Amazon
Managed Service for Prometheus, and Amazon Managed Grafana

For data scientist users, EKS support in HyperPod enables the following.

• Running containerized workloads for training foundation models on the HyperPod cluster

• Running inference on the EKS cluster, leveraging the integration between HyperPod and EKS

• Leveraging the job auto-resume capability for Kubeﬂow PyTorch training (PyTorchJob)

Note

Amazon EKS enables user-managed orchestration of tasks and infrastructure on SageMaker
HyperPod through the Amazon EKS Control Plane. Ensure that user access to the cluster
through the Kubernetes API Server endpoint follows the principle of least-privilege, and
that network egress from the HyperPod cluster is secured.
To learn more about securing access to the Amazon EKS API Server, see Control network
access to cluster API server endpoint.
To learn more about securing network access on HyperPod, see Setting up SageMaker
HyperPod with a custom Amazon VPC.

The high-level architecture of Amazon EKS support in HyperPod involves a 1-to-1 mapping
between an EKS cluster (control plane) and a HyperPod cluster (worker nodes) within a VPC, as
shown in the following diagram.

Amazon EKS orchestration
1755

## Page 785

Amazon SageMaker AI
Developer Guide

![Page 785 Diagram 1](images/page-0785-img-01.png)

Managing SageMaker HyperPod clusters orchestrated by Amazon EKS

This section provides guidance on managing SageMaker HyperPod through the SageMaker AI
console UI or the AWS Command Line Interface (CLI). It explains how to perform various tasks
related to SageMaker HyperPod, whether you prefer a visual interface or working with commands.

Topics

• Getting started with Amazon EKS support in SageMaker HyperPod

• Installing packages on the Amazon EKS cluster using Helm

Amazon EKS orchestration
1756

## Page 786

Amazon SageMaker AI
Developer Guide

• Setting up Kubernetes role-based access control

• Custom Amazon Machine Images (AMIs) for SageMaker HyperPod clusters

• Managing SageMaker HyperPod EKS clusters using the SageMaker console

• Creating SageMaker HyperPod clusters using CloudFormation templates

• Managing SageMaker HyperPod EKS clusters using the AWS CLI

• HyperPod managed tiered checkpointing

• SageMaker HyperPod task governance

• Usage reporting for cost attribution in SageMaker HyperPod

• Conﬁguring storage for SageMaker HyperPod clusters orchestrated by Amazon EKS

• Using the Amazon EBS CSI driver on SageMaker HyperPod EKS clusters

• Conﬁguring custom Kubernetes labels and taints in Amazon SageMaker HyperPod

Getting started with Amazon EKS support in SageMaker HyperPod

In addition to the general the section called “Prerequisites” for SageMaker HyperPod, check the
following requirements and considerations for orchestrating SageMaker HyperPod clusters using
Amazon EKS.

Important

You can set up resources conﬁguration for creating SageMaker HyperPod clusters using the
AWS Management Console and CloudFormation. For more information, see ??? and ???.

Requirements

Note

Before creating a HyperPod cluster, you need a running Amazon EKS cluster conﬁgured
with VPC and installed using Helm.

• If using the SageMaker AI console, you can create an Amazon EKS cluster within the HyperPod
cluster console page. For more information, see the section called “Creating a cluster”.

Amazon EKS orchestration
1757

## Page 787

Amazon SageMaker AI
Developer Guide

• If using AWS CLI, you should create an Amazon EKS cluster before creating a HyperPod cluster to
associate with. For more information, see Create an Amazon EKS cluster in the Amazon EKS User
Guide.

When provisioning your Amazon EKS cluster, consider the following:

1. Kubernetes version support

• SageMaker HyperPod supports Kubernetes versions 1.28, 1.29, 1.30, 1.31, 1.32, and 1.33.

2. Amazon EKS cluster authentication mode

• The authentication mode of an Amazon EKS cluster supported by SageMaker HyperPod are

API and API_AND_CONFIG_MAP.

3. Networking

• SageMaker HyperPod requires the Amazon VPC Container Network Interface (CNI) plug-in
version 1.18.3 or later.

Note

AWS VPC CNI plugin for Kubernetes is the only CNI supported by SageMaker
HyperPod.

• The type of the subnet in your VPC must be private for HyperPod clusters.

4. IAM roles

• Ensure the necessary IAM roles for HyperPod are set up as guided in the the section called
“IAM for HyperPod” section.

5. Amazon EKS cluster add-ons

• You can continue using the various add-ons provided by Amazon EKS such as Kube-proxy,
CoreDNS, the Amazon VPC Container Network Interface (CNI) plugin, Amazon EKS pod
identity, the GuardDuty agent, the Amazon FSx Container Storage Interface (CSI) driver,
the Mountpoint for Amazon S3 CSI driver, the AWS Distro for OpenTelemetry, and the
CloudWatch Observability agent.

Considerations for conﬁguring SageMaker HyperPod clusters with Amazon EKS

Amazon EKS orchestration
1758

## Page 788

Amazon SageMaker AI
Developer Guide

• You must use distinct IAM roles based on the type of your nodes. For HyperPod nodes, use a role
based on IAM role for SageMaker HyperPod. For Amazon EKS nodes, see Amazon EKS node IAM
role.

• You can provision and mount additional Amazon EBS volumes on SageMaker HyperPod nodes
using two approaches: use InstanceStorageConﬁgs for cluster-level volume provisioning
(available when creating or updating instance groups), or use the Amazon Elastic Block
Store (Amazon EBS) Container Storage Interface (CSI) driver for dynamic pod-level volume

management. With InstanceStorageConﬁgs, set the local path to /opt/sagemaker to properly
mount the volumes to your Amazon EKS pods. For information about how to deploy the Amazon
EBS CSI controller on HyperPod nodes, see ???.

• If you use instance-type labels for deﬁning scheduling constraints, ensure that you use

the SageMaker AI ML instance types preﬁxed with ml.. For example, for P5 instances, use

ml.p5.48xlarge instead of p5.48xlarge.

Considerations for conﬁguring network for SageMaker HyperPod clusters with Amazon EKS

• Each HyperPod cluster instance supports one Elastic Network Interface (ENI). For the maximum
number of Pods per instance type, refer to the following table.

Instance type
Max number of pods

ml.p4d.24xlarge
49

ml.p4de.24xlarge
49

ml.p5.48xlarge
49

ml.trn1.32xlarge
49

ml.trn1n.32xlarge
49

ml.g5.xlarge
14

ml.g5.2xlarge
14

ml.g5.4xlarge
29

ml.g5.8xlarge
29

Amazon EKS orchestration
1759

## Page 789

Amazon SageMaker AI
Developer Guide

Instance type
Max number of pods

ml.g5.12xlarge
49

ml.g5.16xlarge
29

ml.g5.24xlarge
49

ml.g5.48xlarge
49

ml.c5.large
9

ml.c5.xlarge
14

ml.c5.2xlarge
14

ml.c5.4xlarge
29

ml.c5.9xlarge
29

ml.c5.12xlarge
29

ml.c5.18xlarge
49

ml.c5.24xlarge
49

ml.c5n.large
9

ml.c5n.2xlarge
14

ml.c5n.4xlarge
29

ml.c5n.9xlarge
29

ml.c5n.18xlarge
49

ml.m5.large
9

ml.m5.xlarge
14

ml.m5.2xlarge
14

Amazon EKS orchestration
1760

## Page 790

Amazon SageMaker AI
Developer Guide

Instance type
Max number of pods

ml.m5.4xlarge
29

ml.m5.8xlarge
29

ml.m5.12xlarge
29

ml.m5.16xlarge
49

ml.m5.24xlarge
49

ml.t3.medium
5

ml.t3.large
11

ml.t3.xlarge
14

ml.t3.2xlarge
14

ml.g6.xlarge
14

ml.g6.2xlarge
14

ml.g6.4xlarge
29

ml.g6.8xlarge
29

ml.g6.12xlarge
29

ml.g6.16xlarge
49

ml.g6.24xlarge
49

ml.g6.48xlarge
49

ml.gr6.4xlarge
29

ml.gr6.8xlarge
29

ml.g6e.xlarge
14

Amazon EKS orchestration
1761

## Page 791

Amazon SageMaker AI
Developer Guide

Instance type
Max number of pods

ml.g6e.2xlarge
14

ml.g6e.4xlarge
29

ml.g6e.8xlarge
29

ml.g6e.12xlarge
29

ml.g6e.16xlarge
49

ml.g6e.24xlarge
49

ml.g6e.48xlarge
49

ml.p5e.48xlarge
49

• Only Pods with hostNetwork = true have access to the Amazon EC2 Instance Metadata
Service (IMDS) by default. Use the Amazon EKS Pod identity or the IAM roles for service accounts
(IRSA) to manage access to the AWS credentials for Pods.

• EKS-orchestrated HyperPod clusters support dual IP addressing modes, allowing conﬁguration
with IPv4 or IPv6 for IPv6 Amazon EKS clusters in IPv6-enabled VPC and subnet environments.
For more information, see the section called “Setting up SageMaker HyperPod with a custom
Amazon VPC”.

Considerations for using the HyperPod cluster resiliency features

• Node auto-replacement is not supported for CPU instances.

• The HyperPod health monitoring agent needs to be installed for node auto-recovery to work.
The agent can be installed using Helm. For more information, see the section called “Installing
packages”.

• The HyperPod deep health check and health monitoring agent supports GPU and Trn instances.

• SageMaker AI applies the following taint to nodes when they are undergoing deep health checks:

effect: NoSchedule
key: sagemaker.amazonaws.com/node-health-status
value: Unschedulable

Amazon EKS orchestration
1762

## Page 792

Amazon SageMaker AI
Developer Guide

Note

You cannot add custom taints to nodes in instance groups with DeepHealthChecks
turned on.

Once your Amazon EKS cluster is running, conﬁgure your cluster using the Helm package manager
as instructed in the section called “Installing packages” before creating your HyperPod cluster.

Installing packages on the Amazon EKS cluster using Helm

Before creating a SageMaker HyperPod cluster and attaching it to an Amazon EKS cluster, you
should install packages using Helm, a package manager for Kubernetes. Helm is an open-source
tool for setting up a installation process for Kubernetes clusters. It enables the automation and
streamlining of dependency installations and simpliﬁes various setups needed for preparing the
Amazon EKS cluster as the orchestrator (control plane) for a SageMaker HyperPod cluster.

The SageMaker HyperPod service team provides a Helm chart package, which bundles key
dependencies such as device/EFA plug-ins, plug-ins, Kubeﬂow Training Operator, and associated
permission conﬁgurations.

Important

This Helm installation step is required. If you set up your Amazon EKS cluster using
the AWS Management Console or CloudFormation, you can skip this step because the
installation is handled automatically during the setup process. If you set up the cluster
directly using the APIs, use the provided Helm chart to conﬁgure your Amazon EKS cluster.
Failure to conﬁgure your Amazon EKS cluster using the provided Helm chart might result
in the SageMaker HyperPod cluster not functioning correctly or the creation process failing

entirely. The aws-hyperpod namespace name cannot be modiﬁed.

1. Install Helm on your local machine.

2. Download the Helm charts provided by SageMaker HyperPod located at helm_chart/

HyperPodHelmChart in the SageMaker HyperPod CLI repository.

git clone https://github.com/aws/sagemaker-hyperpod-cli.git

Amazon EKS orchestration
1763

## Page 793

Amazon SageMaker AI
Developer Guide

cd sagemaker-hyperpod-cli/helm_chart

3. Update the dependencies of the Helm chart, preview the changes that will be made to your

Kubernetes cluster, and install the Helm chart.

helm dependencies update HyperPodHelmChart

helm install hyperpod-dependencies HyperPodHelmChart --namespace kube-system --dry-
run

helm install hyperpod-dependencies HyperPodHelmChart --namespace kube-system

In summary, the Helm installation sets up various components for your Amazon EKS cluster,

including job scheduling and queueing (Kueue), storage management, MLﬂow integration, and
Kubeﬂow. Additionally, the charts install the following components for integrating with the
SageMaker HyperPod cluster resiliency features, which are required components.

• Health monitoring agent – This installs the health-monitoring agent provided by SageMaker
HyperPod. This is required if you want to get your HyperPod cluster be monitored. Health-

monitoring agents are provided as Docker images as follows. In the provided values.yaml in
the Helm charts, the image is preset. The agent support GPU-based instances and Trainium-

accelerator-based instances (trn1, trn1n, inf2). It is installed to the aws-hyperpod
namespace. To ﬁnd your supported URI, see Supported Regions and their ECR URIs in the
sagemaker-hyperpod-cli repository on GitHub.

• Deep health check – This sets up a ClusterRole, a ServiceAccount (deep-health-check-

service-account) in the aws-hyperpod namespace, and a ClusterRoleBinding to enable
the SageMaker HyperPod deep health check feature. For more information about the Kubernetes

RBAC ﬁle for deep health check, see the conﬁguration ﬁle deep-health-check-rbac.yaml in
the SageMaker HyperPod CLI GitHub repository.

• job-auto-restart - This sets up a ClusterRole, a ServiceAccount (job-auto-restart)

in the aws-hyperpod namespace, and a ClusterRoleBinding, to enable the auto-restart
feature for PyTorch training jobs in SageMaker HyperPod. For more information about the

Kubernetes RBAC ﬁle for job-auto-restart, see the conﬁguration ﬁle job-auto-restart-

rbac.yaml in the SageMaker HyperPod CLI GitHub repository.

• Kubeﬂow MPI operator – The MPI Operator is a Kubernetes operator that simpliﬁes running
distributed Machine Learning (ML) and High-Performance Computing (HPC) workloads using

Amazon EKS orchestration
1764

## Page 794

Amazon SageMaker AI
Developer Guide

the Message Passing Interface (MPI) on Kubernetes clusters. It installs MPI Operator v0.5. It is

installed to the mpi-operator namespace.

• nvidia-device-plugin – This is a Kubernetes device plug-in that allows you to automatically
expose NVIDIA GPUs for consumption by containers in your Amazon EKS cluster. It allows
Kubernetes to allocate and provide access to the requested GPUs for that container. Required
when using an instance type with GPU.

• neuron-device-plugin – This is a Kubernetes device plug-in that allows you to automatically
expose AWS Inferentia chips for consumption by containers in your Amazon EKS cluster. It allows
Kubernetes to access and utilize the AWS Inferentia chips on the cluster nodes. Required when
using a Neuron instance type.

• aws-efa-k8s-device-plugin – This is a Kubernetes device plug-in that enables the use of
AWS Elastic Fabric Adapter (EFA) on Amazon EKS clusters. EFA is a network device that provides
low-latency and high-throughput communication between instances in a cluster. Required when
using an EFA supported instance type.

For more information about the installation procedure using the provided Helm charts, see the
README ﬁle in the SageMaker HyperPod CLI repository.

Setting up Kubernetes role-based access control

Cluster admin users also need to set up Kubernetes role-based access control (RBAC) for data
scientist users to use the SageMaker HyperPod CLI to run workloads on HyperPod clusters
orchestrated with Amazon EKS.

Option 1: Set up RBAC using Helm chart

The SageMaker HyperPod service team provides a Helm sub-chart for setting up RBAC. To learn
more, see the section called “Installing packages”.

Option 2: Set up RBAC manually

Create ClusterRole and ClusterRoleBinding with the minimum privilege, and create Role

and RoleBinding with mutation permissions.

To create ClusterRole & ClusterRoleBinding for data scientist IAM role

Create a cluster-level conﬁguration ﬁle cluster_level_config.yaml as follows.

kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1

Amazon EKS orchestration
1765

## Page 795

Amazon SageMaker AI
Developer Guide

metadata:
name: hyperpod-scientist-user-cluster-role
rules:
- apiGroups: [""]
resources: ["pods"]
verbs: ["list"]
- apiGroups: [""]
resources: ["nodes"]
verbs: ["list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
name: hyperpod-scientist-user-cluster-role-binding
subjects:
- kind: Group
name: hyperpod-scientist-user-cluster-level

apiGroup: rbac.authorization.k8s.io
roleRef:
kind: ClusterRole
name: hyperpod-scientist-user-cluster-role # this must match the name of the Role or
ClusterRole you wish to bind to
apiGroup: rbac.authorization.k8s.io

Apply the conﬁguration to the EKS cluster.

kubectl apply -f cluster_level_config.yaml

To create Role and RoleBinding in namespace

This is the namespace training operator that run training jobs and Resiliency will monitor by

default. Job auto-resume can only support in kubeflow namespace or namespace preﬁxed aws-

hyperpod.

Create a role conﬁguration ﬁle namespace_level_role.yaml as follows. This example creates a

role in the kubeflow namespace

kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
namespace: kubeflow
name: hyperpod-scientist-user-namespace-level-role
###

Amazon EKS orchestration
1766

## Page 796

Amazon SageMaker AI
Developer Guide

#  1) add/list/describe/delete pods
#  2) get/list/watch/create/patch/update/delete/describe kubeflow pytroch job
#  3) get pod log
###
rules:
- apiGroups: [""]
resources: ["pods"]
verbs: ["create", "get"]
- apiGroups: [""]
resources: ["nodes"]
verbs: ["get", "list"]
- apiGroups: [""]
resources: ["pods/log"]
verbs: ["get", "list"]
- apiGroups: [""]
resources: ["pods/exec"]
verbs: ["get", "create"]

- apiGroups: ["kubeflow.org"]
resources: ["pytorchjobs", "pytorchjobs/status"]
verbs: ["get", "list", "create", "delete", "update", "describe"]
- apiGroups: [""]
resources: ["configmaps"]
verbs: ["create", "update", "get", "list", "delete"]
- apiGroups: [""]
resources: ["secrets"]
verbs: ["create", "get", "list", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
namespace: kubeflow
name: hyperpod-scientist-user-namespace-level-role-binding
subjects:
- kind: Group
name: hyperpod-scientist-user-namespace-level
apiGroup: rbac.authorization.k8s.io
roleRef:
kind: Role
name: hyperpod-scientist-user-namespace-level-role # this must match the name of the
Role or ClusterRole you wish to bind to
apiGroup: rbac.authorization.k8s.io

Apply the conﬁguration to the EKS cluster.

Amazon EKS orchestration
1767

## Page 797

Amazon SageMaker AI
Developer Guide

kubectl apply -f namespace_level_role.yaml

Create an access entry for Kubernetes groups

After you have set up RBAC using one of the two options above, use the following sample
command replacing the necessary information.

aws eks create-access-entry \
--cluster-name <eks-cluster-name> \
--principal-arn arn:aws:iam::<AWS_ACCOUNT_ID_SCIENTIST_USER>:role/ScientistUserRole
\
--kubernetes-groups '["hyperpod-scientist-user-namespace-level","hyperpod-
scientist-user-cluster-level"]'

For the principal-arn parameter, you need to use the the section called “IAM users for

scientists”.

Custom Amazon Machine Images (AMIs) for SageMaker HyperPod clusters

Using base Amazon Machine Images (AMIs) provided and made public by Amazon SageMaker
HyperPod, you can build custom AMIs. With a custom AMI, you can create specialized environments
for AI workloads with pre-conﬁgured software stacks, driver customizations, proprietary
dependencies, and security agents. This capability eliminates the need for complex post-launch
bootstrapping using lifecycle conﬁguration scripts.

With custom AMIs, you can standardize environments across diﬀerent stages, accelerate startup
times, and have full control over your runtime environment while leveraging SageMaker
HyperPod's infrastructure capabilities and scaling advantages. This helps you maintain control over
your AI infrastructure while still beneﬁting from SageMaker HyperPod's optimized base runtime.

You can build upon the SageMaker HyperPod performance-tuned base images by adding
security agents, compliance tools, and specialized libraries while preserving all the distributed
training beneﬁts. This capability removes the previously required choice between infrastructure
optimization and organizational security policies.

The custom AMI experience integrates seamlessly with established enterprise security workﬂows.
Security teams build hardened images using SageMaker HyperPod's public AMIs as a base, and AI
platform teams can specify these custom AMIs when creating or updating clusters through the
SageMaker HyperPod APIs. The APIs validate image compatibility, handle necessary permissions,
and maintain backwards compatibility so existing workﬂows continue functioning. Organizations

Amazon EKS orchestration
1768

## Page 798

Amazon SageMaker AI
Developer Guide

with stringent security protocols can eliminate the error-prone alternative of installing security
agents at runtime through lifecycle scripts. By aligning with enterprise security practices rather
than forcing organizations to adapt their protocols to SageMaker HyperPod's limitations, custom
AMIs remove a common barrier to adoption for security-conscious organizations running critical AI
workloads.

For release notes on updates to the public AMIs, see Public AMI releases. To learn how to get
started with building a custom AMI and using it in your HyperPod clusters, see the following topics.

Topics

• Build a custom AMI

• Cluster management with custom AMIs

Build a custom AMI

The following page explains how to build a custom Amazon Machine Image (AMI) using Amazon
SageMaker HyperPod base AMIs. You begin by selecting a base AMI, and then you create your own
customized AMI using any of the common methods for creating new images, such as the AWS CLI.

Select a SageMaker HyperPod base AMI

You can select a SageMaker HyperPod base AMI through one of the following methods.

AWS console selection

You can select public SageMaker HyperPod AMIs through the AWS console or by using the

DescribeImages API call. SageMaker HyperPod AMIs are public and visible in every AWS account.
You can ﬁnd them in the Amazon EC2 AMI catalog by applying a ﬁlter to search for public AMIs
owned by Amazon.

To ﬁnd SageMaker HyperPod AMIs in the console:

1. Sign in to the Amazon EC2 console.

2. In the left navigation pane, choose AMIs.

3. For the Image type dropdown, select Public images.

4. In the search bar ﬁlters, set the Owner alias ﬁlter to amazon.

5. Search for AMIs preﬁxed as HyperPod EKS and select the AMI (preferably latest) that works for

your use case. For instance, you can choose an AMI between Kubernetes 1.31 versus Kubernetes
1.30.

Amazon EKS orchestration
1769

## Page 799

Amazon SageMaker AI
Developer Guide

Fetch latest public AMI ID through the AWS CLI

If you want to always use the latest release public AMI, it is more eﬃcient to use the public
SageMaker HyperPod SSM parameter that contains the value of the latest AMI ID released by
SageMaker HyperPod.

The following example shows how to retrieve the latest AMI ID using the AWS CLI:

aws ssm get-parameter \
--name "/aws/service/sagemaker-hyperpod/ami/x86_64/eks-1.31-amazon-linux-2/latest/
ami-id" \
--region us-east-1 \
--query "Parameter.Value" \
--output text

Note

Replace the parameter name with the corresponding Kubernetes version as required.

For example, if you want to use Kubernetes 1.30, use the following parameter: /aws/

service/hyperpod/ami/x86_64/eks-1.30-amazon-linux-2/latest/ami-id.

Build your custom AMI

After you have selected a SageMaker HyperPod public AMI, use that as the base AMI to build your
own custom AMI with one of the following methods. Note that this is not an exhaustive list for
building AMIs. You can use any method of your choice for building AMIs. SageMaker HyperPod does
not have any speciﬁc recommendation.

• AWS Management Console: You can launch an Amazon EC2 instance using the SageMaker
HyperPod AMI, make desired customizations, and then create an AMI from that instance.

• AWS CLI: You can also use the aws ec2 create-image command to create an AMI from an
existing Amazon EC2 instance after performing the customization.

• HashiCorp Packer: Packer is an open-source tool from HashiCorp that enables you to create
identical machine images for multiple platforms from a single source conﬁguration. It supports
creating AMIs for AWS, as well as images for other cloud providers and virtualization platforms.

• Image Builder: EC2 Image Builder is a fully managed AWS service that makes it easier to
automate the creation, maintenance, validation, sharing, and deployment of Linux or Windows
Server images. For more information, see the EC2 Image Builder User Guide.

Amazon EKS orchestration
1770

## Page 800

Amazon SageMaker AI
Developer Guide

Build a custom AMI with customer managed AWS KMS encryption

The following sections describe how to build a custom AMI with a customer managed AWS KMS
key to encrypt your HyperPod cluster volumes. For more information about customer managed
keys in HyperPod and granting the required IAM and KMS key policy permissions, see Customer
managed AWS KMS key encryption for SageMaker HyperPod. If you plan to use a custom AMI that
is encrypted with a customer managed key, ensure that you also encrypt your HyperPod cluster's
Amazon EBS root volume with the same key.

AWS CLI example: Create a new AMI using EC2 Image Builder and a HyperPod base image

The following example shows how to create an AMI using Image Builder with AWS KMS encryption:

aws imagebuilder create-image-recipe \
name "hyperpod-custom-recipe" \
version "1.0.0" \
parent-image "<hyperpod-base-image-id>" \
block-device-mappings DeviceName="/dev/
xvda",Ebs={VolumeSize=100,VolumeType=gp3,Encrypted=true,KmsKeyId=arn:aws:kms:us-
east-1:111122223333:key/key-id,DeleteOnTermination=true}

Amazon EC2 console: Create a new AMI from an Amazon EC2

To create an AMI from an Amazon EC2 instance using the Amazon EC2 console:

1. Right-click on your customized Amazon EC2 instance and choose Create Image.

2. In the Encryption section, select Encrypt snapshots.

3. Select your KMS key from the dropdown. For example: arn:aws:kms:us-

east-2:111122223333:key/<your-kms-key-id> or use the key alias: alias/<your-

hyperpod-key>.

AWS CLI example: Create a new AMI from an Amazon EC2 instance

Use the aws ec2 create-image command with AWS KMS encryption:

aws ec2 create-image \
instance-id "<instance-id>" \
name "MyCustomHyperPodAMI" \
description "Custom HyperPod AMI" \
block-device-mappings '[
{

Amazon EKS orchestration
1771

## Page 801

Amazon SageMaker AI
Developer Guide

"DeviceName": "/dev/xvda",
"Ebs": {
"Encrypted": true,
"KmsKeyId": "arn:aws:kms:us-east-1:111122223333:key/key-id",
"VolumeType": "gp2"
}
}
]'

Cluster management with custom AMIs

After the custom AMI is built, you can use it for creating or updating an Amazon SageMaker
HyperPod cluster. You can also scale up or add instance groups that use the new AMI.

Permissions required for cluster operations

Add the following permissions to the cluster admin user who operates and conﬁgures SageMaker
HyperPod clusters. The following policy example includes the minimum set of permissions for
cluster administrators to run the SageMaker HyperPod core APIs and manage SageMaker HyperPod
clusters with custom AMI.

Note that AMI and AMI EBS snapshot sharing permissions are included through

ModifyImageAttribute and ModifySnapshotAttribute API permissions as part of the
following policy. For scoping down the sharing permissions, you can take the following steps:

• Add tags to control the AMI sharing permissions to AMI and AMI snapshot. For example, you can

tag the AMI with AllowSharing as true.

• Add the context key in the policy to only allow AMI sharing for AMIs tagged with certain tags.

The following policy is a scoped down policy to ensure only AMIs tagged with AllowSharing as

true are allowed.

JSON

{
"Version":"2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Action": "iam:PassRole",

Amazon EKS orchestration
1772

## Page 802

Amazon SageMaker AI
Developer Guide

"Resource": "arn:aws:iam::111122223333:role/your-execution-role-name"
},
{
"Effect": "Allow",
"Action": [
"sagemaker:CreateCluster",
"sagemaker:DeleteCluster",
"sagemaker:DescribeCluster",
"sagemaker:DescribeClusterNode",
"sagemaker:ListClusterNodes",
"sagemaker:ListClusters",
"sagemaker:UpdateCluster",
"sagemaker:UpdateClusterSoftware",
"sagemaker:BatchDeleteClusterNodes",
"eks:DescribeCluster",
"eks:CreateAccessEntry",
"eks:DescribeAccessEntry",

"eks:DeleteAccessEntry",
"eks:AssociateAccessPolicy",
"iam:CreateServiceLinkedRole",
"ec2:DescribeImages",
"ec2:DescribeSnapshots"
],
"Resource": "*"
},
{
"Effect": "Allow",
"Action": [
"ec2:ModifyImageAttribute",
"ec2:ModifySnapshotAttribute"
],
"Resource": "*",
"Condition": {
"StringEquals": {
"ec2:ResourceTag/AllowSharing": "true"
}
}
}
]
}

Amazon EKS orchestration
1773

## Page 803

Amazon SageMaker AI
Developer Guide

Important

If you plan to use an encrypted custom AMI, then make sure that your KMS key meets
the permissions described in Customer managed AWS KMS key encryption for SageMaker
HyperPod. Additionally, ensure that your custom AMI's KMS key is also used to encrypt your
cluster's Amazon EBS root volume.

Create a cluster

You can specify your custom AMI in the ImageId ﬁeld for the CreateCluster operation.

The following examples show how to create a cluster with a custom AMI, both with and without an
AWS KMS customer managed key for encrypting the cluster volumes.

Standard example

The following example shows how to create a cluster with a custom AMI.

aws sagemaker create-cluster \
--cluster-name <exampleClusterName> \
--orchestrator 'Eks={ClusterArn='<eks_cluster_arn>'}' \
--node-provisioning-mode Continuous \
--instance-groups '{
"InstanceGroupName": "<exampleGroupName>",
"InstanceType": "ml.c5.2xlarge",
"InstanceCount": 2,
"LifeCycleConfig": {
"SourceS3Uri": "<s3://amzn-s3-demo-bucket>",
"OnCreate": "on_create_noop.sh"
},
"ImageId": "<your_custom_ami>",
"ExecutionRole": "<arn:aws:iam::444455556666:role/Admin>",
"ThreadsPerCore": 1,
"InstanceStorageConfigs": [
{
"EbsVolumeConfig": {
"VolumeSizeInGB": 200
}
}
]

Amazon EKS orchestration
1774

## Page 804

Amazon SageMaker AI
Developer Guide

}' --vpc-config '{
"SecurityGroupIds": ["<security_group>"],
"Subnets": ["<subnet>"]
}'

Customer managed key example

The following example shows how to create a cluster with a custom AMI while specifying your
own AWS KMS customer managed key for encrypting the cluster's Amazon EBS volumes. It
is possible to specify diﬀerent customer managed keys for the root volume and the instance

storage volume. If you don't use customer managed keys in the InstanceStorageConfigs
ﬁeld, then an AWS owned KMS key is used to encrypt the volumes. If you use diﬀerent keys
for the root volume and secondary instance storage volumes, then set the required KMS key
policies on both of your keys.

aws sagemaker create-cluster \
--cluster-name <exampleClusterName> \
--orchestrator 'Eks={ClusterArn='<eks_cluster_arn>'}' \
--node-provisioning-mode Continuous \
--instance-groups '{
"InstanceGroupName": "<exampleGroupName>",
"InstanceType": "ml.c5.2xlarge",
"InstanceCount": 2,
"LifeCycleConfig": {
"SourceS3Uri": "<s3://amzn-s3-demo-bucket>",
"OnCreate": "on_create_noop.sh"
},
"ImageId": "<your_custom_ami>",
"ExecutionRole": "<arn:aws:iam:us-east-1:444455556666:role/Admin>",
"ThreadsPerCore": 1,
"InstanceStorageConfigs": [
# Root volume configuration
{
"EbsVolumeConfig": {
"RootVolume": True,
"VolumeKmsKeyId": "arn:aws:kms:us-east-1:111122223333:key/key-
id"
}
},
# Instance storage volume configuration
{
"EbsVolumeConfig": {

Amazon EKS orchestration
1775

## Page 805

Amazon SageMaker AI
Developer Guide

"VolumeSizeInGB": 100,
"VolumeKmsKeyId": "arn:aws:kms:us-east-1:111122223333:key/key-
id"
}
}
]
}' --vpc-config '{
"SecurityGroupIds": ["<security_group>"],
"Subnets": ["<subnet>"]
}'

Update the cluster software

If you want to update an existing instance group on your cluster with your custom AMI, you can use

the UpdateClusterSoftware operation and specify your custom AMI in the ImageId ﬁeld. Note

that unless you specify the name of a speciﬁc instance group in your request, then the new image
is applied to all of the instance groups in your cluster.

The following example shows how to update a cluster's platform software with a custom AMI:

aws sagemaker update-cluster-software \
--cluster-name <exampleClusterName> \
--instance-groups <instanceGroupToUpdate> \
--image-id <customAmiId>

Scale up an instance group

The following examples show how to scale up an instance group for a cluster using a custom AMI,
both with and without using an AWS KMS customer managed key for encryption.

Standard example

The following example shows how to scale up an instance group with a custom AMI.

aws sagemaker update-cluster \
--cluster-name <exampleClusterName> --instance-groups '[{
"InstanceGroupName": "<exampleGroupName>",
"InstanceType": "ml.c5.2xlarge",
"InstanceCount": 2,
"LifeCycleConfig": {
"SourceS3Uri": "<s3://amzn-s3-demo-bucket>",

Amazon EKS orchestration
1776

## Page 806

Amazon SageMaker AI
Developer Guide

"OnCreate": "on_create_noop.sh"
},
"ExecutionRole": "<arn:aws:iam::444455556666:role/Admin>",
"ThreadsPerCore": 1,
"ImageId": "<your_custom_ami>"
}]'

Customer managed key example

The following example shows how to update and scale up your cluster with a custom AMI
while specifying your own AWS KMS customer managed key for encrypting the cluster's
Amazon EBS volumes. It is possible to specify diﬀerent customer managed keys for the
root volume and the instance storage volume. If you don't use customer managed keys in

the InstanceStorageConfigs ﬁeld, then an AWS owned KMS key is used to encrypt the
volumes. If you use diﬀerent keys for the root volume and secondary instance storage volumes,
then set the required KMS key policies on both of your keys.

aws sagemaker update-cluster \
--cluster-name <exampleClusterName> --instance-groups '[{
"InstanceGroupName": "<exampleGroupName>",
"InstanceType": "ml.c5.2xlarge",
"InstanceCount": 2,
"LifeCycleConfig": {
"SourceS3Uri": "<s3://amzn-s3-demo-bucket>",
"OnCreate": "on_create_noop.sh"
},
"ExecutionRole": "<arn:aws:iam::444455556666:role/Admin>",
"ThreadsPerCore": 1,
"ImageId": "<your_custom_ami>",
"InstanceStorageConfigs": [
# Root volume configuration
{
"EbsVolumeConfig": {
"RootVolume": True,
"VolumeKmsKeyId": "arn:aws:kms:us-east-1:111122223333:key/key-
id"
}
},
# Instance storage volume configuration
{
"EbsVolumeConfig": {
"VolumeSizeInGB": 100,

Amazon EKS orchestration
1777

## Page 807

Amazon SageMaker AI
Developer Guide

"VolumeKmsKeyId": "arn:aws:kms:us-east-1:111122223333:key/key-
id"
}
}
]
}]'

Add an instance group

The following example shows how to add an instance group to a cluster using a custom AMI:

aws sagemaker update-cluster \
--cluster-name "<exampleClusterName>" \
--instance-groups '{
"InstanceGroupName": "<exampleGroupName>",
"InstanceType": "ml.c5.2xlarge",
"InstanceCount": 2,
"LifeCycleConfig": {
"SourceS3Uri": "<s3://amzn-s3-demo-bucket>",
"OnCreate": "on_create_noop.sh"
},
"ExecutionRole": "<arn:aws:iam::444455556666:role/Admin>",
"ThreadsPerCore": 1,
"ImageId": "<your_custom_ami>"
}' '{
"InstanceGroupName": "<exampleGroupName2>",
"InstanceType": "ml.c5.2xlarge",
"InstanceCount": 1,
"LifeCycleConfig": {
"SourceS3Uri": "<s3://amzn-s3-demo-bucket>",
"OnCreate": "on_create_noop.sh"
},
"ExecutionRole": "<arn:aws:iam::444455556666:role/Admin>",
"ThreadsPerCore": 1,
"ImageId": "<your_custom_ami>"
}'

Managing SageMaker HyperPod EKS clusters using the SageMaker console

The following topics provide guidance on how to manage SageMaker HyperPod in the SageMaker
AI console.

Amazon EKS orchestration
1778

## Page 808

Amazon SageMaker AI
Developer Guide

Topics

• Creating a SageMaker HyperPod cluster with Amazon EKS orchestration

• Browsing, viewing, and editing SageMaker HyperPod clusters

• Deleting a SageMaker HyperPod cluster

Creating a SageMaker HyperPod cluster with Amazon EKS orchestration

The following tutorial demonstrates how to create a new SageMaker HyperPod cluster and set it up
with Amazon EKS orchestration through the SageMaker AI console UI.

In this topic:

• Create cluster

• Deploy resources

Create cluster

To navigate to the SageMaker HyperPod Clusters page and choose Amazon EKS orchestration,
follow these steps.

1.
Open the Amazon SageMaker AI console at https://console.aws.amazon.com/sagemaker/.

2.
Choose HyperPod Clusters in the left navigation pane and then Cluster Management.

3.
On the SageMaker HyperPod Clusters page, choose Create HyperPod cluster.

4.
On the Create HyperPod cluster drop-down, choose Orchestrated by Amazon EKS.

5.
On the EKS cluster creation page, you will see two options, choose the option that best ﬁts
your needs.

a.
Quick setup - To get started immediately with default settings, choose Quick setup. With
this option, SageMaker AI will create new resources such as VPC, subnets, security groups,
Amazon S3 bucket, IAM role, and FSx for Lustre in the process of creating your cluster.

b.
Custom setup - To integrate with existing AWS resources or have speciﬁc networking,
security, or storage requirements, choose Custom setup. With this option, you can choose
to use the existing resources or create new ones, and you can customize the conﬁguration
that best ﬁts your needs.

Amazon EKS orchestration
1779

## Page 809

Amazon SageMaker AI
Developer Guide

Quick setup

On the Quick setup section, follow these steps to create your HyperPod cluster with Amazon EKS
orchestration.

General settings

Specify a name for the new cluster. You can’t change the name after the cluster is created.

Instance groups

To add an instance group, choose Add group. Each instance group can be conﬁgured diﬀerently,
and you can create a heterogeneous cluster that consists of multiple instance groups with various
instance types. To deploy a cluster, you must add at least one instance group. Follow these steps to
add an instance group.

1.
For Instance group type, choose Standard or Restricted Instance Group (RIG). Typically, you

will choose Standard, which provides a general purpose computing environment without
additional security restrictions. Restricted Instance Group (RIG) is a specialized environment
for foundational models customization such as Amazon Nova. For more information about
setting up RIG for Amazon Nova model customization, see ???.

2.
For Name, specify a name for the instance group.

3.
For Instance capacity, choose either on-demand capacity or a training plan to reserve your
compute resources.

4.
For Instance type, choose the instance for the instance group.

Important

Ensure that you choose an instance type with suﬃcient quotas and enough unassigned
IP addresses for your account. To view or request additional quotas, see the section
called “SageMaker HyperPod quotas”.

5.
For Instance quantity, specify an integer not exceeding the instance quota for cluster usage.
For this tutorial, enter 1 for all three groups.

6.
For Target Availability Zone, choose the Availability Zone where your instances will be
provisioned. The Availability Zone should correspond to the location of your accelerated
compute capacity.

7.
For Additional storage volume per instance (GB) - optional, specify an integer between 1 and
16384 to set the size of an additional Elastic Block Store (EBS) volume in gigabytes (GB). The

Amazon EKS orchestration
1780

## Page 810

Amazon SageMaker AI
Developer Guide

EBS volume is attached to each instance of the instance group. The default mount path for

the additional EBS volume is /opt/sagemaker. After the cluster is successfully created, you
can SSH into the cluster instances (nodes) and verify if the EBS volume is mounted correctly

by running the df -h command. Attaching an additional EBS volume provides stable, oﬀ-

instance, and independently persisting storage, as described in the Amazon EBS volumes
section in the Amazon Elastic Block Store User Guide.

8.
For Instance deep health checks, choose your option. Deep health checks monitor instance
health during creation and after software updates, automatically recovering faulty instances
through reboots or replacements when enabled.

9.
If your instance type supports GPU partitioning with Multi-Instance GPU (MIG), you can enable
GPU partition conﬁguration for the instance group. GPU partitioning allows you to divide
GPUs into smaller, isolated partitions for improved resource utilization. For more information,
see Using GPU partitions in Amazon SageMaker HyperPod.

a.
Toggle Use GPU partition to enable GPU partitioning for this instance group.

b.
Select a GPU partition proﬁle from the available options for your instance type. Each
proﬁle deﬁnes the GPU slice conﬁguration and memory allocation.

10. Choose Add instance group.

Quick setup defaults

This section lists all the default settings for your cluster creation, including all the new AWS
resources that will be created during the cluster creation process. Review the default settings.

Custom setup

On the Custom setup section, follow these steps to create your ﬁrst HyperPod cluster with Amazon
EKS orchestration.

General settings

Specify a name for the new cluster. You can’t change the name after the cluster is created.

For Instance recovery, choose Automatic - recommended or None.

Amazon EKS orchestration
1781

## Page 811

Amazon SageMaker AI
Developer Guide

Networking

Conﬁgure network settings within the cluster and in-and-out of the cluster. For orchestration of
SageMaker HyperPod cluster with Amazon EKS, the VPC is automatically set to the one conﬁgured
with the EKS cluster you selected.

1.
For VPC, choose your own VPC if you already have one that gives SageMaker AI access to your
VPC. To create a new VPC, follow the instructions at Create a VPC in the Amazon Virtual Private
Cloud User Guide. You can leave it as None to use the default SageMaker AI VPC.

2.
For VPC IPv4 CIDR block, enter the starting IP of your VPC.

3.
For Availability Zones, choose the Availability Zones (AZ) where HyperPod will create subnets
for your cluster. Choose AZs that match the location of your accelerated compute capacity.

4.
For Security group(s), choose security groups that are either attached to the Amazon EKS
cluster or whose inbound traﬃc is permitted by the security group associated with the Amazon
EKS cluster. To create new security groups, go to the Amazon VPC console.

Orchestration

Follow these steps to create or select an Amazon EKS cluster to use as an orchestrator.

1.
For EKS cluster, choose either create a new Amazon EKS cluster or use an existing one.

If you need to create a new EKS cluster, you can create it from the EKS cluster section without
having to open the Amazon EKS console.

Note

The VPC subnet you choose for HyperPod has to be private.
After submitting a new EKS cluster creation request, wait until the EKS cluster

becomes Active.

2.
For Kubernetes version, choose a version from the drop-down menu. For more information
about Kubernetes versions, see Understand the Kubernetes version lifecycle on EKS from the
Amazon EKS User Guide.

3.
For Operators, choose Use default Helm charts and add-ons or Don't install operators.
The option defaults to Use default Helm charts and add-ons, which will be used to install
operators on the EKS cluster. For more information about the default Helm charts and add-

Amazon EKS orchestration
1782

## Page 812

Amazon SageMaker AI
Developer Guide

ons, see helm_chart from the GitHub repository. For more information, see the section called
“Installing packages”.

4.
For Enabled operators, view the list of enabled operators. To edit the operators, uncheck the
box at top and choose operators to enable for the EKS cluster.

Note

To use HyperPod with EKS, you must install Helm charts and add-ons that enable
operators on the EKS cluster. These components conﬁgure EKS as the control plane
for HyperPod and provide the necessary setup for workload management and
orchestration.

Instance groups

To add an instance group, choose Add group. Each instance group can be conﬁgured diﬀerently,
and you can create a heterogeneous cluster that consists of multiple instance groups with various
instance types. To deploy a cluster, you must add at least one instance group. Follow these steps to
add an instance group.

1.
For Instance group type, choose Standard or Restricted Instance Group (RIG). Typically, you
will choose Standard, which provides a general purpose computing environment without
additional security restrictions. Restricted Instance Group (RIG) is a specialized environment
for foundational models customization such as Amazon Nova. For more information about
setting up RIG for Amazon Nova model customization, see ???.

2.
For Name, specify a name for the instance group.

3.
For Instance capacity, choose either on-demand capacity or a training plan to reserve your
compute resources.

4.
For Instance type, choose the instance for the instance group.

Important

Ensure that you choose an instance type with suﬃcient quotas and enough unassigned
IP addresses for your account. To view or request additional quotas, see the section
called “SageMaker HyperPod quotas”.

Amazon EKS orchestration
1783

## Page 813

Amazon SageMaker AI
Developer Guide

5.
For Instance quantity, specify an integer not exceeding the instance quota for cluster usage.
For this tutorial, enter 1 for all three groups.

6.
For Target Availability Zone, choose the Availability Zone where your instances will be
provisioned. The Availability Zone should correspond to the location of your accelerated
compute capacity.

7.
For Additional storage volume per instance (GB) - optional, specify an integer between 1 and
16384 to set the size of an additional Elastic Block Store (EBS) volume in gigabytes (GB). The
EBS volume is attached to each instance of the instance group. The default mount path for

the additional EBS volume is /opt/sagemaker. After the cluster is successfully created, you
can SSH into the cluster instances (nodes) and verify if the EBS volume is mounted correctly

by running the df -h command. Attaching an additional EBS volume provides stable, oﬀ-
instance, and independently persisting storage, as described in the Amazon EBS volumes
section in the Amazon Elastic Block Store User Guide.

8.
For Instance deep health checks, choose your option. Deep health checks monitor instance
health during creation and after software updates, automatically recovering faulty instances
through reboots or replacements when enabled. To learn more, see the section called “Deep
health checks”

9.
For Use GPU partition - optional, if your instance type supports GPU partitioning with Multi-
Instance GPU (MIG), you can enable this option to conﬁgure the GPU partition proﬁle for the
instance group. GPU partitioning allows you to divide GPUs into smaller, isolated partitions
for improved resource utilization. For more information, see Using GPU partitions in Amazon
SageMaker HyperPod.

a.
Toggle Use GPU partition to enable GPU partitioning for this instance group.

b.
Select a GPU partition proﬁle from the available options for your instance type. Each
proﬁle deﬁnes the GPU slice conﬁguration and memory allocation.

10. Choose Add instance group.

Lifecycle scripts

You can choose to use the default lifecycle scripts or the custom lifecycle scripts, which will be
stored in your Amazon S3 bucket. You can view the default lifecycle scripts in the Awesome
Distributed Training GitHub repository. To learn more about the lifecycle scripts, see the section
called “Lifecycle scripts”.

1.
For Lifecycle scripts, choose to use default or custom lifecycle scripts.

Amazon EKS orchestration
1784

## Page 814

Amazon SageMaker AI
Developer Guide

2.
For S3 bucket for lifecycle scripts, choose to create a new bucket or use an existing bucket to
store the lifecycle scripts.

Permissions

Choose or create an IAM role that allows HyperPod to run and access necessary AWS resources on
your behalf. For more information, see the section called “IAM role for SageMaker HyperPod”.

Storage

Conﬁgure the FSx for Lustre ﬁle system to be provisioned on the HyperPod cluster.

1.
For File system, choose an existing FSx for Lustre ﬁle system, to create a new FSx for Lustre
ﬁle system, or don't provision an FSx for Lustre ﬁle system.

2.
For Throughput per unit of storage, choose the throughput that will be available per TiB of
provisioned storage.

3.
For Storage capacity, enter a capacity value in TB.

4.
For Data compression type, choose LZ4 to enable data compression.

5.
For Lustre version, view the value that's recommended for the new ﬁle systems.

Tags - optional

For Tags - optional, add key and value pairs to the new cluster and manage the cluster as an AWS
resource. To learn more, see Tagging your AWS resources.

Deploy resources

After you complete the cluster conﬁgurations using either Quick setup or Custom setup, choose
the following option to start resource provisioning and cluster creation.

• Submit - SageMaker AI will start provisioning the default conﬁguration resources and creating
the cluster.

• Download CloudFormation template parameters - You will download the conﬁguration
parameter JSON ﬁle and run AWS CLI command to deploy the CloudFormation stack to provision
the conﬁguration resources and creating the cluster. You can edit the downloaded parameter
JSON ﬁle if needed. If you choose this option, see more instructions in the section called “
CloudFormation”.

Amazon EKS orchestration
1785

## Page 815

Amazon SageMaker AI
Developer Guide

Browsing, viewing, and editing SageMaker HyperPod clusters

Use the following instructions to browse, view, and edit SageMaker HyperPod clusters orchestrated
by Amazon EKS in the SageMaker AI console.

Topics

• To browse your SageMaker HyperPod clusters

• To view details of each SageMaker HyperPod cluster

• To edit a SageMaker HyperPod cluster

To browse your SageMaker HyperPod clusters

Under Clusters on the SageMaker HyperPod page in the SageMaker AI console, all created clusters
should be listed under the Clusters section, which provides a summary view of clusters, their ARNs,
status, and creation time.

To view details of each SageMaker HyperPod cluster

Under Clusters on the SageMaker HyperPod page in the SageMaker AI console, the cluster names
are activated as links. Choose the cluster name link to see details of each cluster.

To edit a SageMaker HyperPod cluster

1. Under Clusters in the main pane of the SageMaker HyperPod console, choose the cluster you

want to update.

2. Select your cluster, and choose Edit.

3. In the Edit <your-cluster> page, you can edit the conﬁgurations of existing instance groups,

add more instance groups, delete instance groups, and change tags for the cluster. After making
changes, choose Submit.

a. In the Conﬁgure instance groups section, you can add more instance groups by choosing

Create instance group.

b. In the Conﬁgure instance groups section, you can choose Edit to change its conﬁguration or

Delete to remove the instance group permanently.

Important

When deleting an instance group, consider the following points:

Amazon EKS orchestration
1786

## Page 816

Amazon SageMaker AI
Developer Guide

• Your SageMaker HyperPod cluster must always maintain at least one instance
group.

• Ensure all critical data is backed up before removal.

• The removal process cannot be undone.

Note

Deleting an instance group will terminate all compute resources associated with that
group.

c. In the Tags section, you can update tags for the cluster.

Deleting a SageMaker HyperPod cluster

Use the following instructions to delete SageMaker HyperPod clusters orchestrated by Amazon EKS
in the SageMaker AI console.

1.
Under Clusters in the main pane of the SageMaker HyperPod console, choose the cluster you
want to delete.

2.
Select your cluster, and choose Delete.

3.
In the pop-up window for cluster deletion, review the cluster information carefully to conﬁrm
that you chose the right cluster to delete.

4.
After you reviewed the cluster information, choose Yes, delete cluster.

5.
In the text ﬁeld to conﬁrm this deletion, type delete.

6.
Choose Delete on the lower right corner of the pop-up window to ﬁnish sending the cluster
deletion request.

Note

When cluster deletion fails due to attached SageMaker HyperPod task governance policies,
you will need to Delete policies.

Amazon EKS orchestration
1787

## Page 817

Amazon SageMaker AI
Developer Guide

Creating SageMaker HyperPod clusters using CloudFormation templates

You can create SageMaker HyperPod clusters using the CloudFormation templates for HyperPod.
You must install AWS CLI to proceed.

In this topic:

• Conﬁgure resources in the console and deploy using CloudFormation

• Conﬁgure and deploy resources using CloudFormation

Conﬁgure resources in the console and deploy using CloudFormation

You can conﬁgure resources using the AWS Management Console and deploy using the
CloudFormation templates.

Follow these steps.

1.
Instead of choosing Submit, choose Download CloudFormation template parameters at the
end of the tutorial in ???. The tutorial contains important conﬁguration information you will
need to create your cluster successfully.

Important

If you choose Submit, you will not be able to deploy a cluster with the same name
until you delete the cluster.

After you choose Download CloudFormation template parameters, the Using the
conﬁguration ﬁle to create the cluster using the AWS CLI window will appear on the right
side of the page.

2.
On the Using the conﬁguration ﬁle to create the cluster using the AWS CLI window, choose
Download conﬁguration parameters ﬁle. The ﬁle will be downloaded to your machine. You
can edit the conﬁguration JSON ﬁle based on your needs or leave it as-is, if no change is
required.

3.
In the terminal, navigate to the location of the parameter ﬁle file://params.json.

4.
Run the create-stack AWS CLI command to deploy the CloudFormation stack that will
provision the conﬁgured resources and create the HyperPod cluster.

aws cloudformation create-stack

Amazon EKS orchestration
1788

## Page 818

Amazon SageMaker AI
Developer Guide

--stack-name my-stack
--template-url https://aws-sagemaker-hyperpod-cluster-setup.amazonaws.com/
templates-slurm/main-stack-slurm-based-template.yaml
--parameters file://params.json
--capabilities CAPABILITY_IAM CAPABILITY_NAMED_IAM

5.
To view the status of the resources provisioning, navigate to the CloudFormation console.

After the cluster creation completes, view the new cluster under Clusters in the main pane of
the SageMaker HyperPod console. You can check the status of it displayed under the Status
column.

6.
After the status of the cluster turns to InService, you can start logging into the cluster
nodes. To access the cluster nodes and start running ML workloads, see the section called “Jobs
on HyperPod clusters”.

Conﬁgure and deploy resources using CloudFormation

You can conﬁgure and deploy resources using the CloudFormation templates for SageMaker
HyperPod.

Follow these steps.

1.
Download a CloudFormation template for SageMaker HyperPod from the sagemaker-
hyperpod-cluster-setup GitHub repository.

2.
Run the create-stack AWS CLI command to deploy the CloudFormation stack that will
provision the conﬁgured resources and create the HyperPod cluster.

aws cloudformation create-stack
--stack-name my-stack
--template-url URL_of_the_file_that_contains_the_template_body
--parameters file://params.json
--capabilities CAPABILITY_IAM CAPABILITY_NAMED_IAM

3.
To view the status of the resources provisioning, navigate to the CloudFormation console.

After the cluster creation completes, view the new cluster under Clusters in the main pane of
the SageMaker HyperPod console. You can check the status of it displayed under the Status
column.

4.
After the status of the cluster turns to InService, you can start logging into the cluster
nodes.

Amazon EKS orchestration
1789

## Page 819

Amazon SageMaker AI
Developer Guide

Managing SageMaker HyperPod EKS clusters using the AWS CLI

The following topics provide guidance on writing SageMaker HyperPod API request ﬁles in JSON
format and run them using the AWS CLI commands.

Topics

• Creating a SageMaker HyperPod cluster

• Retrieving SageMaker HyperPod cluster details

• Updating SageMaker HyperPod cluster conﬁguration

• Updating the SageMaker HyperPod platform software

• Accessing SageMaker HyperPod cluster nodes

• Scaling down a SageMaker HyperPod cluster

• Deleting a SageMaker HyperPod cluster

Creating a SageMaker HyperPod cluster

Learn how to create SageMaker HyperPod clusters orchestrated by Amazon EKS using the AWS CLI.

1.
Before creating an SageMaker HyperPod cluster:

a.
Ensure that you have an existing Amazon EKS cluster up and running. For detailed
instructions about how to set up an Amazon EKS cluster, see Create an Amazon EKS
cluster in the Amazon EKS User Guide.

b.
Install the Helm chart as instructed in the section called “Installing packages”. If you
create a the section called “HP cluster setup”, you will need a separate Helm chart.

2.
Prepare a lifecycle conﬁguration script and upload to an Amazon S3 bucket, such as

s3://amzn-s3-demo-bucket/Lifecycle-scripts/base-config/.

For a quick start, download the sample script on_create.sh from the AWSome Distributed
Training GitHub repository, and upload it to the S3 bucket. You can also include additional
setup instructions, a series of setup scripts, or commands to be executed during the HyperPod
cluster provisioning stage.

Amazon EKS orchestration
1790

## Page 820

Amazon SageMaker AI
Developer Guide

Important

If you create an the section called “IAM role for SageMaker HyperPod” attaching only

the managed AmazonSageMakerClusterInstanceRolePolicy, your cluster has

access to Amazon S3 buckets with the speciﬁc preﬁx sagemaker-.

If you create a restricted instance group, you don't need to download and run the lifecycle

script. Instead, you need to run install_rig_dependencies.sh.

The prerequisites to run the install_rig_dependencies.sh script include:

• AWS Node (CNI) and CoreDNS should both be enabled. These are standard EKS add-ons that
are not managed by the standard SageMaker HyperPod Helm, but can be easily enabled in
the EKS console under Add-ons.

• The standard SageMaker HyperPod Helm chart should be installed before running this
script.

The install_rig_dependencies.sh script performs the following actions.

• aws-node (CNI): New rig-aws-node Daemonset created; existing aws-node patched to
avoid RIG nodes.

• coredns: Converted to Daemonset for RIGs to support multi-RIG use and prevent
overloading.

• training-operators: Updated with RIG Worker taint tolerations and nodeAﬃnity favoring
non-RIG instances.

• Elastic Fabric Adapter (EFA): Updated to tolerate RIG worker taint and use correct container
images for each Region.

3.
Prepare a CreateCluster API request ﬁle in JSON format. For ExecutionRole,
provide the ARN of the IAM role you created with the managed

AmazonSageMakerClusterInstanceRolePolicy from the section the section called “IAM
role for SageMaker HyperPod”.

Amazon EKS orchestration
1791

## Page 821

Amazon SageMaker AI
Developer Guide

Note

Ensure that your SageMaker HyperPod cluster is deployed within the same Virtual
Private Cloud (VPC) as your Amazon EKS cluster. The subnets and security groups
speciﬁed in the SageMaker HyperPod cluster conﬁguration must allow network
connectivity and communication with the Amazon EKS cluster's API server endpoint.

// create_cluster.json
{
"ClusterName": "string",
"InstanceGroups": [{
"InstanceGroupName": "string",
"InstanceType": "string",
"InstanceCount": number,
"LifeCycleConfig": {
"SourceS3Uri": "s3://amzn-s3-demo-bucket-sagemaker/lifecycle-script-
directory/src/",
"OnCreate": "on_create.sh"
},
"ExecutionRole": "string",
"ThreadsPerCore": number,
"OnStartDeepHealthChecks": [
"InstanceStress", "InstanceConnectivity"
]
}],
"RestrictedInstanceGroups": [
{
"EnvironmentConfig": {
"FSxLustreConfig": {
"PerUnitStorageThroughput": number,
"SizeInGiB": number
}
},
"ExecutionRole": "string",
"InstanceCount": number,
"InstanceGroupName": "string",
"InstanceStorageConfigs": [
{ ... }
],
"InstanceType": "string",

Amazon EKS orchestration
1792

## Page 822

Amazon SageMaker AI
Developer Guide

"OnStartDeepHealthChecks": [ "string" ],
"OverrideVpcConfig": {
"SecurityGroupIds": [ "string" ],
"Subnets": [ "string" ]
},
"ScheduledUpdateConfig": {
"DeploymentConfig": {
"AutoRollbackConfiguration": [
{
"AlarmName": "string"
}
],
"RollingUpdatePolicy": {
"MaximumBatchSize": {
"Type": "string",
"Value": number
},

"RollbackMaximumBatchSize": {
"Type": "string",
"Value": number
}
},
"WaitIntervalInSeconds": number
},
"ScheduleExpression": "string"
},
"ThreadsPerCore": number,
"TrainingPlanArn": "string"
}
],
"VpcConfig": {
"SecurityGroupIds": ["string"],
"Subnets": ["string"]
},
"Tags": [{
"Key": "string",
"Value": "string"
}],
"Orchestrator": {
"Eks": {
"ClusterArn": "string",
"KubernetesConfig": {
"Labels": {
"nvidia.com/mig.config": "all-3g.40gb"

Amazon EKS orchestration
1793

## Page 823

Amazon SageMaker AI
Developer Guide

}
}
}
},
"NodeRecovery": "Automatic"
}

Note the following when conﬁguring to create a new SageMaker HyperPod cluster associating
with an EKS cluster.

• You can conﬁgure up to 20 instance groups under the InstanceGroups parameter.

• For Orchestator.Eks.ClusterArn, specify the ARN of the EKS cluster you want to use as
the orchestrator.

• For OnStartDeepHealthChecks, add InstanceStress and InstanceConnectivity to
enable the section called “Deep health checks”.

• For NodeRecovery, specify Automatic to enable automatic node recovery. SageMaker
HyperPod replaces or reboots instances (nodes) when issues are found by the health-
monitoring agent.

• For the Tags parameter, you can add custom tags for managing the SageMaker HyperPod
cluster as an AWS resource. You can add tags to your cluster in the same way you add them
in other AWS services that support tagging. To learn more about tagging AWS resources in
general, see Tagging AWS Resources User Guide.

• For the VpcConfig parameter, specify the information of the VPC used in the EKS cluster.
The subnets must be private.

• For Orchestrator.Eks.KubernetesConfig.Labels, you can optionally specify
Kubernetes labels to apply to the nodes. To enable GPU partitioning with Multi-Instance

GPU (MIG), add the nvidia.com/mig.config label with the desired MIG proﬁle. For

example, "nvidia.com/mig.config": "all-3g.40gb" conﬁgures all GPUs with the
3g.40gb partition proﬁle. For more information about GPU partitioning and available
proﬁles, see Using GPU partitions in Amazon SageMaker HyperPod.

4.
Run the create-cluster command as follows.

Important

When running the create-cluster command with the --cli-input-json

parameter, you must include the file:// preﬁx before the complete path to the

Amazon EKS orchestration
1794

## Page 824

Amazon SageMaker AI
Developer Guide

JSON ﬁle. This preﬁx is required to ensure that the AWS CLI recognizes the input as a

ﬁle path. Omitting the file:// preﬁx results in a parsing parameter error.

aws sagemaker create-cluster \
--cli-input-json file://complete/path/to/create_cluster.json

This should return the ARN of the new cluster.

Important

You can use the update-cluster operation to remove a restricted instance group (RIG).
When a RIG is scaled down to 0, the FSx for Lustre ﬁle system won't be deleted. To
completely remove the FSx for Lustre ﬁle system, you must remove the RIG entirely.
Removing a RIG will not delete any artifacts stored in the service-managed Amazon
S3 bucket. However, you should ensure all artifacts in the FSx for Lustre ﬁle system are
fully synchronized to Amazon S3 before removal. We recommend waiting at least 30
minutes after job completion to ensure complete synchronization of all artifacts from
the FSx for Lustre ﬁle system to the service-managed Amazon S3 bucket.

Important

When using an onboarded On-Demand Capacity Reservation (ODCR), you must map
your instance group to the same Availability Zone ID (AZ ID) as the ODCR by setting

OverrideVpcConfig with a subnet in the matching AZ ID.

CRITICAL: Verify OverrideVpcConfig conﬁguration before deployment to avoid
incurring duplicate charges for both ODCR and On-Demand Capacity.

Retrieving SageMaker HyperPod cluster details

Learn how to retrieve SageMaker HyperPod cluster details using the AWS CLI.

Describe a cluster

Run describe-cluster to check the status of the cluster. You can specify either the name or the ARN
of the cluster.

Amazon EKS orchestration
1795

## Page 825

Amazon SageMaker AI
Developer Guide

aws sagemaker describe-cluster --cluster-name your-hyperpod-cluster

After the status of the cluster turns to InService, proceed to the next step. Using this API, you
can also retrieve failure messages from running other HyperPod API operations.

List details of cluster nodes

Run list-cluster-nodes to check the key information of the cluster nodes.

aws sagemaker list-cluster-nodes --cluster-name your-hyperpod-cluster

This returns a response, and the InstanceId is what you need to use for logging (using aws ssm)
into them.

Describe details of a cluster node

Run describe-cluster-node to retrieve details of a cluster node. You can get the cluster node ID
from list-cluster-nodes output. You can specify either the name or the ARN of the cluster.

aws sagemaker describe-cluster-node \
--cluster-name your-hyperpod-cluster \
--node-id i-111222333444555aa

List clusters

Run list-clusters to list all clusters in your account.

aws sagemaker list-clusters

You can also add additional ﬂags to ﬁlter the list of clusters down. To learn more about what this
command runs at low level and additional ﬂags for ﬁltering, see the ListClusters API reference.

Updating SageMaker HyperPod cluster conﬁguration

Run update-cluster to update the conﬁguration of a cluster.

Note

Important considerations:

Amazon EKS orchestration
1796

## Page 826

Amazon SageMaker AI
Developer Guide

• You cannot change the EKS cluster information that your HyperPod cluster is associated
after the cluster is created.

• If deep health checks are running on the cluster, this API will not function as expected.
You might encounter an error message stating that deep health checks are in progress.
To update the cluster, you should wait until the deep health checks ﬁnish.

1. Create an UpdateCluster API request ﬁle in JSON format. Make sure that you specify the right

cluster name and instance group name to update. For each instance group, you can change the
instance type, the number of instances, the lifecycle conﬁguration entrypoint script, and the
path to the script.

Note

You can use the UpdateCluster to scale down or remove entire instance groups from
your SageMaker HyperPod cluster. For additional instructions on how to scale down or
delete instance groups, see the section called “Scaling down a cluster”.

a. For ClusterName, specify the name of the cluster you want to update.

b. For InstanceGroupName

i. To update an existing instance group, specify the name of the instance group you want to

update.

ii. To add a new instance group, specify a new name not existing in your cluster.

c. For InstanceType

i. To update an existing instance group, you must match the instance type you initially

speciﬁed to the group.

ii. To add a new instance group, specify an instance type you want to conﬁgure the group

with.

d. For InstanceCount

i. To update an existing instance group, specify an integer that corresponds to your desired

number of instances. You can provide a higher or lower value (down to 0) to scale the
instance group up or down.

ii. To add a new instance group, specify an integer greater or equal to 1.

Amazon EKS orchestration
1797

## Page 827

Amazon SageMaker AI
Developer Guide

e. For LifeCycleConfig, you can change the values for both SourceS3Uri and OnCreate as

you want to update the instance group.

f. For ExecutionRole

i. For updating an existing instance group, keep using the same IAM role you attached during

cluster creation.

ii. For adding a new instance group, specify an IAM role you want to attach.

g. For ThreadsPerCore

i. For updating an existing instance group, keep using the same value you speciﬁed during

cluster creation.

ii. For adding a new instance group, you can choose any value from the allowed options per

instance type. For more information, search the instance type and see the Valid threads
per core column in the reference table at CPU cores and threads per CPU core per instance
type in the Amazon EC2 User Guide.

h. For OnStartDeepHealthChecks, add InstanceStress and InstanceConnectivity to

enable the section called “Deep health checks”.

i. For NodeRecovery, specify Automatic to enable automatic node recovery. SageMaker

HyperPod replaces or reboots instances (nodes) when issues are found by the health-
monitoring agent.

The following code snippet is a JSON request ﬁle template you can use. For more information
about the request syntax and parameters of this API, see the UpdateCluster API reference.

// update_cluster.json
{
// Required
"ClusterName": "name-of-cluster-to-update",
// Required
"InstanceGroups": [{
"InstanceGroupName": "string",
"InstanceType": "string",
"InstanceCount": number,
"LifeCycleConfig": {
"SourceS3Uri": "string",
"OnCreate": "string"
},
"ExecutionRole": "string",
"ThreadsPerCore": number,
"OnStartDeepHealthChecks": [

Amazon EKS orchestration
1798

## Page 828

Amazon SageMaker AI
Developer Guide

"InstanceStress", "InstanceConnectivity"
]
}],
"NodeRecovery": "Automatic"
}

2. Run the following update-cluster command to submit the request.

aws sagemaker update-cluster \
--cli-input-json file://complete/path/to/update_cluster.json

Updating the SageMaker HyperPod platform software

When you create your SageMaker HyperPod cluster, SageMaker HyperPod selects an Amazon
Machine Image (AMI) corresponding to the Kubernetes version of your Amazon EKS cluster.

Run update-cluster-software to update existing clusters with software and security patches

provided by the SageMaker HyperPod service. For --cluster-name, specify either the name or
the ARN of the cluster to update.

Important

• When this API is called, SageMaker HyperPod doesn’t drain or redistribute the jobs (Pods)
running on the nodes. Make sure to check if there are any jobs running on the nodes
before calling this API.

• The patching process replaces the root volume with the updated AMI, which means
that your previous data stored in the instance root volume will be lost. Make sure that
you back up your data from the instance root volume to Amazon S3 or Amazon FSx for
Lustre.

• All cluster nodes experience downtime (nodes appear as <NotReady> in the output

of kubectl get node) while the patching is in progress. We recommend that you
terminate all workloads before patching and resume them after the patch completes.

If the security patch fails, you can retrieve failure messages by running the

DescribeCluster API as instructed at the section called “Describe a cluster”.

Amazon EKS orchestration
1799

## Page 829

Amazon SageMaker AI
Developer Guide

aws sagemaker update-cluster-software --cluster-name your-hyperpod-cluster

When calling the UpdateClusterSoftware API, SageMaker HyperPod updates the Kubernetes
version of the nodes by selecting the latest the section called “SageMaker HyperPod DLAMI” based
on the Kubernetes version of your Amazon EKS cluster. It then runs the lifecycle scripts in the
Amazon S3 bucket that you speciﬁed during the cluster creation or update.

You can verify the kubelet version of a node by running the kubectl describe node command.

The Kubernetes version of SageMaker HyperPod cluster nodes does not automatically update
when you update your Amazon EKS cluster version. After updating the Kubernetes version for your

Amazon EKS cluster, you must use the UpdateClusterSoftware API to update your SageMaker
HyperPod cluster nodes to the same Kubernetes version.

It is recommended to update your SageMaker HyperPod cluster after updating your Amazon EKS
nodes, and avoid having more than one version diﬀerence between the Amazon EKS cluster version
and the SageMaker HyperPod cluster nodes version.

The SageMaker HyperPod service team regularly rolls out new the section called “SageMaker
HyperPod DLAMI”s for enhancing security and improving user experiences. We recommend
that you always keep updating to the latest SageMaker HyperPod DLAMI. For future SageMaker
HyperPod DLAMI updates for security patching, follow up with the section called “HyperPod
release notes”.

Note

You can only run this API programmatically. The patching functionality is not implemented
in the SageMaker HyperPod console UI.

Accessing SageMaker HyperPod cluster nodes

You can directly access the nodes of a SageMaker HyperPod cluster in service using the AWS CLI

commands for AWS Systems Manager (SSM). Run aws ssm start-session with the host name

of the node in format of sagemaker-cluster:[cluster-id]_[instance-group-name]-

[instance-id]. You can retrieve the cluster ID, the instance ID, and the instance group name

from the SageMaker HyperPod console or by running describe-cluster and list-cluster-

nodes from the AWS CLI commands for SageMaker HyperPod. For example, if your cluster ID

Amazon EKS orchestration
1800

## Page 830

Amazon SageMaker AI
Developer Guide

is aa11bbbbb222, the cluster node name is controller-group, and the cluster node ID is

i-111222333444555aa, the SSM start-session command should be the following.

Note

If you haven't set up AWS Systems Manager, follow the instructions provided at the section
called “Setting up AWS Systems Manager and Run As for cluster user access control”.

$ aws ssm start-session \
--target sagemaker-cluster:aa11bbbbb222_controller-group-i-111222333444555aa \
--region us-west-2
Starting session with SessionId: s0011223344aabbccdd
root@ip-111-22-333-444:/usr/bin#

Scaling down a SageMaker HyperPod cluster

You can scale down the number of instances running on your Amazon SageMaker HyperPod
cluster. You might want to scale down a cluster for various reasons, such as reduced resource
utilization or cost optimization.

The following page outlines two main approaches to scaling down:

• Scale down at the instance group level: This approach uses the UpdateCluster API, with
which you can:

• Scale down the instance counts for speciﬁc instance groups independently. SageMaker AI
handles the termination of nodes in a way that reaches the new target instance counts you've
set for each group. See the section called “Scale down an instance group”.

• Completely delete instance groups from your cluster. See the section called “Delete instance
groups”.

• Scale down at the instance level: This approach uses the BatchDeleteClusterNodes API,
with which you can specify the individual nodes you want to terminate. See the section called
“Scale down at the instance level”.

Amazon EKS orchestration
1801

## Page 831

Amazon SageMaker AI
Developer Guide

Note

When scaling down at the instance level with BatchDeleteCusterNodes, you can only

terminate a maximum of 99 instances at a time. UpdateCluster supports terminating any

number of instances.

Important considerations

• When scaling down a cluster, you should ensure that the remaining resources are suﬃcient to
handle your workload and that any necessary data migration or rebalancing is properly handled
to avoid disruptions.

• Make sure to back up your data to Amazon S3 or an FSx for Lustre ﬁle system before invoking
the API on a worker node group. This can help prevent any potential data loss from the instance
root volume. For more information about backup, see Use the backup script provided by
SageMaker HyperPod.

• To invoke this API on an existing cluster, you must ﬁrst patch the cluster by running the
UpdateClusterSoftware API. For more information about patching a cluster, see Update the
SageMaker HyperPod platform software of a cluster.

• Metering/billing for on-demand instances will automatically be stopped after scale down. To
stop metering for scaled-down reserved instances, you should reach out to your AWS account
team for support.

• You can use the released capacity from the scaled-down reserved instances to scale up another
SageMaker HyperPod cluster.

Scale down at the instance group level

The UpdateCluster operation allows you to make changes to the conﬁguration of your
SageMaker HyperPod cluster, such as scaling down the number of instances of an instance group
or removing entire instance groups. This can be useful when you want to adjust the resources
allocated to your cluster based on changes in your workload, optimize costs, or change the instance
type of an instance group.

Scale down an instance group

Use this approach when you have an instance group that is idle and it's safe to terminate any of the

instances for scaling down. When you submit an UpdateCluster request to scale down, HyperPod

Amazon EKS orchestration
1802

## Page 832

Amazon SageMaker AI
Developer Guide

randomly chooses instances for termination and scales down to the speciﬁed number of nodes for
the instance group.

Note

When you scale the number of instances in an instance group down to 0, all the instances
within that group will be terminated. However, the instance group itself will still exist as
part of the SageMaker HyperPod cluster. You can scale the instance group back up at a later
time, using the same instance group conﬁguration.
Alternatively, you can choose to remove an instance group permanently. For more
information, see the section called “Delete instance groups”.

To scale down with UpdateCluster

1.
Follow the steps outlined in Updating SageMaker HyperPod cluster conﬁguration. When you
reach step 1.d where you specify the InstanceCount ﬁeld, enter a number that is smaller than
the current number of instances to scale down the cluster.

2.
Run the update-cluster AWS CLI command to submit your request.

The following is an example of an UpdateCluster JSON object. Consider the case where your
instance group currently has 2 running instances. If you set the InstanceCount ﬁeld to 1, as shown
in the example, then HyperPod randomly selects one of the instances and terminates it.

{
"ClusterName": "name-of-cluster-to-update",
"InstanceGroups": [
{
"InstanceGroupName": "training-instances",
"InstanceType": "instance-type",
"InstanceCount": 1,
"LifeCycleConfig": {
"SourceS3Uri": "s3://amzn-s3-demo-bucket/training-script.py",
"OnCreate": "s3://amzn-s3-demo-bucket/setup-script.sh"
},
"ExecutionRole": "arn:aws:iam::123456789012:role/SageMakerRole",
"ThreadsPerCore": number-of-threads,
"OnStartDeepHealthChecks": [
"InstanceStress",
"InstanceConnectivity"

Amazon EKS orchestration
1803

## Page 833

Amazon SageMaker AI
Developer Guide

]
}
],
"NodeRecovery": "Automatic"
}

Delete instance groups

You can use the UpdateCluster operation to remove entire instance groups from your
SageMaker HyperPod cluster when they are no longer needed. This goes beyond simple
scaling down, allowing you to completely eliminate speciﬁc instance groups from your cluster's
conﬁguration.

Note

When removing an instance group:

• All instances within the targeted group are terminated.

• The entire group conﬁguration is deleted from the cluster.

• Any workloads running on that instance group are stopped.

To delete instance groups with UpdateCluster

1.
When following the steps outlined in Updating SageMaker HyperPod cluster conﬁguration:

a.
Set the optional InstanceGroupsToDelete parameter in your UpdateCluster JSON
and pass the comma-separated list of instance group names that you want to delete.

b.
When you specify the InstanceGroups list, ensure that the speciﬁcations of the instance

groups you are removing are no longer listed in the InstanceGroups list.

2.
Run the update-cluster AWS CLI command to submit your request.

Important

• Your SageMaker HyperPod cluster must always maintain at least one instance group.

• Ensure all critical data is backed up before removal.

• The removal process cannot be undone.

Amazon EKS orchestration
1804

## Page 834

Amazon SageMaker AI
Developer Guide

The following is an example of an UpdateCluster JSON object. Consider the case where a cluster
currently has 3 instance groups, a training, a prototype-training, and an inference-serving group.
You want to delete the prototype-training group.

{
"ClusterName": "name-of-cluster-to-update",
"InstanceGroups": [
{
"InstanceGroupName": "training",
"InstanceType": "instance-type",
"InstanceCount": ,
"LifeCycleConfig": {
"SourceS3Uri": "s3://amzn-s3-demo-bucket/training-script.py",
"OnCreate": "s3://amzn-s3-demo-bucket/setup-script.sh"
},
"ExecutionRole": "arn:aws:iam::123456789012:role/SageMakerRole",
"ThreadsPerCore": number-of-threads,
"OnStartDeepHealthChecks": [
"InstanceStress",
"InstanceConnectivity"
]
},
{
"InstanceGroupName": "inference-serving",
"InstanceType": "instance-type",
"InstanceCount": 2,
[...]
},
],
"InstanceGroupsToDelete": [ "prototype-training" ],
"NodeRecovery": "Automatic"
}

Scale down at the instance level

The BatchDeleteClusterNodes operation allows you to scale down a SageMaker HyperPod

cluster by specifying the individual nodes you want to terminate. BatchDeleteClusterNodes
provides more granular control for targeted node removal and cluster optimization. For example,

you might use BatchDeleteClusterNodes to delete targeted nodes for maintenance, rolling
upgrades, or rebalancing resources geographically.

API request and response

Amazon EKS orchestration
1805

## Page 835

Amazon SageMaker AI
Developer Guide

When you submit a BatchDeleteClusterNodes request, SageMaker HyperPod deletes nodes
by their instance IDs. The API accepts a request with the cluster name and a list of node IDs to be
deleted.

The response includes two sections:

• Failed: A list of errors of type  BatchDeleteClusterNodesError  - one per instance ID.

• Successful: The list of instance IDs successfully terminated.

Validation and error handling

The API performs various validations, such as:

• Verifying the node ID format (preﬁx of i- and Amazon EC2 instance ID structure).

• Checking the node list length, with a limit of 99 or fewer node IDs in a single

BatchDeleteClusterNodes request.

• Ensuring a valid SageMaker HyperPod cluster with the input cluster-name is present and that no
cluster-level operations (update, system update, patching, or deletion) are in progress.

• Handling cases where instances are not found, have invalid status, or are in use.

API Response Codes

• The API returns a 200 status code for successful (e.g., all input nodes succeeded validation) or
partially successful requests (e.g., some input nodes fail validation).

• If all of these validations fail (e.g., all input nodes fail validation), the API will return a 400 Bad
Request response with the appropriate error messages and error codes.

Example

The following is an example of scaling down a cluster at the instance level using the AWS CLI:

aws sagemaker batch-delete-cluster-nodes --cluster-name "cluster-name" --node-
ids '["i-111112222233333", "i-111112222233333"]'

Deleting a SageMaker HyperPod cluster

Run delete-cluster to delete a cluster. You can specify either the name or the ARN of the cluster.

Amazon EKS orchestration
1806

## Page 836

Amazon SageMaker AI
Developer Guide

aws sagemaker delete-cluster --cluster-name your-hyperpod-cluster

This API only cleans up the SageMaker HyperPod resources and doesn’t delete any resources of

the associated EKS cluster. This includes the Amazon EKS cluster, EKS Pod identities, Amazon
FSx volumes, and EKS add-ons. This also includes the initial conﬁguration you added to your EKS
cluster. If you want to clean up all resources, make sure that you also clean up the EKS resources
separately.

Make sure that you ﬁrst delete the SageMaker HyperPod resources, followed by the EKS resources.
Performing the deletion in the reverse order may result in lingering resources.

Important

When this API is called, SageMaker HyperPod doesn’t drain or redistribute the jobs (Pods)
running on the nodes. Make sure to check if there are any jobs running on the nodes before

calling this API.

HyperPod managed tiered checkpointing

This section explains how managed tiered checkpointing works and the beneﬁts it provides for
large-scale model training.

Amazon SageMaker HyperPod managed tiered checkpointing helps you train large-scale generative
AI models more eﬃciently. It uses multiple storage tiers, including your cluster’s CPU memory.
This approach reduces your time to recovery and minimizes loss in training progress. It also uses
underutilized memory resources in your training infrastructure.

Managed tiered checkpointing enables saving checkpoints at a higher frequency to memory. It
periodically persists them to durable storage. This maintains both performance and reliability
during your training process.

This guide covers how to set up, conﬁgure, and use managed tiered checkpointing with PyTorch
frameworks on Amazon EKS HyperPod clusters.

How managed tiered checkpointing works

Managed tiered checkpointing uses a multi-tier storage approach. CPU memory serves as the
primary tier to store model checkpoints. Secondary tiers include persistent storage options like
Amazon S3.

Amazon EKS orchestration
1807

## Page 837

Amazon SageMaker AI
Developer Guide

When you save a checkpoint, the system stores it in allocated memory space across your cluster
nodes. It automatically replicates data across adjacent compute nodes for enhanced reliability. This
replication strategy protects against single or multiple node failures while providing fast access for
recovery operations.

The system also periodically saves checkpoints to persistent storage according to your
conﬁguration. This ensures long-term durability of your training progress.

Key components include:

• Memory management system: A memory management daemon that provides disaggregated
memory as a service for checkpoint storage

• HyperPod Python library: Interfaces with the disaggregated storage APIs and provides utilities
for saving, loading, and managing checkpoints across tiers

• Checkpoint replication: Automatically replicates checkpoints across multiple nodes for fault
tolerance

The system integrates seamlessly with PyTorch training loops through simple API calls. It requires
minimal changes to your existing code.

Beneﬁts

Managed tiered checkpointing delivers several advantages for large-scale model training:

• Improved usability: Manages checkpoint save, replication, persistence, and recovery

• Faster checkpoint operations: Memory-based storage provides faster save and load times
compared to disk-based checkpointing, leading to faster recovery

• Fault tolerance: Automatic checkpoint replication across nodes protects against hardware node
failures

• Minimal code changes: Simple API integration requires only minor modiﬁcations to existing
training scripts

• Improved training throughput: Reduced checkpoint overhead means more time spent on actual
training

Topics

• Set up managed tiered checkpointing

Amazon EKS orchestration
1808

## Page 838

Amazon SageMaker AI
Developer Guide

• Removing managed tiered checkpointing

• Security considerations for managed tiered checkpointing

Set up managed tiered checkpointing

This section contains setup process for managed tiered checkpointing for Amazon SageMaker
HyperPod. You’ll learn how to enable the capability on your cluster and implement checkpointing
in your training code.

Topics

• Prerequisites

• Step 1: Enable managed tiered checkpointing for your cluster

• Step 2: Install the Python library in your training image

• Step 3: Save checkpoints in your training loop

• Step 4: Load checkpoints for recovery

• Validate your managed tiered checkpointing operations

Prerequisites

Before setting up managed tiered checkpointing, ensure you have:

• An Amazon EKS HyperPod cluster with suﬃcient CPU memory available for checkpoint
allocation

• PyTorch training workloads and DCP jobs (both are supported)

• Appropriate IAM permissions for cluster management, including:

• Amazon CloudWatch and Amazon S3 write permissions for the training pod to read/write
checkpoints and push metrics

• These permissions can be conﬁgured via EKS OIDC setup

Step 1: Enable managed tiered checkpointing for your cluster

Important

You must opt in to use managed tiered checkpointing.

Amazon EKS orchestration
1809

## Page 839

Amazon SageMaker AI
Developer Guide

Enable managed tiered checkpointing through the HyperPod APIs when creating or updating your
cluster. The service automatically installs the memory management system when you specify the

TieredStorageConfig parameter.

For new clusters, you can use create-cluster AWS CLI.

aws sagemaker create-cluster \
--cluster-name cluster-name \
--orchestrator "Eks={ClusterArn=eks-cluster-arn}" \
--instance-groups '{
"InstanceGroupName": "instance-group-name",
"InstanceType": "instance-type",
"InstanceCount": instance-count,
"LifeCycleConfig": {
"SourceS3Uri": "s3-path-to-lifecycle-scripts",
"OnCreate": "lifecycle-script-name"
},
"ExecutionRole": "instance-group-iam-role",
"ThreadsPerCore": threads-per-core,
"InstanceStorageConfigs": [
{ "EbsVolumeConfig": {"VolumeSizeInGB": volume-size} }
]
}' \
--vpc-config '{
"SecurityGroupIds": ["security-group-ids"],
"Subnets": ["subnets"]
}' \
--tiered-storage-config '{
"Mode": "Enable"
}'

The InstanceMemoryAllocationPercentage parameter speciﬁes the percentage (int) of
cluster memory to allocate for checkpointing. The range is 20-100.

Step 2: Install the Python library in your training image

Install the Amazon SageMaker checkpointing library and its dependencies in your training image by
adding it to your Dockerﬁle:

# Add this line to your training image Dockerfile
RUN pip install amzn-sagemaker-checkpointing s3torchconnector tenacity torch boto3
s3torchconnector

Amazon EKS orchestration
1810

## Page 840

Amazon SageMaker AI
Developer Guide

Step 3: Save checkpoints in your training loop

In your training loop, you can asynchronously save checkpoints using PyTorch DCP. The following is
an example on how to do so.

import torch
import torch.distributed as dist
from torch.distributed.checkpoint import async_save, load

from amzn_sagemaker_checkpointing.checkpointing.filesystem.filesystem import (
SageMakerTieredStorageWriter,
SageMakerTieredStorageReader
)

# Initialize distributed training
dist.init_process_group(backend="nccl")

# Configure checkpointing
checkpoint_config = SageMakerCheckpointConfig(
# Unique ID for your training job
# Allowed characters in ID include: alphanumeric, hyphens, and underscores
namespace=os.environ.get('TRAINING_JOB_NAME', f'job-{int(time.time())}'),

# Number of distributed processes/available GPUs
world_size=dist.get_world_size(),

# S3 storage location, required for SageMakerTieredStorageReader for read fallbacks
# Required for SageMakerTieredStorageWriter when save_to_s3 is True
s3_tier_base_path="s3://my-bucket/checkpoints"
)

# Your model and optimizer
model = MyModel()
optimizer = torch.optim.AdamW(model.parameters())

# Training loop
future = None
in_memory_ckpt_freq = 10
s3_ckpt_freq = 50

for training_step in range(1000):
# ... training code ...
# Save checkpoint
if (training_step % in_memory_ckpt_freq == 0 or

Amazon EKS orchestration
1811

## Page 841

Amazon SageMaker AI
Developer Guide

training_step % s3_ckpt_freq == 0):
# Create state dictionary
state_dict = {
"model": model.state_dict(),
"optimizer": optimizer.state_dict(),
"step": training_step,
"epoch": epoch
}
# Create storage writer for current step
checkpoint_config.save_to_s3 = training_step % s3_ckpt_freq == 0
storage_writer = SageMakerTieredStorageWriter(
checkpoint_config=checkpoint_config,
step=training_step
)

# wait for previous checkpoint to get completed

if future is not None:
exc = future.exception()
if exc:
print(f"Failure in saving previous checkpoint:{str(exc)}")
# Handle failures as required
else:
result = future.result()
# Process results from save, if required
# Async save checkpoint using PyTorch DCP
future = async_save(state_dict=state_dict, storage_writer=storage_writer)
# Continue training while checkpoint saves in background

Step 4: Load checkpoints for recovery

The following is an example on loading a checkpoint.

# Create state dictionary template
state_dict = {
"model": model.state_dict(),
"optimizer": optimizer.state_dict(),
"step": 0,
"epoch": 0
}

# Load latest checkpoint

Amazon EKS orchestration
1812

## Page 842

Amazon SageMaker AI
Developer Guide

storage_reader = SageMakerTieredStorageReader(checkpoint_config=checkpoint_config)
load(state_dict, storage_reader=storage_reader)

# Load specific checkpoint step
storage_reader = SageMakerTieredStorageReader(
checkpoint_config=checkpoint_config,
step=500 # Or don't pass step if you have to load the latest available step.
)
try:
load(state_dict, storage_reader=storage_reader)
except BaseException as e:
print(f"Checkpoint load failed: {str(e)}")
# Add additional exception handling

Validate your managed tiered checkpointing operations

You can validate your managed tiered checkpointing operations with logs.

Custom logging (optional)

You can integrate checkpointing logs with other logs by passing a custom logger to the library. For
example, you can add a custom logger to your training code so that all logs from the library are
also collected in the training logger.

Enhanced service logging (optional)

For enhanced debugging and service visibility, you can mount the checkpointing log path

/var/log/sagemaker_checkpointing from within your pod to a path /var/logs/

sagemaker_checkpointing on your host. This ensures that only library-speciﬁc logs are
collected separately. This provides the service team with enhanced visibility for debugging and
support.

Removing managed tiered checkpointing

This section explains how to disable managed tiered checkpointing when you no longer need it.

To disable managed tiered checkpointing, use the update-cluster AWS CLI to update your
cluster conﬁguration:

aws sagemaker update-cluster \
--cluster-name cluster-name \
--tiered-storage-config '{ "Mode": "Disable" }'

Amazon EKS orchestration
1813

## Page 843

Amazon SageMaker AI
Developer Guide

This removes the memory management daemon from your cluster. The daemon is implemented as
a standard Kubernetes DaemonSet and follows standard Kubernetes lifecycle management.

Security considerations for managed tiered checkpointing

This section covers important security considerations when using managed tiered checkpointing. It
includes Python pickle usage, Amazon S3 encryption, and network endpoint security.

Python pickle usage

Managed tiered checkpointing uses Python’s pickle module to deserialize checkpoint data stored in
Amazon S3. This implementation has important security implications:

• Extended trust boundary: When using managed tiered checkpointing with Amazon S3, the
Amazon S3 bucket becomes part of your cluster’s trust boundary.

• Code execution risk: Python’s pickle module can execute arbitrary code during deserialization.
If an unauthorized user gains write access to your checkpoint Amazon S3 bucket, they
could potentially craft malicious pickle data that executes when loaded by managed tiered
checkpointing.

Best practices for Amazon S3 storage

When using managed tiered checkpointing with Amazon S3 storage:

• Restrict Amazon S3 bucket access: Ensure that only authorized users and roles associated with
your training cluster have access to the Amazon S3 bucket used for checkpointing.

• Implement bucket policies: Conﬁgure appropriate bucket policies to prevent unauthorized
access or modiﬁcations.

• Validate access patterns: Implement logging for validating access patterns to your checkpoint
Amazon S3 buckets.

• Validate bucket names: Use caution with bucket name selection to avoid potential bucket
hijacking.

Network endpoints

Managed tiered checkpointing enables network endpoints on each of your compute nodes on the
following ports: 9200/TCP, 9209/UDP, 9210/UDP, 9219/UDP, 9220/UDP, 9229/UDP, 9230/UDP,

Amazon EKS orchestration
1814

## Page 844

Amazon SageMaker AI
Developer Guide

9239/UDP, 9240/UDP. These ports are necessary for the checkpointing service to function and
maintain data synchronization.

By default, SageMaker’s network conﬁguration restricts access to these endpoints for security
purposes. We recommend that you maintain these default restrictions.

When conﬁguring your network settings for your nodes and VPC, follow AWS best practices for
VPCs, security groups, and ACLs. For more information, see the following:

• Amazon SageMaker HyperPod prerequisites

• VPC security best practices

SageMaker HyperPod task governance

SageMaker HyperPod task governance is a robust management system designed to streamline

resource allocation and ensure eﬃcient utilization of compute resources across teams and projects
for your Amazon EKS clusters. This provides administrators with the capability to set:

• Priority levels for various tasks

• Compute allocation for each team

• How each team lends and borrows idle compute

• If a team preempts their own tasks

HyperPod task governance also provides Amazon EKS cluster Observability, oﬀering real-time
visibility into cluster capacity. This includes compute availability and usage, team allocation and
utilization, and task run and wait time information, setting you up for informed decision-making
and proactive resource management.

The following sections cover how to set up, understand key concepts, and use HyperPod task
governance for your Amazon EKS clusters.

Topics

• Setup for SageMaker HyperPod task governance

• Dashboard

• Tasks

• Policies

Amazon EKS orchestration
1815

## Page 845

Amazon SageMaker AI
Developer Guide

• Example HyperPod task governance AWS CLI commands

• Troubleshoot

• Attribution document for Amazon SageMaker HyperPod task governance

Setup for SageMaker HyperPod task governance

The following section provides information on how to get set up with the Amazon CloudWatch
Observability EKS and SageMaker HyperPod task governance add-ons.

Ensure that you have the minimum permission policy for HyperPod cluster administrators with
Amazon EKS, in IAM users for cluster admin. This includes permissions to run the SageMaker
HyperPod core APIs and manage SageMaker HyperPod clusters within your AWS account,
performing the tasks in Managing SageMaker HyperPod clusters orchestrated by Amazon EKS.

Topics

• Dashboard setup

• Task governance setup

Dashboard setup

Use the following information to get set up with Amazon SageMaker HyperPod Amazon
CloudWatch Observability EKS add-on. This sets you up with a detailed visual dashboard that
provides a view into metrics for your EKS cluster hardware, team allocation, and tasks.

If you are having issues setting up, please see Troubleshoot for known troubleshooting solutions.

Topics

• HyperPod Amazon CloudWatch Observability EKS add-on prerequisites

• HyperPod Amazon CloudWatch Observability EKS add-on setup

HyperPod Amazon CloudWatch Observability EKS add-on prerequisites

The following section includes the prerequisites needed before installing the Amazon EKS
Observability add-on.

• Ensure that you have the minimum permission policy for HyperPod cluster administrators, in IAM
users for cluster admin.

Amazon EKS orchestration
1816

## Page 846

Amazon SageMaker AI
Developer Guide

• Attach the CloudWatchAgentServerPolicy IAM policy to your worker nodes. To do so,

enter the following command. Replace my-worker-node-role with the IAM role used by your

Kubernetes worker nodes.

aws iam attach-role-policy \
--role-name my-worker-node-role \
--policy-arn arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy

HyperPod Amazon CloudWatch Observability EKS add-on setup

Use the following options to set up the Amazon SageMaker HyperPod Amazon CloudWatch
Observability EKS add-on.

Setup using the SageMaker AI console

The following permissions are required for setup and visualizing the HyperPod task governance
dashboard. This section expands upon the permissions listed in IAM users for cluster admin.

To manage task governance, use the sample policy:

JSON

{
"Version":"2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Action": [
"sagemaker:ListClusters",
"sagemaker:DescribeCluster",
"sagemaker:ListComputeQuotas",
"sagemaker:CreateComputeQuota",
"sagemaker:UpdateComputeQuota",
"sagemaker:DescribeComputeQuota",
"sagemaker:DeleteComputeQuota",
"sagemaker:ListClusterSchedulerConfigs",
"sagemaker:DescribeClusterSchedulerConfig",
"sagemaker:CreateClusterSchedulerConfig",
"sagemaker:UpdateClusterSchedulerConfig",
"sagemaker:DeleteClusterSchedulerConfig",
"eks:ListAddons",

Amazon EKS orchestration
1817

## Page 847

Amazon SageMaker AI
Developer Guide

"eks:CreateAddon",
"eks:DescribeAddon",
"eks:DescribeCluster",
"eks:DescribeAccessEntry",
"eks:ListAssociatedAccessPolicies",
"eks:AssociateAccessPolicy",
"eks:DisassociateAccessPolicy"
],
"Resource": "*"
}
]
}

To grant permissions to manage Amazon CloudWatch Observability Amazon EKS and view the
HyperPod cluster dashboard through the SageMaker AI console, use the sample policy below:

JSON

{
"Version":"2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Action": [
"eks:ListAddons",
"eks:CreateAddon",
"eks:UpdateAddon",
"eks:DescribeAddon",
"eks:DescribeAddonVersions",
"sagemaker:DescribeCluster",
"sagemaker:DescribeClusterNode",
"sagemaker:ListClusterNodes",
"sagemaker:ListClusters",
"sagemaker:ListComputeQuotas",
"sagemaker:DescribeComputeQuota",
"sagemaker:ListClusterSchedulerConfigs",
"sagemaker:DescribeClusterSchedulerConfig",
"eks:DescribeCluster",
"cloudwatch:GetMetricData",
"eks:AccessKubernetesApi"
],

Amazon EKS orchestration
1818

## Page 848

Amazon SageMaker AI
Developer Guide

"Resource": "*"
}
]
}

Navigate to the Dashboard tab in the SageMaker HyperPod console to install the Amazon
CloudWatch Observability EKS. To ensure task governance related metrics are included in
the Dashboard, enable the Kueue metrics checkbox. Enabling the Kueue metrics enables
CloudWatch Metrics costs, after free-tier limit is reached. For more information, see Metrics in
Amazon CloudWatch Pricing.

Setup using the EKS AWS CLI

Use the following EKS AWS CLI command to install the add-on:

aws eks create-addon --cluster-name cluster-name
--addon-name amazon-cloudwatch-observability
--configuration-values "configuration json"

Below is an example of the JSON of the conﬁguration values:

{
"agent": {
"config": {
"logs": {
"metrics_collected": {
"kubernetes": {
"kueue_container_insights": true,
"enhanced_container_insights": true
},
"application_signals": { }
}
},
"traces": {
"traces_collected": {
"application_signals": { }
}
}
},
},
}

Amazon EKS orchestration
1819

## Page 849

Amazon SageMaker AI
Developer Guide

Setup using the EKS Console UI

1.
Navigate to the EKS console.

2.
Choose your cluster.

3.
Choose Add-ons.

4.
Find the Amazon CloudWatch Observability add-on and install. Install version >= 2.4.0 for
the add-on.

5.
Include the following JSON, Conﬁguration values:

{
"agent": {
"config": {
"logs": {
"metrics_collected": {
"kubernetes": {
"kueue_container_insights": true,
"enhanced_container_insights": true
},
"application_signals": { }
},
},
"traces": {
"traces_collected": {
"application_signals": { }
}
}
},
},
}

Once the EKS Observability add-on has been successfully installed, you can view your EKS cluster
metrics under the HyperPod console Dashboard tab.

Task governance setup

This section includes information on how to set up the Amazon SageMaker HyperPod task
governance EKS add-on. This includes granting permissions that allows you to set task
prioritization, compute allocation for teams, how idle compute is shared, and task preemption for
teams.

Amazon EKS orchestration
1820

## Page 850

Amazon SageMaker AI
Developer Guide

If you are having issues setting up, please see Troubleshoot for known troubleshooting solutions.

Topics

• Kueue Settings

• HyperPod Task governance prerequisites

• HyperPod task governance setup

Kueue Settings

HyperPod task governance EKS add-on installs Kueue for your HyperPod EKS clusters. Kueue is a
kubernetes-native system that manages quotas and how jobs consume them.

EKS HyperPod task governance add-on
version

Version of Kueue that is installed as part of
the add-on

v1.1.3
v0.12.0

Note

Kueue v.012.0 and higher don't include kueue-rbac-proxy as part of the installation.
Previous versions might have kueue-rbac-proxy installed. For example, if you're using
Kueue v0.8.1, you might have kueue-rbac-proxy v0.18.1.

HyperPod task governance leverages Kueue for Kubernetes-native job queueing, scheduling,
and quota management, and is installed with the HyperPod task governance EKS add-on. When
installed, HyperPod creates and modiﬁes SageMaker AI-managed Kubernetes resources such

as KueueManagerConfig, ClusterQueues, LocalQueues, WorkloadPriorityClasses,

ResourceFlavors, and ValidatingAdmissionPolicies. While Kubernetes administrators
have the ﬂexibility to modify the state of these resources, it is possible that any changes made to a
SageMaker AI-managed resource may be updated and overwritten by the service.

The following information outlines the conﬁguration settings utilized by the HyperPod task
governance add-on for setting up Kueue.

apiVersion: config.kueue.x-k8s.io/v1beta1
kind: Configuration

Amazon EKS orchestration
1821

## Page 851

Amazon SageMaker AI
Developer Guide

health:
healthProbeBindAddress: :8081
metrics:
bindAddress: :8443
enableClusterQueueResources: true
webhook:
port: 9443
manageJobsWithoutQueueName: false
leaderElection:
leaderElect: true
resourceName: c1f6bfd2.kueue.x-k8s.io
controller:
groupKindConcurrency:
Job.batch: 5
Pod: 5
Workload.kueue.x-k8s.io: 5
LocalQueue.kueue.x-k8s.io: 1

ClusterQueue.kueue.x-k8s.io: 1
ResourceFlavor.kueue.x-k8s.io: 1
clientConnection:
qps: 50
burst: 100
integrations:
frameworks:
- "batch/job"
- "kubeflow.org/mpijob"
- "ray.io/rayjob"
- "ray.io/raycluster"
- "jobset.x-k8s.io/jobset"
- "kubeflow.org/mxjob"
- "kubeflow.org/paddlejob"
- "kubeflow.org/pytorchjob"
- "kubeflow.org/tfjob"
- "kubeflow.org/xgboostjob"
- "pod"
- "deployment"
- "statefulset"
- "leaderworkerset.x-k8s.io/leaderworkerset"
podOptions:
namespaceSelector:
matchExpressions:
- key: kubernetes.io/metadata.name
operator: NotIn
values: [ kube-system, kueue-system ]

Amazon EKS orchestration
1822

## Page 852

Amazon SageMaker AI
Developer Guide

fairSharing:
enable: true
preemptionStrategies: [LessThanOrEqualToFinalShare, LessThanInitialShare]
resources:
excludeResourcePrefixes: []

For more information about each conﬁguration entry, see Conﬁguration in the Kueue
documentation.

HyperPod Task governance prerequisites

• Ensure that you have the minimum permission policy for HyperPod cluster administrators, in
IAM users for cluster admin. This includes permissions to run the SageMaker HyperPod core APIs,
manage SageMaker HyperPod clusters within your AWS account, and performing the tasks in
Managing SageMaker HyperPod clusters orchestrated by Amazon EKS.

• You will need to have your Kubernetes version >= 1.30. For instructions, see Update existing
clusters to the new Kubernetes version.

• If you already have Kueue installed in their clusters, uninstall Kueue before installing the EKS
add-on.

• A HyperPod node must already exist in the EKS cluster before installing the HyperPod task
governance add-on.

HyperPod task governance setup

The following provides information on how to get set up with HyperPod task governance.

Setup using the SageMaker AI console

The following provides information on how to get set up with HyperPod task governance using
the SageMaker HyperPod console.

You already have all of the following permissions attached if you have already granted
permissions to manage Amazon CloudWatch Observability EKS and view the HyperPod
cluster dashboard through the SageMaker AI console in the HyperPod Amazon CloudWatch
Observability EKS add-on setup. If you have not set this up, use the sample policy below to
grant permissions to manage the HyperPod task governance add-on and view the HyperPod
cluster dashboard through the SageMaker AI console.

Amazon EKS orchestration
1823

## Page 853

Amazon SageMaker AI
Developer Guide

JSON

{
"Version":"2012-10-17",
"Statement": [
{
"Effect": "Allow",
"Action": [
"eks:ListAddons",
"eks:CreateAddon",
"eks:UpdateAddon",
"eks:DescribeAddon",
"eks:DescribeAddonVersions",
"sagemaker:DescribeCluster",
"sagemaker:DescribeClusterNode",
"sagemaker:ListClusterNodes",

"sagemaker:ListClusters",
"eks:DescribeCluster",
"eks:AccessKubernetesApi"
],
"Resource": "*"
}
]
}

Navigate to the Dashboard tab in the SageMaker HyperPod console to install the Amazon
SageMaker HyperPod task governance Add-on.

Setup using the Amazon EKS AWS CLI

Use the example create-addon EKS AWS CLI command to set up the HyperPod task
governance Amazon EKS API and console UI using the AWS CLI:

aws eks create-addon --region region --cluster-name cluster-name --addon-name
amazon-sagemaker-hyperpod-taskgovernance

You can view the Policies tab in the HyperPod SageMaker AI console if the install was successful.

You can also use the following example describe-addon EKS AWS CLI command to check the
status.

Amazon EKS orchestration
1824

## Page 854

Amazon SageMaker AI
Developer Guide

aws eks describe-addon --region region --cluster-name cluster-name --addon-name amazon-
sagemaker-hyperpod-taskgovernance

Dashboard

Amazon SageMaker HyperPod task governance provides a comprehensive dashboard view of your
Amazon EKS cluster utilization metrics, including hardware, team, and task metrics. The following
provides information on your HyperPod EKS cluster dashboard.

The dashboard provides a comprehensive view of cluster utilization metrics, including hardware,
team, and task metrics. You will need to install the EKS add-on to view the dashboard. For more
information, see Dashboard setup.

In the Amazon SageMaker AI console, under HyperPod Clusters, you can navigate to the HyperPod
console and view your list of HyperPod clusters in your Region. Choose your cluster and navigate to

the Dashboard tab. The dashboard contains the following metrics. You can download the data for
a section by choosing the corresponding Export.

Utilization

Provides health of the EKS cluster point-in-time and trend-based metrics for critical compute
resources. By default, All Instance Groups are shown. Use the dropdown menu to ﬁlter your
instance groups. The metrics included in this section are:

• Number of total, running, and pending recovery instances. The number of pending recovery
instances refer to the number of instances that need attention for recovery.

• GPUs, GPU memory, vCPUs, and vCPUs memory.

• GPU utilization, GPU memory utilization, vCPU utilization, and vCPU memory utilization.

• An interactive graph of your GPU and vCPU utilization.

Teams

Provides information into team-speciﬁc resource management. This includes:

• Instance and GPU allocation.

• GPU utilization rates.

• Borrowed GPU statistics.

• Task status (running or pending).

Amazon EKS orchestration
1825

## Page 855

Amazon SageMaker AI
Developer Guide

• A bar chart view of GPU utilization versus compute allocation across teams.

• Team detailed GPU and vCPU-related information. By default, the information displayed includes
All teams. You can ﬁlter by team and instances by choosing the dropdown menus. In the
interactive plot you can ﬁlter by time.

Tasks

Note

To view your HyperPod EKS cluster tasks in the dashboard:

• Conﬁgure Kubernetes Role-Based Access Control (RBAC) for data scientist users in
the designated HyperPod namespace to authorize task execution on Amazon EKS-

orchestrated clusters. Namespaces follow the format hyperpod-ns-team-name. To
establish RBAC permissions, refer to the team role creation instructions.

• Ensure that your job is submitted with the appropriate namespace and priority class
labels. For a comprehensive example, see Submit a job to SageMaker AI-managed queue
and namespace.

Provides information on task-related metrics. This includes number of running, pending, and
preempted tasks, and run and wait time statistics. By default, the information displayed includes
All teams. You can ﬁlter by team by choosing the dropdown menu. In the interactive plot you can
ﬁlter by time.

Tasks

The following provides information on Amazon SageMaker HyperPod EKS cluster tasks. Tasks
are operations or jobs that are sent to the cluster. These can be machine learning operations, like
training, running experiments, or inference. The viewable task details list include status, run time,
and how much compute is being used per task.

In the Amazon SageMaker AI console, under HyperPod Clusters, you can navigate to the HyperPod
console and view your list of HyperPod clusters in your Region. Choose your cluster and navigate to
the Tasks tab.

For the Tasks tab to be viewable from anyone besides the administrator, the administrator needs
to add an access entry to the EKS cluster for the IAM role.

Amazon EKS orchestration
1826

## Page 856

Amazon SageMaker AI
Developer Guide

Note

To view your HyperPod EKS cluster tasks in the dashboard:

• Conﬁgure Kubernetes Role-Based Access Control (RBAC) for data scientist users in
the designated HyperPod namespace to authorize task execution on Amazon EKS-

orchestrated clusters. Namespaces follow the format hyperpod-ns-team-name. To
establish RBAC permissions, refer to the team role creation instructions.

• Ensure that your job is submitted with the appropriate namespace and priority class
labels. For a comprehensive example, see Submit a job to SageMaker AI-managed queue
and namespace.

For EKS clusters, kubeﬂow (PyTorch, MPI, TensorFlow) tasks are shown. By default, PyTorch tasks

are shown. You can ﬁlter for PyTorch, MPI, TensorFlow tasks by choosing the dropdown menu or
using the search ﬁeld. The information that is shown for each task includes the task name, status,
namespace, priority class, and creation time.

Using topology-aware scheduling in Amazon SageMaker HyperPod task governance

Topology-aware scheduling in Amazon SageMaker HyperPod task governance optimizes the
training eﬃciency of distributed machine learning workloads by placing pods based on the physical
network topology of your Amazon EC2 instances. By considering the hierarchical structure of AWS
infrastructure, including Availability Zones, network blocks, and physical racks, topology-aware
scheduling ensures that pods requiring frequent communication are scheduled in close proximity
to minimize network latency. This intelligent placement is particularly beneﬁcial for large-scale
machine learning training jobs that involve intensive pod-to-pod communication, resulting in
reduced training times and more eﬃcient resource utilization across your cluster.

Note

To use topology-aware scheduling, make sure that your version of HyperPod task
governance is v1.2.2-eksbuild.1 or higher.

Topology-aware scheduling supports the following instance types:

• ml.p3dn.24xlarge

Amazon EKS orchestration
1827

## Page 857

Amazon SageMaker AI
Developer Guide

• ml.p4d.24xlarge

• ml.p4de.24xlarge

• ml.p5.48xlarge

• ml.p5e.48xlarge

• ml.p5en.48xlarge

• ml.p6e-gb200.36xlarge

• ml.trn1.2xlarge

• ml.trn1.32xlarge

• ml.trn1n.32xlarge

• ml.trn2.48xlarge

• ml.trn2u.48xlarge

Topology-aware scheduling integrates with your existing HyperPod workﬂows while providing
ﬂexible topology preferences through both kubectl YAML ﬁles and the HyperPod CLI. HyperPod
task governance automatically conﬁgures cluster nodes with topology labels and works with
HyperPod task governance policies and resource borrowing mechanisms, ensuring that topology-
aware scheduling doesn't disrupt your current operational processes. With built-in support for both
preferred and required topology speciﬁcations, you can ﬁne-tune workload placement to match
your speciﬁc performance requirements while maintaining the ﬂexibility to fall back to standard
scheduling when topology constraints cannot be satisﬁed.

By leveraging topology-aware labels in HyperPod, you can enhance their machine learning
workloads through intelligent pod placement that considers the physical network infrastructure.
HyperPod task governance automatically optimizes pod scheduling based on the hierarchical
data center topology, which directly translates to reduced network latency and improved training
performance for distributed ML tasks. This topology awareness is particularly valuable for large-
scale machine learning workloads, as it minimizes communication overhead by strategically placing
related pods closer together in the network hierarchy. The result is optimized communication
network latency between pods, more eﬃcient resource utilization, and better overall performance
for compute-intensive AI/ML applications, all achieved without you needing to manually manage
complex network topology conﬁgurations.

The following are labels for the available topology network layers that HyperPod task governance
can schedule pods in:

Amazon EKS orchestration
1828

## Page 858

Amazon SageMaker AI
Developer Guide

• topology.k8s.aws/network-node-layer-1

• topology.k8s.aws/network-node-layer-2

• topology.k8s.aws/network-node-layer-3

• topology.k8s.aws/ultraserver-id

To use topology-aware scheduling, include the following labels in your YAML ﬁle:

• kueue.x-k8s.io/podset-required-topology - indicates that this job must have the required pods
and that all pods in the nodes must be scheduled within the same topology layer.

• kueue.x-k8s.io/podset-preferred-topology - indicates that this job must have the pods, but that
scheduling pods within the same topology layer is preferred but not required. HyperPod task
governance will try to schedule the pods within one layer before trying the next topology layer.

If resources don’t share the same topology label, the job will be suspended. The job will be in the
waitlist. Once Kueue sees that there are enough resources, it will admit and run the job.

The following example demonstrates how to use the labels in your YAML ﬁles:

apiVersion: batch/v1
kind: Job
metadata:
name: test-tas-job
namespace: hyperpod-ns-team-name
labels:
kueue.x-k8s.io/queue-name: hyperpod-ns-team-name-localqueue
kueue.x-k8s.io/priority-class: PRIORITY_CLASS-priority
spec:
parallelism: 10
completions: 10
suspend: true
template:
metadata:
labels:
kueue.x-k8s.io/queue-name: hyperpod-ns-team-name-localqueue
annotations:
kueue.x-k8s.io/podset-required-topology: "topology.k8s.aws/network-node-
layer-3"
or
kueue.x-k8s.io/podset-preferred-topology: "topology.k8s.aws/network-node-
layer-3"

Amazon EKS orchestration
1829

## Page 859

Amazon SageMaker AI
Developer Guide

spec:
nodeSelector:
topology.k8s.aws/network-node-layer-3: TOPOLOGY_LABEL_VALUE
containers:
- name: dummy-job
image: gcr.io/k8s-staging-perf-tests/sleep:v0.1.0
args: ["3600s"]
resources:
requests:
cpu: "100"
restartPolicy: Never

The following table explains the new parameters you can use in the kubectl YAML ﬁle.

Parameter
Description

kueue.x-k8s.io/queue-name
The name of the queue to use to run the
job. The format of this queue-name must be

hyperpod-ns- team-name -localque

ue .

kueue.x-k8s.io/priority-class
Lets you specify a priority for pod scheduling.
This speciﬁcation is optional.

annotations
Contains the topology annotation that you
attach to the job. Available topologies are
kueue.x-k8s.io/podset-required-topology and
kueue.x-k8s.io/podset-preferred-topology. You

can use either an annotation or nodeSelector,
but not both at the same time.

nodeSelector
Speciﬁes the network layer that represents
the layer of Amazon EC2 instance placement
. Use either this ﬁeld or an annotation, but
not both at the same time. In your YAML ﬁle,
you can also use the nodeSelector parameter
to choose the exact layer for your pods. To
get the value of your label, use the  DescribeI
nstanceTopology API operation.

Amazon EKS orchestration
1830

## Page 860

Amazon SageMaker AI
Developer Guide

You can also use the HyperPod CLI to run your job and use topology aware scheduling. For more
information about the HyperPod CLI, see SageMaker HyperPod CLI commands.

hyp create hyp-pytorch-job \
--version 1.1 \
--job-name sample-pytorch-job \
--image 123456789012.dkr.ecr.us-west-2.amazonaws.com/ptjob:latest \
--pull-policy "Always" \

--tasks-per-node 1 \
--max-retry 1 \
--priority high-priority \
--namespace hyperpod-ns-team-name \
--queue-name hyperpod-ns-team-name-localqueue \
--preferred-topology-label topology.k8s.aws/network-node-layer-1

The following is an example conﬁguration ﬁle that you might use to run a PytorchJob with
topology labels. The ﬁle is largely similar if you want to run MPI and Tensorﬂow jobs. If you want
to run those jobs instead, remember to change the conﬁguration ﬁle accordingly, such as using
the correct image instead of PyTorchJob. If you’re running a PyTorchJob, you can assign diﬀerent
topologies to the master and worker nodes. PyTorchJob always has one master node, so we
recommend that you use topology to support worker pods instead.

apiVersion: kubeflow.org/v1
kind: PyTorchJob
metadata:
annotations: {}
labels:
kueue.x-k8s.io/queue-name: hyperpod-ns-team-name-localqueue
name: tas-test-pytorch-job
namespace: hyperpod-ns-team-name
spec:
pytorchReplicaSpecs:
Master:
replicas: 1
restartPolicy: OnFailure
template:
metadata:
labels:
kueue.x-k8s.io/queue-name: hyperpod-ns-team-name-localqueue
spec:
containers:
- command:

Amazon EKS orchestration
1831

## Page 861

Amazon SageMaker AI
Developer Guide

- python3
- /opt/pytorch-mnist/mnist.py
- --epochs=1
image: docker.io/kubeflowkatib/pytorch-mnist:v1beta1-45c5727
imagePullPolicy: Always
name: pytorch
Worker:
replicas: 10
restartPolicy: OnFailure
template:
metadata:
# annotations:
# kueue.x-k8s.io/podset-required-topology: "topology.k8s.aws/network-node-
layer-3"
labels:
kueue.x-k8s.io/queue-name: hyperpod-ns-team-name-localqueue
spec:

containers:
- command:
- python3
- /opt/pytorch-mnist/mnist.py
- --epochs=1
image: docker.io/kubeflowkatib/pytorch-mnist:v1beta1-45c5727
imagePullPolicy: Always
name: pytorch
resources:
limits:
cpu: 1
requests:
memory: 200Mi
cpu: 1
#nodeSelector:
#  topology.k8s.aws/network-node-layer-3: xxxxxxxxxxx

To see the topologies for your cluster, use the  DescribeInstanceTopology API operation. By default,
the topologies are hidden in the AWS Management Console and Amazon SageMaker Studio. Follow
these steps to see them in the interface that you’re using.

SageMaker Studio

1.
In SageMaker Studio, navigate to your cluster.

2.
In the Tasks view, choose the options menu in the Name column, then choose Manage
columns.

Amazon EKS orchestration
1832

## Page 862

Amazon SageMaker AI
Developer Guide

3.
Select Requested topology and Topology constraint to add the columns to see the topology
information in the list of Kubernetes pods.

AWS Management Console

1.
Open the Amazon SageMaker AI console at https://console.aws.amazon.com/sagemaker/.

2.
Under HyperPod clusters, choose Cluster management.

3.
Choose the Tasks tab, then choose the gear icon.

4.
Under instance attributes, toggle Requested topology and Topology constraint.

5.
Choose Conﬁrm to see the topology information in the table.

Policies

Amazon SageMaker HyperPod task governance simpliﬁes how your Amazon EKS cluster resources
are allocated and how tasks are prioritized. The following provides information on HyperPod EKS
cluster policies. For information on how to set up task governance, see Task governance setup.

The policies are divided up into Compute prioritization and Compute allocation. The policy
concepts below will be organized in the context of these policies.

Compute prioritization, or cluster policy, determines how idle compute is borrowed and how tasks
are prioritized by teams.

• Idle compute allocation deﬁnes how idle compute is allocated across teams. That is, how
unused compute can be borrowed from teams. When choosing an Idle compute allocation, you
can choose between:

• First-come ﬁrst-serve: When applied, teams are not prioritized against each other and each
incoming task is equally likely to obtain over-quota resources. Tasks are prioritized based on
order of submission. This means a user may be able to use 100% of the idle compute if they
request it ﬁrst.

• Fair-share: When applied, teams borrow idle compute based on their assigned Fair-share
weight. These weights are deﬁned in Compute allocation. For more information on how this
can be used, see Sharing idle compute resources examples.

• Task prioritization deﬁnes how tasks are queued as compute becomes available. When choosing
a Task prioritization, you can choose between:

• First-come ﬁrst-serve: When applied, tasks are queued in the order they are requested.

Amazon EKS orchestration
1833

## Page 863

Amazon SageMaker AI
Developer Guide

• Task ranking: When applied, tasks are queued in the order deﬁned by their prioritization.
If this option is chosen, you must add priority classes along with the weights at which they
should be prioritized. Tasks of the same priority class will be executed on a ﬁrst-come ﬁrst-
serve basis. When enabled in Compute allocation, tasks are preempted from lower priority
tasks by higher priority tasks within the team.

When data scientists submit jobs to the cluster, they use the priority class name in the YAML

ﬁle. The priority class is in the format priority-class-name-priority. For an example,
see Submit a job to SageMaker AI-managed queue and namespace.

• Priority classes: These classes establish a relative priority for tasks when borrowing capacity.
When a task is running using borrowed quota, it may be preempted by another task of higher
priority than it, if no more capacity is available for the incoming task. If Preemption is enabled
in the Compute allocation, a higher priority task may also preempt tasks within its own team.

• Unallocated resource sharing enables teams to borrow compute resources that are not
allocated to any team through compute quota. When enabled, unallocated cluster capacity
becomes available for teams to borrow automatically. For more information, see How
unallocated resource sharing works.

Compute allocation, or compute quota, deﬁnes a team’s compute allocation and what weight (or
priority level) a team is given for fair-share idle compute allocation.

• Team name: The team name. A corresponding Namespace will be created, of type hyperpod-

ns-team-name.

• Members: Members of the team namespace. You will need to set up a Kubernetes role-based
access control (RBAC) for data scientist users that you want to be part of this team, to run tasks
on HyperPod clusters orchestrated with Amazon EKS. To set up a Kubernetes RBAC, use the
instructions in create team role.

• Fair-share weight: This is the level of prioritization assigned to the team when Fair-share is
applied for Idle compute allocation. The highest priority has a weight of 100 and the lowest
priority has a weight of 0. Higher weight enables a team to access unutilized resources within
shared capacity sooner. A zero weight signiﬁes the lowest priority, implying this team will always
be at a disadvantage compared to other teams.

The fair-share weight provides a comparative edge to this team when vying for available
resources against others. Admission prioritizes scheduling tasks from teams with the highest
weights and the lowest borrowing. For example, if Team A has a weight of 10 and Team B has a

Amazon EKS orchestration
1834

## Page 864

Amazon SageMaker AI
Developer Guide

weight of 5, Team A would have priority in accessing unutilized resources as in would have jobs
that are scheduled earlier than Team B.

• Task preemption: Compute is taken over from a task based on priority. By default, the team
loaning idle compute will preempt tasks from other teams.

• Lending and borrowing: How idle compute is being lent by the team and if the team can borrow
from other teams.

• Percentage-based borrow limit: The limit of idle compute that a team is allowed to borrow,
expressed as a percentage of their guaranteed quota. A team can borrow up to 10,000% of
allocated compute. The value you provide here is interpreted as a percentage. For example, a
value of 500 will be interpreted as 500%. This percentage applies uniformly across all resource
types (CPU, GPU, Memory) and instance types in the team's quota.

• Absolute borrow limit: The limit of idle compute that a team is allowed to borrow, deﬁned
as absolute resource values per instance type. This provides granular control over borrowing
behavior for speciﬁc instance types. You need to specify absolute limits using the same schema
as Compute quota, including instance count, accelerators, vCPU, memory, or accelerator
partitions. You can specify absolute limits for one or more instance types in your team's quota.

For information on how these concepts are used, such as priority classes and name spaces, see
Example HyperPod task governance AWS CLI commands.

Sharing idle compute resources examples

The total reserved quota should not surpass the cluster's available capacity for that resource,

to ensure proper quota management. For example, if a cluster comprises 20 ml.c5.2xlarge
instances, the cumulative quota assigned to teams should remain under 20.

If the Compute allocation policies for teams allow for Lend and Borrow or Lend, the idle capacity
is shared between these teams. For example, Team A and Team B have Lend and Borrow enabled.
Team A has a quota of 6 but is using only 2 for its jobs, and Team B has a quota of 5 and is using 4
for its jobs. A job that is submitted to Team B requiring 4 resources. 3 will be borrowed from Team
A.

If any team's Compute allocation policy is set to Don't Lend, the team would not be able to
borrow any additional capacity beyond its own allocations.

Amazon EKS orchestration
1835

## Page 865

Amazon SageMaker AI
Developer Guide

How unallocated resource sharing works

Unallocated resource sharing automatically manages the pool of resources that are not allocated
to any compute quota in your cluster. This means HyperPod continuously monitors your cluster
state and automatically updates to the correct conﬁguration over time.

Initial Setup

• When you set IdleResourceSharing to Enabled in your ClusterSchedulerConﬁg (by default it

is Disabled), HyperPod task governance begins monitoring your cluster and calculates available
idle resources by subtracting team quotas from total node capacity.

• Unallocated resource sharing ClusterQueues are created to represent the borrowable resource
pool.

• When you ﬁrst enable unallocated resource sharing, infrastructure setup takes several

mins. You can monitor the progress through policy Status and DetailedStatus in

ClusterSchedulerConﬁg.

Ongoing Reconciliation

• HyperPod task governance continuously monitors for changes such as node additions or
removals and cluster queue quota updates.

• When changes occur, unallocated resource sharing recalculates quota and updates
ClusterQueues. Reconciliation typically completes within seconds.

Monitoring

You can verify that unallocated resource sharing is fully conﬁgured by checking for unallocated
resource sharing ClusterQueues:

kubectl get clusterqueue | grep hyperpod-ns-idle-resource-sharing

When you see ClusterQueues with names like hyperpod-ns-idle-resource-sharing-

cq-1, unallocated resource sharing is active. Note that multiple unallocated resource sharing
ClusterQueues may exist depending on the number of resource ﬂavors in your cluster.

Node eligibility for unallocated resource sharing

Unllocated Resource Sharing only includes nodes that meet the following requirements:

Amazon EKS orchestration
1836

## Page 866

Amazon SageMaker AI
Developer Guide

1. Node Ready Status

• Nodes must be in Ready status to contribute to the unallocated resource pool.

• Nodes in NotReady or other non-ready states are excluded from capacity calculations.

• When a node becomes Ready, it is automatically included in the next reconciliation cycle.

2. Node Schedulable Status

• Nodes with spec.unschedulable: true are excluded from unallocated resource sharing.

• When a node becomes schedulable again, it is automatically included in the next reconciliation
cycle.

3. MIG Conﬁguration (GPU nodes only)

• For GPU nodes with MIG (Multi-Instance GPU) partitioning, the nvidia.com/

mig.config.state label must show success for the node to contribute MIG proﬁles to

unallocated resource sharing.

• These nodes will be retried automatically once MIG conﬁguration completes successfully.

4. Supported Instance Types

• The instance must be a supported SageMaker HyperPod instance type.

• See the list of supported instance types in the SageMaker HyperPod cluster.

Topics

• Create policies

• Edit policies

• Delete policies

• Allocating compute quota in Amazon SageMaker HyperPod task governance

Create policies

You can create your Cluster policy and Compute allocation conﬁgurations in the Policies tab. The
following provides instructions on how to create the following conﬁgurations.

• Create your Cluster policy to update how tasks are prioritized and idle compute is allocated.

• Create Compute allocation to create a new compute allocation policy for a team.

Amazon EKS orchestration
1837

## Page 867

Amazon SageMaker AI
Developer Guide

Note

When you create a Compute allocation you will need to set up a Kubernetes role-based
access control (RBAC) for data scientist users in the corresponding namespace to run
tasks on HyperPod clusters orchestrated with Amazon EKS. The namespaces have the

format hyperpod-ns-team-name. To set up a Kubernetes RBAC, use the instructions in
create team role.

For information about the HyperPod task governance EKS cluster policy concepts, see Policies.

Create HyperPod task governance policies

This procedure assumes that you have already created an Amazon EKS cluster set up with
HyperPod. If you have not already done so, see Creating a SageMaker HyperPod cluster with
Amazon EKS orchestration.

1.
Navigate to the Amazon SageMaker AI console.

2.
On the left navigation pane, under HyperPod Clusters, choose Cluster Management.

3.
Choose your Amazon EKS cluster listed under SageMaker HyperPod clusters.

4.
Choose the Policies tab.

5.
To create your Cluster policy:

a.
Choose the corresponding Edit to update how tasks are prioritized and idle compute is
allocated.

b.
After you have made your changes, choose Submit.

6.
To create a Compute allocation:

7.
a.
Choose the corresponding Create. This takes you to the compute allocation creation page.

b.
After you have made your changes, choose Submit.

Edit policies

You can edit your Cluster policy and Compute allocation conﬁgurations in the Policies tab. The
following provides instructions on how to edit the following conﬁgurations.

• Edit your Cluster policy to update how tasks are prioritized and idle compute is allocated.

Amazon EKS orchestration
1838

## Page 868

Amazon SageMaker AI
Developer Guide

• Edit Compute allocation to create a new compute allocation policy for a team.

Note

When you create a Compute allocation you will need to set up a Kubernetes role-based
access control (RBAC) for data scientist users in the corresponding namespace to run
tasks on HyperPod clusters orchestrated with Amazon EKS. The namespaces have the

format hyperpod-ns-team-name. To set up a Kubernetes RBAC, use the instructions in
create team role.

For more information about the HyperPod task governance EKS cluster policy concepts, see
Policies.

Edit HyperPod task governance policies

This procedure assumes that you have already created an Amazon EKS cluster set up with
HyperPod. If you have not already done so, see Creating a SageMaker HyperPod cluster with
Amazon EKS orchestration.

1.
Navigate to the Amazon SageMaker AI console.

2.
On the left navigation pane, under HyperPod Clusters, choose Cluster Management.

3.
Choose your Amazon EKS cluster listed under SageMaker HyperPod clusters.

4.
Choose the Policies tab.

5.
To edit your Cluster policy:

a.
Choose the corresponding Edit to update how tasks are prioritized and idle compute is
allocated.

b.
After you have made your changes, choose Submit.

6.
To edit your Compute allocation:

7.
a.
Choose the conﬁguration you wish to edit under Compute allocation. This takes you to
the conﬁguration details page.

b.
If you wish to edit these conﬁgurations, choose Edit.

c.
After you have made your changes, choose Submit.

Amazon EKS orchestration
1839

## Page 869

Amazon SageMaker AI
Developer Guide

Delete policies

You can delete your Cluster policy and Compute allocation conﬁgurations using the SageMaker
AI console or AWS CLI. The following page provides instructions on how to delete your SageMaker

HyperPod task governance policies and conﬁgurations.

For more information about the HyperPod task governance EKS cluster policy concepts, see
Policies.

Note

If you are having issues with listing or deleting task governance policies, you may need to
update your cluster administrator minimum set of permissions. See the Amazon EKS tab in
the IAM users for cluster admin section. For additional information, see Deleting clusters.

Delete HyperPod task governance policies (console)

The following uses the SageMaker AI console to delete your HyperPod task governance policies.

Note

You cannot delete your Cluster policy (ClusterSchedulerConfig) using the SageMaker
AI console. To learn how to do so using the AWS CLI, see Delete HyperPod task governance
policies (AWS CLI).

To delete task governance policies (console)

1.
Navigate to the Amazon SageMaker AI console.

2.
On the left navigation pane, under HyperPod Clusters, choose Cluster Management.

3.
Choose your Amazon EKS cluster listed under SageMaker HyperPod clusters.

4.
Choose the Policies tab.

5.
To delete your Compute allocation (ComputeQuota):

a.
In the Compute allocation section, select the conﬁguration you want to delete.

b.
In the Actions dropdown menu, choose Delete.

Amazon EKS orchestration
1840

## Page 870

Amazon SageMaker AI
Developer Guide

c.
Follow the instructions in the UI to complete the task.

Delete HyperPod task governance policies (AWS CLI)

The following uses the AWS CLI to delete your HyperPod task governance policies.

Note

If you are having issues using the following commands, you may need to update your AWS
CLI. For more information, see Installing or updating to the latest version of the AWS CLI.

To delete task governance policies (AWS CLI)

First set your variables for the AWS CLI commands that follow.

REGION=aws-region

1.
Get the cluster-arn associated with the policies you wish to delete. You can use the
following AWS CLI command to list the clusters in your AWS Region.

aws sagemaker list-clusters \
--region ${REGION}

2.
To delete your compute allocations (ComputeQuota):

a.
List all of the compute quotas associated with the HyperPod cluster.

aws sagemaker list-compute-quotas \
--cluster-arn cluster-arn \
--region ${REGION}

b.
For each compute-quota-id you wish to delete, run the following command to delete
the compute quota.

aws sagemaker delete-compute-quota \
--compute-quota-id compute-quota-id \
--region ${REGION}

3.
To delete your cluster policies (ClusterSchedulerConfig):

Amazon EKS orchestration
1841

## Page 871

Amazon SageMaker AI
Developer Guide

a.
List all of the cluster policies associated with the HyperPod cluster.

aws sagemaker list-cluster-scheduler-configs \
--cluster-arn cluster-arn \
--region ${REGION}

b.
For each cluster-scheduler-config-id you wish to delete, run the following
command to delete the compute quota.

aws sagemaker delete-cluster-scheduler-config
--cluster-scheduler-config-id scheduler-config-id \
--region ${REGION}

Allocating compute quota in Amazon SageMaker HyperPod task governance

Cluster administrators can decide how the organization uses purchased compute. Doing so
reduces waste and idle resources. You can allocate compute quota such that teams can borrow
unused resources from each other. Compute quota allocation in HyperPod task governance
lets administrators allocate resources at the instance level and at a more granular resource
level. This capability provides ﬂexible and eﬃcient resource management for teams by allowing
granular control over individual compute resources instead of requiring entire instance allocations.
Allocating at a granular level eliminates ineﬃciencies of traditional instance-level allocation.
Through this approach, you can optimize resource utilization and reduce idle compute.

Compute quota allocation supports three types of resource allocation: accelerators, vCPU, and
memory. Accelerators are components in accelerated computer instances that that perform
functions, such as ﬂoating point number calculations, graphics processing, or data pattern
matching. Accelerators include GPUs, Trainium accelerators, and neuron cores. For multi-team
GPU sharing, diﬀerent teams can receive speciﬁc GPU allocations from the same instance type,
maximizing utilization of accelerator hardware. For memory-intensive workloads that require
additional RAM for data preprocessing or model caching scenarios, you can allocate memory
quota beyond the default GPU-to-memory ratio. For CPU-heavy preprocessing tasks that need
substantial CPU resources alongside GPU training, you can allocate independent CPU resource
allocation.

Once you provide a value, HyperPod task governance calculates the ratio using the formula
allocated resource divided by the total amount of resources available in the instance. HyperPod
task governance then uses this ratio to apply default allocations to other resources, but you can

Amazon EKS orchestration
1842

## Page 872

Amazon SageMaker AI
Developer Guide

override these defaults and customize them based on your use case. The following are sample
scenarios of how HyperPod task governance allocates resources based on your values:

• Only accelerator speciﬁed - HyperPod task governance applies the default ratio to vCPU and
memory based on the accelerator values.

• Only vCPU speciﬁed - HyperPod task governance calculates the ratio and applies it to memory.
Accelerators are set to 0.

• Only memory speciﬁed - HyperPod task governance calculates the ratio and applies it to vCPU
because compute is required to run memory-speciﬁed workloads. Accelerators are set to 0.

To programmatically control quota allocation, you can use the  ComputeQuotaResourceConﬁg
object and specify your allocations in integers.

{
"ComputeQuotaConfig": {
"ComputeQuotaResources": [{
"InstanceType": "ml.g5.24xlarge",
"Accelerators": "16",
"vCpu": "200.0",
"MemoryInGiB": "2.0"
}]
}
}

To see all of the allocated allocations, including the defaults, use the  DescribeComputeQuota
operation. To update your allocations, use the  UpdateComputeQuota operation.

You can also use the HyperPod CLI to allocate compute quotas. For more information about the
HyperPod CLI, see Running jobs on SageMaker HyperPod clusters orchestrated by Amazon EKS. The
following example demonstrates how to set compute quotas using the HyperPod CLI.

hyp create hyp-pytorch-job --version 1.1 --job-name sample-job \
--image 123456789012.dkr.ecr.us-west-2.amazonaws.com/ptjob:latest \
--pull-policy "Always" \
--tasks-per-node 1 \
--max-retry 1 \
--priority high-priority \
--namespace hyperpod-ns-team-name \
--queue-name hyperpod-ns-team-name-localqueue \
--instance-type sample-instance-type \

Amazon EKS orchestration
1843

## Page 873

Amazon SageMaker AI
Developer Guide

--accelerators 1 \
--vcpu 3 \
--memory 1 \
--accelerators-limit 1 \
--vcpu-limit 4 \
--memory-limit 2

To allocate quotas using the AWS console, follow these steps.

1.
Open the Amazon SageMaker AI console at https://console.aws.amazon.com/sagemaker/.

2.
Under HyperPod clusters, choose Cluster management.

3.
Under Compute allocations, choose Create.

4.
If you don’t already have instances, choose Add allocation to add an instance.

5.
Under Allocations, choose to allocate by instances or individual resources. If you allocate by
individual resources, SageMaker AI automatically assigns allocations to other resources by the
ratio that you chose. To override this ratio-based allocation, use the corresponding toggle to
override that compute.

6.
Repeat steps 4 and 5 to conﬁgure additional instances.

After allocating compute quota, you can then submit jobs through the HyperPod CLI or kubectl.
HyperPod eﬃciently schedules workloads based on available quota.

Allocating GPU partition quota

You can extend compute quota allocation to support GPU partitioning, enabling ﬁne-grained
resource sharing at the GPU partition level. When GPU partitioning is enabled on supported
GPUs in the cluster, each physical GPU can be partitioned into multiple isolated GPUs with
deﬁned compute, memory, and streaming multiprocessor allocations. For more information about
GPU partitioning, see Using GPU partitions in Amazon SageMaker HyperPod. You can allocate
speciﬁc GPU partitions to teams, allowing multiple teams to share a single GPU while maintaining
hardware-level isolation and predictable performance.

For example, an ml.p5.48xlarge instance with 8 H100 GPUs can be partitioned into GPU partitions,
and you can allocate individual partitions to diﬀerent teams based on their task requirements.
When you specify GPU partition allocations, HyperPod task governance calculates proportional
vCPU and memory quotas based on the GPU partition, similar to GPU-level allocation. This
approach maximizes GPU utilization by eliminating idle capacity and enabling cost-eﬀective
resource sharing across multiple concurrent tasks on the same physical GPU.

Amazon EKS orchestration
1844

## Page 874

Amazon SageMaker AI
Developer Guide

Creating Compute Quotas

aws sagemaker create-compute-quota \
--name "fractional-gpu-quota" \
--compute-quota-config '{
"ComputeQuotaResources": [
{
"InstanceType": "ml.p4d.24xlarge",
"AcceleratorPartition": {
"Count": 4,
"Type": "mig-1g.5gb"
}
}
],
"ResourceSharingConfig": {
"Strategy": "LendAndBorrow",
"BorrowLimit": 100

}
}'

Verifying Quota Resources

# Check ClusterQueue
kubectl get clusterqueues
kubectl describe clusterqueue QUEUE_NAME

# Check ResourceFlavors
kubectl get resourceflavor
kubectl describe resourceflavor FLAVOR_NAME

Example HyperPod task governance AWS CLI commands

You can use HyperPod with EKS through Kubectl or through HyperPod custom CLI. You can use
these commands through Studio or AWS CLI. The following provides SageMaker HyperPod task
governance examples, on how to view cluster details using the HyperPod AWS CLI commands. For
more information, including how to install, see the HyperPod CLI Github repository.

Topics

• Get cluster accelerator device quota information

• Submit a job to SageMaker AI-managed queue and namespace

• List jobs

Amazon EKS orchestration
1845

## Page 875

Amazon SageMaker AI
Developer Guide

• Get job detailed information

• Suspend and unsuspend jobs

• Debugging jobs

Get cluster accelerator device quota information

The following example command gets the information on the cluster accelerator device quota.

hyperpod get-clusters -n hyperpod-ns-test-team

The namespace in this example, hyperpod-ns-test-team, is created in Kubernetes based on the

team name provided, test-team, when the compute allocation is created. For more information,
see Edit policies.

Example response:

[
{
"Cluster": "hyperpod-eks-test-cluster-id",
"InstanceType": "ml.g5.xlarge",
"TotalNodes": 2,
"AcceleratorDevicesAvailable": 1,
"NodeHealthStatus=Schedulable": 2,
"DeepHealthCheckStatus=Passed": "N/A",
"Namespaces": {
"hyperpod-ns-test-team": {
"TotalAcceleratorDevices": 1,
"AvailableAcceleratorDevices": 1
}
}
}
]

Submit a job to SageMaker AI-managed queue and namespace

The following example command submits a job to your HyperPod cluster. If you have access to
only one team, the HyperPod AWS CLI will automatically assign the queue for you in this case.
Otherwise if multiple queues are discovered, we will display all viable options for you to select.

hyperpod start-job --job-name hyperpod-cli-test --job-kind kubeflow/PyTorchJob --image
docker.io/kubeflowkatib/pytorch-mnist-cpu:v1beta1-bc09cfd --entry-script /opt/pytorch-

Amazon EKS orchestration
1846

## Page 876

Amazon SageMaker AI
Developer Guide

mnist/mnist.py --pull-policy IfNotPresent --instance-type ml.g5.xlarge --node-count 1
--tasks-per-node 1 --results-dir ./result --priority training-priority

The priority classes are deﬁned in the Cluster policy, which deﬁnes how tasks are prioritized
and idle compute is allocated. When a data scientist submits a job, they use one of the priority

class names with the format priority-class-name-priority. In this example, training-

priority refers to the priority class named “training”. For more information on policy concepts,
see Policies.

If a priority class is not speciﬁed, the job is treated as a low priority job, with a task ranking value of
0.

If a priority class is speciﬁed, but does not correspond to one of the priority classes deﬁned in
the Cluster policy, the submission fails and an error message provides the deﬁned set of priority
classes.

You can also submit the job using a YAML conﬁguration ﬁle using the following command:

hyperpod start-job --config-file ./yaml-configuration-file-name.yaml

The following is an example YAML conﬁguration ﬁle that is equivalent to submitting a job as
discussed above.

defaults:
- override hydra/job_logging: stdout
hydra:
run:
dir: .
output_subdir: null
training_cfg:
entry_script: /opt/pytorch-mnist/mnist.py
script_args: []
run:
name: hyperpod-cli-test
nodes: 1
ntasks_per_node: 1
cluster:
cluster_type: k8s
instance_type: ml.g5.xlarge
custom_labels:
kueue.x-k8s.io/priority-class: training-priority
cluster_config:

Amazon EKS orchestration
1847

## Page 877

Amazon SageMaker AI
Developer Guide

label_selector:
required:
sagemaker.amazonaws.com/node-health-status:
- Schedulable
preferred:
sagemaker.amazonaws.com/deep-health-check-status:
- Passed
weights:
- 100
pullPolicy: IfNotPresent
base_results_dir: ./result
container: docker.io/kubeflowkatib/pytorch-mnist-cpu:v1beta1-bc09cfd
env_vars:
NCCL_DEBUG: INFO

Alternatively, you can submit a job using kubectl to ensure the task appears in the Dashboard
tab. The following is an example kubectl command.

kubectl apply -f ./yaml-configuration-file-name.yaml

When submitting the job, include your queue name and priority class labels. For example, with

the queue name hyperpod-ns-team-name-localqueue and priority class priority-class-

name-priority, you must include the following labels:

• kueue.x-k8s.io/queue-name: hyperpod-ns-team-name-localqueue

• kueue.x-k8s.io/priority-class: priority-class-name-priority

The following YAML conﬁguration snippet demonstrates how to add labels to your original
conﬁguration ﬁle to ensure your task appears in the Dashboard tab:

metadata:
name: job-name
namespace: hyperpod-ns-team-name
labels:
kueue.x-k8s.io/queue-name: hyperpod-ns-team-name-localqueue
kueue.x-k8s.io/priority-class: priority-class-name-priority

List jobs

The following command lists the jobs and their details.

Amazon EKS orchestration
1848

## Page 878

Amazon SageMaker AI
Developer Guide

hyperpod list-jobs

Example response:

{
"jobs": [
{
"Name": "hyperpod-cli-test",
"Namespace": "hyperpod-ns-test-team",
"CreationTime": "2024-11-18T21:21:15Z",
"Priority": "training",
"State": "Succeeded"
}
]
}

Get job detailed information

The following command provides a job’s details. If no namespace is speciﬁed, HyperPod AWS CLI
will fetch a namespace managed by SageMaker AI that you have access to.

hyperpod get-job --job-name hyperpod-cli-test

Example response:

{
"Name": "hyperpod-cli-test",
"Namespace": "hyperpod-ns-test-team",
"Label": {
"app": "hyperpod-cli-test",
"app.kubernetes.io/managed-by": "Helm",
"kueue.x-k8s.io/priority-class": "training"
},
"CreationTimestamp": "2024-11-18T21:21:15Z",
"Status": {
"completionTime": "2024-11-18T21:25:24Z",
"conditions": [
{
"lastTransitionTime": "2024-11-18T21:21:15Z",
"lastUpdateTime": "2024-11-18T21:21:15Z",
"message": "PyTorchJob hyperpod-cli-test is created.",
"reason": "PyTorchJobCreated",

Amazon EKS orchestration
1849

## Page 879

Amazon SageMaker AI
Developer Guide

"status": "True",
"type": "Created"
},
{
"lastTransitionTime": "2024-11-18T21:21:17Z",
"lastUpdateTime": "2024-11-18T21:21:17Z",
"message": "PyTorchJob hyperpod-ns-test-team/hyperpod-cli-test is
running.",
"reason": "PyTorchJobRunning",
"status": "False",
"type": "Running"
},
{
"lastTransitionTime": "2024-11-18T21:25:24Z",
"lastUpdateTime": "2024-11-18T21:25:24Z",
"message": "PyTorchJob hyperpod-ns-test-team/hyperpod-cli-test
successfully completed.",

"reason": "PyTorchJobSucceeded",
"status": "True",
"type": "Succeeded"
}
],
"replicaStatuses": {
"Worker": {
"selector": "training.kubeflow.org/job-name=hyperpod-cli-
test,training.kubeflow.org/operator-name=pytorchjob-controller,training.kubeflow.org/
replica-type=worker",
"succeeded": 1
}
},
"startTime": "2024-11-18T21:21:15Z"
},
"ConsoleURL": "https://us-west-2.console.aws.amazon.com/sagemaker/home?region=us-
west-2#/cluster-management/hyperpod-eks-test-cluster-id“
}

Suspend and unsuspend jobs

If you want to remove some submitted job from the scheduler, HyperPod AWS CLI provides

suspend command to temporarily remove the job from orchestration. The suspended job will no

longer be scheduled unless the job is manually unsuspended by the unsuspend command

To temporarily suspend a job:

Amazon EKS orchestration
1850

## Page 880

Amazon SageMaker AI
Developer Guide

hyperpod patch-job suspend --job-name hyperpod-cli-test

To add a job back to the queue:

hyperpod patch-job unsuspend --job-name hyperpod-cli-test

Debugging jobs

The HyperPod AWS CLI also provides other commands for you to debug job submission issues. For

example list-pods and get-logs in the HyperPod AWS CLI Github repository.

Troubleshoot

The following page contains known solutions for troubleshooting your HyperPod EKS clusters.

Topics

• Dashboard tab

• Tasks tab

• Policies

• Deleting clusters

• Unallocated resource sharing

Dashboard tab

The EKS add-on fails to install

For the EKS add-on installation to succeed, you will need to have a Kubernets version >= 1.30. To
update, see Update Kubernetes version.

For the EKS add-on installation to succeed, all of the nodes need to be in Ready status and all of
the pods need to be in Running status.

To check the status of your nodes, use the list-cluster-nodes AWS CLI command or navigate
to your EKS cluster in the EKS console and view the status of your nodes. Resolve the issue for
each node or reach out to your administrator. If the node status is Unknown, delete the node.
Once all nodes statuses are Ready, retry installing the EKS add-on in HyperPod from the Amazon
SageMaker AI console.

Amazon EKS orchestration
1851

## Page 881

Amazon SageMaker AI
Developer Guide

To check the status of your pods, use the Kubernetes CLI command kubectl get pods -n

cloudwatch-agent or navigate to your EKS cluster in the EKS console and view the status of your

pods with the namespace cloudwatch-agent. Resolve the issue for the pods or reach out to your
administrator to resolve the issues. Once all pod statuses are Running, retry installing the EKS add-

on in HyperPod from the Amazon SageMaker AI console.

For more troubleshooting, see Troubleshooting the Amazon CloudWatch Observability EKS add-on.

Tasks tab

If you see the error message about how the Custom Resource Deﬁnition (CRD) is not conﬁgured

on the cluster, grant EKSAdminViewPolicy and ClusterAccessRole policies to your domain
execution role.

• For information on how to get your execution role, see Get your execution role.

• To learn how to attach policies to an IAM user or group, see Adding and removing IAM identity
permissions.

Policies

The following lists solutions to errors relating to policies using the HyperPod APIs or console.

• If the policy is in CreateFailed or CreateRollbackFailed status, you need to delete the
failed policy and create a new one.

• If the policy is in UpdateFailed status, retry the update with the same policy ARN.

• If the policy is in UpdateRollbackFailed status, you need to delete the failed policy and then
create a new one.

• If the policy is in DeleteFailed or DeleteRollbackFailed status, retry the delete with the
same policy ARN.

• If you ran into an error while trying to delete the Compute prioritization, or cluster policy,

using the HyperPod console, try to delete the cluster-scheduler-config using the API.
To check the status of the resource, go to the details page of a compute allocation.

To see more details into the failure, use the describe API.

Deleting clusters

The following lists known solutions to errors relating to deleting clusters.

Amazon EKS orchestration
1852

## Page 882

Amazon SageMaker AI
Developer Guide

• When cluster deletion fails due to attached SageMaker HyperPod task governance policies, you
will need to Delete policies.

• When cluster deletion fails due to the missing the following permissions, you will need to update
your cluster administrator minimum set of permissions. See the Amazon EKS tab in the IAM
users for cluster admin section.

• sagemaker:ListComputeQuotas

• sagemaker:ListClusterSchedulerConfig

• sagemaker:DeleteComputeQuota

• sagemaker:DeleteClusterSchedulerConfig

Unallocated resource sharing

If your unallocated resource pool capacity is less than expected:

1. Check node ready status

kubectl get nodes

Verify all nodes show Ready status in the STATUS column.

2. Check node schedulable status

kubectl get nodes -o custom-
columns=NAME:.metadata.name,UNSCHEDULABLE:.spec.unschedulable

Verify nodes show <none> or false (not true).

3. List unallocated resource sharing ClusterQueues:

kubectl get clusterqueue | grep hyperpod-ns-idle-resource-sharing

This shows all unallocated resource sharing ClusterQueues. If the ClusterQueues are not showing

up, check the FailureReason under ClusterSchedulerConﬁg policy to see if there are any
failure messages to continue the debugging.

4. Verify unallocated resource sharing quota:

kubectl describe clusterqueue hyperpod-ns-idle-resource-sharing-<index>

Amazon EKS orchestration
1853

## Page 883

Amazon SageMaker AI
Developer Guide

Check the spec.resourceGroups[].flavors[].resources section to see the quota
allocated for each resource ﬂavor.

Multiple unallocated resource sharing ClusterQueues may exist depending on the number of

resource ﬂavors in your cluster.

5. Check MIG conﬁguration status (GPU nodes):

kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}
{.metadata.labels.nvidia\.com/mig\.config\.state}{"\n"}{end}'

Verify MIG-enabled nodes show success state.

Attribution document for Amazon SageMaker HyperPod task governance

In the following you can learn about attributions and third-party licenses for material used in
Amazon SageMaker HyperPod task governance.

Topics

• base-ﬁles

• netbase

• golang-lru

base-ﬁles

This is the Debian prepackaged version of the Debian Base System
Miscellaneous files. These files were written by Ian Murdock
<imurdock@debian.org> and Bruce Perens <bruce@pixar.com>.

This package was first put together by Bruce Perens <Bruce@Pixar.com>,
from his own sources.

The GNU Public Licenses in /usr/share/common-licenses were taken from
ftp.gnu.org and are copyrighted by the Free Software Foundation, Inc.

The Artistic License in /usr/share/common-licenses is the one coming
from Perl and its SPDX name is "Artistic License 1.0 (Perl)".

Amazon EKS orchestration
1854

## Page 884

Amazon SageMaker AI
Developer Guide

Copyright © 1995-2011 Software in the Public Interest.

This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

On Debian systems, the complete text of the GNU General
Public License can be found in `/usr/share/common-licenses/GPL'.

netbase

Format: https://www.debian.org/doc/packaging-manuals/copyright-format/1.0/
Comment:
This package was created by Peter Tobias tobias@et-inf.fho-emden.de on
Wed, 24 Aug 1994 21:33:28 +0200 and maintained by Anthony Towns
<ajt@debian.org> until 2001.
It is currently maintained by Marco d'Itri <md@linux.it>.

Files: *
Copyright:
Copyright © 1994-1998 Peter Tobias
Copyright © 1998-2001 Anthony Towns
Copyright © 2002-2022 Marco d'Itri
License: GPL-2
This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License, version 2, as
published by the Free Software Foundation.
.
This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.
.
You should have received a copy of the GNU General Public License along
with this program; if not, write to the Free Software Foundation,
Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.
.

Amazon EKS orchestration
1855

## Page 885

Amazon SageMaker AI
Developer Guide

On Debian systems, the complete text of the GNU General Public License
version 2 can be found in '/usr/share/common-licenses/GPL-2'.

golang-lru

Copyright © 2014 HashiCorp, Inc.

Mozilla Public License, version 2.0

1. Definitions

1.1. "Contributor"

means each individual or legal entity that creates, contributes to the
creation of, or owns Covered Software.

1.2. "Contributor Version"

means the combination of the Contributions of others (if any) used by a
Contributor and that particular Contributor's Contribution.

1.3. "Contribution"

means Covered Software of a particular Contributor.

1.4. "Covered Software"

means Source Code Form to which the initial Contributor has attached the
notice in Exhibit A, the Executable Form of such Source Code Form, and
Modifications of such Source Code Form, in each case including portions
thereof.

1.5. "Incompatible With Secondary Licenses"
means

a. that the initial Contributor has attached the notice described in
Exhibit B to the Covered Software; or

b. that the Covered Software was made available under the terms of
version 1.1 or earlier of the License, but not also under the terms of
a Secondary License.

1.6. "Executable Form"

Amazon EKS orchestration
1856

## Page 886

Amazon SageMaker AI
Developer Guide

means any form of the work other than Source Code Form.

1.7. "Larger Work"

means a work that combines Covered Software with other material, in a
separate file or files, that is not Covered Software.

1.8. "License"

means this document.

1.9. "Licensable"

means having the right to grant, to the maximum extent possible, whether
at the time of the initial grant or subsequently, any and all of the
rights conveyed by this License.

1.10. "Modifications"

means any of the following:

a. any file in Source Code Form that results from an addition to,
deletion from, or modification of the contents of Covered Software; or

b. any new file in Source Code Form that contains any Covered Software.

1.11. "Patent Claims" of a Contributor

means any patent claim(s), including without limitation, method,
process, and apparatus claims, in any patent Licensable by such
Contributor that would be infringed, but for the grant of the License,
by the making, using, selling, offering for sale, having made, import,
or transfer of either its Contributions or its Contributor Version.

1.12. "Secondary License"

means either the GNU General Public License, Version 2.0, the GNU Lesser
General Public License, Version 2.1, the GNU Affero General Public
License, Version 3.0, or any later versions of those licenses.

1.13. "Source Code Form"

means the form of the work preferred for making modifications.

Amazon EKS orchestration
1857

## Page 887

Amazon SageMaker AI
Developer Guide

1.14. "You" (or "Your")

means an individual or a legal entity exercising rights under this
License. For legal entities, "You" includes any entity that controls, is
controlled by, or is under common control with You. For purposes of this
definition, "control" means (a) the power, direct or indirect, to cause
the direction or management of such entity, whether by contract or
otherwise, or (b) ownership of more than fifty percent (50%) of the
outstanding shares or beneficial ownership of such entity.

2. License Grants and Conditions

2.1. Grants

Each Contributor hereby grants You a world-wide, royalty-free,

non-exclusive license:

a. under intellectual property rights (other than patent or trademark)
Licensable by such Contributor to use, reproduce, make available,
modify, display, perform, distribute, and otherwise exploit its
Contributions, either on an unmodified basis, with Modifications, or
as part of a Larger Work; and

b. under Patent Claims of such Contributor to make, use, sell, offer for
sale, have made, import, and otherwise transfer either its
Contributions or its Contributor Version.

2.2. Effective Date

The licenses granted in Section 2.1 with respect to any Contribution
become effective for each Contribution on the date the Contributor first
distributes such Contribution.

2.3. Limitations on Grant Scope

The licenses granted in this Section 2 are the only rights granted under
this License. No additional rights or licenses will be implied from the
distribution or licensing of Covered Software under this License.
Notwithstanding Section 2.1(b) above, no patent license is granted by a
Contributor:

a. for any code that a Contributor has removed from Covered Software; or

Amazon EKS orchestration
1858

## Page 888

Amazon SageMaker AI
Developer Guide

b. for infringements caused by: (i) Your and any other third party's
modifications of Covered Software, or (ii) the combination of its
Contributions with other software (except as part of its Contributor
Version); or

c. under Patent Claims infringed by Covered Software in the absence of
its Contributions.

This License does not grant any rights in the trademarks, service marks,
or logos of any Contributor (except as may be necessary to comply with
the notice requirements in Section 3.4).

2.4. Subsequent Licenses

No Contributor makes additional grants as a result of Your choice to
distribute the Covered Software under a subsequent version of this

License (see Section 10.2) or under the terms of a Secondary License (if
permitted under the terms of Section 3.3).

2.5. Representation

Each Contributor represents that the Contributor believes its
Contributions are its original creation(s) or it has sufficient rights to
grant the rights to its Contributions conveyed by this License.

2.6. Fair Use

This License is not intended to limit any rights You have under
applicable copyright doctrines of fair use, fair dealing, or other
equivalents.

2.7. Conditions

Sections 3.1, 3.2, 3.3, and 3.4 are conditions of the licenses granted in
Section 2.1.

3. Responsibilities

3.1. Distribution of Source Form

All distribution of Covered Software in Source Code Form, including any
Modifications that You create or to which You contribute, must be under

Amazon EKS orchestration
1859

## Page 889

Amazon SageMaker AI
Developer Guide

the terms of this License. You must inform recipients that the Source
Code Form of the Covered Software is governed by the terms of this
License, and how they can obtain a copy of this License. You may not
attempt to alter or restrict the recipients' rights in the Source Code
Form.

3.2. Distribution of Executable Form

If You distribute Covered Software in Executable Form then:

a. such Covered Software must also be made available in Source Code Form,
as described in Section 3.1, and You must inform recipients of the
Executable Form how they can obtain a copy of such Source Code Form by
reasonable means in a timely manner, at a charge no more than the cost
of distribution to the recipient; and

b. You may distribute such Executable Form under the terms of this

License, or sublicense it under different terms, provided that the
license for the Executable Form does not attempt to limit or alter the
recipients' rights in the Source Code Form under this License.

3.3. Distribution of a Larger Work

You may create and distribute a Larger Work under terms of Your choice,
provided that You also comply with the requirements of this License for
the Covered Software. If the Larger Work is a combination of Covered
Software with a work governed by one or more Secondary Licenses, and the
Covered Software is not Incompatible With Secondary Licenses, this
License permits You to additionally distribute such Covered Software
under the terms of such Secondary License(s), so that the recipient of
the Larger Work may, at their option, further distribute the Covered
Software under the terms of either this License or such Secondary
License(s).

3.4. Notices

You may not remove or alter the substance of any license notices
(including copyright notices, patent notices, disclaimers of warranty, or
limitations of liability) contained within the Source Code Form of the
Covered Software, except that You may alter any license notices to the
extent required to remedy known factual inaccuracies.

3.5. Application of Additional Terms

Amazon EKS orchestration
1860

## Page 890

Amazon SageMaker AI
Developer Guide

You may choose to offer, and to charge a fee for, warranty, support,
indemnity or liability obligations to one or more recipients of Covered
Software. However, You may do so only on Your own behalf, and not on
behalf of any Contributor. You must make it absolutely clear that any
such warranty, support, indemnity, or liability obligation is offered by
You alone, and You hereby agree to indemnify every Contributor for any
liability incurred by such Contributor as a result of warranty, support,
indemnity or liability terms You offer. You may include additional
disclaimers of warranty and limitations of liability specific to any
jurisdiction.

4. Inability to Comply Due to Statute or Regulation

If it is impossible for You to comply with any of the terms of this License
with respect to some or all of the Covered Software due to statute,
judicial order, or regulation then You must: (a) comply with the terms of
this License to the maximum extent possible; and (b) describe the

limitations and the code they affect. Such description must be placed in a
text file included with all distributions of the Covered Software under
this License. Except to the extent prohibited by statute or regulation,
such description must be sufficiently detailed for a recipient of ordinary
skill to be able to understand it.

5. Termination

5.1. The rights granted under this License will terminate automatically if You
fail to comply with any of its terms. However, if You become compliant,
then the rights granted under this License from a particular Contributor
are reinstated (a) provisionally, unless and until such Contributor
explicitly and finally terminates Your grants, and (b) on an ongoing
basis, if such Contributor fails to notify You of the non-compliance by
some reasonable means prior to 60 days after You have come back into
compliance. Moreover, Your grants from a particular Contributor are
reinstated on an ongoing basis if such Contributor notifies You of the
non-compliance by some reasonable means, this is the first time You have
received notice of non-compliance with this License from such
Contributor, and You become compliant prior to 30 days after Your receipt
of the notice.

5.2. If You initiate litigation against any entity by asserting a patent
infringement claim (excluding declaratory judgment actions,
counter-claims, and cross-claims) alleging that a Contributor Version
directly or indirectly infringes any patent, then the rights granted to
You by any and all Contributors for the Covered Software under Section

Amazon EKS orchestration
1861

## Page 891

Amazon SageMaker AI
Developer Guide

2.1 of this License shall terminate.

5.3. In the event of termination under Sections 5.1 or 5.2 above, all end user
license agreements (excluding distributors and resellers) which have been
validly granted by You or Your distributors under this License prior to
termination shall survive termination.

6. Disclaimer of Warranty

Covered Software is provided under this License on an "as is" basis,
without warranty of any kind, either expressed, implied, or statutory,
including, without limitation, warranties that the Covered Software is free
of defects, merchantable, fit for a particular purpose or non-infringing.
The entire risk as to the quality and performance of the Covered Software
is with You. Should any Covered Software prove defective in any respect,
You (not any Contributor) assume the cost of any necessary servicing,
repair, or correction. This disclaimer of warranty constitutes an essential

part of this License. No use of  any Covered Software is authorized under
this License except under this disclaimer.

7. Limitation of Liability

Under no circumstances and under no legal theory, whether tort (including
negligence), contract, or otherwise, shall any Contributor, or anyone who
distributes Covered Software as permitted above, be liable to You for any
direct, indirect, special, incidental, or consequential damages of any
character including, without limitation, damages for lost profits, loss of
goodwill, work stoppage, computer failure or malfunction, or any and all
other commercial damages or losses, even if such party shall have been
informed of the possibility of such damages. This limitation of liability
shall not apply to liability for death or personal injury resulting from
such party's negligence to the extent applicable law prohibits such
limitation. Some jurisdictions do not allow the exclusion or limitation of
incidental or consequential damages, so this exclusion and limitation may
not apply to You.

8. Litigation

Any litigation relating to this License may be brought only in the courts
of a jurisdiction where the defendant maintains its principal place of
business and such litigation shall be governed by laws of that
jurisdiction, without reference to its conflict-of-law provisions. Nothing
in this Section shall prevent a party's ability to bring cross-claims or
counter-claims.

Amazon EKS orchestration
1862

## Page 892

Amazon SageMaker AI
Developer Guide

9. Miscellaneous

This License represents the complete agreement concerning the subject
matter hereof. If any provision of this License is held to be
unenforceable, such provision shall be reformed only to the extent
necessary to make it enforceable. Any law or regulation which provides that
the language of a contract shall be construed against the drafter shall not
be used to construe this License against a Contributor.

10. Versions of the License

10.1. New Versions

Mozilla Foundation is the license steward. Except as provided in Section
10.3, no one other than the license steward has the right to modify or

publish new versions of this License. Each version will be given a
distinguishing version number.

10.2. Effect of New Versions

You may distribute the Covered Software under the terms of the version
of the License under which You originally received the Covered Software,
or under the terms of any subsequent version published by the license
steward.

10.3. Modified Versions

If you create software not governed by this License, and you want to
create a new license for such software, you may create and use a
modified version of this License if you rename the license and remove
any references to the name of the license steward (except to note that
such modified license differs from this License).

10.4. Distributing Source Code Form that is Incompatible With Secondary
Licenses If You choose to distribute Source Code Form that is
Incompatible With Secondary Licenses under the terms of this version of
the License, the notice described in Exhibit B of this License must be
attached.

Exhibit A - Source Code Form License Notice

This Source Code Form is subject to the

Amazon EKS orchestration
1863

## Page 893

Amazon SageMaker AI
Developer Guide

terms of the Mozilla Public License, v.
2.0. If a copy of the MPL was not
distributed with this file, You can
obtain one at
http://mozilla.org/MPL/2.0/.

If it is not possible or desirable to put the notice in a particular file,
then You may include the notice in a location (such as a LICENSE file in a
relevant directory) where a recipient would be likely to look for such a
notice.

You may add additional accurate notices of copyright ownership.

Exhibit B - "Incompatible With Secondary Licenses" Notice

This Source Code Form is "Incompatible
With Secondary Licenses", as defined by

the Mozilla Public License, v. 2.0.

Usage reporting for cost attribution in SageMaker HyperPod

Usage reporting in SageMaker HyperPod EKS-orchestrated clusters provides granular visibility into
compute resource consumption. The capability allows organizations to implement transparent cost
attribution, allocating cluster costs to teams, projects, or departments based on their actual usage.
By tracking metrics such as GPU/CPU hours, and Neuron Core utilization - captured in both team-
level aggregates and task-speciﬁc breakdowns - usage reporting complements HyperPod's Task
Governance functionality, ensuring fair cost distribution in shared multi-tenant clusters by:

• Eliminating guesswork in cost allocation

• Directly linking expenses to measurable resource consumption

• Enforcing usage-based accountability in shared infrastructure environments

Prerequisites

To use this capability:

• You need:

• An active SageMaker HyperPod environment with a running EKS-orchestrated cluster.

• (Strongly recommended) Task Governance conﬁgured with compute quotas and priority rules.
For setup instructions, see Task Governance setup.

Amazon EKS orchestration
1864

## Page 894

Amazon SageMaker AI
Developer Guide

• Familiarize yourself with these core concepts:

• Allocated compute quota: Resources reserved for a team based on predeﬁned quotas in their
Task Governance policies. This is guaranteed capacity for their workloads.

• Borrowed compute: Idle resources from the shared cluster pool that teams can temporarily
use beyond their allocated quota. Borrowed compute is assigned dynamically based on priority
rules in the Task Governance policies and availability of unused resources.

• Compute usage: The measurement of resources (GPU, CPU, Neuron Core hours) consumed by
a team, tracked as:

• Allocated utilization: Usage within the team's quota.

• Borrowed utilization: Usage beyond the quota, drawn from the shared pool.

• Cost attribution: The process of allocating cluster costs to teams based on their actual
compute usage, including both resources consumed within their predeﬁned quota and
resources temporarily used from the shared cluster pool beyond their quota.

Reports types

HyperPod's usage reports provide varying operational granularity:

• Summary reports provide organization-wide visibility into compute usage, aggregating total
GPU/CPU/Neuron Core hours per team (namespace) while distinguishing between regular usage
(resources from a team's allocated quota) and borrowed compute (overﬂow capacity from shared
pools).

• Detailed reports oﬀer task-level breakdowns by team, tracking exact compute hours spent
running speciﬁc tasks – including preempted tasks, hourly utilization patterns, and namespace-
speciﬁc allocations.

Important

HyperPod usage reporting tracks compute utilization across all Kubernetes namespaces
in a cluster—including those managed by Task Governance, default namespaces, and
namespaces created outside of Task Governance (e.g., via direct Kubernetes API calls or
external tools). This infrastructure-level monitoring ensures comprehensive usage-based
accountability, preventing gaps in cost attribution for shared clusters regardless of how
namespaces are managed.

Amazon EKS orchestration
1865

## Page 895

Amazon SageMaker AI
Developer Guide

Reports formats and time range

Using the Python script provided in the section called “Generate a report”, administrators can
generate usage reports on demand in CSV or PDF formats, selecting time ranges from daily
snapshots to 180-day (6-month) historical windows.

Note

You can conﬁgure the historical window to extend beyond the default 180-day maximum
when setting up the reporting infrastructure. For more information on conﬁguring the data
retention period, see Install Usage Report Infrastructure using CloudFormation.

Illustrative use cases

This capability addresses critical scenarios in multi-tenant AI/ML environments such as:

1. Cost allocation for shared clusters: An administrator manages a HyperPod cluster shared by

20 teams training generative AI models. Using a summary usage report, they analyze daily GPU
utilization over 180 days and discover Team A consumed 200 GPU hours of a speciﬁc instance
type—170 from their allocated quota and 30 from borrowed compute. The administrator
invoices Team A based on this reported usage.

2. Auditing and dispute resolution: A ﬁnance team questions cost attribution accuracy, citing

inconsistencies. The administrator can export a detailed task-level report to audit discrepancies.
By cross-referencing timestamps, instance types, and preempted jobs within the team's
namespace, the report transparently reconcile disputed usage data.

Reports details and data breakdown

SageMaker HyperPod's usage reports provide two distinct lenses for analyzing compute resource
consumption: summary reports for cost allocation and detailed reports for granular auditing.
Summary reports aggregate cluster-wide usage by team or namespace, highlighting trends in
allocated versus borrowed compute across GPU, CPU, and Neuron Core resources. Detailed reports
drill into individual tasks, exposing metrics such as execution windows, task status, and priority-
class utilization. In this section, we break down the structure of these reports, understand their
key metrics, and demonstrate how administrators and ﬁnance teams can cross-reference summary
trends with task-level data to validate cost attribution accuracy, resolve discrepancies, and optimize
shared infrastructure.

Amazon EKS orchestration
1866

## Page 896

Amazon SageMaker AI
Developer Guide

Common report headers

Both summary and detailed reports include the following metadata to contextualize the usage
data:

• ClusterName: The EKS-orchestrated Hyperpod cluster name where resources were consumed.

• Type: The report category (Summary Utilization Report or Detailed Utilization

Report).

• Date Generated: When the report was created (e.g., 2025-04-18).

• Date Range (UTC): The timeframe covered (e.g., 2025-04-16 to 2025-04-18).

• Missing data periods: Gaps in data collection due to cluster downtime or monitoring issues (e.g.,

2025-04-16 00:00:00 to 2025-04-19 00:00:00).

Summary reports

Summary reports provide a per-day high-level overview of compute resource consumption across
teams/namespaces, and instance types distinguishing between allocated (reserved quota) and
borrowed (lended pool) utilization. These reports are ideal for invoice generation, cost attribution
statements, or capacity forecasting.

Example: A summary report might show that Team A used 200 GPU hours—170 from their allocated
quota and 30 borrowed.

Here's a structured breakdown of the key columns in a summary report:

• Date: The date of the reported usage (e.g., 2025-04-18).

• Namespace: The Kubernetes namespace associated with the team (e.g., hyperpod-ns-ml-

team).

• Team: The Owning team/department (e.g., ml-team).

• Instance Type: The compute instance used (e.g., ml.g5.4xlarge).

• Total/Allocated/Borrowed Utilization (Hours): The breakdown of GPU, CPU, or Neuron Core
usage by category.

Where:

• Total utilization = Allocated utilization + Borrowed utilization

• Allocated utilization is the actual GPU CPU, or Neuron Core hours a team has used, capped at
100% of their allocated quota.

Amazon EKS orchestration
1867

## Page 897

Amazon SageMaker AI
Developer Guide

• Borrowed utilization is the actual GPU, CPU, or Neuron Core hours a team has used beyond
their allocated quota, drawn from the shared cluster pool based on Task Governance priority
rules and resource availability.

Example: 72 GPU hours total (48 allocated, 24 borrowed).

Note

Only total utilization is displayed for namespaces not managed by Task Governance.

Detailed reports

Detailed reports provide forensic-level visibility into compute usage, breaking down resource
consumption by task, exposing granular metrics like task execution windows, status (e.g.,
Succeeded, Failed), and priority-class usage. These reports are ideal for billing discrepancies
validation, or ensuring compliance with governance policies.

Here's a structured breakdown of the key columns in a detailed report:

• Date: The date of the reported usage (e.g., 2025-04-18).

• Period Start/End: Exact execution window (UTC) for the task. (e.g., 19:54:34)

• Namespace: The Kubernetes namespace associated with the team (e.g., hyperpod-ns-ml-

team).

• Team: The Owning team/department (e.g., ml-team).

• Task: The identiﬁer for the job/pod (e.g., pytorchjob-ml-pytorch-job-2p5zt-db686).

• Instance: The compute instance used (e.g., ml.g5.4xlarge).

• Status: Task outcome (Succeeded, Failed, Preempted).

• Total Utilization: Total consumption (hours and instance count) of GPU, CPU, or Neuron Core
resources.

• Priority Class: The priority tier assigned (e.g., training-priority).

Amazon EKS orchestration
1868

## Page 898

Amazon SageMaker AI
Developer Guide

Generate reports

This guide provides step-by-step instructions to conﬁgure and manage usage reporting for your
SageMaker HyperPod clusters. Follow these procedures to deploy infrastructure, generate custom
reports, and remove resources when no longer needed.

Set up usage reporting

Note

Before conﬁguring the SageMaker HyperPod usage report infrastructure in your SageMaker

HyperPod cluster, ensure you have met all prerequisites detailed in this README.md.

Usage reporting in HyperPod requires:

• Deploying SageMaker HyperPod usage report AWS resources using an CloudFormation stack

• Installing the SageMaker HyperPod usage report Kubernetes operator via a Helm chart

You can ﬁnd comprehensive installation instructions in the SageMaker HyperPod usage report
GitHub repository. Speciﬁcally, follow the steps in the Set up section.

Generate usage reports on demand

Once the usage reporting infrastructure and Kubernetes operator are installed, job data for your
SageMaker HyperPod cluster is automatically collected and stored in the S3 bucket you conﬁgured
during setup. The operator continuously captures detailed usage metrics in the background,

creating raw data ﬁles in the raw directory of your designated S3 bucket.

To generate an on-demand usage report, you can use the run.py script provided in the SageMaker
HyperPod usage report GitHub repository to extract and export usage metrics. Speciﬁcally, you
can ﬁnd the script and comprehensive instructions for generating a report in the Generate Reports
section.

The script allows you to:

• Specify custom date ranges for report generation

• Choose between detailed and summary report types

• Export reports in CSV or PDF formats

Amazon EKS orchestration
1869

## Page 899

Amazon SageMaker AI
Developer Guide

• Direct reports to a speciﬁc S3 location

Clean up usage reporting resources

When you no longer need your SageMaker HyperPod usage reporting infrastructure, follow the
steps in Clean Up Resources to clean up the Kubernetes operator and AWS resources (in that order).
Proper resource deletion helps prevent unnecessary costs.

Conﬁguring storage for SageMaker HyperPod clusters orchestrated by Amazon EKS

Cluster admin needs to conﬁgure storage for data scientist users to manage input and output data
and storing checkpoints during training on SageMaker HyperPod clusters.

Handling large datasets (input/output data)

• Data access and management: Data scientists often work with large datasets that are required
for training machine learning models. Specifying storage parameters in the job submission
allows them to deﬁne where these datasets are located (e.g., Amazon S3 buckets, persistent
volumes in Kubernetes) and how they are accessed during the job execution.

• Performance optimization: The eﬃciency of accessing input data can signiﬁcantly impact the
performance of the training job. By optimizing storage parameters, data scientists can ensure
that data is read and written eﬃciently, reducing I/O bottlenecks.

Storing checkpoints

• Checkpointing in training: During long-running training jobs, it’s common practice to save
checkpoints—intermediate states of the model. This allows data scientists to resume training
from a speciﬁc point in case of a failure, rather than starting from scratch.

• Data recovery and experimentation: By specifying the storage location for checkpoints, data
scientists can ensure that these checkpoints are securely stored, potentially in a distributed
storage system that oﬀers redundancy and high availability. This is crucial for recovering from
interruptions and for experimenting with diﬀerent training strategies.

Tip

For a hands-on experience and guidance on how to set up storage for SageMaker HyperPod
cluster orchestrated with Amazon EKS, see the following sections in the Amazon EKS
Support in SageMaker HyperPod workshop.

Amazon EKS orchestration
1870

## Page 900

Amazon SageMaker AI
Developer Guide

• Set up Amazon FSx for Lustre on SageMaker HyperPod

• Set up a mountpoint for Amazon S3 using Mountpoint for Amazon S3

Using the Amazon EBS CSI driver on SageMaker HyperPod EKS clusters

SageMaker HyperPod supports the Amazon Elastic Block Store (Amazon EBS) Container Storage
Interface (CSI) driver, which manages the lifecycle of Amazon EBS volumes as storage for the
Kubernetes volumes that you create. With the Amazon EBS CSI driver, you can create, attach, and
manage your Amazon EBS volumes for your machine learning workloads running on SageMaker
HyperPod clusters with Amazon EKS orchestration.

In this topic:

• Key storage capabilities

• Use cases

• Setting up the Amazon EBS CSI driver on SageMaker HyperPod EKS clusters

• Using the APIs

Key storage capabilities

The Amazon EBS CSI driver on SageMaker HyperPod supports the following storage capabilities.

• Static provisioning: Associates pre-created Amazon EBS volumes with Kubernetes persistent
volumes for use in your pods.

• Dynamic provisioning: Automatically creates Amazon EBS volumes and associated persistent

volumes from PersistentVolumeClaims. Parameters can be passed via StorageClass for
ﬁne-grained control over volume creation.

• Volume resizing: Expands existing volumes by updating the PersistentVolumeClaims size
speciﬁcation without disrupting running workloads. This can be essential for handling growing
model repositories or adapting to larger nodes without service disruption.

• Volume snapshots: Creates point-in-time snapshots of volumes for backup, recovery, and data
versioning.

• Block volumes: Provides raw block device access for high-performance applications requiring
direct storage access.

Amazon EKS orchestration
1871

## Page 901

Amazon SageMaker AI
Developer Guide

• Volume modiﬁcation: Changes volume properties such as type, input or output operations per
second (IOPS), or throughput using volume attributes classes.

For more information about the Amazon EBS CSI driver, see Use Kubernetes volume storage with
Amazon EBS from the Amazon EKS User Guide.

For more information about storage to pods in your cluster, see Storage from the Kubernetes
Documentation.

Use cases

The Amazon EBS CSI driver integration enables several key use cases for both training and
inference workloads on SageMaker HyperPod EKS clusters.

Training workloads

• Dataset storage: Provision volumes for training datasets that persist across pod restarts

• Checkpoint storage: Save model checkpoints and intermediate training results

• Shared artifacts: Access common datasets and model artifacts across multiple training jobs

Inference workloads

• Model storage: Dynamically provision appropriately sized volumes based on model requirements

• Container caching: Create ephemeral storage for improved inference performance

• Event logging: Store inference results and logs with persistent storage

Setting up the Amazon EBS CSI driver on SageMaker HyperPod EKS clusters

The Amazon Elastic Block Store (Amazon EBS) Container Storage Interface (CSI) driver allows you
to dynamically provision and manage Amazon EBS volumes for your containerized workloads
running on SageMaker HyperPod clusters with EKS orchestration. This section walks you through
installing and conﬁguring the Amazon EBS CSI driver to enable persistent storage for your machine
learning workloads.

Amazon EKS orchestration
1872

## Page 902

Amazon SageMaker AI
Developer Guide

Prerequisites

Before you begin, do the following:

• Install and conﬁgure the AWS CLI

• Create a SageMaker HyperPod cluster with Amazon EKS orchestration

• Install the Amazon EBS CSI driver with the version of v1.47.0

Additional permissions

To set up the Amazon EBS CSI driver add-on, follow the instructions in Use Kubernetes volume
storage with Amazon EBS from the Amazon EKS User Guide. You should also add the following
additional permissions to the IAM role used to run the driver add-on. Note that this is the IAM role
speciﬁed in your service account conﬁguration for the driver add-on, not the HyperPod cluster
execution role.

JSON

{
"Version":"2012-10-17",
"Statement":
[
{
"Effect": "Allow",
"Action":
[
"sagemaker:AttachClusterNodeVolume",
"sagemaker:DetachClusterNodeVolume"
],
"Resource": "arn:aws:sagemaker:us-east-1:111122223333:cluster/*"
},
{
"Effect": "Allow",
"Action":
[
"eks:DescribeCluster"
],
"Resource": "arn:aws:eks:us-east-1:111122223333:cluster/my-cluster-
name"
}
]

Amazon EKS orchestration
1873

## Page 903

Amazon SageMaker AI
Developer Guide

}

Using the APIs

As an alternative, you can use the AttachClusterNodeVolume and DetachClusterNodeVolume API
operations to attach and detach your Amazon EBS volumes to SageMaker HyperPod EKS cluster
instances.

Key requirements for using these APIs include the following.

• Both the Amazon EBS volume and SageMaker HyperPod EKS cluster must be owned by the same
AWS account.

• The calling principal needs speciﬁc minimum permissions to successfully perform the attach
or detach operation. For more information about the minimum permissions, see the following
sections.

• After attaching a volume to your HyperPod node, follow the instructions in Accessing SageMaker
HyperPod cluster nodes to access the cluster node, and Make a volume available for use to
mount the attached volume.

Required permissions for sagemaker:AttachClusterNodeVolume

JSON

{
"Version":"2012-10-17",
"Statement":
[
{
"Effect": "Allow",
"Action":
[
"sagemaker:AttachClusterNodeVolume"
],
"Resource": "arn:aws:sagemaker:us-east-1:111122223333:cluster/*"
},
{
"Effect": "Allow",
"Action":
[

Amazon EKS orchestration
1874

## Page 904

Amazon SageMaker AI
Developer Guide

"eks:DescribeCluster"
],
"Resource": "arn:aws:eks:us-east-1:111122223333:cluster/my-cluster-
name"
},
{
"Effect": "Allow",
"Action":
[
"ec2:AttachVolume",
"ec2:DescribeVolumes"
],
"Resource": "arn:aws:ec2:us-east-1:111122223333:volume/*"
}
]
}

Required permissions for sagemaker:DetachClusterNodeVolume

JSON

{
"Version":"2012-10-17",
"Statement":
[
{
"Effect": "Allow",
"Action":
[
"sagemaker:DetachClusterNodeVolume"
],
"Resource": "arn:aws:sagemaker:us-east-1:111122223333:cluster/*"
},
{
"Effect": "Allow",
"Action":
[
"eks:DescribeCluster"
],
"Resource": "arn:aws:eks:us-east-1:111122223333:cluster/my-cluster-
name"
},

Amazon EKS orchestration
1875

## Page 905

Amazon SageMaker AI
Developer Guide

{
"Effect": "Allow",
"Action":
[
"ec2:DetachVolume",
"ec2:DescribeVolumes"
],
"Resource": "arn:aws:ec2:us-east-1:111122223333:volume/*"
}
]
}

Required permissions for AWS KMS keys

Add the following AWS KMS permissions only if you're using customer managed KMS keys to
encrypt your Amazon EBS volumes attached to HyperPod cluster nodes. These permissions are not
required if you're using AWS-managed KMS keys (the default encryption option).

JSON

{
"Version":"2012-10-17",
"Id": "key-default-1",
"Statement":
[
{
"Effect": "Allow",
"Principal":
{
"AWS": "arn:aws:iam::111122223333:role/caller-role"
},
"Action": "kms:DescribeKey",
"Resource": "*"
},
{
"Effect": "Allow",
"Principal":
{
"AWS": "arn:aws:iam::111122223333:role/caller-role"
},
"Action": "kms:CreateGrant",

Amazon EKS orchestration
1876

## Page 906

Amazon SageMaker AI
Developer Guide

"Resource": "*",
"Condition":
{
"StringEquals":
{
"kms:CallerAccount": "111122223333",
"kms:ViaService": "ec2.us-east-1.amazonaws.com"
},
"ForAnyValue:StringEquals":
{
"kms:EncryptionContextKeys": "aws:ebs:id"
},
"Bool":
{
"kms:GrantIsForAWSResource": true
},
"ForAllValues:StringEquals":

{
"kms:GrantOperations":
[
"Decrypt"
]
}
}
}
]
}

Note

These AWS KMS permissions are not required for

sagemaker:DetachClusterNodeVolume when detaching a Cluster Auto Volume
Attachment (CAVA) volume encrypted with customer managed KMS keys.

Conﬁguring custom Kubernetes labels and taints in Amazon SageMaker HyperPod

Amazon SageMaker HyperPod clusters with Amazon Elastic Kubernetes Service (Amazon EKS)
orchestrator support custom Kubernetes labels and taints for nodes within instance groups. Labels
and taints are fundamental scheduling and organization mechanisms in Kubernetes that give you
ﬁne-grained control over pod placement and resource utilization.

Amazon EKS orchestration
1877

## Page 907

Amazon SageMaker AI
Developer Guide

Labels are key-value pairs that can be attached to Kubernetes objects, allowing you to organize
and select resources based on attributes. Taints, working in conjunction with tolerations, are node-
speciﬁc properties that inﬂuence pod scheduling by repelling pods that don't have matching
tolerations. Together, these mechanisms enable you to isolate workloads, assign them according to
hardware speciﬁcations, and ensure optimal resource utilization.

Common use cases

The following are common scenarios where custom labels and taints are beneﬁcial:

• Preventing system pods on expensive instances - Apply taints to GPU instances to prevent
system pods and other non-critical workloads from consuming expensive compute resources

• Integration with existing tooling - Apply labels that match your organization's established
infrastructure patterns and node aﬃnity conﬁgurations

Conﬁguring labels and taints

You can conﬁgure custom Kubernetes labels and taints at the instance group level using the

KubernetesConfig parameter in your cluster conﬁguration. Labels and taints are applied to all
nodes in the instance group and persist throughout the cluster's lifecycle.

The KubernetesConfig parameter is declarative, meaning you specify the complete desired state
of labels and taints for an instance group. SageMaker HyperPod then reconciles the actual state of
the nodes to match this desired state.

• Adding labels or taints - Include the new labels or taints in the KubernetesConfig along with
any existing ones you want to keep

• Updating labels or taints - Modify the values in the KubernetesConfig for the labels or taints
you want to change, and include all others you want to keep

• Removing labels or taints - Omit the labels or taints you want to remove from the

KubernetesConfig, keeping only those you want to retain

Creating a cluster with labels and taints

When creating a new SageMaker HyperPod cluster, include the KubernetesConfig parameter
in your instance group conﬁguration. The following example shows how to create a cluster with
custom labels and taints:

Amazon EKS orchestration
1878

## Page 908

Amazon SageMaker AI
Developer Guide

{
"ClusterName": "my-cluster",
"InstanceGroups": [{
"InstanceGroupName": "worker-group-1",
"InstanceType": "ml.p4d.24xlarge",
"InstanceCount": 4,
"LifeCycleConfig": {
"SourceS3Uri": "s3://my-bucket/lifecycle-config.sh",
"OnCreate": "on-create.sh"
},
"ExecutionRole": "arn:aws:iam::123456789012:role/HyperPodExecutionRole",
"ThreadsPerCore": 1,
"KubernetesConfig": {
"Labels": {
"env": "prod",
"team": "ml-training",

"gpu-type": "a100"
},
"Taints": [{
"key": "gpu",
"value": "true",
"effect": "NoSchedule"
},
{
"key": "dedicated",
"value": "ml-workloads",
"effect": "NoExecute"
}]
}
}],
"VpcConfig": {
"SecurityGroupIds": ["sg-0123456789abcdef0"],
"Subnets": ["subnet-0123456789abcdef0", "subnet-0123456789abcdef1"]
},
"Orchestrator": {
"Eks": {
"ClusterArn": "arn:aws:eks:us-west-2:123456789012:cluster/my-eks-cluster"
}
}
}

In this example:

Amazon EKS orchestration
1879

## Page 909

Amazon SageMaker AI
Developer Guide

• Labels - Three custom labels are applied: env=prod, team=ml-training, and gpu-

type=a100

• Taints - Two taints are conﬁgured to prevent unwanted pod scheduling

Updating labels and taints on an existing cluster

You can modify labels and taints on an existing cluster using the UpdateCluster API. The

following example shows how to update the KubernetesConfig for an instance group:

{
"ClusterName": "my-cluster",
"InstanceGroups": [{
"InstanceGroupName": "worker-group-1",
"KubernetesConfig": {
"Labels": {
"env": "prod",
"team": "ml-training",
"gpu-type": "a100",
"cost-center": "ml-ops"
},
"Taints": [{
"key": "gpu",
"value": "true",
"effect": "NoSchedule"
}]
}
}]
}

When you update labels and taints, SageMaker HyperPod applies the changes to all nodes in the
instance group. The service manages the transition from current to desired state, which you can

monitor using the DescribeCluster API.

Monitoring label and taint application

SageMaker HyperPod provides APIs to monitor the status of labels and taints as they are applied to
your cluster nodes.

Amazon EKS orchestration
1880

## Page 910

Amazon SageMaker AI
Developer Guide

Checking cluster-level status

Use the DescribeCluster API to view the current and desired states of labels and taints at the
instance group level. The following example shows the response structure:

{
"ClusterName": "my-cluster",
"ClusterStatus": "InService",
"InstanceGroups": [{
"InstanceGroupName": "worker-group-1",
"InstanceType": "ml.p4d.24xlarge",
"CurrentInstanceCount": 4,
"TargetInstanceCount": 4,
"KubernetesConfig": {
"CurrentLabels": {
"env": "prod",

"team": "ml-training",
"gpu-type": "a100"
},
"DesiredLabels": {
"env": "prod",
"team": "ml-training",
"gpu-type": "a100"
},
"CurrentTaints": [{
"key": "gpu",
"value": "true",
"effect": "NoSchedule"
}],
"DesiredTaints": [{
"key": "gpu",
"value": "true",
"effect": "NoSchedule"
}]
}
}]
}

When the CurrentLabels match DesiredLabels and CurrentTaints match

DesiredTaints, all nodes in the instance group have the speciﬁed conﬁguration applied. If they
diﬀer, the cluster is still in the process of applying the changes.

Amazon EKS orchestration
1881

## Page 911

Amazon SageMaker AI
Developer Guide

Checking individual node status

For node-level details, use the DescribeClusterNode API to check the label and taint
conﬁguration of individual nodes. The following example shows the response structure:

{
"NodeDetails": {
"InstanceId": "i-0123456789abcdef0",

"InstanceGroupName": "worker-group-1",
"InstanceType": "ml.p4d.24xlarge",
"InstanceStatus": {
"Status": "Running",
"Message": "Node is healthy"
},
"LifeCycleConfig": {
"SourceS3Uri": "s3://my-bucket/lifecycle-config.sh",
"OnCreate": "on-create.sh"
},
"LaunchTime": 1699564800.0,
"KubernetesConfig": {
"CurrentLabels": {
"env": "prod",
"team": "ml-training",
"gpu-type": "a100"
},
"DesiredLabels": {
"env": "prod",
"team": "ml-training",
"gpu-type": "a100"
},
"CurrentTaints": [{
"key": "gpu",
"value": "true",
"effect": "NoSchedule"
}],
"DesiredTaints": [{
"key": "gpu",
"value": "true",
"effect": "NoSchedule"
}]
}
}
}

Amazon EKS orchestration
1882

## Page 912

Amazon SageMaker AI
Developer Guide

Node-level monitoring is useful for troubleshooting when labels or taints are not applying correctly
to speciﬁc nodes, or when you need to verify the conﬁguration of a particular instance.

Reserved preﬁxes

Certain preﬁxes are reserved for system use and should not be used for custom labels or taints. The
following preﬁxes are reserved:

• kubernetes.io/ - Reserved for Kubernetes core components

• k8s.io/ - Reserved for Kubernetes core components

• sagemaker.amazonaws.com/ - Reserved for SageMaker HyperPod

• eks.amazonaws.com/ - Reserved for Amazon EKS

• k8s.aws/ - Reserved for Amazon EKS

• karpenter.sh/ - Reserved for Karpenter autoscaling

Labels and taints with these preﬁxes are managed by system components and should not be
overwritten with custom values.

Checkpointless training in Amazon SageMaker HyperPod

Checkpointless training on Amazon SageMaker HyperPod enables faster recovery from training
infrastructure faults. The following documentation helps you get started with checkpointless
training and ﬁne-tuning for NeMo-supported models.

Checkpointless training has the following pre-requisites:

• the section called “Getting started”

• the section called “Installation”. You must install v1.2.0 or above.

Checkpointless training on SageMaker HyperPod is built on top of the  NVIDIA NeMo Framework
User Guide. You can run checkpointless training with pre-created SageMaker HyperPod recipes. If
you're familiar with NeMo, the process of using the checkpointless training recipes is similar. With
minor changes, you can start training a model using checkpointless training features that enable
you to recover quickly from training faults.

The following HyperPod recipes are pre-conﬁgured with checkpointless training optimizations.
You can specify your data paths as part of the recipe and use the associated launch script to run
training (see the quick start guide below):

Amazon EKS orchestration
1883

## Page 913

Amazon SageMaker AI
Developer Guide

Model
Method
Size
Nodes
Instance
Accelerat
or

Recipe
Script
Tutorial

GPT

Full

120b
16
p5.48xlar

GPU

link
link
link

OSS

ﬁnetune
example

ge

H100

GPT
OSS

LoRA-
exam
ple

120b
2
p5.48xlar
ge

GPU
H100

link
link
link

Llama3
Pretrain
example

70b
16
p5.48xlar
ge

GPU
H100

link
link
link

Llama3
LoRA-
exam
ple

70b
2
p5.48xlar
ge

GPU
H100

link
link
link

The following quick-start guide provides tutorials for using checkpointless training recipes:

Getting started examples

• the section called “Full Finetuning GPT OSS 120b”

• the section called “PEFT-LoRA GPT OSS 120b”

• the section called “Pretraining Llama 3 70b”

• the section called “PEFT-LoRA Llama 3 70b”

If you’d like to pre-train or ﬁne-tune custom models, see the section called “Pretraining or
ﬁnetuning custom models”.

To learn more about incorporating speciﬁc checkpointless training components, HyperPod
checkpointless training features.

Amazon SageMaker HyperPod checkpointless training tutorials

HyperPod checkpointless training recipes are predeﬁned job conﬁgurations with checkpointless
training features enabled. Using these recipes, makes it easier to get started with checkpointless
training on HyperPod.

Amazon EKS orchestration
1884

## Page 914

Amazon SageMaker AI
Developer Guide

Topics

• Tutorials - Amazon SageMaker HyperPod Checkpointless Full Finetuning GPT OSS 120b

• Tutorials - Amazon SageMaker HyperPod Checkpointless PEFT-LoRA GPT OSS 120b

• Tutorials - Amazon SageMaker HyperPod Checkpointless Pretraining Llama 3 70b

• Tutorials - Amazon SageMaker HyperPod Checkpointless PEFT-LoRA Llama 3 70b

• Tutorials - Amazon SageMaker HyperPod Checkpointless Pretraining or Finetuning Custom
Models

Tutorials - Amazon SageMaker HyperPod Checkpointless Full Finetuning GPT OSS 120b

The following sequence of steps is required to run checkpointless training recipes on HyperPod.

Prerequisites

Before you start setting up your environment, make sure you have:

• Enabled Amazon EKS support in Amazon SageMaker HyperPod

• Set up HyperPod training operator (v1.2+)

• A shared storage location. It can be an Amazon FSx ﬁle system or NFS system that's accessible
from the cluster nodes.

• Data in one of the following formats:

• JSON

• JSONGZ (Compressed JSON)

• ARROW

• Pick a supported checkpointless training recipe for Llama 70B or GPT-OSS 120B from the source.

• Download the hugging face model weights and covert to  Nemo supported format.

• Setup your environment

Kubernetes environment setup

To set up your Kubernetes environment, do the following:

1.
Set up the virtual environment. Make sure your version of Python is greater than or equal to
3.10 and lower than 3.14.

python3 -m venv ${PWD}/venv

Amazon EKS orchestration
1885

## Page 915

Amazon SageMaker AI
Developer Guide

source venv/bin/activate

2.
Set up kubectl and eksctl

3.
Install Helm

4.
Connect to your Kubernetes cluster

aws eks update-kubeconfig --region "${CLUSTER_REGION}" --name "${CLUSTER_NAME}"

5.
Install dependencies using one of the following methods:

1. Method 1: SageMaker HyperPod recipes method:

# install SageMaker HyperPod Recipes.
git clone --recursive git@github.com:aws/sagemaker-hyperpod-recipes.git
cd sagemaker-hyperpod-recipes

pip3 install -r requirements.txt

2. Method 2: kubectl with pre-deﬁned job yaml method

# install SageMaker HyperPod checkpointless training.
git clone git@github.com:aws/sagemaker-hyperpod-checkpointless-training.git
cd sagemaker-hyperpod-checkpointless-training

You can now launch the checkpointless training recipe using either the NeMo-style launcher or
using kubectl.

Launch training jobs with the recipes launcher

You can use the Amazon SageMaker HyperPod recipes to submit your training job. Using the
recipes involves updating k8s.yaml, conﬁg.yaml and running the launch script.

1.
Update launcher_scripts/gpt_oss/

run_checkpointless_gpt_oss_120b_full_fine_tuning.sh

your_container: A Deep Learning container. To ﬁnd the most recent release of the
checkpointless training container, see  checkpointless training release notes.

#!/bin/bash

SAGEMAKER_TRAINING_LAUNCHER_DIR=${SAGEMAKER_TRAINING_LAUNCHER_DIR:-"$(pwd)"}

Amazon EKS orchestration
1886

## Page 916

Amazon SageMaker AI
Developer Guide

TRAIN_DIR="${TRAIN_DIR}"
VAL_DIR="${VAL_DIR}"
EXP_DIR="${EXP_DIR}"
LOG_DIR="${LOG_DIR}"
CONTAINER_MOUNT="/data"
CONTAINER="${CONTAINER}"
MODEL_NAME_OR_PATH="${MODEL_NAME_OR_PATH}"

HYDRA_FULL_ERROR=1 python3 "${SAGEMAKER_TRAINING_LAUNCHER_DIR}/main.py" \
recipes=fine-tuning/gpt_oss/checkpointless_gpt_oss_120b_full_fine_tuning \
recipes.dataset.dataset_path="${TRAIN_DIR}" \
recipes.exp_manager.exp_dir="${EXP_DIR}" \
recipes.log_dir="${LOG_DIR}" \
recipes.resume.restore_config.path="${MODEL_NAME_OR_PATH}" \
base_results_dir="${SAGEMAKER_TRAINING_LAUNCHER_DIR}/results" \
git.use_default=false \
cluster=k8s \

cluster_type=k8s \
container="${CONTAINER}" \
+cluster.hostNetwork=true \
+cluster.persistent_volume_claims.0.claimName=fsx-claim \
+cluster.persistent_volume_claims.0.mountPath="${CONTAINER_MOUNT}" \
+recipes.dataset.val_dataset_path="${VAL_DIR}" \
++recipes.callbacks.3.test_fault_config.fault_prob_between_lock=1 \

2.
Launch the training job

bash launcher_scripts/gpt_oss/run_checkpointless_gpt_oss_120b_full_fine_tuning.sh

After you've submitted the training job, you can use the following command to verify if you
submitted it successfully.

kubectl get pods

NAME                             READY   STATUS             RESTARTS        AGE
gpt-oss-120b-worker-0             0/1    running               0            36s

If the STATUS is at PENDING or ContainerCreating, run the following command to get more details

kubectl describe pod <name of pod>

Amazon EKS orchestration
1887

## Page 917

Amazon SageMaker AI
Developer Guide

After the job STATUS changes to Running, you can examine the log by using the following
command.

kubectl logs <name of pod>

The STATUS will turn to COMPLETED when you run kubectl get pods.

Launch the training job with kubectl with pre-deﬁned yaml

Another option is to launch the training through kubectl with a pre-deﬁned job yaml.

1. update the examples/gpt_oss/launch/full_ﬁnetune_gpt_oss_120b_checkpointless_p5.yaml

• image: A Deep Learning container. To ﬁnd the most recent release of the checkpointless
training container, see checkpointless training release notes.

• resume.restore_conﬁg.path=<path_to_pretrained_weights>: The path to downloaded
pretrained model weigths in Nemo format in  Prerequisites step.

• dataset.dataset_path=<path_to_dataset>: The path to the dataset that stored in the shared
storage

2. Submit the job using kubectl with full_ﬁnetune_gpt_oss_120b_checkpointless_p5.yaml

kubectl apply -f examples/gpt_oss/launch/
full_finetune_gpt_oss_120b_checkpointless_p5.yaml

After you've submitted the training job, you can use the following command to verify if you
submitted it successfully.

kubectl get pods

NAME                             READY   STATUS             RESTARTS        AGE
gpt-oss-120b-worker-0             0/1    running               0            36s

If the STATUS is at PENDING or ContainerCreating, run the following command to get more details

kubectl describe pod <name of pod>

After the job STATUS changes to Running, you can examine the log by using the following
command.

Amazon EKS orchestration
1888

## Page 918

Amazon SageMaker AI
Developer Guide

kubectl logs <name of pod>

The STATUS will turn to Completed when you run kubectl get pods

Tutorials - Amazon SageMaker HyperPod Checkpointless PEFT-LoRA GPT OSS 120b

The following sequence of steps is required to run checkpointless training recipes on HyperPod.

Prerequisites

Before you start setting up your environment, make sure you have:

• Enabled Amazon EKS support in Amazon SageMaker HyperPod

• Set up HyperPod training operator (v1.2+)

• A shared storage location. It can be an Amazon FSx ﬁle system or NFS system that's accessible
from the cluster nodes.

• Data in one of the following formats:

• JSON

• JSONGZ (Compressed JSON)

• ARROW

• Pick a supported checkpointless training recipe for Llama 70B or GPT-OSS 120B from the source.

• Download the hugging face model weights and covert to  Nemo supported format.

• Setup your environment

Kubernetes environment setup

To set up your Kubernetes environment, do the following:

1.
Set up the virtual environment. Make sure you're using Python greater than or equal to 3.10
and < 3.14.

python3 -m venv ${PWD}/venv
source venv/bin/activate

2.
Set up kubectl and eksctl

3.
Install Helm

Amazon EKS orchestration
1889

## Page 919

Amazon SageMaker AI
Developer Guide

4.
Connect to your Kubernetes cluster

aws eks update-kubeconfig --region "${CLUSTER_REGION}" --name "${CLUSTER_NAME}"

5.
Install dependencies using one of the following methods:

• SageMaker HyperPod recipes method:

# install SageMaker HyperPod Recipes.
git clone --recursive git@github.com:aws/sagemaker-hyperpod-recipes.git
cd sagemaker-hyperpod-recipes
pip3 install -r requirements.txt

• kubectl with pre-deﬁned job yaml method

# install SageMaker HyperPod checkpointless training.

git clone git@github.com:aws/sagemaker-hyperpod-checkpointless-training.git
cd sagemaker-hyperpod-checkpointless-training

You can now launch the checkpointless training recipe using either the NeMo-style launcher or
using kubectl.

Launch the training job with the recipes launcher

Alternatively, you can use the SageMaker HyperPod recipes to submit your training job. Using the
recipes involves updating k8s.yaml, conﬁg.yaml and running the launch script.

1. Update launcher_scripts/gpt_oss/run_checkpointless_gpt_oss_120b_lora.sh

your_contrainer: A Deep Learning container. To ﬁnd the most recent release of the
checkpointless training container, see  checkpointless training release notes.

#!/bin/bash
SAGEMAKER_TRAINING_LAUNCHER_DIR=${SAGEMAKER_TRAINING_LAUNCHER_DIR:-"$(pwd)"}
TRAIN_DIR="${TRAIN_DIR}"
VAL_DIR="${VAL_DIR}"
EXP_DIR="${EXP_DIR}"
LOG_DIR="${LOG_DIR}"
CONTAINER_MOUNT="/data"
CONTAINER="${CONTAINER}"
MODEL_NAME_OR_PATH="${MODEL_NAME_OR_PATH}"

Amazon EKS orchestration
1890

## Page 920

Amazon SageMaker AI
Developer Guide

HYDRA_FULL_ERROR=1 python3 "${SAGEMAKER_TRAINING_LAUNCHER_DIR}/main.py" \
recipes=fine-tuning/gpt_oss/checkpointless_gpt_oss_120b_lora \
recipes.dataset.dataset_path="${TRAIN_DIR}" \
recipes.exp_manager.exp_dir="${EXP_DIR}" \
recipes.log_dir="${LOG_DIR}" \
recipes.resume.restore_config.path="${MODEL_NAME_OR_PATH}" \
base_results_dir="${SAGEMAKER_TRAINING_LAUNCHER_DIR}/results" \
git.use_default=false \
cluster=k8s \
cluster_type=k8s \
container="${CONTAINER}" \
+cluster.hostNetwork=true \
+cluster.persistent_volume_claims.0.claimName=fsx-claim \
+cluster.persistent_volume_claims.0.mountPath="${CONTAINER_MOUNT}" \
+recipes.dataset.val_dataset_path="${VAL_DIR}" \
++recipes.callbacks.3.test_fault_config.fault_prob_between_lock=1 \

2. Launch the training job

bash launcher_scripts/gpt_oss/run_checkpointless_gpt_oss_120b_lora.sh

After you've submitted the training job, you can use the following command to verify if you
submitted it successfully.

kubectl get pods

NAME                             READY   STATUS             RESTARTS        AGE
gpt-oss-120b-worker-0             0/1    running               0            36s

If the STATUS is at PENDING or ContainerCreating, run the following command to get more details

kubectl describe pod <name of pod>

After the job STATUS changes to Running, you can examine the log by using the following
command.

kubectl logs <name of pod>

The STATUS will turn to Completed when you run kubectl get pods

Amazon EKS orchestration
1891

## Page 921

Amazon SageMaker AI
Developer Guide

Launch the training job with kubectl with pre-deﬁned yaml

Another option is to launch the training through kubectl with a pre-deﬁned job yaml.

1. update the examples/gpt_oss/launch/peft_gpt_oss_120b_checkpointless_p5.yaml

• image: A Deep Learning container. To ﬁnd the most recent release of the checkpointless
training container, see  checkpointless training release notes.

• resume.restore_conﬁg.path=<path_to_pretrained_weights>: The path to downloaded
pretrained model weights in Nemo format in  Prerequisites step.

• dataset.dataset_path=<path_to_dataset>: The path to the dataset that stored in the shared
storage

2. Submit the job using kubectl with peft_gpt_oss_120b_checkpointless_p5.yaml

kubectl apply -f examples/gpt_oss/launch/peft_gpt_oss_120b_checkpointless_p5.yaml

After you've submitted the training job, you can use the following command to verify if you
submitted it successfully.

kubectl get pods

NAME                                             READY   STATUS             RESTARTS
AGE
gpt-120b-lora-checkpointless-worker-0             0/1    running               0
36s

If the STATUS is at PENDING or ContainerCreating, run the following command to get more details

kubectl describe pod <name of pod>

After the job STATUS changes to Running, you can examine the log by using the following
command.

kubectl logs <name of pod>

The STATUS will turn to Completed when you run kubectl get pods

Amazon EKS orchestration
1892

## Page 922

Amazon SageMaker AI
Developer Guide

Tutorials - Amazon SageMaker HyperPod Checkpointless Pretraining Llama 3 70b

The following sequence of steps is required to run checkpointless training recipes on HyperPod.

Prerequisites

Before you start setting up your environment, make sure you have:

• Enabled Amazon EKS support in Amazon SageMaker HyperPod

• Set up HyperPod training operator (v1.2+)

• A shared storage location. It can be an Amazon FSx ﬁle system or NFS system that's accessible
from the cluster nodes.

• Data in one of the following formats:

• JSON

• JSONGZ (Compressed JSON)

• ARROW

• Pick a supported checkpointless training recipe for Llama 70B or GPT-OSS 120B from the
source.

• Download the hugging face model weights and covert to  Nemo supported format.

• Setup your environment

Kubernetes environment setup

To set up your Kubernetes environment, do the following:

1.
Set up the virtual environment. Make sure you're using Python greater than or equal to 3.10
and lower than 3.14.

python3 -m venv ${PWD}/venv
source venv/bin/activate

2.
Set up kubectl and eksctl

3.
Install Helm

4.
Connect to your Kubernetes cluster

aws eks update-kubeconfig --region "${CLUSTER_REGION}" --name "${CLUSTER_NAME}"

5.
Install dependencies using one of the following methods:

Amazon EKS orchestration
1893

## Page 923

Amazon SageMaker AI
Developer Guide

a.
Method 1: SageMaker HyperPod recipes method:

# install SageMaker HyperPod Recipes.
git clone --recursive git@github.com:aws/sagemaker-hyperpod-recipes.git
cd sagemaker-hyperpod-recipes
pip3 install -r requirements.txt

b.
Method 2: kubectl with pre-deﬁned job yaml method

# install SageMaker HyperPod checkpointless training.
git clone git@github.com:aws/sagemaker-hyperpod-checkpointless-training.git
cd sagemaker-hyperpod-checkpointless-training

You can now launch the checkpointless training recipe using either the NeMo-style launcher or
using kubectl.

Method 1: Launch the training job with the recipes launcher

Alternatively, you can use the SageMaker HyperPod recipes to submit your training job. Using the
recipes involves updating k8s.yaml, conﬁg.yaml and running the launch script.

1.
Update launcher_scripts/llama/run_checkpointless_llama3_70b_pretrain.sh

A Deep Learning container. To ﬁnd the most recent release of the checkpointless training
container, see  checkpointless training release notes.

#!/bin/bash

SAGEMAKER_TRAINING_LAUNCHER_DIR=${SAGEMAKER_TRAINING_LAUNCHER_DIR:-"$(pwd)"}
TRAIN_DIR="${TRAIN_DIR}"
VAL_DIR="${VAL_DIR}"
EXP_DIR="${EXP_DIR}"
LOG_DIR="${LOG_DIR}"
CONTAINER_MOUNT="/data"
CONTAINER="${CONTAINER}"

HYDRA_FULL_ERROR=1 python3 "${SAGEMAKER_TRAINING_LAUNCHER_DIR}/main.py" \
recipes=training/llama/checkpointless_llama3_70b_pretrain \
recipes.dataset.dataset_path="${TRAIN_DIR}" \
recipes.exp_manager.exp_dir="${EXP_DIR}" \
recipes.log_dir="${LOG_DIR}" \

Amazon EKS orchestration
1894

## Page 924

Amazon SageMaker AI
Developer Guide

recipes.data.global_batch_size=16 \
recipes.data.micro_batch_size=4 \
base_results_dir="${SAGEMAKER_TRAINING_LAUNCHER_DIR}/results" \
git.use_default=false \
cluster=k8s \
cluster_type=k8s \
container="${CONTAINER}" \
+cluster.hostNetwork=true \
+cluster.persistent_volume_claims.0.claimName=fsx-claim \
+cluster.persistent_volume_claims.0.mountPath="${CONTAINER_MOUNT}" \
+recipes.dataset.val_dataset_path="${VAL_DIR}" \
++recipes.callbacks.3.test_fault_config.fault_prob_between_lock=1 \

2.
Launch the training job

bash launcher_scripts/llama/run_checkpointless_llama3_70b_pretrain.sh

3.
After you've submitted the training job, you can use the following command to verify if you
submitted it successfully.

kubectl get pods

NAME                             READY   STATUS             RESTARTS        AGE
llama-3-70b-worker-0             0/1    running               0            36s

4.
If the STATUS is at PENDING or ContainerCreating, run the following command to get more
details

kubectl describe pod <name of pod>

5.
After the job STATUS changes to Running, you can examine the log by using the following
command.

kubectl logs <name of pod>

The STATUS will turn to Completed when you run kubectl get pods

Method 2: Launch the training job with kubectl with pre-deﬁned yaml

Another option is to launch the training through kubectl with a pre-deﬁned job yaml.

Amazon EKS orchestration
1895

## Page 925

Amazon SageMaker AI
Developer Guide

1.
Update the examples/llama3/launch/

pretrain_llama3_70b_checkpointless_p5.yaml

• image: A Deep Learning container. To ﬁnd the most recent release of the checkpointless
training container, see  checkpointless training release notes.

• resume.restore_config.path=<path_to_pretrained_weights>: The path to
downloaded pretrained model weights in Nemo format in  Prerequisites step.

• dataset.dataset_path=<path_to_dataset>: The path to the dataset that stored in
the shared storage

2.
Submit the job using kubectl with pretrain_llama3_70b_checkpointless_p5.yaml

kubectl apply -f examples/llama3/launch/pretrain_llama3_70b_checkpointless_p5.yaml

3.
After you've submitted the training job, you can use the following command to verify if you

submitted it successfully.

kubectl get pods

NAME                                             READY   STATUS
RESTARTS        AGE
llama3-pretrain-checkpointless-worker-0             0/1    running               0
36s

4.
If the STATUS is at PENDING or ContainerCreating, run the following command to get more
details

kubectl describe pod <name of pod>

5.
After the job STATUS changes to Running, you can examine the log by using the following
command.

kubectl logs <name of pod>

The STATUS will turn to Completed when you run kubectl get pods

Tutorials - Amazon SageMaker HyperPod Checkpointless PEFT-LoRA Llama 3 70b

The following sequence of steps is required to run checkpointless training recipes on HyperPod.

Amazon EKS orchestration
1896

## Page 926

Amazon SageMaker AI
Developer Guide

Prerequisites

Before you start setting up your environment, make sure you have:

• Enabled Amazon EKS support in Amazon SageMaker HyperPod

• Set up HyperPod training operator (v1.2+)

• A shared storage location. It can be an Amazon FSx ﬁle system or NFS system that's accessible
from the cluster nodes.

• Data in one of the following formats:

• JSON

• JSONGZ (Compressed JSON)

• ARROW

• Pick a supported checkpointless training recipe for Llama 70B or GPT-OSS 120B from the
source.

• Download the hugging face model weights and covert to  Nemo supported format.

• Setup your environment

Kubernetes environment setup

To set up your Kubernetes environment, do the following:

1.
Set up the virtual environment. Make sure you're using Python greater than or equal to 3.10
and lower than 3.14.

python3 -m venv ${PWD}/venv
source venv/bin/activate

2.
Set up kubectl and eksctl

3.
Install Helm

4.
Connect to your Kubernetes cluster

aws eks update-kubeconfig --region "${CLUSTER_REGION}" --name "${CLUSTER_NAME}"

5.
Install dependencies using one of the following methods:

a.
Method 1: SageMaker HyperPod recipes method:

Amazon EKS orchestration
1897

## Page 927

Amazon SageMaker AI
Developer Guide

# install SageMaker HyperPod Recipes.
git clone --recursive git@github.com:aws/sagemaker-hyperpod-recipes.git
cd sagemaker-hyperpod-recipes
pip3 install -r requirements.txt

b.
Method 2: kubectl with pre-deﬁned job yaml method

# install SageMaker HyperPod checkpointless training.
git clone git@github.com:aws/sagemaker-hyperpod-checkpointless-training.git
cd sagemaker-hyperpod-checkpointless-training

You can now launch the checkpointless training recipe using either the NeMo-style launcher or
using kubectl.

Method 1: Launch the training job with the recipes launcher

Alternatively, you can use the SageMaker HyperPod recipes to submit your training job. Using the
recipes involves updating k8s.yaml, conﬁg.yaml and running the launch script.

1.
Update launcher_scripts/llama/run_checkpointless_llama3_70b_lora.sh

A Deep Learning container. To ﬁnd the most recent release of the checkpointless training
container, see  checkpointless training release notes.

#!/bin/bash

SAGEMAKER_TRAINING_LAUNCHER_DIR=${SAGEMAKER_TRAINING_LAUNCHER_DIR:-"$(pwd)"}
TRAIN_DIR="${TRAIN_DIR}"
VAL_DIR="${VAL_DIR}"
EXP_DIR="${EXP_DIR}"
LOG_DIR="${LOG_DIR}"
CONTAINER_MOUNT="/data"
CONTAINER="${CONTAINER}"
MODEL_NAME_OR_PATH="${MODEL_NAME_OR_PATH}"

HYDRA_FULL_ERROR=1 python3 "${SAGEMAKER_TRAINING_LAUNCHER_DIR}/main.py" \
recipes=fine-tuning/llama/checkpointless_llama3_70b_lora \
recipes.dataset.dataset_path="${TRAIN_DIR}" \
recipes.exp_manager.exp_dir="${EXP_DIR}" \
recipes.log_dir="${LOG_DIR}" \
recipes.resume.restore_config.path="${MODEL_NAME_OR_PATH}" \

Amazon EKS orchestration
1898

## Page 928

Amazon SageMaker AI
Developer Guide

base_results_dir="${SAGEMAKER_TRAINING_LAUNCHER_DIR}/results" \
git.use_default=false \
cluster=k8s \
cluster_type=k8s \
container="${CONTAINER}" \
+cluster.hostNetwork=true \
+cluster.persistent_volume_claims.0.claimName=fsx-claim \
+cluster.persistent_volume_claims.0.mountPath="${CONTAINER_MOUNT}" \
+recipes.dataset.val_dataset_path="${VAL_DIR}" \
++recipes.callbacks.3.test_fault_config.fault_prob_between_lock=1 \

2.
Launch the training job

bash launcher_scripts/llama/run_checkpointless_llama3_70b_lora.sh

3.
After you've submitted the training job, you can use the following command to verify if you
submitted it successfully.

kubectl get pods

NAME                             READY   STATUS             RESTARTS        AGE
llama-3-70b-worker-0             0/1    running               0            36s

4.
If the STATUS is at PENDING or ContainerCreating, run the following command to get more
details

kubectl describe pod <name of pod>

5.
After the job STATUS changes to Running, you can examine the log by using the following
command.

kubectl logs <name of pod>

The STATUS will turn to Completed when you run kubectl get pods

Method 2: Launch the training job with kubectl with pre-deﬁned yaml

Another option is to launch the training through kubectl with a pre-deﬁned job yaml.

1.
Update the examples/llama3/launch/peft_llama3_70b_checkpointless_p5.yaml

Amazon EKS orchestration
1899

## Page 929

Amazon SageMaker AI
Developer Guide

• image: A Deep Learning container. To ﬁnd the most recent release of the checkpointless
training container, see  checkpointless training release notes.

• resume.restore_config.path=<path_to_pretrained_weights>: The path to
downloaded pretrained model weights in Nemo format in  Prerequisites step.

• dataset.dataset_path=<path_to_dataset>: The path to the dataset that stored in
the shared storage

2.
Submit the job using kubectl with peft_llama3_70b_checkpointless_p5.yaml

kubectl apply -f examples/llama3/launch/peft_llama3_70b_checkpointless_p5.yaml

3.
After you've submitted the training job, you can use the following command to verify if you
submitted it successfully.

kubectl get pods

NAME                                             READY   STATUS
RESTARTS        AGE
llama3-70b-lora-checkpointless-worker-0             0/1    running               0
36s

4.
If the STATUS is at PENDING or ContainerCreating, run the following command to get more
details

kubectl describe pod <name of pod>

5.
After the job STATUS changes to Running, you can examine the log by using the following
command.

kubectl logs <name of pod>

The STATUS will turn to Completed when you run kubectl get pods

Tutorials - Amazon SageMaker HyperPod Checkpointless Pretraining or Finetuning Custom
Models

The following sequence of steps is required to run checkpointless training with your custom model
on HyperPod.

Amazon EKS orchestration
1900

## Page 930

Amazon SageMaker AI
Developer Guide

Prerequisites

Before you start setting up your environment, make sure you have:

• Enabled Amazon EKS support in Amazon SageMaker HyperPod

• Set up HyperPod training operator (v1.2+)

• A shared storage location. It can be an Amazon FSx ﬁle system or NFS system that's accessible
from the cluster nodes.

• Data in one of the following formats:

• JSON

• JSONGZ (Compressed JSON)

• ARROW

• Download the hugging face model weights and covert to  Nemo supported format.

• Setup your environment

Kubernetes environment setup

To set up your Kubernetes environment, do the following:

1.
Set up the virtual environment. Make sure you're using Python greater than or equal to 3.10
and lower than 3.14.

python3 -m venv ${PWD}/venv
source venv/bin/activate

2.
Set up kubectl and eksctl

3.
Connect to your Kubernetes cluster

aws eks update-kubeconfig --region "${CLUSTER_REGION}" --name "${CLUSTER_NAME}"

4.
Install dependencies

# install SageMaker HyperPod checkpointless training.
git clone git@github.com:aws/sagemaker-hyperpod-checkpointless-training.git
cd sagemaker-hyperpod-checkpointless-training

Amazon EKS orchestration
1901

## Page 931

Amazon SageMaker AI
Developer Guide

Checkpointless training modiﬁcation instructions

To incrementally adopt checkpointless training for custom models, follow the integration guide
(here we use Llama 3 70b pretraining as an example), which involves:

• Fast communicator creation

• Memory-mapped dataloader (MMAP)

• In-process & Checkpointless recovery

Component 1: Fast communicator creation

This is to optimize time to establish connections between the workers. There is no code changes
needed and only requires setting env variables

# Enable Rootless features
export HPCT_USE_ROOTLESS=1 && \
sysctl -w net.ipv4.ip_local_port_range="20000 65535" && \

hyperpodrun --nproc_per_node=8 \
...
--inprocess-restart \
...

The full change can be found in the  llama3 70 pretrain launch job conﬁg.

Component 2: Memory-mapped dataloader (MMAP)

MMAP caches to store pre-fetched data samples & enable immediate training start without
needing to wait for data preprocessing. It requires minimal code changes to adopt by wrapping
existing dataloader.

data_module = MMAPDataModule(
data_module=base_data_module,
mmap_config=CacheResumeMMAPConfig(cache_dir=…)
)

Components 3 and 4: In-process and checkpointless recovery

This enables failure recovery without restart training processes or loading from checkpoints.
Additional code changes needed (strategy & training conﬁg update, wrap existing main)

Amazon EKS orchestration
1902

## Page 932

Amazon SageMaker AI
Developer Guide

@HPWrapper(
health_check=CudaHealthCheck(),
hp_api_factory=HPAgentK8sAPIFactory(),
abort_timeout=60.0,
...)
def run_main(
cfg,
caller: Optional[HPCallWrapper] = None):
...

CheckpointlessMegatronStrategy(
**self.cfg.strategy,
ddp=self.ddp,
)

The full change can be found in the llama3 70 pretrain entry script and the corresponding training
conﬁg change can be found in the  llama3 70b training conﬁg.

Launch training

You can now launch the checkpointless training using kubectl.

kubectl apply -f your_job_config.yaml

HyperPod checkpointless training features

See the following pages to learn about the training features in checkpointless training.

Topics

• Amazon SageMaker HyperPod checkpointless training repositories

• Collective communication initialization improvements

• Memory mapped dataloader

• In-process recovery and checkpointless training

Amazon SageMaker HyperPod checkpointless training repositories

HyperPod checkpointless training accelerates recovery from cluster faults in large-scale distributed
training environments through framework-level optimizations. These optimizations are delivered

Amazon EKS orchestration
1903

## Page 933

Amazon SageMaker AI
Developer Guide

via a base container image that includes enhanced NCCL initialization improvements, data
loading optimizations, and in-process and checkpointless recovery components. The HyperPod
checkpointless training package is built on this foundation.

Checkpointless training is enabled via three optimization tracks that run in concert:

• Communication initilization improvements (NCCL and Gloo) - Eliminate communication
bottlenecks by decentralizing rank peer and ring information (red box below).

• Data loading optimizations - Reduce the time required to serve the ﬁrst batch of data during
restart operations (orange boxes below).

• Program restart overhead reduction - Minimize restart costs and enable checkpointless
replenishment through process recovery on healthy nodes (blue and green boxes below).

![Page 933 Diagram 1](images/page-0933-img-01.png)

Collective communication initialization improvements

NCCL and Gloo are fundamental communication libraries that enable collective operations (such as
all-reduce and broadcast) across distributed training processes. However, traditional NCCL and Gloo
initialization can create bottlenecks during fault recovery.

The standard recovery process requires all processes to connect to a centralized TCPStore and
coordinate through a root process, introducing an expensive overhead that becomes particularly

Amazon EKS orchestration
1904

## Page 934

Amazon SageMaker AI
Developer Guide

problematic during restarts. This centralized design creates three critical issues: coordination
overhead from mandatory TCPStore connections, recovery delays as each restart must repeat the
full initialization sequence, and a single point of failure in the root process itself. This imposes an
expensive, centralized coordination steps every time training initializes or restarts.

HyperPod checkpointless training eliminates these coordination bottlenecks, enabling the faster
recovery from faults by making initialization "rootless" and "TCPStoreless."

Rootless conﬁgurations

To enable Rootless, one can simply expose the following environment variables.

export HPCT_USE_ROOTLESS=1 && \
sysctl -w net.ipv4.ip_local_port_range="20000 65535" && \

HPCT_USE_ROOTLESS: 0 or 1. Use to turn on and oﬀ rootless

sysctl -w net.ipv4.ip_local_port_range="20000 65535": Set the system port range

See the example for enabling Rootless.

Rootless

HyperPod checkpointless training oﬀers novel initialization methods, Rootless and TCPStoreless,
for NCCL and Gloo process groups.

The implementation of these optimizations involves modifying NCCL, Gloo, and PyTorch:

• Extending third-party library APIs to enable Rootless and Storeless NCCL and Gloo optimizations
while maintaining backward compatibility

• Updating process group backends to conditionally use optimized paths and handle in-process
recovery issues

• Bypassing expensive TCPStore creation at the PyTorch distributed layer while maintaining
symmetric address patterns through global group counters

The following graph shows the architecture of the distributed training libraries and the changes
made in checkpointless training.

Amazon EKS orchestration
1905

## Page 935

Amazon SageMaker AI
Developer Guide

![Page 935 Diagram 1](images/page-0935-img-01.png)

NCCL and Gloo

These are independent packages that perform the core functionality of collective communications.
They provide key APIs, such as ncclCommInitRank, to initialize communication networks, manage
the underlying resources, and perform collective communications. After making custom changes
in NCCL and Gloo, the Rootless and Storeless optimizes (e.g., skip connecting to the TCPStore)
initialization of the communication network. You can switch between using the the original code
paths or optimized code paths ﬂexibly.

PyTorch process group backend

The process group backends, speciﬁcally ProcessGroupNCCL and ProcessGroupGloo, implement
the ProcessGroup APIs by invoking the APIs of their corresponding underlying libraries. Since
we extend the third-party libraries' APIs, we have to invoke them properly and make code path
switches based on customers' conﬁgurations.

In addition to optimization code paths, we also change the process group backend to support in-
process recovery.

Amazon EKS orchestration
1906

## Page 936

Amazon SageMaker AI
Developer Guide

Memory mapped dataloader

Another restart overhead stems from data loading: the training cluster remains idle while the
dataloader initializes, downloads data from remote ﬁle systems, and processes it into batches.

To address this, we introduce the Memory Mapped DataLoader(MMAP) Dataloader, which caches
prefetched batches in persistent memory, ensuring they remain available even after a fault-
induced restart. This approach eliminates dataloader setup time and enables training to resume
immediately using cached batches, while the dataloader concurrently reinitializes and fetches
subsequent data in the background. The data cache resides on each rank that requires training data
and maintains two types of batches: recently consumed batches that have been used for training,
and prefetched batches ready for immediate use.

![Page 936 Diagram 1](images/page-0936-img-01.png)

Amazon EKS orchestration
1907

## Page 937

Amazon SageMaker AI
Developer Guide

MMAP dataloader oﬀers two following features:

• Data Prefetching - Proactively fetches and caches data generated by the dataloader

• Persistent Caching - Stores both consumed and prefetched batches in a temporary ﬁlesystem
that survives process restarts

Using the cache, the training job will beneﬁt from:

• Reduced Memory Footprint - Leverages memory-mapped I/O to maintain a single shared copy
of data in host CPU memory, eliminating redundant copies across GPU processes (e.g., reduces
from 8 copies to 1 on a p5 instance with 8 GPUs)

• Faster Recovery - Reduces Mean Time to Restart (MTTR) by enabling training to resume
immediately from cached batches, eliminating the wait for dataloader reinitialization and ﬁrst-
batch generation

MMAP conﬁgurations

To use MMAP, simply pass in the your original data module into MMAPDataModule

data_module=MMAPDataModule(
data_module=MY_DATA_MODULE(...),
mmap_config=CacheResumeMMAPConfig(
cache_dir=self.cfg.mmap.cache_dir,
checkpoint_frequency=self.cfg.mmap.checkpoint_frequency),
)

CacheResumeMMAPConfig: MMAP Dataloader parameters control cache directory location,
size limits, and data fetching delegation. By default, only TP rank 0 per node fetches data from
the source, while other ranks in the same data replication group read from the shared cache,
eliminating redundant transfers.

MMAPDataModule: It wraps the original data module and returns the mmap dataloader for both
train and validation.

See the example for enabling MMAP.

Amazon EKS orchestration
1908

## Page 938

Amazon SageMaker AI
Developer Guide

API reference

CacheResumeMMAPConﬁg

class hyperpod_checkpointless_training.dataloader.config.CacheResumeMMAPConfig(
cache_dir='/dev/shm/pdl_cache',
prefetch_length=10,
val_prefetch_length=10,

lookback_length=2,
checkpoint_frequency=None,
model_parallel_group=None,
enable_batch_encryption=False)

Conﬁguration class for cache-resume memory-mapped (MMAP) dataloader functionality in
HyperPod checkpointless training.

This conﬁguration enables eﬃcient data loading with caching and prefetching capabilities,
allowing training to resume quickly after failures by maintaining cached data batches in memory-
mapped ﬁles.

Parameters

• cache_dir (str, optional) – Directory path for storing cached data batches. Default: "/dev/shm/
pdl_cache"

• prefetch_length (int, optional) – Number of batches to prefetch ahead during training. Default:
10

• val_prefetch_length (int, optional) – Number of batches to prefetch ahead during validation.
Default: 10

• lookback_length (int, optional) – Number of previously used batches to keep in cache for
potential reuse. Default: 2

• checkpoint_frequency (int, optional) – Frequency of model checkpointing steps. Used for cache
performance optimization. Default: None

• model_parallel_group (object, optional) – Process group for model parallelism. If None, will be
created automatically. Default: None

• enable_batch_encryption (bool, optional) – Whether to enable encryption for cached batch
data. Default: False

Methods

Amazon EKS orchestration
1909

## Page 939

Amazon SageMaker AI
Developer Guide

create(dataloader_init_callable,
parallel_state_util,
step,
is_data_loading_rank,
create_model_parallel_group_callable,
name='Train',
is_val=False,
cached_len=0)

Creates and returns a conﬁgured MMAP dataloader instance.

Parameters

• dataloader_init_callable (Callable) – Function to initialize the underlying dataloader

• parallel_state_util (object) – Utility for managing parallel state across processes

• step (int) – The data step to resume from during training

• is_data_loading_rank (Callable) – Function that returns True if current rank should load data

• create_model_parallel_group_callable (Callable) – Function to create model parallel process
group

• name (str, optional) – Name identiﬁer for the dataloader. Default: "Train"

• is_val (bool, optional) – Whether this is a validation dataloader. Default: False

• cached_len (int, optional) – Length of cached data if resuming from existing cache. Default: 0

Returns CacheResumePrefetchedDataLoader or CacheResumeReadDataLoader – Conﬁgured
MMAP dataloader instance

Raises ValueError if the step parameter is None.

Example

from hyperpod_checkpointless_training.dataloader.config import CacheResumeMMAPConfig

# Create configuration
config = CacheResumeMMAPConfig(
cache_dir="/tmp/training_cache",
prefetch_length=20,
checkpoint_frequency=100,
enable_batch_encryption=False

Amazon EKS orchestration
1910

## Page 940

Amazon SageMaker AI
Developer Guide

)

# Create dataloader
dataloader = config.create(
dataloader_init_callable=my_dataloader_init,
parallel_state_util=parallel_util,
step=current_step,
is_data_loading_rank=lambda: rank == 0,
create_model_parallel_group_callable=create_mp_group,
name="TrainingData"
)

Notes

• The cache directory should have suﬃcient space and fast I/O performance (e.g., /dev/shm for in-
memory storage).

• Setting checkpoint_frequency improves cache performance by aligning cache management
with model checkpointing

• For validation dataloaders (is_val=True), the step is reset to 0 and cold start is forced

• Diﬀerent dataloader implementations are used based on whether the current rank is responsible
for data loading

MMAPDataModule

class hyperpod_checkpointless_training.dataloader.mmap_data_module.MMAPDataModule(
data_module,
mmap_config,
parallel_state_util=MegatronParallelStateUtil(),
is_data_loading_rank=None)

A PyTorch Lightning DataModule wrapper that applies memory-mapped (MMAP) data loading
capabilities to existing DataModules for checkpointless training.

This class wraps an existing PyTorch Lightning DataModule and enhances it with MMAP
functionality, enabling eﬃcient data caching and fast recovery during training failures. It maintains
compatibility with the original DataModule interface while adding checkpointless training
capabilities.

Parameters

Amazon EKS orchestration
1911

## Page 941

Amazon SageMaker AI
Developer Guide

data_module (pl.LightningDataModule)

The underlying DataModule to wrap (e.g., LLMDataModule)

mmap_conﬁg (MMAPConﬁg)

The MMAP conﬁguration object that deﬁnes caching behavior and parameters

parallel_state_util (MegatronParallelStateUtil, optional)

Utility for managing parallel state across distributed processes. Default:
MegatronParallelStateUtil()

is_data_loading_rank (Callable, optional)

Function that returns True if the current rank should load data. If None, defaults to
parallel_state_util.is_tp_0. Default: None

Attributes

global_step (int)

Current global training step, used for resuming from checkpoints

cached_train_dl_len (int)

Cached length of the training dataloader

cached_val_dl_len (int)

Cached length of the validation dataloader

Methods

setup(stage=None)

Setup the underlying data module for the speciﬁed training stage.

stage (str, optional)

Stage of training ('ﬁt', 'validate', 'test', or 'predict'). Default: None

train_dataloader()

Amazon EKS orchestration
1912

## Page 942

Amazon SageMaker AI
Developer Guide

Create the training DataLoader with MMAP wrapping.

Returns: DataLoader – MMAP-wrapped training DataLoader with caching and prefetching
capabilities

val_dataloader()

Create the validation DataLoader with MMAP wrapping.

Returns: DataLoader – MMAP-wrapped validation DataLoader with caching capabilities

test_dataloader()

Create the test DataLoader if the underlying data module supports it.

Returns: DataLoader or None – Test DataLoader from the underlying data module, or None if not
supported

predict_dataloader()

Create the predict DataLoader if the underlying data module supports it.

Returns: DataLoader or None – Predict DataLoader from the underlying data module, or None if not
supported

load_checkpoint(checkpoint)

Load checkpoint information to resume training from a speciﬁc step.

checkpoint (dict)

Checkpoint dictionary containing 'global_step' key

get_underlying_data_module()

Get the underlying wrapped data module.

Returns: pl.LightningDataModule – The original data module that was wrapped

Amazon EKS orchestration
1913

## Page 943

Amazon SageMaker AI
Developer Guide

state_dict()

Get the state dictionary of the MMAP DataModule for checkpointing.

Returns: dict – Dictionary containing cached dataloader lengths

load_state_dict(state_dict)

Load the state dictionary to restore MMAP DataModule state.

state_dict (dict)

State dictionary to load

Properties

data_sampler

Expose the underlying data module's data sampler to NeMo framework.

Returns: object or None – The data sampler from the underlying data module

Example

from hyperpod_checkpointless_training.dataloader.mmap_data_module import MMAPDataModule
from hyperpod_checkpointless_training.dataloader.config import CacheResumeMMAPConfig
from my_project import MyLLMDataModule

# Create MMAP configuration
mmap_config = CacheResumeMMAPConfig(
cache_dir="/tmp/training_cache",
prefetch_length=20,
checkpoint_frequency=100
)

# Create original data module
original_data_module = MyLLMDataModule(
data_path="/path/to/data",
batch_size=32
)

Amazon EKS orchestration
1914

## Page 944

Amazon SageMaker AI
Developer Guide

# Wrap with MMAP capabilities
mmap_data_module = MMAPDataModule(
data_module=original_data_module,
mmap_config=mmap_config
)

# Use in PyTorch Lightning Trainer
trainer = pl.Trainer()
trainer.fit(model, data=mmap_data_module)

# Resume from checkpoint
checkpoint = {"global_step": 1000}
mmap_data_module.load_checkpoint(checkpoint)

Notes

• The wrapper delegates most attribute access to the underlying data module using __getattr__

• Only data loading ranks actually initialize and use the underlying data module; other ranks use
fake dataloaders

• Cached dataloader lengths are maintained to optimize performance during training resumption

In-process recovery and checkpointless training

HyperPod checkpointless training uses model redundancy to enable fault-tolerant training. The
core principle is that model and optimizer states are fully replicated across multiple node groups,
with weight updates and optimizer state changes synchronously replicated within each group.
When a failure occurs, healthy replicas complete their optimizer steps and transmit the updated
model/optimizer states to recovering replicas.

This model redundancy-based approach enables several fault handling mechanisms:

• In-process recovery: processes remain active despite faults, keeping all model and optimizer
states in GPU memory with the latest values

• Graceful abort handling: controlled aborts and resource cleanup for aﬀected operations

• Code block re-execution: re-running only the aﬀected code segments within a Re-executable
Code Block (RCB)

• Checkpointless recovery with no lost training progress: since processes persist and states
remain in memory, no training progress is lost; when a fault occurs training resumes from the
previous step, as opposed to resuming from the last saved checkpoint

Amazon EKS orchestration
1915

## Page 945

Amazon SageMaker AI
Developer Guide

Checkpointless Conﬁgurations

Here is the core snippet of checkpointless training.

from hyperpod_checkpointless_training.inprocess.train_utils import wait_rank

wait_rank()
def main():
@HPWrapper(
health_check=CudaHealthCheck(),
hp_api_factory=HPAgentK8sAPIFactory(),
abort_timeout=60.0,
checkpoint_manager=PEFTCheckpointManager(enable_offload=True),
abort=CheckpointlessAbortManager.get_default_checkpointless_abort(),
finalize=CheckpointlessFinalizeCleanup(),
)
def run_main(cfg, caller: Optional[HPCallWrapper] = None):
...
trainer = Trainer(
strategy=CheckpointlessMegatronStrategy(...,
num_distributed_optimizer_instances=2),
callbacks=[..., CheckpointlessCallback(...)],
)
trainer.fresume = resume
trainer._checkpoint_connector = CheckpointlessCompatibleConnector(trainer)
trainer.wrapper = caller

• wait_rank: All ranks will wait for the rank information from the HyperpodTrainingOperator
infrastructure.

• HPWrapper: Python function wrapper that enables restart capabilities for a Re-executable
Code Block (RCB). The implementation uses a context manager rather than a Python decorator
because decorators cannot determine the number of RCBs to monitor at runtime.

• CudaHealthCheck: Ensures the CUDA context for the current process is in a healthy state by
synchronizing with the GPU. Uses the device speciﬁed by the LOCAL_RANK environment variable,
or defaults to the main thread's CUDA device if LOCAL_RANK is not set.

• HPAgentK8sAPIFactory: This API enables checkpointless training to query the training status
of other pods in the Kubernetes training cluster. It also provides an infrastructure-level barrier
that ensures all ranks successfully complete abort and restart operations before proceeding.

• CheckpointManager: Manages in-memory checkpoints and peer-to-peer recovery for
checkpointless fault tolerance. It has the following core responsibilities:

Amazon EKS orchestration
1916

## Page 946

Amazon SageMaker AI
Developer Guide

• In-Memory Checkpoint Management: Saves and manages NeMo model checkpoints in
memory for fast recovery without disk I/O during checkpointless recovery scenarios.

• Recovery Feasibility Validation: Determines if checkpointless recovery is possible by
validating global step consistency, rank health, and model state integrity.

• Peer-to-Peer Recovery Orchestration: Coordinates checkpoint transfer between healthy and
failed ranks using distributed communication for fast recovery.

• RNG State Management: Preserves and restores random number generator states across
Python, NumPy, PyTorch, and Megatron for deterministic recovery.

• [Optional] Checkpoint Oﬄoad: Oﬄoad in memory checkpoint to CPU if GPU does not have
enough memory capacity.

• PEFTCheckpointManager: It extends CheckpointManager by keeping the base model
weights for PEFT ﬁnetuning.

• CheckpointlessAbortManager: Manages abort operations in a background thread when an
error is encountered. By default, it aborts TransformerEngine, Checkpointing, TorchDistributed,
and DataLoader. Users can register custom abort handlers as needed. After the abort completes,
all communication must cease and all processes and threads must terminate to prevent resource
leaks.

• CheckpointlessFinalizeCleanup: Handles ﬁnal cleanup operations in the main thread for
components that cannot be safely aborted or cleaned up in the background thread.

• CheckpointlessMegatronStrategy: This inherits from the MegatronStrategy from in

Nemo. Note that checkpointless training requires num_distributed_optimizer_instances
to be least 2 so that there will be optimizer replication. The strategy also takes care of essential
attribute registration and process group initialization, e.g., rootless.

• CheckpointlessCallback: Lightning callback that integrates NeMo training with
checkpointless training's fault tolerance system. It has the following core responsibilities:

• Training Step Lifecycle Management: Tracks training progress and coordinates with
ParameterUpdateLock to enable/disable checkpointless recovery based on training state (ﬁrst
step vs subsequent steps).

• Checkpoint State Coordination: Manages in-memory PEFT base model checkpoint saving/
restoring.

• CheckpointlessCompatibleConnector: A PTL CheckpointConnector that attempts to
pre-load the checkpoint ﬁle to memory, with the source path determined in this priority:

• try checkpointless recovery

Amazon EKS orchestration
1917

## Page 947

Amazon SageMaker AI
Developer Guide

• if checkpointless return None, fallback to parent.resume_start()

See the example to add checkpointless training features to codes.

Concepts

This section introduces checkpointless training concepts. Checkpointless training on Amazon
SageMaker HyperPod supports in-process recovery. This API interface follows a similar format as
the NVRx APIs.

Concept - Re-Executable Code Block (RCB)

When a failure occurs, healthy processes remain alive, but a portion of the code must be re-
executed to recover the training states and python stacks. A Re-executable Code Block (RCB) is
a speciﬁc code segment that re-runs during failure recovery. In the following example, the RCB

encompasses the entire training script (i.e., everything under main()), meaning that each failure
recovery restarts the training script while preserving the in-memory model and optimizer states.

Concept - Faults control

A fault controller module receives notiﬁcations when failures occur during checkpointless training.
This fault controller includes the following components:

• Fault detection module: Receives infrastructure fault notiﬁcations

• RCB deﬁnition APIs: Enables users to deﬁne the re-executable code block (RCB) in their code

• Restart module: Terminates the RCB, cleans up resources, and restarts the RCB

![Page 947 Diagram 1](images/page-0947-img-01.png)

Amazon EKS orchestration
1918

## Page 948

Amazon SageMaker AI
Developer Guide

Concept - Model redundancy

Large model training usually requires a large enough data parallel size to train models eﬃciently.
In traditional data parallelism like PyTorch DDP and Horovod, the model is fully replicated. More
advanced sharded data parallelism techniques like DeepSpeed ZeRO optimizer and FSDP also
support hybrid sharding mode, which allows sharding the model/optimizer states within the
sharding group and fully replicating across replication groups. NeMo also has this hybrid sharding
feature through an argument num_distributed_optimizer_instances, which allows redundancy.

However, adding redundancy indicates that the model will not be fully sharded across the entire
cluster, resulting in higher device memory usage. The amount of redundant memory will vary
depending on the speciﬁc model sharding techniques implemented by the user. The low-precision
model weights, gradients, and activation memory will not be aﬀected, since they are sharded
through model parallelism. The high-precision master model weights/gradients and optimizer

states will be aﬀected. Adding one redundant model replica increases device memory usage by
roughly the equivalent of one DCP checkpoint size.

Hybrid sharding breaks the collectives across the entire DP groups into relatively smaller
collectives. Previously there was a reduce-scatter and an all-gather across the entire DP group.
After the hybrid sharding, the reduce-scatter is only running inside each model replica, and there
will be an all-reduce across model replica groups. The all-gather is also running inside each model
replica. As a result, the entire communication volume remains roughly unchanged, but collectives
are running with smaller groups, so we expect better latency.

Concept - Failure and Restart Types

The following table records diﬀerent failure types and associated recovery mechanisms.
Checkpointless training ﬁrst attempts failure recovery via an in-process recovery, followed by a
process-level restart. It falls back to a job-level restart only in the event of a catastrophic failure
(e.g., multiple nodes fail at the same time).

Failure Type
Cause
Recovery Type
Recovery Mechanism

In-process failure
Code-level errors,
exceptions

In-Process Recovery
(IPR)

Rerun RCB within
existing process;
healthy processes
remain active

Amazon EKS orchestration
1919

## Page 949

Amazon SageMaker AI
Developer Guide

Failure Type
Cause
Recovery Type
Recovery Mechanism

Process restart
failure

Process Level Restart
(PLR)

Corrupted CUDA
context, terminated
process

SageMaker HyperPod
training operator
restarts processes;
skips K8s pod restart

Node replacement
failure

Job Level Restart
(JLR)

Permanent node/
GPU hardware failure

Replace failed node;
restart entire training
job

Concept - Atomic lock protection for optimizer step

Model execution is divided into three phases: forward propagation, backward propagation, and
optimizer step. Recovery behavior varies based on the failure timing:

• Forward/backward propagation: Roll back to the beginning of the current training step and
broadcast model states to replacement node(s)

• Optimizer step: Allow healthy replicas to complete the step under lock protection, then
broadcast the updated model states to replacement node(s)

This strategy ensures completed optimizer updates are never discarded, helping reduce fault
recovery time.

Amazon EKS orchestration
1920

## Page 950

Amazon SageMaker AI
Developer Guide

![Page 950 Diagram 1](images/page-0950-img-01.png)

Amazon EKS orchestration
1921

## Page 951

Amazon SageMaker AI
Developer Guide

Checkpointless Training Flow Diagram

![Page 951 Diagram 1](images/page-0951-img-01.png)

The following steps outline the failure detection and checkpointless recovery process:

1. Training loop starts

2. Fault occurs

3. Evaluate checkpointless resume feasibility

4. Check if it is feasible to do checkpointless resume

Amazon EKS orchestration
1922

## Page 952

Amazon SageMaker AI
Developer Guide

• If feasible, Attempt checkpointless reusme

• If resumes fails, fallback to checkpoint loading from storage

• If resume succeeds, training continues from recovered state

• If not feasible, fall back to checkpoint loading from storage

5. Clean up resources - abort all process groups and backends and free resources in preparation for

restart.

6. Resume training loop - a new training loop begins, and the process returns to step 1.

API reference

wait_rank

hyperpod_checkpointless_training.inprocess.train_utils.wait_rank()

Waits for and retrieves rank information from HyperPod, then updates the current process
environment with distributed training variables.

This function obtains the correct rank assignment and environment variables for distributed
training. It ensures that each process gets the appropriate conﬁguration for its role in the
distributed training job.

Parameters

None

Returns

None

Behavior

• Process Check: Skips execution if called from a subprocess (only runs in MainProcess)

• Environment Retrieval: Gets current RANK and WORLD_SIZE from environment variables

• HyperPod Communication: Calls hyperpod_wait_rank_info() to retrieve rank information
from HyperPod

• Environment Update: Updates the current process environment with worker-speciﬁc
environment variables received from HyperPod

Amazon EKS orchestration
1923

## Page 953

Amazon SageMaker AI
Developer Guide

Environment Variables

The function reads the following environment variables:

• RANK (int) – Current process rank (default: -1 if not set)

• WORLD_SIZE (int) – Total number of processes in the distributed job (default: 0 if not set)

Raises

• AssertionError – If the response from HyperPod is not in the expected format or if required
ﬁelds are missing

Example

from hyperpod_checkpointless_training.inprocess.train_utils import wait_rank

# Call before initializing distributed training
wait_rank()

# Now environment variables are properly set for this rank
import torch.distributed as dist
dist.init_process_group(backend='nccl')

Notes

• Only executes in the main process; subprocess calls are automatically skipped

• The function blocks until HyperPod provides the rank information

HPWrapper

class hyperpod_checkpointless_training.inprocess.wrap.HPWrapper(
*,
abort=Compose(HPAbortTorchDistributed()),
finalize=None,
health_check=None,
hp_api_factory=None,
abort_timeout=None,
enabled=True,
trace_file_path=None,
async_raise_before_abort=True,

Amazon EKS orchestration
1924

## Page 954

Amazon SageMaker AI
Developer Guide

early_abort_communicator=False,
checkpoint_manager=None,
check_memory_status=True)

Python function wrapper that enables restart capabilities for a Re-executable Code Block (RCB) in
HyperPod checkpointless training.

This wrapper provides fault tolerance and automatic recovery capabilities by monitoring training
execution and coordinating restarts across distributed processes when failures occur. It uses a context
manager approach rather than a decorator to maintain global resources throughout the training
lifecycle.

Parameters

• abort (Abort, optional) – Asynchronously aborts execution when failures are detected. Default:

Compose(HPAbortTorchDistributed())

• ﬁnalize (Finalize, optional) – Rank-local ﬁnalize handler executed during restart. Default: None

• health_check (HealthCheck, optional) – Rank-local health check executed during restart. Default:

None

• hp_api_factory (Callable, optional) – Factory function for creating a HyperPod API to interact

with HyperPod. Default: None

• abort_timeout (ﬂoat, optional) – Timeout for abort call in fault controlling thread. Default: None

• enabled (bool, optional) – Enables the wrapper functionality. When False, the wrapper becomes

a pass-through. Default: True

• trace_ﬁle_path (str, optional) – Path to the trace ﬁle for VizTracer proﬁling. Default: None

• async_raise_before_abort (bool, optional) – Enable raise before abort in fault controlling thread.

Default: True

• early_abort_communicator (bool, optional) – Abort communicator (NCCL/Gloo) before aborting

dataloader. Default: False

• checkpoint_manager (Any, optional) – Manager for handling checkpoints during recovery.

Default: None

• check_memory_status (bool, optional) – Enable memory status checking and logging. Default:

True

Methods

Amazon EKS orchestration
1925

## Page 955

Amazon SageMaker AI
Developer Guide

def __call__(self, fn)

Wraps a function to enable restart capabilities.

Parameters:

• fn (Callable) – The function to wrap with restart capabilities

Returns:

• Callable – Wrapped function with restart capabilities, or original function if disabled

Example

from hyperpod_checkpointless_training.nemo_plugins.checkpoint_manager import
CheckpointManager
from hyperpod_checkpointless_training.nemo_plugins.patches import
patch_megatron_optimizer
from hyperpod_checkpointless_training.nemo_plugins.checkpoint_connector import
CheckpointlessCompatibleConnector
from hyperpod_checkpointless_training.inprocess.train_utils import HPAgentK8sAPIFactory
from hyperpod_checkpointless_training.inprocess.abort import
CheckpointlessFinalizeCleanup, CheckpointlessAbortManager
@HPWrapper(
health_check=CudaHealthCheck(),
hp_api_factory=HPAgentK8sAPIFactory(),
abort_timeout=60.0,
checkpoint_manager=CheckpointManager(enable_offload=False),
abort=CheckpointlessAbortManager.get_default_checkpointless_abort(),
finalize=CheckpointlessFinalizeCleanup(),
)def training_function():
# Your training code here
pass

Notes

• The wrapper requires torch.distributed to be available

Amazon EKS orchestration
1926

## Page 956

Amazon SageMaker AI
Developer Guide

• When enabled=False, the wrapper becomes a pass-through and returns the original function
unchanged

• The wrapper maintains global resources like monitoring threads throughout the training lifecycle

• Supports VizTracer proﬁling when trace_file_path is provided

• Integrates with HyperPod for coordinated fault handling across distributed training

HPCallWrapper

class hyperpod_checkpointless_training.inprocess.wrap.HPCallWrapper(wrapper)

Monitors and manages the state of a Restart Code Block (RCB) during execution.

This class handles the lifecycle of RCB execution, including failure detection, coordination with
other ranks for restarts, and cleanup operations. It manages distributed synchronization and
ensures consistent recovery across all training processes.

Parameters

• wrapper (HPWrapper) – The parent wrapper containing global in-process recovery settings

Attributes

• step_upon_restart (int) – Counter that tracks steps since the last restart, used for determining
restart strategy

Methods

def initialize_barrier()

Wait for HyperPod barrier synchronization after encountering an exception from RCB.

def start_hp_fault_handling_thread()

Start the fault handling thread for monitoring and coordinating failures.

def handle_fn_exception(call_ex)

Process exceptions from the execution function or RCB.

Amazon EKS orchestration
1927

## Page 957

Amazon SageMaker AI
Developer Guide

Parameters:

• call_ex (Exception) – Exception from the monitoring function

def restart(term_ex)

Execute restart handler including ﬁnalization, garbage collection, and health checks.

Parameters:

• term_ex (RankShouldRestart) – Termination exception triggering the restart

def launch(fn, *a, **kw)

Execute the RCB with proper exception handling.

Parameters:

• fn (Callable) – Function to be executed

• a – Function arguments

• kw – Function keyword arguments

def run(fn, a, kw)

Main execution loop that handles restarts and barrier synchronization.

Parameters:

• fn (Callable) – Function to be executed

• a – Function arguments

• kw – Function keyword arguments

def shutdown()

Shutdown fault handling and monitoring threads.

Amazon EKS orchestration
1928

## Page 958

Amazon SageMaker AI
Developer Guide

Notes

• Automatically handles RankShouldRestart exceptions for coordinated recovery

• Manages memory tracking and aborts, garbage collection during restarts

• Supports both in-process recovery and PLR (Process-Level Restart) strategies based on failure
timing

CudaHealthCheck

class
hyperpod_checkpointless_training.inprocess.health_check.CudaHealthCheck(timeout=datetime.timed

Ensures that the CUDA context for the current process is in a healthy state during checkpointless
training recovery.

This health check synchronizes with the GPU to verify that the CUDA context is not corrupted after
a training failure. It performs GPU synchronization operations to detect any issues that might
prevent successful training resumption. The health check is executed after distributed groups are
destroyed and ﬁnalization is complete.

Parameters

• timeout (datetime.timedelta, optional) – Timeout duration for GPU synchronization operations.

Default: datetime.timedelta(seconds=30)

Methods

__call__(state, train_ex=None)

Execute the CUDA health check to verify GPU context integrity.

Parameters:

• state (HPState) – Current HyperPod state containing rank and distributed information

• train_ex (Exception, optional) – The original training exception that triggered the restart. Default:

None

Returns:

Amazon EKS orchestration
1929

## Page 959

Amazon SageMaker AI
Developer Guide

• tuple – A tuple containing (state, train_ex) unchanged if health check passes

Raises:

• TimeoutError – If GPU synchronization times out, indicating a potentially corrupted CUDA
context

State Preservation: Returns the original state and exception unchanged if all checks pass

Example

import datetime
from hyperpod_checkpointless_training.inprocess.health_check import CudaHealthCheck
from hyperpod_checkpointless_training.inprocess.wrap import HPWrapper

# Create CUDA health check with custom timeout
cuda_health_check = CudaHealthCheck(
timeout=datetime.timedelta(seconds=60)
)
# Use with HPWrapper for fault-tolerant training
@HPWrapper(
health_check=cuda_health_check,
enabled=True
)
def training_function():
# Your training code here
pass

Notes

• Uses threading to implement timeout protection for GPU synchronization

• Designed to detect corrupted CUDA contexts that could prevent successful training resumption

• Should be used as part of the fault tolerance pipeline in distributed training scenarios

HPAgentK8sAPIFactory

class hyperpod_checkpointless_training.inprocess.train_utils.HPAgentK8sAPIFactory()

Amazon EKS orchestration
1930

## Page 960

Amazon SageMaker AI
Developer Guide

Factory class for creating HPAgentK8sAPI instances that communicate with HyperPod
infrastructure for distributed training coordination.

This factory provides a standardized way to create and conﬁgure HPAgentK8sAPI objects

that handle communication between training processes and the HyperPod control plane. It
encapsulates the creation of the underlying socket client and API instance, ensuring consistent
conﬁguration across diﬀerent parts of the training system.

Methods

__call__()

Create and return an HPAgentK8sAPI instance conﬁgured for HyperPod communication.

Returns:

• HPAgentK8sAPI – Conﬁgured API instance for communicating with HyperPod infrastructure

Example

from hyperpod_checkpointless_training.inprocess.train_utils import HPAgentK8sAPIFactory
from hyperpod_checkpointless_training.inprocess.wrap import HPWrapper
from hyperpod_checkpointless_training.inprocess.health_check import CudaHealthCheck
# Create the factory
hp_api_factory = HPAgentK8sAPIFactory()
# Use with HPWrapper for fault-tolerant training
hp_wrapper = HPWrapper(
hp_api_factory=hp_api_factory,
health_check=CudaHealthCheck(),
abort_timeout=60.0,
enabled=True
)
@hp_wrapper
def training_function():
# Your distributed training code here
pass

Notes

Amazon EKS orchestration
1931

## Page 961

Amazon SageMaker AI
Developer Guide

• Designed to work seamlessly with HyperPod's Kubernetes-based infrastructure. It is essential for
coordinated fault handling and recovery in distributed training scenarios

CheckpointManager

class
hyperpod_checkpointless_training.nemo_plugins.checkpoint_manager.CheckpointManager(
enable_checksum=False,
enable_offload=False)

Manages in-memory checkpoints and peer-to-peer recovery for checkpointless fault tolerance in
distributed training.

This class provides the core functionality for HyperPod checkpointless training by managing NeMo
model checkpoints in memory, validating recovery feasibility, and orchestrating peer-to-peer
checkpoint transfer between healthy and failed ranks. It eliminates the need for disk I/O during
recovery, signiﬁcantly reducing mean time to recovery (MTTR).

Parameters

• enable_checksum (bool, optional) – Enable model state checksum validation for integrity checks

during recovery. Default: False

• enable_oﬄoad (bool, optional) – Enable checkpoint oﬄoading from GPU to CPU memory to

reduce GPU memory usage. Default: False

Attributes

• global_step (int or None) – Current training step associated with the saved checkpoint

• rng_states (list or None) – Stored random number generator states for deterministic recovery

• checksum_manager (MemoryChecksumManager) – Manager for model state checksum validation

• parameter_update_lock (ParameterUpdateLock) – Lock for coordinating parameter updates
during recovery

Methods

save_checkpoint(trainer)

Amazon EKS orchestration
1932

## Page 962

Amazon SageMaker AI
Developer Guide

Save NeMo model checkpoint in memory for potential checkpointless recovery.

Parameters:

• trainer (pytorch_lightning.Trainer) – PyTorch Lightning trainer instance

Notes:

• Called by CheckpointlessCallback at batch end or during exception handling

• Creates recovery points without disk I/O overhead

• Stores complete model, optimizer, and scheduler states

delete_checkpoint()

Delete the in-memory checkpoint and perform cleanup operations.

Notes:

• Clears checkpoint data, RNG states, and cached tensors

• Performs garbage collection and CUDA cache cleanup

• Called after successful recovery or when checkpoint is no longer needed

try_checkpointless_load(trainer)

Attempt checkpointless recovery by loading state from peer ranks.

Parameters:

• trainer (pytorch_lightning.Trainer) – PyTorch Lightning trainer instance

Returns:

• dict or None – Restored checkpoint if successful, None if fallback to disk needed

Notes:

• Main entry point for checkpointless recovery

Amazon EKS orchestration
1933

## Page 963

Amazon SageMaker AI
Developer Guide

• Validates recovery feasibility before attempting P2P transfer

• Always cleans up in-memory checkpoints after recovery attempt

checkpointless_recovery_feasible(trainer, include_checksum_verification=True)

Determine if checkpointless recovery is possible for the current failure scenario.

Parameters:

• trainer (pytorch_lightning.Trainer) – PyTorch Lightning trainer instance

• include_checksum_veriﬁcation (bool, optional) – Whether to include checksum validation.

Default: True

Returns:

• bool – True if checkpointless recovery is feasible, False otherwise

Validation Criteria:

• Global step consistency across healthy ranks

• Suﬃcient healthy replicas available for recovery

• Model state checksum integrity (if enabled)

store_rng_states()

Store all random number generator states for deterministic recovery.

Notes:

• Captures Python, NumPy, PyTorch CPU/GPU, and Megatron RNG states

• Essential for maintaining training determinism after recovery

load_rng_states()

Restore all RNG states for deterministic recovery continuation.

Amazon EKS orchestration
1934

## Page 964

Amazon SageMaker AI
Developer Guide

Notes:

• Restores all previously stored RNG states

• Ensures training continues with identical random sequences

maybe_offload_checkpoint()

Oﬄoad checkpoint from GPU to CPU memory if oﬄoad is enabled.

Notes:

• Reduces GPU memory usage for large models

• Only executes if enable_offload=True

• Maintains checkpoint accessibility for recovery

Example

from hyperpod_checkpointless_training.inprocess.wrap import HPWrapper
from hyperpod_checkpointless_training.nemo_plugins.checkpoint_manager import
CheckpointManager
# Use with HPWrapper for complete fault tolerance
@HPWrapper(
checkpoint_manager=CheckpointManager(),
enabled=True
)
def training_function():
# Training code with automatic checkpointless recovery
pass

Validation: Veriﬁes checkpoint integrity using checksums (if enabled)

Notes

• Uses distributed communication primitives for eﬃcient P2P transfer

• Automatically handles tensor dtype conversions and device placement

• MemoryChecksumManager – Handles model state integrity validation

Amazon EKS orchestration
1935

## Page 965

Amazon SageMaker AI
Developer Guide

PEFTCheckpointManager

class
hyperpod_checkpointless_training.nemo_plugins.checkpoint_manager.PEFTCheckpointManager(
*args,
**kwargs)

Manages checkpoints for PEFT (Parameter-Eﬃcient Fine-Tuning) with separate base and adapter
handling for optimized checkpointless recovery.

This specialized checkpoint manager extends CheckpointManager to optimize PEFT workﬂows by
separating base model weights from adapter parameters.

Parameters

Inherits all parameters from CheckpointManager:

• enable_checksum (bool, optional) – Enable model state checksum validation. Default: False

• enable_oﬄoad (bool, optional) – Enable checkpoint oﬄoading to CPU memory. Default: False

Additional Attributes

• params_to_save (set) – Set of parameter names that should be saved as adapter parameters

• base_model_weights (dict or None) – Cached base model weights, saved once and reused

• base_model_keys_to_extract (list or None) – Keys for extracting base model tensors during P2P
transfer

Methods

maybe_save_base_model(trainer)

Save base model weights once, ﬁltering out adapter parameters.

Parameters:

• trainer (pytorch_lightning.Trainer) – PyTorch Lightning trainer instance

Notes:

Amazon EKS orchestration
1936

## Page 966

Amazon SageMaker AI
Developer Guide

• Only saves base model weights on ﬁrst call; subsequent calls are no-ops

• Filters out adapter parameters to store only frozen base model weights

• Base model weights are preserved across multiple training sessions

save_checkpoint(trainer)

Save NeMo PEFT adapter model checkpoint in memory for potential checkpointless recovery.

Parameters:

• trainer (pytorch_lightning.Trainer) – PyTorch Lightning trainer instance

Notes:

• Automatically calls maybe_save_base_model() if base model not yet saved

• Filters checkpoint to include only adapter parameters and training state

• Signiﬁcantly reduces checkpoint size compared to full model checkpoints

try_base_model_checkpointless_load(trainer)

Attempt PEFT base model weights checkpointless recovery by loading state from peer ranks.

Parameters:

• trainer (pytorch_lightning.Trainer) – PyTorch Lightning trainer instance

Returns:

• dict or None – Restored base model checkpoint if successful, None if fallback needed

Notes:

• Used during model initialization to recover base model weights

• Does not clean up base model weights after recovery (preserves for reuse)

• Optimized for model-weights-only recovery scenarios

Amazon EKS orchestration
1937

## Page 967

Amazon SageMaker AI
Developer Guide

try_checkpointless_load(trainer)

Attempt PEFT adapter weights checkpointless recovery by loading state from peer ranks.

Parameters:

• trainer (pytorch_lightning.Trainer) – PyTorch Lightning trainer instance

Returns:

• dict or None – Restored adapter checkpoint if successful, None if fallback needed

Notes:

• Recovers only adapter parameters, optimizer states, and schedulers

• Automatically loads optimizer and scheduler states after successful recovery

• Cleans up adapter checkpoints after recovery attempt

is_adapter_key(key)

Check if state dict key belongs to adapter parameters.

Parameters:

• key (str or tuple) – State dict key to check

Returns:

• bool – True if key is adapter parameter, False if base model parameter

Detection Logic:

• Checks if key is in params_to_save set

• Identiﬁes keys containing ".adapter." substring

• Identiﬁes keys ending with ".adapters"

• For tuple keys, checks if parameter requires gradients

Amazon EKS orchestration
1938

## Page 968

Amazon SageMaker AI
Developer Guide

maybe_offload_checkpoint()

Oﬄoad base model weights from GPU to CPU memory.

Notes:

• Extends parent method to handle base model weight oﬄoading

• Adapter weights are typically small and don't require oﬄoading

• Sets internal ﬂag to track oﬄoad state

Notes

• Designed speciﬁcally for Parameter-Eﬃcient Fine-Tuning scenarios (LoRA, Adapters, etc.)

• Automatically handles separation of base model and adapter parameters

Example

from hyperpod_checkpointless_training.inprocess.wrap import HPWrapper
from hyperpod_checkpointless_training.nemo_plugins.checkpoint_manager import
PEFTCheckpointManager
# Use with HPWrapper for complete fault tolerance
@HPWrapper(
checkpoint_manager=PEFTCheckpointManager(),
enabled=True
)
def training_function():
# Training code with automatic checkpointless recovery
pass

CheckpointlessAbortManager

class hyperpod_checkpointless_training.inprocess.abort.CheckpointlessAbortManager()

Factory class for creating and managing abort component compositions for checkpointless fault
tolerance.

This utility class provides static methods to create, customize, and manage abort component
compositions used during fault handling in HyperPod checkpointless training. It simpliﬁes the

Amazon EKS orchestration
1939

## Page 969

Amazon SageMaker AI
Developer Guide

conﬁguration of abort sequences that handle cleanup of distributed training components, data
loaders, and framework-speciﬁc resources during failure recovery.

Parameters

None (all methods are static)

Static Methods

get_default_checkpointless_abort()

Get the default abort compose instance containing all standard abort components.

Returns:

• Compose – Default composed abort instance with all abort components

Default Components:

• AbortTransformerEngine() – Cleans up TransformerEngine resources

• HPCheckpointingAbort() – Handles checkpointing system cleanup

• HPAbortTorchDistributed() – Aborts PyTorch distributed operations

• HPDataLoaderAbort() – Stops and cleans up data loaders

create_custom_abort(abort_instances)

Create a custom abort compose with only the speciﬁed abort instances.

Parameters:

• abort_instances (Abort) – Variable number of abort instances to include in the compose

Returns:

• Compose – New composed abort instance containing only the speciﬁed components

Raises:

Amazon EKS orchestration
1940

## Page 970

Amazon SageMaker AI
Developer Guide

• ValueError – If no abort instances are provided

override_abort(abort_compose, abort_type, new_abort)

Replace a speciﬁc abort component in a Compose instance with a new component.

Parameters:

• abort_compose (Compose) – The original Compose instance to modify

• abort_type (type) – The type of abort component to replace (e.g., HPCheckpointingAbort)

• new_abort (Abort) – The new abort instance to use as replacement

Returns:

• Compose – New Compose instance with the speciﬁed component replaced

Raises:

• ValueError – If abort_compose doesn't have 'instances' attribute

Example

from hyperpod_checkpointless_training.inprocess.wrap import HPWrapper
from hyperpod_checkpointless_training.nemo_plugins.callbacks import
CheckpointlessCallback
from hyperpod_checkpointless_training.inprocess.abort import
CheckpointlessFinalizeCleanup, CheckpointlessAbortManager
# The strategy automatically integrates with HPWrapper
@HPWrapper(
abort=CheckpointlessAbortManager.get_default_checkpointless_abort(),
health_check=CudaHealthCheck(),
finalize=CheckpointlessFinalizeCleanup(),
enabled=True
)
def training_function():
trainer.fit(...)

Notes

Amazon EKS orchestration
1941

## Page 971

Amazon SageMaker AI
Developer Guide

• Custom conﬁgurations allow ﬁne-tuned control over cleanup behavior

• Abort operations are critical for proper resource cleanup during fault recovery

CheckpointlessFinalizeCleanup

class hyperpod_checkpointless_training.inprocess.abort.CheckpointlessFinalizeCleanup()

Performs comprehensive cleanup after fault detection to prepare for in-process recovery during
checkpointless training.

This ﬁnalize handler executes framework-speciﬁc cleanup operations including Megatron/
TransformerEngine abort, DDP cleanup, module reloading, and memory cleanup by destroying
training component references. It ensures that the training environment is properly reset for
successful in-process recovery without requiring full process termination.

Parameters

None

Attributes

• trainer (pytorch_lightning.Trainer or None) – Reference to the PyTorch Lightning trainer instance

Methods

__call__(*a, **kw)

Execute comprehensive cleanup operations for in-process recovery preparation.

Parameters:

• a – Variable positional arguments (inherited from Finalize interface)

• kw – Variable keyword arguments (inherited from Finalize interface)

Cleanup Operations:

• Megatron Framework Cleanup – Calls abort_megatron() to clean up Megatron-speciﬁc
resources

Amazon EKS orchestration
1942

## Page 972

Amazon SageMaker AI
Developer Guide

• TransformerEngine Cleanup – Calls abort_te() to clean up TransformerEngine resources

• RoPE Cleanup – Calls cleanup_rope() to clean up rotary position embedding resources

• DDP Cleanup – Calls cleanup_ddp() to clean up DistributedDataParallel resources

• Module Reloading – Calls reload_megatron_and_te() to reload framework modules

• Lightning Module Cleanup – Optionally clears Lightning module to reduce GPU memory

• Memory Cleanup – Destroys training component references to free memory

register_attributes(trainer)

Register the trainer instance for use during cleanup operations.

Parameters:

• trainer (pytorch_lightning.Trainer) – PyTorch Lightning trainer instance to register

Integration with CheckpointlessCallback

from hyperpod_checkpointless_training.nemo_plugins.callbacks import
CheckpointlessCallback
from hyperpod_checkpointless_training.inprocess.wrap import HPWrapper
# The strategy automatically integrates with HPWrapper
@HPWrapper(
...
finalize=CheckpointlessFinalizeCleanup(),
)
def training_function():
trainer.fit(...)

Notes

• Cleanup operations are executed in a speciﬁc order to avoid dependency issues

• Memory cleanup uses garbage collection introspection to ﬁnd target objects

• All cleanup operations are designed to be idempotent and safe to retry

Amazon EKS orchestration
1943

## Page 973

Amazon SageMaker AI
Developer Guide

CheckpointlessMegatronStrategy

class
hyperpod_checkpointless_training.nemo_plugins.megatron_strategy.CheckpointlessMegatronStrategy
**kwargs)

NeMo Megatron strategy with integrated checkpointless recovery capabilities for fault-tolerant
distributed training.

Note that checkpointless training requires num_distributed_optimizer_instances to be
least 2 so that there will be optimizer replication. The strategy also takes care of essential attribute
registration and process group initialization.

Parameters

Inherits all parameters from MegatronStrategy:

• Standard NeMo MegatronStrategy initialization parameters

• Distributed training conﬁguration options

• Model parallelism settings

Attributes

• base_store (torch.distributed.TCPStore or None) – Distributed store for process group
coordination

Methods

setup(trainer)

Initialize the strategy and register fault tolerance components with the trainer.

Parameters:

• trainer (pytorch_lightning.Trainer) – PyTorch Lightning trainer instance

Setup Operations:

• Parent Setup – Calls parent MegatronStrategy setup

Amazon EKS orchestration
1944

## Page 974

Amazon SageMaker AI
Developer Guide

• Fault Injection Registration – Registers HPFaultInjectionCallback hooks if present

• Finalize Registration – Registers trainer with ﬁnalize cleanup handlers

• Abort Registration – Registers trainer with abort handlers that support it

setup_distributed()

Initialize process group using either TCPStore with preﬁx or rootless connection.

load_model_state_dict(checkpoint, strict=True)

Load model state dict with checkpointless recovery compatibility.

Parameters:

• checkpoint (Mapping[str, Any]) – Checkpoint dictionary containing model state

• strict (bool, optional) – Whether to strictly enforce state dict key matching. Default: True

get_wrapper()

Get the HPCallWrapper instance for fault tolerance coordination.

Returns:

• HPCallWrapper – The wrapper instance attached to the trainer for fault tolerance

is_peft()

Check if PEFT (Parameter-Eﬃcient Fine-Tuning) is enabled in the training conﬁguration by
checking for PEFT callbacks

Returns:

• bool – True if PEFT callback is present, False otherwise

teardown()

Amazon EKS orchestration
1945

## Page 975

Amazon SageMaker AI
Developer Guide

Override PyTorch Lightning native teardown to delegate cleanup to abort handlers.

Example

from hyperpod_checkpointless_training.inprocess.wrap import HPWrapper
# The strategy automatically integrates with HPWrapper
@HPWrapper(

checkpoint_manager=checkpoint_manager,
enabled=True
)
def training_function():
trainer = pl.Trainer(strategy=CheckpointlessMegatronStrategy())
trainer.fit(model, datamodule)

CheckpointlessCallback

class hyperpod_checkpointless_training.nemo_plugins.callbacks.CheckpointlessCallback(
enable_inprocess=False,
enable_checkpointless=False,
enable_checksum=False,
clean_tensor_hook=False,
clean_lightning_module=False)

Lightning callback that integrates NeMo training with checkpointless training's fault tolerance
system.

This callback manages step tracking, checkpoint saving, and parameter update coordination for in-
process recovery capabilities. It serves as the primary integration point between PyTorch Lightning
training loops and HyperPod checkpointless training mechanisms, coordinating fault tolerance
operations throughout the training lifecycle.

Parameters

• enable_inprocess (bool, optional) – Enable in-process recovery capabilities. Default: False

• enable_checkpointless (bool, optional) – Enable checkpointless recovery (requires

enable_inprocess=True). Default: False

• enable_checksum (bool, optional) – Enable model state checksum validation (requires

enable_checkpointless=True). Default: False

Amazon EKS orchestration
1946

## Page 976

Amazon SageMaker AI
Developer Guide

• clean_tensor_hook (bool, optional) – Clear tensor hooks from all GPU tensors during cleanup

(expensive operation). Default: False

• clean_lightning_module (bool, optional) – Enable Lightning module cleanup to free GPU

memory after each restart. Default: False

Attributes

• tried_adapter_checkpointless (bool) – Flag to track if adapter checkpointless restore has been
attempted

Methods

get_wrapper_from_trainer(trainer)

Get the HPCallWrapper instance from the trainer for fault tolerance coordination.

Parameters:

• trainer (pytorch_lightning.Trainer) – PyTorch Lightning trainer instance

Returns:

• HPCallWrapper – The wrapper instance for fault tolerance operations

on_train_batch_start(trainer, pl_module, batch, batch_idx, *args, **kwargs)

Called at the start of each training batch to manage step tracking and recovery.

Parameters:

• trainer (pytorch_lightning.Trainer) – PyTorch Lightning trainer instance

• pl_module (pytorch_lightning.LightningModule) – Lightning module being trained

• batch – Current training batch data

• batch_idx (int) – Index of the current batch

• args – Additional positional arguments

• kwargs – Additional keyword arguments

Amazon EKS orchestration
1947

## Page 977

Amazon SageMaker AI
Developer Guide

on_train_batch_end(trainer, pl_module, outputs, batch, batch_idx)

Release parameter update lock at the end of each training batch.

Parameters:

• trainer (pytorch_lightning.Trainer) – PyTorch Lightning trainer instance

• pl_module (pytorch_lightning.LightningModule) – Lightning module being trained

• outputs (STEP_OUTPUT) – Training step outputs

• batch (Any) – Current training batch data

• batch_idx (int) – Index of the current batch

Notes:

• Lock release timing ensures checkpointless recovery can proceed after parameter updates
complete

• Only executes when both enable_inprocess and enable_checkpointless are True

get_peft_callback(trainer)

Retrieve the PEFT callback from the trainer's callback list.

Parameters:

• trainer (pytorch_lightning.Trainer) – PyTorch Lightning trainer instance

Returns:

• PEFT or None – PEFT callback instance if found, None otherwise

_try_adapter_checkpointless_restore(trainer, params_to_save)

Attempt checkpointless restore for PEFT adapter parameters.

Parameters:

Amazon EKS orchestration
1948

## Page 978

Amazon SageMaker AI
Developer Guide

• trainer (pytorch_lightning.Trainer) – PyTorch Lightning trainer instance

• params_to_save (set) – Set of parameter names to save as adapter parameters

Notes:

• Only executes once per training session (controlled by tried_adapter_checkpointless ﬂag)

• Conﬁgures checkpoint manager with adapter parameter information

Example

from hyperpod_checkpointless_training.nemo_plugins.callbacks import
CheckpointlessCallback
from hyperpod_checkpointless_training.nemo_plugins.checkpoint_manager import
CheckpointManager
import pytorch_lightning as pl
# Create checkpoint manager
checkpoint_manager = CheckpointManager(
enable_checksum=True,
enable_offload=True
)
# Create checkpointless callback with full fault tolerance
checkpointless_callback = CheckpointlessCallback(
enable_inprocess=True,
enable_checkpointless=True,
enable_checksum=True,
clean_tensor_hook=True,
clean_lightning_module=True
)
# Use with PyTorch Lightning trainer
trainer = pl.Trainer(
callbacks=[checkpointless_callback],
strategy=CheckpointlessMegatronStrategy()
)
# Training with fault tolerance
trainer.fit(model, datamodule=data_module)

Memory Management

Amazon EKS orchestration
1949

## Page 979

Amazon SageMaker AI
Developer Guide

• clean_tensor_hook: Removes tensor hooks during cleanup (expensive but thorough)

• clean_lightning_module: Frees Lightning module GPU memory during restarts

• Both options help reduce memory footprint during fault recovery

• Coordinates with ParameterUpdateLock for thread-safe parameter update tracking

CheckpointlessCompatibleConnector

class
hyperpod_checkpointless_training.nemo_plugins.checkpoint_connector.CheckpointlessCompatibleCon

PyTorch Lightning checkpoint connector that integrates checkpointless recovery with traditional
disk-based checkpoint loading.

This connector extends PyTorch Lightning's _CheckpointConnector to provide seamless
integration between checkpointless recovery and standard checkpoint restoration. It attempts
checkpointless recovery ﬁrst, then falls back to disk-based checkpoint loading if checkpointless
recovery is not feasible or fails.

Parameters

Inherits all parameters from _CheckpointConnector

Methods

resume_start(checkpoint_path=None)

Attempt to pre-load checkpoint with checkpointless recovery priority.

Parameters:

• checkpoint_path (str or None, optional) – Path to disk checkpoint for fallback. Default: None

resume_end()

Complete the checkpoint loading process and perform post-load operations.

Notes

Amazon EKS orchestration
1950

## Page 980

Amazon SageMaker AI
Developer Guide

• Extends PyTorch Lightning's internal _CheckpointConnector class with checkpointless
recovery support

• Maintains full compatibility with standard PyTorch Lightning checkpoint workﬂows

CheckpointlessAutoResume

class hyperpod_checkpointless_training.nemo_plugins.resume.CheckpointlessAutoResume()

Extends NeMo's AutoResume with delayed setup to enable checkpointless recovery validation
before checkpoint path resolution.

This class implements a two-phase initialization strategy that allows checkpointless recovery
validation to occur before falling back to traditional disk-based checkpoint loading. It conditionally
delays AutoResume setup to prevent premature checkpoint path resolution, enabling the
CheckpointManager to ﬁrst validate whether checkpointless peer-to-peer recovery is feasible.

Parameters

Inherits all parameters from AutoResume

Methods

setup(trainer, model=None, force_setup=False)

Conditionally delay AutoResume setup to enable checkpointless recovery validation.

Parameters:

• trainer (pytorch_lightning.Trainer or lightning.fabric.Fabric) – PyTorch Lightning trainer or Fabric
instance

• model (optional) – Model instance for setup. Default: None

• force_setup (bool, optional) – If True, bypass delay and execute AutoResume setup immediately.

Default: False

Example

from hyperpod_checkpointless_training.nemo_plugins.resume import
CheckpointlessAutoResume

Amazon EKS orchestration
1951

## Page 981

Amazon SageMaker AI
Developer Guide

from hyperpod_checkpointless_training.nemo_plugins.megatron_strategy import
CheckpointlessMegatronStrategy
import pytorch_lightning as pl
# Create trainer with checkpointless auto-resume
trainer = pl.Trainer(
strategy=CheckpointlessMegatronStrategy(),
resume=CheckpointlessAutoResume()
)

Notes

• Extends NeMo's AutoResume class with delay mechanism for enabling checkpointless recovery

• Works in conjunction with CheckpointlessCompatibleConnector for complete recovery
workﬂow

Special considerations

We collect certain routine aggregated and anonymized operational metrics to provide essential
service availability. The creation of these metrics is fully automated and does not involve human
review of the underlying model training workload. These metrics relate to job operations, resource
management, and essential service functionality.

HyperPod managed tiered checkpointing and elastic training: note that HyperPod checkpointless
training is currently incompatible with HyperPod managed tiered checkpointing and elastic
training.

Checkpointless training recipes for GPT OSS 120B and Llama models are provided to simplify
getting started. These recipes have been veriﬁed on ml.p5 instances. Using other instance types
may require additional modiﬁcations to the underlying recipes. These recipes can be adapted to
full ﬁnetuning workﬂows as well. For custom models, we recommend reviewing the getting started
examples.

Appendix

Monitor training results via HyperPod recipes

SageMaker HyperPod recipes oﬀer Tensorboard integration to analyze training behavior. These
recipes also incorporate VizTracer, which is a low-overhead tool for tracing and visualizing Python
code execution. For more information, see  VizTracer.

Amazon EKS orchestration
1952

## Page 982

Amazon SageMaker AI
Developer Guide

The tensorboard logs are generated and stored within the log_dir. To access and analyze these
logs locally, use the following procedure:

1.
Download the Tensorboard experiment folder from your training environment to your local
machine.

2.
Open a terminal or command prompt on your local machine.

3.
Navigate to the directory containing the downloaded experiment folder.

4.
Launch Tensorboard by running the command:

tensorboard --port=<port> --bind_all --logdir experiment.

5.
Open your web browser and visit http://localhost:8008.

You can now see the status and visualizations of your training jobs within the Tensorboard
interface. Seeing the status and visualizations helps you monitor and analyze the training process.
Monitoring and analyzing the training process helps you gain insights into the behavior and
performance of your models. For more information about how you monitor and analyze the
training with Tensorboard, see the  NVIDIA NeMo Framework User Guide.

VizTracer

To enable VizTracer, you can modify your recipe by setting the environment variable

ENABLE_VIZTRACER to 1. After the training has completed, your VizTracer proﬁle is in the

experiment folder log_dir/viztracer_xxx.json. To analyze your proﬁle, you can download it
and open it using the vizviewer tool:

vizviewer --port <port> viztracer_xxx.json

This command launches the vizviewer on port 9001. You can view your VizTracer by going to
http://localhost:<port> in your browser. After you open VizTracer, you begin analyzing the training.
For more information about using VizTracer, see  VizTracer documentation.

Release notes

See the following release notes to track the latest updates for the SageMaker HyperPod
checkpointless training.

The SageMaker HyperPod checkpointless training v1.0.0

Amazon EKS orchestration
1953

## Page 983

Amazon SageMaker AI
Developer Guide

Date: Dec 03, 2025

SageMaker HyperPod checkpointless training Features

• Collective Communication Initialization Improvements: Oﬀers novel initialization methods,

Rootless and TCPStoreless for NCCL and Gloo.

• Memory-mapped (MMAP) Dataloader: Caches (persist) prefetched batches so that they are
available even when a fault causes a restart of the training job.

• Checkpointless: Enables faster recovery from cluster training faults in large-scale distributed
training environments by making framework-level optimizations

• Built on Nvidia Nemo and PyTorch Lightning: Leverages these powerful frameworks for
eﬃcient and ﬂexible model training

• Nividia NeMo

• PyTorch Lightning

SageMaker HyperPod Checkpointless training Docker container

Checkpointless training on HyperPod is built on top of the  NVIDIA NeMo framework. HyperPod
checkpointless training aims to recover faster from cluster training faults in large-scale distributed
training environments by making framework-level optimizations that will be delivered on a base
container containing the base image with NCCL and PyTorch optimizations.

Availability

Currently images are only available in:

eu-north-1
ap-south-1
us-east-2
eu-west-1
eu-central-1
sa-east-1
us-east-1
eu-west-2
ap-northeast-1
us-west-2
us-west-1
ap-southeast-1
ap-southeast-2

Amazon EKS orchestration
1954

## Page 984

Amazon SageMaker AI
Developer Guide

but not available in the following 3 opt-in Regions:

ap-southeast-3
ap-southeast-4
eu-south-2

Container details

Checkpointless training Docker container for PyTorch v2.6.0 with CUDA v12.9

963403601044.dkr.ecr.eu-north-1.amazonaws.com/hyperpod-checkpointless-training:v1.0.0
423350936952.dkr.ecr.ap-south-1.amazonaws.com/hyperpod-checkpointless-training:v1.0.0
556809692997.dkr.ecr.us-east-2.amazonaws.com/hyperpod-checkpointless-training:v1.0.0
942446708630.dkr.ecr.eu-west-1.amazonaws.com/hyperpod-checkpointless-training:v1.0.0
391061375763.dkr.ecr.eu-central-1.amazonaws.com/hyperpod-checkpointless-training:v1.0.0
311136344257.dkr.ecr.sa-east-1.amazonaws.com/hyperpod-checkpointless-training:v1.0.0
327873000638.dkr.ecr.us-east-1.amazonaws.com/hyperpod-checkpointless-training:v1.0.0
016839105697.dkr.ecr.eu-west-2.amazonaws.com/hyperpod-checkpointless-training:v1.0.0
356859066553.dkr.ecr.ap-northeast-1.amazonaws.com/hyperpod-checkpointless-
training:v1.0.0
920498770698.dkr.ecr.us-west-2.amazonaws.com/hyperpod-checkpointless-training:v1.0.0
827510180725.dkr.ecr.us-west-1.amazonaws.com/hyperpod-checkpointless-training:v1.0.0
885852567298.dkr.ecr.ap-southeast-1.amazonaws.com/hyperpod-checkpointless-
training:v1.0.0
304708117039.dkr.ecr.ap-southeast-2.amazonaws.com/hyperpod-checkpointless-
training:v1.0.0

Pre-installed packages

PyTorch: v2.6.0
CUDA: v12.9
NCCL: v2.27.5
EFA: v1.43.0
AWS-OFI-NCCL v1.16.0
Libfabric version 2.1
Megatron v0.15.0
Nemo v2.6.0rc0

Using GPU partitions in Amazon SageMaker HyperPod

Cluster administrators can choose how to maximize GPU utilization across their organization. You
can enable GPU partitioning with NVIDIA Multi-Instance GPU (MIG) technology to partition GPU

Amazon EKS orchestration
1955

## Page 985

Amazon SageMaker AI
Developer Guide

resources into smaller, isolated instances for better resource utilization. This capability provides the
ability to run multiple smaller sized tasks concurrently on a single GPU instead of dedicating the
entire hardware to a single, often underutilized task. This eliminates wasted compute power and
memory.

GPU partitioning with MIG technology supports GPUs and allows you to partition a single
supported GPU into up to seven separate GPU partitions. Each GPU partition has dedicated
memory, cache, and compute resources, providing predictable isolation.

Beneﬁts

• Improved GPU utilization - Maximize compute eﬃciency by partitioning GPUs based on
compute and memory requirements

• Task isolation - Each GPU partition operates independently with dedicated memory, cache, and
compute resources

• Task ﬂexibility - Support a mix of tasks on a single physical GPU, all running in parallel

• Flexible setup management - Support both Do-it-yourself (DIY) Kubernetes conﬁgurations using

Kubernetes command-line client kubectl, and a managed solution with custom labels to easily
conﬁgure and apply your labels associated with GPU partitions

Supported Instance Types

GPU partitioning with MIG technology is supported on the following HyperPod instance types:

A100 GPU Instances - https://aws.amazon.com/ec2/instance-types/p4/

• ml.p4d.24xlarge - 8 NVIDIA A100 GPUs (80GB HBM2e per GPU)

• ml.p4de.24xlarge - 8 NVIDIA A100 GPUs (80GB HBM2e per GPU)

H100 GPU Instances - https://aws.amazon.com/ec2/instance-types/p5/

• ml.p5.48xlarge - 8 NVIDIA H100 GPUs (80GB HBM3 per GPU)

H200 GPU Instances - https://aws.amazon.com/ec2/instance-types/p5/

• ml.p5e.48xlarge - 8 NVIDIA H200 GPUs (141GB HBM3e per GPU)

• ml.p5en.48xlarge - 8 NVIDIA H200 GPUs (141GB HBM3e per GPU)

Amazon EKS orchestration
1956

## Page 986

Amazon SageMaker AI
Developer Guide

B200 GPU Instances - https://aws.amazon.com/ec2/instance-types/p6/

• ml.p6b.48xlarge - 8 NVIDIA B200 GPUs

GPU partitions

NVIDIA MIG proﬁles deﬁne how GPUs are partitioned. Each proﬁle speciﬁes the compute and

memory allocation per MIG instance. The following are the MIG proﬁles associated with each GPU
type:

A100 GPU (ml.p4d.24xlarge)

Proﬁle
Memory (GB)
Instances per GPU
Total per ml.p4d.24
xlarge

1g.5gb
5
7
56

2g.10gb
10
3
24

3g.20gb
20
2
16

4g.20gb
20
1
8

7g.40gb
40
1
8

H100 GPU (ml.p5.48xlarge)

Proﬁle
Memory (GB)
Instances per GPU
Total per ml.p5.48x
large

1g.10gb
10
7
56

1g.20gb
20
4
32

2g.20gb
20
3
24

3g.40gb
40
2
16

4g.40gb
40
1
8

Amazon EKS orchestration
1957

## Page 987

Amazon SageMaker AI
Developer Guide

Proﬁle
Memory (GB)
Instances per GPU
Total per ml.p5.48x
large

7g.80gb
80
1
8

H200 GPU (ml.p5e.48xlarge and ml.p5en.48xlarge)

Proﬁle
Memory (GB)
Instances per GPU
Total per ml.p5en.4
8xlarge

1g.18gb
18
7
56

1g.35gb
35
4
32

2g.35gb
35
3
24

3g.71gb
71
2
16

4g.71gb
71
1
8

7g.141gb
141
1
8

Topics

• Setting up GPU partitions on Amazon SageMaker HyperPod

• Node Lifecycle and Labels

• Task Submission with MIG

Setting up GPU partitions on Amazon SageMaker HyperPod

Topics

• Prerequisites

• Creating a Cluster with MIG Conﬁguration

• Adding GPU operator to an existing cluster

• Updating MIG Conﬁguration

• Verifying MIG Conﬁguration

Amazon EKS orchestration
1958

## Page 988

Amazon SageMaker AI
Developer Guide

• Common Commands for Debugging MIG Conﬁguration

• Using SageMaker AI Console

Prerequisites

• HyperPod Amazon EKS cluster with supported GPU instances

• NVIDIA GPU Operator installed

• Appropriate IAM permissions for cluster management

Creating a Cluster with MIG Conﬁguration

Using AWS CLI

aws sagemaker create-cluster \
--cluster-name my-mig-cluster \
--orchestrator 'Eks={ClusterArn=arn:aws:eks:region:account:cluster/cluster-name}' \
--instance-groups '{
"InstanceGroupName": "gpu-group",
"InstanceType": "ml.p4d.24xlarge",
"InstanceCount": 1,
"LifeCycleConfig": {
"SourceS3Uri": "s3://my-bucket",
"OnCreate": "on_create_script.sh"
},
"KubernetesConfig": {
"Labels": {
"nvidia.com/mig.config": "all-1g.5gb"
}
},
"ExecutionRole": "arn:aws:iam::account:role/execution-role",
"ThreadsPerCore": 1
}' \
--vpc-config '{
"SecurityGroupIds": ["sg-12345"],
"Subnets": ["subnet-12345"]
}' \
--node-provisioning-mode Continuous

Amazon EKS orchestration
1959

## Page 989

Amazon SageMaker AI
Developer Guide

Using CloudFormation

{
"ClusterName": "my-mig-cluster",
"InstanceGroups": [
{
"InstanceGroupName": "gpu-group",
"InstanceType": "ml.p4d.24xlarge",
"InstanceCount": 1,
"KubernetesConfig": {
"Labels": {
"nvidia.com/mig.config": "all-2g.10gb"
}
},
"ExecutionRole": "arn:aws:iam::account:role/execution-role"
}
],
"Orchestrator": {
"Eks": {
"ClusterArn": "arn:aws:eks:region:account:cluster/cluster-name"
}
},
"NodeProvisioningMode": "Continuous"
}

Adding GPU operator to an existing cluster

Install GPU Operator

Replace {$AWS_REGION} with your cluster region (e.g., us-east-1, us-west-2).

helm install gpuo helm_chart/HyperPodHelmChart/charts/gpu-operator \
-f helm_chart/HyperPodHelmChart/charts/gpu-operator/regional-values/values-
{$AWS_REGION}.yaml \
-n kube-system

Verify Installation (Wait 2-3 minutes)

Check all GPU operator pods are running:

kubectl get pods -n kube-system | grep -E "(gpu-operator|nvidia-)"

Expected pods:

Amazon EKS orchestration
1960

## Page 990

Amazon SageMaker AI
Developer Guide

• gpu-operator-* - 1 instance (cluster controller)

• nvidia-device-plugin-daemonset-* - 1 per GPU node (all GPU instances)

• nvidia-mig-manager-* - 1 per MIG-capable node (A100/H100)

Remove Old Device Plugin

Disable the existing nvidia-device-plugin:

helm upgrade dependencies helm_chart/HyperPodHelmChart \
--set nvidia-device-plugin.devicePlugin.enabled=false \
-n kube-system

Verify GPU Resources

Conﬁrm nodes show GPU capacity. It should display: nvidia.com/gpu: 8 (or your actual GPU count).

kubectl describe nodes | grep "nvidia.com/gpu"

Updating MIG Conﬁguration

Preparing Nodes Before MIG Updates

Before updating MIG conﬁgurations on your instance group, you must prepare the nodes to
prevent workload disruption. Follow these steps to safely drain workloads from the nodes
that will be reconﬁgured.

Step 1: Identify Nodes in the Instance Group

First, identify all nodes that belong to the instance group you want to update:

# List all nodes in the instance group
kubectl get nodes -l sagemaker.amazonaws.com/instance-group-name=INSTANCE_GROUP_NAME

# Example:
kubectl get nodes -l sagemaker.amazonaws.com/instance-group-name=p4d-group

This command returns a list of all nodes in the speciﬁed instance group. Make note of each node
name for the following steps.

Amazon EKS orchestration
1961

## Page 991

Amazon SageMaker AI
Developer Guide

Step 2: Cordon and Drain Each Node

For each node identiﬁed in Step 1, perform the following actions:

Cordon the Node

Cordoning prevents new pods from being scheduled on the node:

# Cordon a single node
kubectl cordon NODE_NAME

# Example:
kubectl cordon hyperpod-i-014a41a7001adca60

Drain Workload Pods from the Node

Drain the node to evict all workload pods while preserving system pods:

# Drain the node (ignore DaemonSets and evict pods)
kubectl drain NODE_NAME \
--ignore-daemonsets \
--delete-emptydir-data \
--force \
--grace-period=300

# Example:
kubectl drain hyperpod-i-014a41a7001adca60 \
--ignore-daemonsets \
--delete-emptydir-data \
--force \
--grace-period=300

Command Options Explained:

• --ignore-daemonsets - Allows the drain operation to proceed even if DaemonSet pods are
present

• --delete-emptydir-data - Deletes pods using emptyDir volumes (required for draining to
succeed)

• --force - Forces deletion of pods not managed by a controller (use with caution)

• --grace-period=300 - Gives pods 5 minutes to terminate gracefully

Amazon EKS orchestration
1962

## Page 992

Amazon SageMaker AI
Developer Guide

Important

• The drain operation may take several minutes depending on the number of pods and
their termination grace periods

• System pods in the following namespaces will remain running: kube-system, cert-

manager, kubeflow, hyperpod-inference-system, kube-public, mpi-operator,

gpu-operator, aws-hyperpod, jupyter-k8s-system, hyperpod-observability,

kueue-system, and keda

• DaemonSet pods will remain on the node (they are ignored by design)

Step 3: Verify No Workload Pods are Running

After draining, verify that no workload pods remain on the nodes (excluding system namespaces):

# Check for any remaining pods outside system namespaces
kubectl get pods --all-namespaces --field-selector spec.nodeName=NODE_NAME \
| grep -v "kube-system" \
| grep -v "cert-manager" \
| grep -v "kubeflow" \
| grep -v "hyperpod-inference-system" \
| grep -v "kube-public" \
| grep -v "mpi-operator" \
| grep -v "gpu-operator" \
| grep -v "aws-hyperpod" \
| grep -v "jupyter-k8s-system" \
| grep -v "hyperpod-observability" \
| grep -v "kueue-system" \
| grep -v "keda"

# Example:
kubectl get pods --all-namespaces --field-selector spec.nodeName=hyperpod-
i-014a41a7001adca60 \
| grep -v "kube-system" \
| grep -v "cert-manager" \
| grep -v "kubeflow" \
| grep -v "hyperpod-inference-system" \
| grep -v "kube-public" \
| grep -v "mpi-operator" \
| grep -v "gpu-operator" \
| grep -v "aws-hyperpod" \

Amazon EKS orchestration
1963

## Page 993

Amazon SageMaker AI
Developer Guide

| grep -v "jupyter-k8s-system" \
| grep -v "hyperpod-observability" \
| grep -v "kueue-system" \
| grep -v "keda"

Expected Output: If the node is properly drained, this command should return no results (or
only show the header row). If any pods are still running, investigate why they weren't evicted and
manually delete them if necessary.

Step 4: Verify Node Readiness Status

Before proceeding with the MIG update, conﬁrm that all nodes are cordoned:

# Check node status - should show "SchedulingDisabled"
kubectl get nodes -l sagemaker.amazonaws.com/instance-group-name=INSTANCE_GROUP_NAME

Nodes should show SchedulingDisabled in the STATUS column, indicating they are cordoned
and ready for the MIG update.

Update MIG Proﬁle on Existing Cluster

You can change MIG proﬁles on existing clusters:

aws sagemaker update-cluster \
--cluster-name my-mig-cluster \
--instance-groups '{
"InstanceGroupName": "gpu-group",
"InstanceType": "ml.p4d.24xlarge",
"InstanceCount": 1,
"KubernetesConfig": {
"Labels": {
"nvidia.com/mig.config": "all-3g.20gb"
}
},
"ExecutionRole": "arn:aws:iam::account:role/execution-role"
}'

Note

If jobs are already running on a node, the MIG partitioning will fail. User will get error
message to drain the nodes before re-attempting the MIG partitioning.

Amazon EKS orchestration
1964

## Page 994

Amazon SageMaker AI
Developer Guide

Verifying MIG Conﬁguration

After cluster creation or update, verify the MIG conﬁguration:

# Update kubeconfig

aws eks update-kubeconfig --name your-eks-cluster --region us-east-2

# Check MIG labels
kubectl get node NODE_NAME -o=jsonpath='{.metadata.labels}' | grep mig

# Check available MIG resources
kubectl describe node NODE_NAME | grep -A 10 "Allocatable:"

Common Commands for Debugging MIG Conﬁguration

Use the following commands to troubleshoot and validate MIG conﬁguration in your cluster:

# Check GPU Operator status
kubectl get pods -n gpu-operator-resources

# View MIG configuration
kubectl exec -n gpu-operator-resources nvidia-driver-XXXXX -- nvidia-smi mig -lgi

# Check device plugin configuration
kubectl logs -n gpu-operator-resources nvidia-device-plugin-XXXXX

# Monitor node events
kubectl get events --field-selector involvedObject.name=NODE_NAME

Note

Replace nvidia-driver-XXXXX and nvidia-device-plugin-XXXXX with the actual

pod names from your cluster, and NODE_NAME with your node's name.

Using SageMaker AI Console

Creating a New Cluster with MIG

1.
Navigate to Amazon SageMaker AI > HyperPod Clusters > Cluster Management > Create
HyperPod cluster

Amazon EKS orchestration
1965

## Page 995

Amazon SageMaker AI
Developer Guide

2.
Select Orchestrated by EKS

3.
Choose Custom setup and verify GPU Operator is enabled by default

4.
Under Instance groups section, click Add group

5.
Conﬁgure the instance group and navigate to Advanced Conﬁguration to enable Use GPU
partition toggle and choose your desired MIG conﬁguration from the dropdown

6.
Click Add Instance group and complete the remaining cluster conﬁguration

7.
Click Submit to create the cluster

Updating MIG Conﬁguration on Existing Cluster

1.
Navigate to Amazon SageMaker AI > HyperPod Clusters > Cluster Management

2.
Select your existing cluster and click Edit on the instance group you want to modify

3.
In Advanced conﬁguration, toggle Use GPU partition if not already enabled and select a
diﬀerent MIG conﬁguration from the dropdown

4.
Click Save changes

Node Lifecycle and Labels

Amazon SageMaker HyperPod performs deep health checks on cluster instances during the
creation and update of HyperPod clusters before GPU partitioning begins. HyperPod health-
monitoring agent continuously monitors the health status of GPU partitioned instances.

MIG Conﬁguration States

Nodes with GPU partition conﬁguration go through several states:

• Pending - Node is being conﬁgured with a MIG proﬁle

• Conﬁguring - GPU Operator is applying MIG partitioning

• Success - GPU partitioning completed successfully

• Failed - GPU partitioning encountered an error

Monitoring Node States

# Check node health status
kubectl get nodes -l sagemaker.amazonaws.com/node-health-status=Schedulable

Amazon EKS orchestration
1966

## Page 996

Amazon SageMaker AI
Developer Guide

# Monitor MIG configuration progress
kubectl get node NODE_NAME -o jsonpath='{.metadata.labels.nvidia\.com/mig\.config
\.state}'

# Check for configuration errors
kubectl describe node NODE_NAME | grep -A 5 "Conditions:"

Custom Labels and Taints

You can manage MIG conﬁguration with custom labels and taints to label your GPU partitions and
apply them across instances:

{
"KubernetesConfig": {
"Labels": {
"nvidia.com/mig.config": "all-2g.10gb",
"task-type": "inference",
"environment": "production"
},
"Taints": [
{
"Key": "gpu-task",
"Value": "mig-enabled",
"Effect": "NoSchedule"
}
]
}
}

Task Submission with MIG

Topics

• Using Kubernetes YAML

• Using HyperPod CLI

• Model Deployment with MIG

• Using HyperPod CLI

Using Kubernetes YAML

apiVersion: batch/v1

Amazon EKS orchestration
1967

## Page 997

Amazon SageMaker AI
Developer Guide

kind: Job
metadata:
name: mig-job
namespace: default
spec:
template:
spec:
containers:
- name: pytorch
image: pytorch/pytorch:latest
resources:
requests:
nvidia.com/mig-1g.5gb: 1
cpu: "100m"
memory: "128Mi"
limits:
nvidia.com/mig-1g.5gb: 1

restartPolicy: Never

Using HyperPod CLI

Use the HyperPod CLI to deploy JumpStart models with MIG support. The following example
demonstrates the new CLI parameters for GPU partitioning:

# Deploy JumpStart model with MIG
hyp create hyp-jumpstart-endpoint \
--model-id deepseek-llm-r1-distill-qwen-1-5b \
--instance-type ml.p5.48xlarge \
--accelerator-partition-type mig-2g.10gb \
--accelerator-partition-validation True \
--endpoint-name my-endpoint \
--tls-certificate-output-s3-uri s3://certificate-bucket/ \
--namespace default

Model Deployment with MIG

HyperPod Inference allows deploying the models on MIG proﬁles via Studio Classic,

kubectl and HyperPod CLI. To deploy JumpStart Models on kubectl, CRDs have

ﬁelds called spec.server.acceleratorPartitionType to deploy the model to the
desired MIG proﬁle. We run validations to ensure models can be deployed on the MIG
proﬁle selected in the CRD. In case you want to disable the MIG validation checks, use

spec.server.validations.acceleratorPartitionValidation to False.

Amazon EKS orchestration
1968

## Page 998

Amazon SageMaker AI
Developer Guide

JumpStart Models

apiVersion: inference.sagemaker.aws.amazon.com/v1
kind: JumpStartModel
metadata:
name: deepseek-model
namespace: default
spec:
sageMakerEndpoint:
name: deepseek-endpoint
model:
modelHubName: SageMakerPublicHub
modelId: deepseek-llm-r1-distill-qwen-1-5b
server:
acceleratorPartitionType: mig-7g.40gb
instanceType: ml.p4d.24xlarge

Deploy model from Amazon S3 using InferenceEndpointConﬁg

InferenceEndpointConﬁg allows you to deploy custom model from Amazon S3. To deploy a model

on MIG, in spec.worker.resources mention MIG proﬁle in requests and limits. Refer to a
simple deployment below:

apiVersion: inference.sagemaker.aws.amazon.com/v1
kind: InferenceEndpointConfig
metadata:
name: custom-model
namespace: default
spec:
replicas: 1
modelName: my-model
endpointName: my-endpoint
instanceType: ml.p4d.24xlarge
modelSourceConfig:
modelSourceType: s3
s3Storage:
bucketName: my-model-bucket
region: us-east-2
modelLocation: model-path
worker:
resources:
requests:
nvidia.com/mig-3g.20gb: 1

Amazon EKS orchestration
1969

## Page 999

Amazon SageMaker AI
Developer Guide

cpu: "5600m"
memory: "10Gi"
limits:
nvidia.com/mig-3g.20gb: 1

Deploy model from FSx for Lustre using InferenceEndpointConﬁg

InferenceEndpointConﬁg allows you to deploy custom model from FSx for Lustre. To deploy a

model on MIG, in spec.worker.resources mention MIG proﬁle in requests and limits. Refer
to a simple deployment below:

apiVersion: inference.sagemaker.aws.amazon.com/v1
kind: InferenceEndpointConfig
metadata:
name: custom-model
namespace: default
spec:
replicas: 1
modelName: my-model
endpointName: my-endpoint
instanceType: ml.p4d.24xlarge
modelSourceConfig:
modelSourceType: fsx
fsxStorage:
fileSystemId: fs-xxxxx
modelLocation: location-on-fsx
worker:
resources:
requests:
nvidia.com/mig-3g.20gb: 1
cpu: "5600m"
memory: "10Gi"
limits:
nvidia.com/mig-3g.20gb: 1

Using Studio Classic UI

Deploying JumpStart Models with MIG

1.
Open Studio Classic and navigate to JumpStart

2.
Browse or search for your desired model (e.g., "DeepSeek", "Llama", etc.)

3.
Click on the model card and select Deploy

Amazon EKS orchestration
1970

## Page 1000

Amazon SageMaker AI
Developer Guide

4.
In the deployment conﬁguration:

• Choose HyperPod as the deployment target

• Select your MIG-enabled cluster from the dropdown

• Under Instance conﬁguration:

• Select instance type (e.g., ml.p4d.24xlarge)

• Choose GPU Partition Type from available options

• Conﬁgure Instance count and Auto-scaling settings

5.
Review and click Deploy

6.
Monitor deployment progress in the Endpoints section

Model Conﬁguration Options

Endpoint Settings:

• Endpoint name - Unique identiﬁer for your deployment

• Variant name - Conﬁguration variant (default: AllTraﬃc)

• Instance type - Must support GPU partition (p series)

• MIG proﬁle - GPU partition

• Initial instance count - Number of instances to deploy

• Auto-scaling - Enable for dynamic scaling based on traﬃc

Advanced Conﬁguration:

• Model data location - Amazon S3 path for custom models

• Container image - Custom inference container (optional)

• Environment variables - Model-speciﬁc conﬁgurations

• Amazon VPC conﬁguration - Network isolation settings

Monitoring Deployed Models

1.
Navigate to Studio Classic > Deployments > Endpoints

2.
Select your MIG-enabled endpoint

3.
View metrics including:

Amazon EKS orchestration
1971

## Page 1001

Amazon SageMaker AI
Developer Guide

• MIG utilization - Per GPU partition usage

• Memory consumption - Per GPU partition

• Inference latency - Request processing time

• Throughput - Requests per second

4.
Set up Amazon CloudWatch alarms for automated monitoring

5.
Conﬁgure auto-scaling policies based on MIG utilization

Using HyperPod CLI

JumpStart Deployment

The HyperPod CLI JumpStart command includes two new ﬁelds for MIG support:

• --accelerator-partition-type - Speciﬁes the MIG conﬁguration (e.g., mig-4g.20gb)

• --accelerator-partition-validation - Validates compatibility between models and MIG
proﬁle (default: true)

hyp create hyp-jumpstart-endpoint \
--version 1.1 \
--model-id deepseek-llm-r1-distill-qwen-1-5b \
--instance-type ml.p4d.24xlarge \
--endpoint-name js-test \
--accelerator-partition-type "mig-4g.20gb" \
--accelerator-partition-validation true \
--tls-certificate-output-s3-uri s3://my-bucket/certs/

Custom Endpoint Deployment

For deploying via custom endpoint, use the existing ﬁelds --resources-requests and --

resources-limits to enable MIG proﬁle functionality:

hyp create hyp-custom-endpoint \
--namespace default \
--metadata-name deepseek15b-mig-10-14-v2 \
--endpoint-name deepseek15b-mig-endpoint \
--instance-type ml.p4d.24xlarge \
--model-name deepseek15b-mig \

Amazon EKS orchestration
1972

